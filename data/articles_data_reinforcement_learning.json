[
    {
        "id": 25001,
        "title": "Q-Learning: Model Free Reinforcement Learning and Temporal Difference Learning",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "Here we describe Q-learning, which is one of the most popular methods in reinforcement learning. Q-learning is a type of temporal difference learning.  We discuss other TD algorithms, such as SARSA, and connections to biological learning through dopamine. Q-learning is also one of the most common frameworks for deep reinforcement learning.",
        "link": "http://dx.doi.org/10.52843/cassyni.ss11hp"
    },
    {
        "id": 25002,
        "title": "Deep Reinforcement Learning",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0012"
    },
    {
        "id": 25003,
        "title": "Reinforcement Learning and Deep Reinforcement Learning",
        "authors": "",
        "published": "2021-4-30",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108955652.016"
    },
    {
        "id": 25004,
        "title": "Overview of Deep Reinforcement Learning Methods",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "This video gives an overview of methods for deep reinforcement learning, including deep Q-learning, actor-critic methods, deep policy networks, and policy gradient optimization algorithms.",
        "link": "http://dx.doi.org/10.52843/cassyni.kfnzpy"
    },
    {
        "id": 25005,
        "title": "Reinforcement learning",
        "authors": "Thomas P. Trappenberg",
        "published": "2019-11-28",
        "citations": 0,
        "abstract": "The discussion here considers a much more common learning condition where an agent, such as a human or a robot, has to learn to make decisions in the environment from simple feedback. Such feedback is provided only after periods of actions in the form of reward or punishment without detailing which of the actions has contributed to the outcome. This type of learning scenario is called reinforcement learning. This learning problem is formalized in a Markov decision-making process with a variety of related algorithms. The second part of this chapter will use function approximators with neural networks which have made recent progress as deep reinforcement learning.",
        "link": "http://dx.doi.org/10.1093/oso/9780198828044.003.0010"
    },
    {
        "id": 25006,
        "title": "Reinforcement Learning and Stochastic Control",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.008"
    },
    {
        "id": 25007,
        "title": "Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-59238-7_3"
    },
    {
        "id": 25008,
        "title": "Quantum Reinforcement Learning—Connecting Reinforcement Learning and Quantum Computing",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1_4"
    },
    {
        "id": 25009,
        "title": "Hierarchical Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_8"
    },
    {
        "id": 25010,
        "title": "Deep Reinforcement Learning for Fluid Dynamics and Control",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "Reinforcement learning based on deep learning is currently being used for impressive control of fluid dynamic systems.  This video will describe recent advances, including for mimicking the behavior of birds and fish, for turbulence closure modeling with sub-grid-scale models, and for robotic flight demonstrations.",
        "link": "http://dx.doi.org/10.52843/cassyni.kvtnvy"
    },
    {
        "id": 25011,
        "title": "Multi-Agent Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_7"
    },
    {
        "id": 25012,
        "title": "Model-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_5"
    },
    {
        "id": 25013,
        "title": "Reinforcement Learning Basics",
        "authors": "Abhishek Nandy, Manisha Biswas",
        "published": "2018",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-3285-9_1"
    },
    {
        "id": 25014,
        "title": "Reinforcement Learning Explained via Reinforcement Learning: Towards Explainable Policies through Predictive Explanation",
        "authors": "Léo Saulières, Martin Cooper, Florence Bannay",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011619600003393"
    },
    {
        "id": 25015,
        "title": "Policy-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_4"
    },
    {
        "id": 25016,
        "title": "Tabular Value-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_2"
    },
    {
        "id": 25017,
        "title": "Deep Value-Based Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_3"
    },
    {
        "id": 25018,
        "title": "Model Based Reinforcement Learning: Policy Iteration, Value Iteration, and Dynamic Programming",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "Here we introduce dynamic programming, which is a cornerstone of model-based reinforcement learning. We demonstrate dynamic programming for policy iteration and value iteration, leading to the quality function and Q-learning.",
        "link": "http://dx.doi.org/10.52843/cassyni.6fs4s9"
    },
    {
        "id": 25019,
        "title": "Applying Python to Reinforcement Learning",
        "authors": "Abhishek Nandy, Manisha Biswas",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-3285-9_4"
    },
    {
        "id": 25020,
        "title": "Reinforcement Learning and Deep Reinforcement Learning",
        "authors": "F. Richard Yu, Ying He",
        "published": "2019",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-10546-4_2"
    },
    {
        "id": 25021,
        "title": "Hierarchical Reinforcement Learning",
        "authors": "Yanhua Huang",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_10"
    },
    {
        "id": 25022,
        "title": "Deep Reinforcement Learning",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 72,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-6"
    },
    {
        "id": 25023,
        "title": "Deep Reinforcement Learning",
        "authors": "Abhilash Majumder",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6503-1_5"
    },
    {
        "id": 25024,
        "title": "Reinforcement Learning for Cybersecurity",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-7"
    },
    {
        "id": 25025,
        "title": "An introduction to dynamic programming and reinforcement learning",
        "authors": "",
        "published": "2017-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439821091-7"
    },
    {
        "id": 25026,
        "title": "Reinforcement Learning with Keras, TensorFlow, and ChainerRL",
        "authors": "Abhishek Nandy, Manisha Biswas",
        "published": "2018",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-3285-9_5"
    },
    {
        "id": 25027,
        "title": "Reinforcement Learning Problems",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-3"
    },
    {
        "id": 25028,
        "title": "Model-Free Reinforcement Learning",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-5"
    },
    {
        "id": 25029,
        "title": "Overview of Reinforcement Learning",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-1"
    },
    {
        "id": 25030,
        "title": "Google’s DeepMind and the Future of Reinforcement Learning",
        "authors": "Abhishek Nandy, Manisha Biswas",
        "published": "2018",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-3285-9_6"
    },
    {
        "id": 25031,
        "title": "Challenges of Reinforcement Learning",
        "authors": "Zihan Ding, Hao Dong",
        "published": "2020",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_7"
    },
    {
        "id": 25032,
        "title": "Model-Based Reinforcement Learning",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-4"
    },
    {
        "id": 25033,
        "title": "Deep Reinforcement Learning",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_10"
    },
    {
        "id": 25034,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_1"
    },
    {
        "id": 25035,
        "title": "Multi-Agent Reinforcement Learning",
        "authors": "Huaqing Zhang, Shanghang Zhang",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_11"
    },
    {
        "id": 25036,
        "title": "Logic + Reinforcement Learning + Deep Learning: A Survey",
        "authors": "Andreas Bueff, Vaishak Belle",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011746300003393"
    },
    {
        "id": 25037,
        "title": "The Machine-Learning Approach of Reinforcement Learning",
        "authors": "Thomas Boraud",
        "published": "2020-10-8",
        "citations": 1,
        "abstract": "This chapter assesses alternative approaches of reinforcement learning that are developed by machine learning. The initial goal of this branch of artificial intelligence, which appeared in the middle of the twentieth century, was to develop and implement algorithms that allow a machine to learn. Originally, they were computers or more or less autonomous robotic automata. As artificial intelligence has developed and cross-fertilized with neuroscience, it has begun to be used to model the learning and decision-making processes for biological agents, broadening the meaning of the word ‘machine’. Theoreticians of this discipline define several categories of learning, but this chapter only deals with those which are related to reinforcement learning. To understand how these algorithms work, it is necessary first of all to explain the Markov chain and the Markov decision-making process. The chapter then goes on to examine model-free reinforcement learning algorithms, the actor-critic model, and finally model-based reinforcement learning algorithms.",
        "link": "http://dx.doi.org/10.1093/oso/9780198824367.003.0016"
    },
    {
        "id": 25038,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Abhilash Majumder",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6503-1_1"
    },
    {
        "id": 25039,
        "title": "Temporal Difference Learning, SARSA, and Q-Learning",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_4"
    },
    {
        "id": 25040,
        "title": "Transfer Learning in Reinforcement Learning",
        "authors": "",
        "published": "2020-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781139061773.010"
    },
    {
        "id": 25041,
        "title": "Graph Neural Networks and Reinforcement Learning: A Survey",
        "authors": "Fatemeh Fathinezhad, Peyman Adibi, Bijan Shoushtarian, Jocelyn Chanussot",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "Graph neural network (GNN) is an emerging field of research that tries to generalize deep learning architectures to work with non-Euclidean data. Nowadays, combining deep reinforcement learning (DRL) with GNN for graph-structured problems, especially in multi-agent environments, is a powerful technique in modern deep learning. From the computational point of view, multi-agent environments are inherently complex, because future rewards depend on the joint actions of multiple agents. This chapter tries to examine different types of applying GNN and DRL techniques in the most common representations of multi-agent problems and their challenges. In general, the fusion of GNN and DRL can be addressed from two different points of view. First, GNN is used to influence the DRL performance and improve its formulation. Here, GNN is applied in relational DRL structures such as multi-agent and multi-task DRL. Second, DRL is used to improve the application of GNN. From this viewpoint, DRL can be used for a variety of purposes including neural architecture search and improving the explanatory power of GNN predictions.",
        "link": "http://dx.doi.org/10.5772/intechopen.111651"
    },
    {
        "id": 25042,
        "title": "Multi-objective safe reinforcement learning",
        "authors": "Naoto Horie, Tohgoroh Matsui, Koichi Moriyama, Atsuko Mutoh, Nobuhiro Inuzuka",
        "published": "2019-1-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10015-019-00524-2"
    },
    {
        "id": 25043,
        "title": "Distributed Reinforcement Learning",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_12"
    },
    {
        "id": 25044,
        "title": "Taxonomy of Reinforcement Learning Algorithms",
        "authors": "Hongming Zhang, Tianyang Yu",
        "published": "2020",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_3"
    },
    {
        "id": 25045,
        "title": "Arena Platform for Multi-Agent Reinforcement Learning",
        "authors": "Zihan Ding",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_17"
    },
    {
        "id": 25046,
        "title": "Reinforcement Learning and Feedback Control",
        "authors": "Changsheng Hua",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-33034-7_3"
    },
    {
        "id": 25047,
        "title": "Dynamic programming and reinforcement learning in large and contin- uous spaces",
        "authors": "",
        "published": "2017-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439821091-8"
    },
    {
        "id": 25048,
        "title": "Reinforcement Learning for Control",
        "authors": "",
        "published": "2021-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.app2"
    },
    {
        "id": 25049,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2022-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108891530.015"
    },
    {
        "id": 25050,
        "title": "Computer Vision, Deep Learning, Deep Reinforcement Learning",
        "authors": "Farshid PirahanSiah",
        "published": "2019-11-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14293/s2199-1006.1.sor-uncat.clzwyuz.v1"
    },
    {
        "id": 25051,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2022-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108891530.016"
    },
    {
        "id": 25052,
        "title": "Reinforcement Learning as a Subfield of Machine Learning",
        "authors": "Uwe Lorenz",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-09030-1_1"
    },
    {
        "id": 25053,
        "title": "Reinforcement Learning Algorithms",
        "authors": "Taweh Beysolow II",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-5127-0_2"
    },
    {
        "id": 25054,
        "title": "World of Reinforcement Learning",
        "authors": "Harini Anand",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2755"
    },
    {
        "id": 25055,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_1"
    },
    {
        "id": 25056,
        "title": "Reinforcement Learning and Causal Models",
        "authors": "Samuel J. Gershman",
        "published": "2017-5-10",
        "citations": 9,
        "abstract": "This chapter reviews the diverse roles that causal knowledge plays in reinforcement learning. The first half of the chapter contrasts a “model-free” system that learns to repeat actions that lead to reward with a “model-based” system that learns a probabilistic causal model of the environment, which it then uses to plan action sequences. Evidence suggests that these two systems coexist in the brain, both competing and cooperating with each other. The interplay of two systems allows the brain to negotiate a balance between cognitively cheap but inaccurate model-free algorithms and accurate but expensive model-based algorithms. The second half of the chapter reviews research on hidden state inference in reinforcement learning. The problem of inferring hidden states can be construed in terms of inferring the latent causes that give rise to sensory data and rewards. Because hidden state inference affects both model-based and model-free reinforcement learning, causal knowledge impinges upon both systems.",
        "link": "http://dx.doi.org/10.1093/oxfordhb/9780199399550.013.20"
    },
    {
        "id": 25057,
        "title": "Reinforcement Learning Algorithms: Q Learning and Its Variants",
        "authors": "Taweh Beysolow II",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-5127-0_3"
    },
    {
        "id": 25058,
        "title": "Reinforcement learning (machine learning)",
        "authors": "Andrew Murphy, Frank Gaillard",
        "published": "2017-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53347/rid-56116"
    },
    {
        "id": 25059,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Taweh Beysolow II",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-5127-0_1"
    },
    {
        "id": 25060,
        "title": "Reflexive Reinforcement Learning: Methods for Self-Referential Autonomous Learning",
        "authors": "B. Lyons, J. Herrmann",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009997503810388"
    },
    {
        "id": 25061,
        "title": "Deep Reinforcement Learning",
        "authors": "Andreas Folkers",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-28886-0_3"
    },
    {
        "id": 25062,
        "title": "Deep Reinforcement Learning: A Study of Reinforcement Learning with Neural Networks in Industrial Automation",
        "authors": "Asiri Iroshan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4386667"
    },
    {
        "id": 25063,
        "title": "Machine Learning Meets Control Theory",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this video, we provide a high level overview of reinforcement learning, along with leading algorithms and impressive applications.",
        "link": "http://dx.doi.org/10.52843/cassyni.x2t0sp"
    },
    {
        "id": 25064,
        "title": "Review for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "",
        "published": "2020-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v1/review2"
    },
    {
        "id": 25065,
        "title": "Review for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "",
        "published": "2020-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v2/review2"
    },
    {
        "id": 25066,
        "title": "Market Making via Reinforcement Learning",
        "authors": "Taweh Beysolow II",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-5127-0_4"
    },
    {
        "id": 25067,
        "title": "Custom OpenAI Reinforcement Learning Environments",
        "authors": "Taweh Beysolow II",
        "published": "2019",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-5127-0_5"
    },
    {
        "id": 25068,
        "title": "Deep Reinforcement Learning",
        "authors": "",
        "published": "2022-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108891530.017"
    },
    {
        "id": 25069,
        "title": "Bridging reinforcement learning and creativity",
        "authors": "Jieliang Luo, Sam Green",
        "published": "2018-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3277644.3277796"
    },
    {
        "id": 25070,
        "title": "Sophisticated Swarm Reinforcement Learning by Incorporating Inverse Reinforcement Learning",
        "authors": "Yasuaki Kuroe, Kenya Takeuchi",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394525"
    },
    {
        "id": 25071,
        "title": "Learning the Return Distribution",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0005"
    },
    {
        "id": 25072,
        "title": "Continuous‐Time Reinforcement Learning for Force Control",
        "authors": "",
        "published": "2021-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.ch7"
    },
    {
        "id": 25073,
        "title": "Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning",
        "authors": "Yuansheng Xie, Soroush Vosoughi, Saeed Hassanpour",
        "published": "2022-8-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956245"
    },
    {
        "id": 25074,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_1"
    },
    {
        "id": 25075,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Zihan Ding, Yanhua Huang, Hang Yuan, Hao Dong",
        "published": "2020",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_2"
    },
    {
        "id": 25076,
        "title": "Reinforcement Learning for Robot Position/Force Control",
        "authors": "",
        "published": "2021-10-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.ch6"
    },
    {
        "id": 25077,
        "title": "Reinforcement Learning and Bellman’s Principle of Optimality",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1_3"
    },
    {
        "id": 25078,
        "title": "Neural Networks for Learning Control Laws",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": " In this video, we provide an overview of developments in deep reinforcement learning, along with leading algorithms and impressive applications.",
        "link": "http://dx.doi.org/10.52843/cassyni.9tngmc"
    },
    {
        "id": 25079,
        "title": "Recommender System using Reinforcement Learning: A Survey",
        "authors": "Mehrdad Rezaei, Nasseh Tabrizi",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011300300003277"
    },
    {
        "id": 25080,
        "title": "Reinforcement Learning: Practice",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4135/9781529798531"
    },
    {
        "id": 25081,
        "title": "Multi-objective safe reinforcement learning: the relationship between multi-objective reinforcement learning and safe reinforcement learning",
        "authors": "Naoto Horie, Tohgoroh Matsui, Koichi Moriyama, Atsuko Mutoh, Nobuhiro Inuzuka",
        "published": "2019-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10015-019-00523-3"
    },
    {
        "id": 25082,
        "title": "Reinforcement learning in learning automata and cellular learning automata via multiple reinforcement signals",
        "authors": "Reza Vafashoar, Mohammad Reza Meybodi",
        "published": "2019-4",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2019.01.021"
    },
    {
        "id": 25083,
        "title": "Robot ℋ2 Neural Control Using Reinforcement Learning",
        "authors": "",
        "published": "2021-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.ch10"
    },
    {
        "id": 25084,
        "title": "Meta-reinforcement learning",
        "authors": "Lan Zou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-323-89931-4.00011-0"
    },
    {
        "id": 25085,
        "title": "RLAR: A Reinforcement Learning Abductive Reasoner",
        "authors": "Mostafa ElHayani",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012425000003636"
    },
    {
        "id": 25086,
        "title": "Basic Concepts of Reinforcement Learning",
        "authors": "Uwe Lorenz",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-09030-1_2"
    },
    {
        "id": 25087,
        "title": "Deep Reinforcement Learning Framework with Representation Learning for Concurrent Negotiation",
        "authors": "Ryoga Miyajima, Katsuhide Fujita",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012336000003636"
    },
    {
        "id": 25088,
        "title": "Reinforcement Learning as an Approach for Flexible Scheduling",
        "authors": "Schirin Bär",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-39179-9_3"
    },
    {
        "id": 25089,
        "title": "Policy-Based Reinforcement Learning Approaches",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_10"
    },
    {
        "id": 25090,
        "title": "Pareto Envelope Augmented with Reinforcement Learning Multi-Objective Reinforcement Learning-Based Approach for Large-Scale Constrained Pressurized Water Reactor Optimization",
        "authors": "Paul Seurin, Koroush Shirvan",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4673021"
    },
    {
        "id": 25091,
        "title": "Redundant Robots Control Using Multi‐Agent Reinforcement Learning",
        "authors": "",
        "published": "2021-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.ch9"
    },
    {
        "id": 25092,
        "title": "Learning to Play",
        "authors": "Aske Plaat",
        "published": "2020",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-59238-7"
    },
    {
        "id": 25093,
        "title": "Multi-agent Transfer Learning in Reinforcement Learning-based Ride-sharing Systems",
        "authors": "Alberto Castagna, Ivana Dusparic",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010785200003116"
    },
    {
        "id": 25094,
        "title": "Robust Adversarial Deep Reinforcement Learning",
        "authors": "Di Wang",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Deep reinforcement learning has shown remarkable results across various tasks. However, recent studies highlight the susceptibility of DRL to targeted adversarial disruptions. Furthermore, discrepancies between simulated settings and real-world applications often make it challenging to transfer these DRL policies, particularly in situations where safety is essential. Several solutions have been proposed to address these issues to enhance DRL's robustness. This chapter delves into the significance of adversarial attack and defense strategies in machine learning, emphasizing the unique challenges in adversarial DRL settings. It also presents an overview of recent advancements, DRL foundations, adversarial Markov decision process models, and comparisons among different attacks and defenses. The chapter further evaluates the effectiveness of various attacks and the efficacy of multiple defense mechanisms using simulation data, specifically focusing on policy success rates and average rewards. Potential limitations and prospects for future research are also explored.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch005"
    },
    {
        "id": 25095,
        "title": "Reinforcement",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108553179.007"
    },
    {
        "id": 25096,
        "title": "Combining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations",
        "authors": "Igor Halperin, Jiayu Liu, Xiao Zhang, Xiao Zhang",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4002715"
    },
    {
        "id": 25097,
        "title": "Deep Reinforcement Learning and Its Applications",
        "authors": "",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119873747.ch1"
    },
    {
        "id": 25098,
        "title": "Reinforcement Learning Theory",
        "authors": "Samit Ahlawat",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-8835-1_5"
    },
    {
        "id": 25099,
        "title": "Correction to: Reinforcement Learning for Sequential Decision and Optimal Control",
        "authors": "Shengbo Eben Li",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_12"
    },
    {
        "id": 25100,
        "title": "Robot Control in Worst‐Case Uncertainty Using Reinforcement Learning",
        "authors": "",
        "published": "2021-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.ch8"
    },
    {
        "id": 25101,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch3"
    },
    {
        "id": 25102,
        "title": "REIN-2: Giving birth to prepared reinforcement learning agents using reinforcement learning agents",
        "authors": "Aristotelis Lazaridis, Ioannis Vlahavas",
        "published": "2022-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2022.05.004"
    },
    {
        "id": 25103,
        "title": "Estimation of Reward Function Maximizing Learning Efficiency in Inverse Reinforcement Learning",
        "authors": "Yuki Kitazato, Sachiyo Arai",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006729502760283"
    },
    {
        "id": 25104,
        "title": "Notation",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0014"
    },
    {
        "id": 25105,
        "title": "Search for Robust Policies in Reinforcement Learning",
        "authors": "Qi Li",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008917404210428"
    },
    {
        "id": 25106,
        "title": "Causal Campbell-Goodhart’s Law and Reinforcement Learning",
        "authors": "Hal Ashton",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010197300670073"
    },
    {
        "id": 25107,
        "title": "The Advance of Reinforcement Learning and Deep Reinforcement Learning",
        "authors": "Le Lyu, Yang Shen, Sicheng Zhang",
        "published": "2022-2-25",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eebda53927.2022.9744760"
    },
    {
        "id": 25108,
        "title": "Was ist Reinforcement Learning?",
        "authors": "Alexander Zai, Brandon Brown",
        "published": "2020-10-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446466081.001"
    },
    {
        "id": 25109,
        "title": "Inverse Reinforcement Learning Integrated Reinforcement Learning for Single Intersection Traffic Signal Control",
        "authors": "Shiyi Gu, Tingting Zhang, Ya Zhang",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iai59504.2023.10327510"
    },
    {
        "id": 25110,
        "title": "Reinforcement Learning",
        "authors": "Zhi-Hua Zhou",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-1967-3_16"
    },
    {
        "id": 25111,
        "title": "Review for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "Amar Barik",
        "published": "2020-5-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v1/review1"
    },
    {
        "id": 25112,
        "title": "Meta-Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_9"
    },
    {
        "id": 25113,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2023-8-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009093057.019"
    },
    {
        "id": 25114,
        "title": "Review for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "Amar Barik",
        "published": "2020-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v2/review1"
    },
    {
        "id": 25115,
        "title": "Deep Reinforcement Learning Processor Design for Mobile Applications",
        "authors": "Juhyoung Lee, Hoi-Jun Yoo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-36793-9_1"
    },
    {
        "id": 25116,
        "title": "Future Steps in Quantum Reinforcement Learning for Complex Scenarios",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1_9"
    },
    {
        "id": 25117,
        "title": "Advanced Quantum Policy Approximation in Policy Gradient Reinforcement Learning",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1_6"
    },
    {
        "id": 25118,
        "title": "Deep Reinforcement Learning Models and Techniques",
        "authors": "",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119873747.ch3"
    },
    {
        "id": 25119,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Mark Liu",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b23383-14"
    },
    {
        "id": 25120,
        "title": "Evaluation of Reinforcement Learning Methods for a Self-learning System",
        "authors": "David Bechtold, Alexander Wendt, Axel Jantsch",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008909500360047"
    },
    {
        "id": 25121,
        "title": "Distributed Methods for Reinforcement Learning Survey",
        "authors": "Johannes Czech",
        "published": "2021",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41188-6_13"
    },
    {
        "id": 25122,
        "title": "Introduction",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0003"
    },
    {
        "id": 25123,
        "title": "Reinforcement Learning Guided by Provable Normative Compliance",
        "authors": "Emery Neufeld",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010835600003116"
    },
    {
        "id": 25124,
        "title": "Reward Function Design in Reinforcement Learning",
        "authors": "Jonas Eschmann",
        "published": "2021",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41188-6_3"
    },
    {
        "id": 25125,
        "title": "Reinforcement Learning",
        "authors": "Jugal Kalita",
        "published": "2022-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003002611-5"
    },
    {
        "id": 25126,
        "title": "Index",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0016"
    },
    {
        "id": 25127,
        "title": "Decision letter for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "",
        "published": "2020-5-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v1/decision1"
    },
    {
        "id": 25128,
        "title": "Decision letter for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": "",
        "published": "2020-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v2/decision1"
    },
    {
        "id": 25129,
        "title": "Reinforcement and Deep Reinforcement Machine Learning",
        "authors": "Parag Kulkarni",
        "published": "2017",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-55312-2_4"
    },
    {
        "id": 25130,
        "title": "Review for \"&lt;scp&gt;Multi‐Agent&lt;/scp&gt; Reinforcement Learning for Process Control: Exploring the intersection between fields of Reinforcement Learning, Control Theory and Game Theory\"",
        "authors": "",
        "published": "2022-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.24878/v1/review1"
    },
    {
        "id": 25131,
        "title": "Reinforcement Applications",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108553179.008"
    },
    {
        "id": 25132,
        "title": "References",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0015"
    },
    {
        "id": 25133,
        "title": "Preface",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0002"
    },
    {
        "id": 25134,
        "title": "Inverse Reinforcement Learning for Healthcare Applications: A  Survey",
        "authors": "Mohamed-Amine Chadi, Hajar Mousannif",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010729200003101"
    },
    {
        "id": 25135,
        "title": "Introduction",
        "authors": "",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119699057.ch1"
    },
    {
        "id": 25136,
        "title": "Control",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0009"
    },
    {
        "id": 25137,
        "title": "Policy-Based Deep Reinforcement Learning",
        "authors": "Mark Liu",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b23383-18"
    },
    {
        "id": 25138,
        "title": "Markov Decision Process and Reinforcement Learning",
        "authors": "",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119873747.ch2"
    },
    {
        "id": 25139,
        "title": "Reinforcement meta-learning optimizes visuomotor learning",
        "authors": "Taisei Sugiyama, Nicolas Schweighofer, Jun Izawa",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractReinforcement learning enables the brain to learn optimal action selection, such as go or not go, by forming state-action and action-outcome associations. Does this mechanism also optimize the brain’s willingness to learn, such as learn or not learn? Learning to learn by rewards, i.e., reinforcement meta-learning, is a crucial mechanism for machines to develop flexibility in learning, which is also considered in the brain without empirical examinations. Here, we show that humans learn to learn or not learn to maximize rewards in visuomotor learning tasks. We also show that this regulation of learning is not a motivational bias but is a result of an instrumental, active process, which takes into account the learning-outcome structure. Our results thus demonstrate the existence of reinforcement meta-learning in the human brain. Because motor learning is a process of minimizing sensory errors, our findings uncover an essential mechanism of interaction between reward and error.",
        "link": "http://dx.doi.org/10.1101/2020.01.19.912048"
    },
    {
        "id": 25140,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2022-12-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009122092.017"
    },
    {
        "id": 25141,
        "title": "Index",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.020"
    },
    {
        "id": 25142,
        "title": "[ Front Matter ]",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0001"
    },
    {
        "id": 25143,
        "title": "Modern General Game Playing with Reinforcement Learning",
        "authors": "",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "In computational science and robotics, game playing and artificial intelligence (AI) are research areas that have long been studied. RL systems represent a major step towards autonomous systems that comprehend the visual world at an incredibly deep level and are poised to revolutionize the field of artificial intelligence (AI). Game Play is the testbench where environmental variables can be evaluated to generate adaptive, intelligent, or responsive behavior or simulate real-world scenarios. In this survey, we will explore general gameplay (GGP) with reinforcement learning and its various applications. This survey will cover central algorithms in deep RL, including the deep Qnetwork (DQN) and general mathematical and pragmatic approaches taken in the field. The objective of the current review is to examine the history, associations, and recent advances in the field of general game playing with reinforcement learning and its subfields. We conclude by describing several current areas of research within this field.",
        "link": "http://dx.doi.org/10.33140/jeee.02.04.16"
    },
    {
        "id": 25144,
        "title": "Reinforcement Learning for Physical Layer Communications",
        "authors": "",
        "published": "2022-6-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108966559.011"
    },
    {
        "id": 25145,
        "title": "Indirect Reinforcement Learning",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_100211"
    },
    {
        "id": 25146,
        "title": "Mathematical and Algorithmic Understanding of Reinforcement Learning",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_2"
    },
    {
        "id": 25147,
        "title": "Reinforcement Learning: An Industrial Perspective",
        "authors": "Amit Surana",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_21"
    },
    {
        "id": 25148,
        "title": "Merging Reinforcement Learning and Inverse Reinforcement Learning via Auxiliary Reward System",
        "authors": "Wadhah Zeyad Tareq, Mehmet Fatih Amasyali",
        "published": "2022-2-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaiic54071.2022.9722629"
    },
    {
        "id": 25149,
        "title": "Learning Multi-intersection Traffic Signal Control via Coevolutionary Multi-Agent Reinforcement Learning",
        "authors": "Wubing Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Effective management of multi-intersection traffic  signal control (MTSC) is vital for intelligent transportation  systems.  Multi-agent reinforcement learning (MARL) has shown  promise in achieving MTSC.  However, existing MARL-based  MTSC algorithms have primarily focused on capturing the  spatial relationship between multi-intersection traffic signals  but have overlooking the importance of the temporally stable  traffic pattern.  This pattern refers to the fixed positions and  relatively stable traffic flow between intersections over short  periods in real-world MTSC scenarios, which indicates that the  learned spatial relationships between traffic signals should co-  evolve over time.  To this end, we propose a novel algorithm  called Coevolutionary Multi-Agent Reinforcement Learning (Co-  evoMARL).  CoevoMARL employs a graph neural network to  capture the complex spatial interaction network among traffic  signals.  Furthermore, we propose a relationship-driven progres-  sive LSTM (RDP-LSTM) that dynamically evolves the learned  spatial interaction network over time by leveraging insights from  the temporally stable traffic pattern.  To accelerate convergence,  we also propose the mutual information reward optimization  (MIRO) technique, which strengthens the correlation between  policy learning and high-performance samples by using a mutual  information-based intrinsic reward.  Experimental results on both  synthetic and real-world datasets demonstrate the superiority of  CoevoMARL over existing MTSC algorithms, providing valuable  insights into incorporating the temporally stable traffic pattern. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23254547"
    },
    {
        "id": 25150,
        "title": "Lernen mit Multi-Agent Reinforcement Learning",
        "authors": "Alexander Zai, Brandon Brown",
        "published": "2020-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446466081.009"
    },
    {
        "id": 25151,
        "title": "Model-Free Deep Reinforcement Learning—Algorithms and Applications",
        "authors": "Fabian Otto",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41188-6_10"
    },
    {
        "id": 25152,
        "title": "Appendices",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.014"
    },
    {
        "id": 25153,
        "title": "Introduction",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.002"
    },
    {
        "id": 25154,
        "title": "Review for \"Deep reinforcement learning for conservation decisions\"",
        "authors": "",
        "published": "2022-3-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13954/v2/review2"
    },
    {
        "id": 25155,
        "title": "Model-Based Reinforcement Learning from PILCO to PETS",
        "authors": "Pascal Klink",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41188-6_14"
    },
    {
        "id": 25156,
        "title": "Explainable Reinforcement Learning for Longitudinal Control",
        "authors": "Roman Liessner, Jan Dohmen, Marco Wiering",
        "published": "2021",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010256208740881"
    },
    {
        "id": 25157,
        "title": "Theories of Reinforcement",
        "authors": "",
        "published": "2020-9-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108553179.010"
    },
    {
        "id": 25158,
        "title": "Preface",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.001"
    },
    {
        "id": 25159,
        "title": "Deep Reinforcement Learning",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1"
    },
    {
        "id": 25160,
        "title": "Review for \"Deep reinforcement learning for conservation decisions\"",
        "authors": "",
        "published": "2022-3-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13954/v2/review1"
    },
    {
        "id": 25161,
        "title": "Reinforcement Learning",
        "authors": "Taeho Jo",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-65900-4_16"
    },
    {
        "id": 25162,
        "title": "Safe Reinforcement Learning for Learning from Human Demonstrations",
        "authors": "Jorge Ramirez, Wen Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nLearning optimal policies in Reinforcement Learning (RL) can present significant challenges in real-world applications, where it is crucial for agents to demonstrate specific behaviors while ensuring safety and efficiency. For human behavior learning using RL, the matter of safety during the learning process and its subsequent deployment in real-world scenarios has not been adequately addressed. This paper introduces a novel reinforcement learning approach that combines behavior learning with safe exploration in RL, offering a practical and effective method for acquiring specific behaviors while ensuring safety. The proposed algorithm's performance is evaluated in guiding a 2-degree-of-freedom planar robot in its task-space, demonstrating its ability to converge to an optimal policy while strictly adhering to safety constraints. This research has the potential to have a profound impact on various real-world applications, including robotics and virtual assistants.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3195158/v1"
    },
    {
        "id": 25163,
        "title": "A confidence-based reinforcement learning model for perceptual learning",
        "authors": "Matthias Guggenmos, Philipp Sterzer",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractIt is well established that learning can occur without external feedback, yet normative reinforcement learning theories have difficulties explaining such instances of learning. Recently, we reported on a confidence-based reinforcement learn-ing model for the model case of perceptual learning (Guggenmos, Wilbertz, Hebart, & Sterzer, 2016), according to which the brain capitalizes on internal monitoring processes when no external feedback is available. In the model, internal confidence prediction errors – the difference between current confidence and expected confidence – serve as teaching signals to guide learning. In the present paper, we explore an extension to this idea. The main idea is that the neural information processing pathways activated for a given sensory stimulus are subject to fluctuations, where some pathway configurations lead to higher confidence than others. Confidence prediction errors strengthen pathway configurations for which fluctuations lead to above-average confidence and weaken those that are associated with below-average con-fidence. We show through simulation that the model is capable of self-reinforced perceptual learning and can benefit from exploratory network fluctuations. In addition, by simulating different model parameters, we show that the ideal confidence-based learner should (i) exhibit high degrees of network fluctuation in the initial phase of learning, but re-duced fluctuations as learning progresses, (ii) have a high learning rate for network updates for immediate performance gains, but a low learning rate for long-term maximum performance, and (iii) be neither too under-nor too overconfident. In sum, we present a model in which confidence prediction errors strengthen favorable network fluctuations and enable learning in the absence of external feedback. The model can be readily applied to data of real-world perceptual tasks in which observers provided both choice and confidence reports.",
        "link": "http://dx.doi.org/10.1101/136903"
    },
    {
        "id": 25164,
        "title": "Learning Multi-intersection Traffic Signal Control via Coevolutionary Multi-Agent Reinforcement Learning",
        "authors": "Wubing Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Effective management of multi-intersection traffic  signal control (MTSC) is vital for intelligent transportation  systems.  Multi-agent reinforcement learning (MARL) has shown  promise in achieving MTSC.  However, existing MARL-based  MTSC algorithms have primarily focused on capturing the  spatial relationship between multi-intersection traffic signals  but have overlooking the importance of the temporally stable  traffic pattern.  This pattern refers to the fixed positions and  relatively stable traffic flow between intersections over short  periods in real-world MTSC scenarios, which indicates that the  learned spatial relationships between traffic signals should co-  evolve over time.  To this end, we propose a novel algorithm  called Coevolutionary Multi-Agent Reinforcement Learning (Co-  evoMARL).  CoevoMARL employs a graph neural network to  capture the complex spatial interaction network among traffic  signals.  Furthermore, we propose a relationship-driven progres-  sive LSTM (RDP-LSTM) that dynamically evolves the learned  spatial interaction network over time by leveraging insights from  the temporally stable traffic pattern.  To accelerate convergence,  we also propose the mutual information reward optimization  (MIRO) technique, which strengthens the correlation between  policy learning and high-performance samples by using a mutual  information-based intrinsic reward.  Experimental results on both  synthetic and real-world datasets demonstrate the superiority of  CoevoMARL over existing MTSC algorithms, providing valuable  insights into incorporating the temporally stable traffic pattern. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23254547.v1"
    },
    {
        "id": 25165,
        "title": "Interpretierbares Reinforcement Learning: Aufmerksamkeitsmodelle und relationale Modelle",
        "authors": "Alexander Zai, Brandon Brown",
        "published": "2020-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446466081.010"
    },
    {
        "id": 25166,
        "title": "Average-Payoff Reinforcement Learning",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_100029"
    },
    {
        "id": 25167,
        "title": "One-Step Reinforcement Learning",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_100350"
    },
    {
        "id": 25168,
        "title": "Kernel-Based Reinforcement Learning",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_100235"
    },
    {
        "id": 25169,
        "title": "Reinforcement learning using associative memory networks",
        "authors": "Ricardo Salmon",
        "published": "No Date",
        "citations": 0,
        "abstract": "It is shown that associative memory networks are capable of solving immediate and general reinforcement learning (RL) problems by combining techniques from associative neural networks and reinforcement learning and in particular Q-learning. The modified model is shown to outperform native RL techniques on a stochastic grid world task by developing correct policies. In addition, we formulated an analogous method to add feature extraction as dimensional reduction and eligibility traces as another mechanism to help solve the credit assignment problem. The network contrary to pure RL methods is based on associative memory principles such as distribution of information, pattern completion, Hebbian learning, and noise tolerance (limit cycles, one to many associations, chaos, etc). Because of this, it can be argued that the model possesses more cognitive explanative power than other RL or hybrid models. It may be an effective tool for bridging the gap between biological memory models and computational memory models.",
        "link": "http://dx.doi.org/10.32920/ryerson.14656347"
    },
    {
        "id": 25170,
        "title": "Reinforcement learning using associative memory networks",
        "authors": "Ricardo Salmon",
        "published": "No Date",
        "citations": 0,
        "abstract": "It is shown that associative memory networks are capable of solving immediate and general reinforcement learning (RL) problems by combining techniques from associative neural networks and reinforcement learning and in particular Q-learning. The modified model is shown to outperform native RL techniques on a stochastic grid world task by developing correct policies. In addition, we formulated an analogous method to add feature extraction as dimensional reduction and eligibility traces as another mechanism to help solve the credit assignment problem. The network contrary to pure RL methods is based on associative memory principles such as distribution of information, pattern completion, Hebbian learning, and noise tolerance (limit cycles, one to many associations, chaos, etc). Because of this, it can be argued that the model possesses more cognitive explanative power than other RL or hybrid models. It may be an effective tool for bridging the gap between biological memory models and computational memory models.",
        "link": "http://dx.doi.org/10.32920/ryerson.14656347.v1"
    },
    {
        "id": 25171,
        "title": "References",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.018"
    },
    {
        "id": 25172,
        "title": "Overview of Methods",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "This video introduces the variety of methods for model-based and model-free reinforcement learning, including: dynamic programming, value and policy iteration, Q-learning, deep RL, TD-learning, SARSA, policy gradient optimization, among others.",
        "link": "http://dx.doi.org/10.52843/cassyni.jcgdvc"
    },
    {
        "id": 25173,
        "title": "Review for \"Deep reinforcement learning for conservation decisions\"",
        "authors": "",
        "published": "2022-3-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13954/v2/review3"
    },
    {
        "id": 25174,
        "title": "Using Reinforcement Learning to Optimize Quantum Circuits in the Presence of Noise",
        "authors": "Khalil Guy, Gabriel Purdue",
        "published": "2020-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1648527"
    },
    {
        "id": 25175,
        "title": "Turn-Based Multi-Agent Reinforcement Learning Model Checking",
        "authors": "Dennis Gross",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011872800003393"
    },
    {
        "id": 25176,
        "title": "Incremental Algorithms",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0008"
    },
    {
        "id": 25177,
        "title": "Statistical Functionals",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0010"
    },
    {
        "id": 25178,
        "title": "Bayesian Reinforcement Learning",
        "authors": "Christos Dimitrakakis, Ronald Ortner",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-07614-5_9"
    },
    {
        "id": 25179,
        "title": "Reinforcement Learning",
        "authors": "Miroslav Kubat",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-63913-0_17"
    },
    {
        "id": 25180,
        "title": "Learning by Reinforcement and Exposure",
        "authors": "Wladyslaw Sluckin",
        "published": "2017-7-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9780203788950-9"
    },
    {
        "id": 25181,
        "title": "Active Learning with Reinforcement Learning for Data-Efficient Classification",
        "authors": "Xaolin Chun",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper introduces a novel approach that combines active learning and reinforcement learning to achieve data-efficient classification. Traditional supervised learning algorithms require a large amount of labeled data for training, which can be time-consuming and costly to obtain. By integrating active learning techniques with reinforcement learning, this study aims to address the challenge of data scarcity in classification tasks. The proposed framework employs an agent that interacts with the learning environment, actively selecting the most informative instances to label from a pool of unlabeled data. The agent's decision-making process is guided by a reinforcement learning algorithm, which learns an optimal policy through trial and error. By considering uncertainty measures, exploration-exploitation trade-offs, and the goal of maximizing classification performance, the agent learns to select instances that provide the most valuable information for model training. Experimental evaluations on various datasets demonstrate the effectiveness of the proposed approach, showcasing significant improvements in classification accuracy compared to traditional supervised learning methods with limited labeled data. This work contributes to the field of data-efficient classification by demonstrating the potential of active learning combined with reinforcement learning, offering a promising avenue for reducing the annotation burden and accelerating the training process in scenarios with limited labeled data.",
        "link": "http://dx.doi.org/10.31219/osf.io/qj94x"
    },
    {
        "id": 25182,
        "title": "Reinforcement Learning",
        "authors": "Peter Stone",
        "published": "2017",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_720"
    },
    {
        "id": 25183,
        "title": "10. Reinforcement learning",
        "authors": "",
        "published": "2020-5-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9783110595567-011"
    },
    {
        "id": 25184,
        "title": "A Supervised Learning Approach to Robust Reinforcement Learning for Job Shop Scheduling",
        "authors": "Christoph Schmidl, Thiago Simão, Nils Jansen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012473600003636"
    },
    {
        "id": 25185,
        "title": "General Game Playing with Reinforcement Learning",
        "authors": "Shaun Pritchard",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.163941421.14580141/v1"
    },
    {
        "id": 25186,
        "title": "Stochastic Control",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.010"
    },
    {
        "id": 25187,
        "title": "Stochastic Approximation",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.011"
    },
    {
        "id": 25188,
        "title": "Distributional Dynamic Programming",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0007"
    },
    {
        "id": 25189,
        "title": "Reinforcement Learning Considering Worst Case and Equality within Episodes",
        "authors": "Toshihiro Matsui",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009178603350342"
    },
    {
        "id": 25190,
        "title": "Probabilistic Model Checking of Stochastic Reinforcement Learning Policies",
        "authors": "Dennis Gross, Helge Spieker",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012357700003636"
    },
    {
        "id": 25191,
        "title": "DEEP REINFORCEMENT LEARNING ON STOCK DATA",
        "authors": "Abdullayev Nurmuhammet,  ",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "This study proposes using Deep Reinforcement Learning (DRL) for stock trading decisions and prediction. DRL is a machine learning technique that enables agents to learn optimal strategies by interacting with their environment. The proposed model surpasses traditional models and can make informed trading decisions in real-time. The study highlights  the feasibility of applying DRL in financial markets and its advantages in strategic decision- making. The model's ability to learn from market dynamics makes it a promising approach  for stock market forecasting. Overall, this paper provides valuable insights into the use of DRL for stock trading decisions and prediction, establishing a strong case for its adoption in financial markets. Keywords: reinforcement learning, stock market, deep reinforcement learning.",
        "link": "http://dx.doi.org/10.17015/aas.2023.232.49"
    },
    {
        "id": 25192,
        "title": "Monte-Carlo Based Reinforcement Learning (MCRL)",
        "authors": "Muath Alrammal,  , Munir Naveed",
        "published": "2020-2",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2020.10.2.924"
    },
    {
        "id": 25193,
        "title": "What May Lie Ahead in Reinforcement Learning",
        "authors": "Derya Cansever",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_1"
    },
    {
        "id": 25194,
        "title": "Contextual Online Imitation Learning (COIL): Using Guide Policies in Reinforcement Learning",
        "authors": "Alexander Hill, Marc Groefsema, Matthia Sabatelli, Raffaella Carloni, Marco Grzegorczyk",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012312700003636"
    },
    {
        "id": 25195,
        "title": "Relational Reinforcement Learning",
        "authors": "Kurt Driessens",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_726"
    },
    {
        "id": 25196,
        "title": "Reinforcement Learning in Structured Domains",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_100404"
    },
    {
        "id": 25197,
        "title": "An Introduction to Reinforcement Learning and Its Application in Various Domains",
        "authors": "Samina Amin",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) is a dynamic and evolving subfield of machine learning that focuses on training intelligent agents to learn and adapt through interactions with their environment. This introductory article provides an overview of the fundamental concepts and principles of RL, elucidating its core components, such as the agent, environment, actions, and rewards. This study aims to give readers an in-depth introduction to RL and show examples of its different uses in various domains. RL can allow agents to learn through interaction with an environment, which has led to its enormous interest. The core ideas of RL and its essential elements will be covered in this study, after which it will go into applications in industries including robotics, gaming, finance, healthcare, and more. The fundamental ideas of RL will become clearer to readers, and they will recognize how transformative it can be when used to address challenging decision-making issues. These applications demonstrate the versatility and significance of RL in shaping the future of technology and automation.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch001"
    },
    {
        "id": 25198,
        "title": "Distribution-Free Reinforcement Learning",
        "authors": "Christos Dimitrakakis, Ronald Ortner",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-07614-5_10"
    },
    {
        "id": 25199,
        "title": "Decision letter for \"&lt;scp&gt;Multi‐Agent&lt;/scp&gt; Reinforcement Learning for Process Control: Exploring the intersection between fields of Reinforcement Learning, Control Theory and Game Theory\"",
        "authors": "",
        "published": "2022-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.24878/v1/decision1"
    },
    {
        "id": 25200,
        "title": "Markov Chains",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.009"
    },
    {
        "id": 25201,
        "title": "A Reinforcement Learning Approach for Traffic Control",
        "authors": "Urs Baumgart, Michael Burger",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010448500002932"
    },
    {
        "id": 25202,
        "title": "A Reinforcement Learning Approach for Traffic Control",
        "authors": "Urs Baumgart, Michael Burger",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010448501330141"
    },
    {
        "id": 25203,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2021-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108860604.011"
    },
    {
        "id": 25204,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2022-5-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009089517.016"
    },
    {
        "id": 25205,
        "title": "Distribution-Free Reinforcement Learning",
        "authors": "Christos Dimitrakakis, Ronald Ortner",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-07614-5_10"
    },
    {
        "id": 25206,
        "title": "Decision letter for \"&lt;scp&gt;Multi‐Agent&lt;/scp&gt; Reinforcement Learning for Process Control: Exploring the intersection between fields of Reinforcement Learning, Control Theory and Game Theory\"",
        "authors": "",
        "published": "2022-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.24878/v1/decision1"
    },
    {
        "id": 25207,
        "title": "Multi-Agent Deep Reinforcement Learning for Collaborative Task Scheduling",
        "authors": "Mali Gergely",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012434700003636"
    },
    {
        "id": 25208,
        "title": "Task Scheduling: A Reinforcement Learning Based Approach",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011826100003393"
    },
    {
        "id": 25209,
        "title": "Reinforcement Learning and Inverse Reinforcement Learning with System 1 and System 2",
        "authors": "Alexander Peysakhovich",
        "published": "2019-1-27",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3306618.3314259"
    },
    {
        "id": 25210,
        "title": "Markov Chains",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.009"
    },
    {
        "id": 25211,
        "title": "Operators and Metrics",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0006"
    },
    {
        "id": 25212,
        "title": "Review for \"Connectivity conservation planning through deep reinforcement learning\"",
        "authors": "",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.14300/v1/review1"
    },
    {
        "id": 25213,
        "title": "Linear Function Approximation",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0011"
    },
    {
        "id": 25214,
        "title": "Reinforcement Learning",
        "authors": "Abhishek Nandy, Manisha Biswas",
        "published": "2018",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-3285-9"
    },
    {
        "id": 25215,
        "title": "Learning from other minds: An optimistic critique of reinforcement learning models of social learning",
        "authors": "Natalia Vélez, Hyowon Gweon",
        "published": "No Date",
        "citations": 1,
        "abstract": "In the past decade, reinforcement learning models have been productively applied to examine neural signatures that track the value of social information over repeated observations. However, by operationalizing social information as a lean, reward-predictive cue, this literature underestimates the richness of human social learning: Humans readily go beyond action-outcome mappings and can draw flexible inferences even from a single observation. We argue that reinforcement learning models need minds, i.e, a generative model of how other agents’ unobservable mental states cause their observable actions. Recent advances in inferential social learning suggest that even young children learn from others via a generative model of other minds. Bridging these perspectives can enrich our understanding of the neural bases of distinctively human social learning.",
        "link": "http://dx.doi.org/10.31234/osf.io/q4bxr"
    },
    {
        "id": 25216,
        "title": "General Game Playing with Reinforcement Learning",
        "authors": "Shaun Pritchard",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.163941421.14580141/v1"
    },
    {
        "id": 25217,
        "title": "Stochastic Control",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.010"
    },
    {
        "id": 25218,
        "title": "Stochastic Approximation",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.011"
    },
    {
        "id": 25219,
        "title": "Distributional Dynamic Programming",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0007"
    },
    {
        "id": 25220,
        "title": "Reinforcement Learning Considering Worst Case and Equality within Episodes",
        "authors": "Toshihiro Matsui",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009178603350342"
    },
    {
        "id": 25221,
        "title": "DEEP REINFORCEMENT LEARNING ON STOCK DATA",
        "authors": "Abdullayev Nurmuhammet,  ",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "This study proposes using Deep Reinforcement Learning (DRL) for stock trading decisions and prediction. DRL is a machine learning technique that enables agents to learn optimal strategies by interacting with their environment. The proposed model surpasses traditional models and can make informed trading decisions in real-time. The study highlights  the feasibility of applying DRL in financial markets and its advantages in strategic decision- making. The model's ability to learn from market dynamics makes it a promising approach  for stock market forecasting. Overall, this paper provides valuable insights into the use of DRL for stock trading decisions and prediction, establishing a strong case for its adoption in financial markets. Keywords: reinforcement learning, stock market, deep reinforcement learning.",
        "link": "http://dx.doi.org/10.17015/aas.2023.232.49"
    },
    {
        "id": 25222,
        "title": "Probabilistic Model Checking of Stochastic Reinforcement Learning Policies",
        "authors": "Dennis Gross, Helge Spieker",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012357700003636"
    },
    {
        "id": 25223,
        "title": "Monte-Carlo Based Reinforcement Learning (MCRL)",
        "authors": "Muath Alrammal,  , Munir Naveed",
        "published": "2020-2",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2020.10.2.924"
    },
    {
        "id": 25224,
        "title": "Q-Learning Algorithm",
        "authors": "Rafael Ris-Ala",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-37345-9_3"
    },
    {
        "id": 25225,
        "title": "Mathematical Background",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.015"
    },
    {
        "id": 25226,
        "title": "Autonomous Drone Takeoff and Navigation Using Reinforcement Learning",
        "authors": "Sana Ikli, Ilhem Quenel",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012296300003636"
    },
    {
        "id": 25227,
        "title": "AIM-RL: A New Framework Supporting Reinforcement Learning Experiments",
        "authors": "Ionuţ-Cristian Pistol, Andrei Arusoaie",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012091100003538"
    },
    {
        "id": 25228,
        "title": "Reinforcement Learning Theory",
        "authors": "Philip Osborne, Kajal Singh, Matthew E. Taylor",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-79167-3_2"
    },
    {
        "id": 25229,
        "title": "Tabular Reinforcement learning for Robust, Explainable CropRotation Policies Matching Deep Reinforcement LearningPerformance",
        "authors": "Georg Goldenits, Kevin Mallinger, Thomas Neubauer, Edgar Weippl",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDigital Twins are becoming an increasingly researched area in agriculture due to the pressure on food security caused by growing population numbers and climate change. They provide a necessary push towards more efficient and sustainable agricultural methods to secure and increase crop yields.Digital Twins often use Machine Learning, and more recently, deep learning methods in their architecture to process data and predict future outcomes based on input data. However, concerns about the trustworthiness of the output from deep learning models persist due to the lack of clarity regarding the reasoning behind their outputs.\nIn our work, we have developed crop rotation policies using explainable tabular reinforcement learning techniques. We have compared these policies to those generated by a deep Q-learning approach, using both five-step and seven-step rotations. The aim of the rotations is to maximise crop yields while maintaining a healthy nitrogen level in the soil and adhering to established planting rules. Crop yields may vary due to external factors such as weather patterns, so perturbations were added to the reward signal to account for these influences. The deployed explainable tabular reinforcement learning methods perform similarly to the deep Q-learning approach in terms of collected reward when the rewards are not perturbed. However, in the perturbed reward setting, robust tabular reinforcement learning methods outperform the deep learning approach while maintaining interpretable policies. By consulting with farmers and crop rotation experts, we demonstrate that the derived policies are reasonable and that the use of interpretable reinforcement learning has increased confidence in the resulting policies, thereby increasing the likelihood that farmers will adopt the suggested policies.\nKeywords: Digital Twin, Reinforcement Learning, Explainable AI, Agriculture, Crop Rotation Planning, Climate Change, Food Security",
        "link": "http://dx.doi.org/10.5194/egusphere-egu24-9018"
    },
    {
        "id": 25230,
        "title": "Chapter 6: NLP and Reinforcement Learning",
        "authors": "",
        "published": "2020-2-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683924654-007"
    },
    {
        "id": 25231,
        "title": "Temporal-Related Convolutional-Restricted-Boltzmann-Machine Capable of Learning Relational Order via Reinforcement Learning Procedure",
        "authors": "Zizhuang Wang,  ",
        "published": "2017-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2017.7.1.610"
    },
    {
        "id": 25232,
        "title": "Risk-averse Distributional Reinforcement Learning: A CVaR Optimization Approach",
        "authors": "Silvestr Stanko, Karel Macek",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008175604120423"
    },
    {
        "id": 25233,
        "title": "Automating XSS Vulnerability Testing Using Reinforcement Learning",
        "authors": "Kento Hasegawa, Seira Hidano, Kazuhide Fukushima",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011653600003405"
    },
    {
        "id": 25234,
        "title": "Molecule Builder: Environment for Testing Reinforcement Learning Agents",
        "authors": "Petr Hyner, Jan Hůla, Mikoláš Janota",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012257900003595"
    },
    {
        "id": 25235,
        "title": "Reinforcement learning",
        "authors": "Guillaume Coqueret, Tony Guida",
        "published": "2023-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003121596-20"
    },
    {
        "id": 25236,
        "title": "Reinforcement learning",
        "authors": "Guillaume Coqueret, Tony Guida",
        "published": "2020-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003034858-20"
    },
    {
        "id": 25237,
        "title": "Associative Reinforcement Learning",
        "authors": "Alexander L. Strehl",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_40"
    },
    {
        "id": 25238,
        "title": "Gaussian Process Reinforcement Learning",
        "authors": "Yaakov Engel",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_109"
    },
    {
        "id": 25239,
        "title": "Deep Learning in Medical Imaging",
        "authors": "Narjes Benameur, Ramzi Mahmoudi",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "Medical image processing tools play an important role in clinical routine in helping doctors to establish whether a patient has or does not have a certain disease. To validate the diagnosis results, various clinical parameters must be defined. In this context, several algorithms and mathematical tools have been developed in the last two decades to extract accurate information from medical images or signals. Traditionally, the extraction of features using image processing from medical data are time-consuming which requires human interaction and expert validation. The segmentation of medical images, the classification of medical images, and the significance of deep learning-based algorithms in disease detection are all topics covered in this chapter.",
        "link": "http://dx.doi.org/10.5772/intechopen.111686"
    },
    {
        "id": 25240,
        "title": "Introduction to Optimal Control and Reinforcement Learning",
        "authors": "Syed Ali Asad Rizvi, Zongli Lin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-15858-2_1"
    },
    {
        "id": 25241,
        "title": "Teaching Reinforcement Learning Agents via Reinforcement Learning",
        "authors": "Kun Yang, Chengshuai Shi, Cong Shen",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089695"
    },
    {
        "id": 25242,
        "title": "Interleaved Robust Reinforcement Learning",
        "authors": "Jinna Li, Frank L. Lewis, Jialu Fan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-28394-9_4"
    },
    {
        "id": 25243,
        "title": "Reactive Power Optimization Using Feed Forward Neural Deep Reinforcement Learning Method : (Deep Reinforcement Learning DQN algorithm)",
        "authors": "Mazhar Ali, Asad Mujeeb, Hameed Ullah, Saran Zeb",
        "published": "2020-5",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aeees48850.2020.9121492"
    },
    {
        "id": 25244,
        "title": "Adaptive Sampling",
        "authors": "Aske Plaat",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-59238-7_5"
    },
    {
        "id": 25245,
        "title": "Reinforcement Learning",
        "authors": "T V Geetha, S Sendhilkumar",
        "published": "2023-3-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003290100-11"
    },
    {
        "id": 25246,
        "title": "Variation-resistant Q-learning: Controlling and Utilizing Estimation Bias in Reinforcement Learning for Better Performance",
        "authors": "Andreas Pentaliotis, Marco Wiering",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010168000170028"
    },
    {
        "id": 25247,
        "title": "Function Approximation",
        "authors": "Aske Plaat",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-59238-7_6"
    },
    {
        "id": 25248,
        "title": "Deep reinforcement learning architectures",
        "authors": "Shajulin Benedict",
        "published": "2022-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/978-0-7503-4024-3ch9"
    },
    {
        "id": 25249,
        "title": "AI-Powered Precision: Revolutionizing Lunar Landings with Deep Learning and Reinforcement Learning",
        "authors": "Saaketh Suvarna, Cody Waldecker",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.58445/rars.950"
    },
    {
        "id": 25250,
        "title": "Comparison of Multiple Reinforcement Learning and Deep Reinforcement Learning Methods for the Task Aimed at Achieving the Goal",
        "authors": "Roman Parak, Radomil Matousek",
        "published": "2021-6-21",
        "citations": 5,
        "abstract": "Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) methods are a promising approach to solving complex tasks in the real world with physical robots. In this paper, we compare several reinforcement  learning (Q-Learning, SARSA) and deep reinforcement learning (Deep Q-Network, Deep Sarsa) methods for a task aimed at achieving a specific goal using robotics arm UR3. The main optimization problem of this experiment is to find the best solution for each RL/DRL scenario and minimize the Euclidean distance accuracy error and smooth the resulting path by the Bézier spline method. The simulation and real word applications are controlled by the Robot Operating System (ROS). The learning environment is implemented using the OpenAI Gym library which uses the RVIZ simulation tool and the Gazebo 3D modeling tool for dynamics and kinematics.",
        "link": "http://dx.doi.org/10.13164/mendel.2021.1.001"
    },
    {
        "id": 25251,
        "title": "Workshop on Distributed Reinforcement Learning and Reinforcement-Learning Games [Conference Reports]",
        "authors": "Kyriakos G. Vamvoudakis, Yan Wan, Frank L. Lewis",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mcs.2019.2938053"
    },
    {
        "id": 25252,
        "title": "Reliability-Based Reinforcement Learning Under Uncertainty",
        "authors": "Zequn Wang, Narendra Patwardhan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.0001814v"
    },
    {
        "id": 25253,
        "title": "Markov Decision Processes",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.016"
    },
    {
        "id": 25254,
        "title": "Imitation Learning",
        "authors": "Zihan Ding",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_8"
    },
    {
        "id": 25255,
        "title": "Average-Reward Reinforcement Learning",
        "authors": "Prasad Tadepalli",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_17"
    },
    {
        "id": 25256,
        "title": "Hierarchical Reinforcement Learning Introducing Genetic Algorithm for POMDPs Environments",
        "authors": "Kohei Suzuki, Shohei Kato",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007405403180327"
    },
    {
        "id": 25257,
        "title": "Deep Reinforcement Learning for Mobile Social Networks",
        "authors": "F. Richard Yu, Ying He",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-10546-4_4"
    },
    {
        "id": 25258,
        "title": "Author response for \"&lt;scp&gt;Multi‐Agent&lt;/scp&gt; Reinforcement Learning for Process Control: Exploring the intersection between fields of Reinforcement Learning, Control Theory and Game Theory\"",
        "authors": " Yue Yifei,  S. Lakshminarayanan",
        "published": "2023-1-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/cjce.24878/v2/response1"
    },
    {
        "id": 25259,
        "title": "Farsighter: Efficient Multi-Step Exploration for Deep Reinforcement Learning",
        "authors": "Yongshuai Liu, Xin Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011800600003393"
    },
    {
        "id": 25260,
        "title": "Decentralized Multi-agent Formation Control via Deep Reinforcement Learning",
        "authors": "Aniket Gutpa, Raghava Nallanthighal",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010241302890295"
    },
    {
        "id": 25261,
        "title": "Reinforcement Learning-based Real-time Fair Online Resource Matching",
        "authors": "Pankaj Mishra, Ahmed Moustafa",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010834600003116"
    },
    {
        "id": 25262,
        "title": "Nonholonomic Robot Navigation of Mazes using Reinforcement Learning",
        "authors": "Daniel Gleason, Michael Jenkin",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011123600003271"
    },
    {
        "id": 25263,
        "title": "Self-Play",
        "authors": "Aske Plaat",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-59238-7_7"
    },
    {
        "id": 25264,
        "title": "Heuristic Planning",
        "authors": "Aske Plaat",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-59238-7_4"
    },
    {
        "id": 25265,
        "title": "Mutual Learning: Part II --Reinforcement Learning",
        "authors": "Kumpati S. Narendra, Snehasis Mukhopadhyay",
        "published": "2020-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147838"
    },
    {
        "id": 25266,
        "title": "Solving Maximal Stable Set Problem via Deep Reinforcement Learning",
        "authors": "Taiyi Wang, Jiahao Shi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010179904830489"
    },
    {
        "id": 25267,
        "title": "Multi-task Deep Reinforcement Learning for IoT Service Selection",
        "authors": "Hiroki Matsuoka, Ahmed Moustafa",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010857800003116"
    },
    {
        "id": 25268,
        "title": "Measuring Inflation within Virtual Economies using Deep Reinforcement Learning",
        "authors": "Conor Stephens, Chris Exton",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010392804440453"
    },
    {
        "id": 25269,
        "title": "Review for \"A reinforcement‐learning approach for individual pitch control\"",
        "authors": "",
        "published": "2022-1-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2734/v1/review2"
    },
    {
        "id": 25270,
        "title": "Control Systems and Reinforcement Learning",
        "authors": "Sean Meyn",
        "published": "2022-5-31",
        "citations": 17,
        "abstract": "A high school student can create deep Q-learning code to control her robot, without any understanding of the meaning of 'deep' or 'Q', or why the code sometimes fails. This book is designed to explain the science behind reinforcement learning and optimal control in a way that is accessible to students with a background in calculus and matrix algebra. A unique focus is algorithm design to obtain the fastest possible speed of convergence for learning algorithms, along with insight into why reinforcement learning sometimes fails. Advanced stochastic process theory is avoided at the start by substituting random exploration with more intuitive deterministic probing for learning. Once these ideas are understood, it is not difficult to master techniques rooted in stochastic control. These topics are covered in the second part of the book, starting with Markov chain theory and ending with a fresh look at actor-critic methods for reinforcement learning.",
        "link": "http://dx.doi.org/10.1017/9781009051873"
    },
    {
        "id": 25271,
        "title": "Temporal Difference Methods",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.012"
    },
    {
        "id": 25272,
        "title": "Review for \"A reinforcement‐learning approach for individual pitch control\"",
        "authors": "",
        "published": "2022-1-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2734/v1/review1"
    },
    {
        "id": 25273,
        "title": "Value Function Approximations",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.007"
    },
    {
        "id": 25274,
        "title": "Adaptive Reinforcement Learning with LLM-augmented Reward Functions",
        "authors": "Alex Place",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Learning in sequential decision making problems can be significantly affected by the choice of reward function. Although crucial for learning success, reward function design remains a complex task, requiring expertise and not always aligned with human preferences. Large Language Models (LLMs) offer a promising avenue for reward design through textual prompts that leverage the prior knowledge and contextual reasoning of LLMs. Despite their potential, LLM responses lack guarantees and their reasoning abilities are poorly understood. To mitigate potential LLM errors, we introduce an alternative approach: learning a new reward function utilizing LLM outputs as auxiliary rewards. This problem is tackled through a bi-level optimization framework, showcasing the method's proficiency in both optimal reward acquisition and adaptive reward shaping. The proposed approach demonstrates robustness and effectiveness, offering a novel strategy for enhancing reinforcement learning outcomes.      </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24708198.v1"
    },
    {
        "id": 25275,
        "title": "Fundamentals without Noise",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.003"
    },
    {
        "id": 25276,
        "title": "Control Crash Course",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.004"
    },
    {
        "id": 25277,
        "title": "The Distribution of Returns",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0004"
    },
    {
        "id": 25278,
        "title": "Fundamentals of Reinforcement Learning",
        "authors": "Rafael Ris-Ala",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-37345-9"
    },
    {
        "id": 25279,
        "title": "Powertrain Modeling and Reinforcement Learning",
        "authors": "Teng Liu",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01503-8_2"
    },
    {
        "id": 25280,
        "title": "Adaptive Reinforcement Learning with LLM-augmented Reward Functions",
        "authors": "Alex Place",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Learning in sequential decision making problems can be significantly affected by the choice of reward function. Although crucial for learning success, reward function design remains a complex task, requiring expertise and not always aligned with human preferences. Large Language Models (LLMs) offer a promising avenue for reward design through textual prompts that leverage the prior knowledge and contextual reasoning of LLMs. Despite their potential, LLM responses lack guarantees and their reasoning abilities are poorly understood. To mitigate potential LLM errors, we introduce an alternative approach: learning a new reward function utilizing LLM outputs as auxiliary rewards. This problem is tackled through a bi-level optimization framework, showcasing the method's proficiency in both optimal reward acquisition and adaptive reward shaping. The proposed approach demonstrates robustness and effectiveness, offering a novel strategy for enhancing reinforcement learning outcomes.      </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24708198"
    },
    {
        "id": 25281,
        "title": "Review for \"A reinforcement‐learning approach for individual pitch control\"",
        "authors": "",
        "published": "2022-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2734/v2/review2"
    },
    {
        "id": 25282,
        "title": "Review for \"A reinforcement‐learning approach for individual pitch control\"",
        "authors": "",
        "published": "2022-4-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2734/v2/review1"
    },
    {
        "id": 25283,
        "title": "Off-Policy Game Reinforcement Learning",
        "authors": "Jinna Li, Frank L. Lewis, Jialu Fan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-28394-9_7"
    },
    {
        "id": 25284,
        "title": "Using Deep Reinforcement Learning to Build Intelligent Tutoring Systems",
        "authors": "Ciprian Paduraru, Miruna Paduraru, Stefan Iordache",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011267400003266"
    },
    {
        "id": 25285,
        "title": "A Meta-Learning Reinforcement Training Method for Machine Learning Image-To-Image Optical Proximity Correction",
        "authors": "Albert Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/3197"
    },
    {
        "id": 25286,
        "title": "Reinforcement Learning",
        "authors": "Indranath Chatterjee",
        "published": "2021-12-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2174/9781681089409121010008"
    },
    {
        "id": 25287,
        "title": "Industrial Applications of Game Reinforcement Learning",
        "authors": "Jinna Li, Frank L. Lewis, Jialu Fan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-28394-9_8"
    },
    {
        "id": 25288,
        "title": "Infrared Camera Assisted UAV Autonomous Control via Deep Reinforcement Learning",
        "authors": "",
        "published": "2020-1-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2021-1121.vid"
    },
    {
        "id": 25289,
        "title": "Background: Deep Reinforcement Learning",
        "authors": "Yuecheng Li, Hongwen He",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-79206-9_2"
    },
    {
        "id": 25290,
        "title": "Efficient Exploration in Reinforcement Learning",
        "authors": "John Langford",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_244"
    },
    {
        "id": 25291,
        "title": "Intelligence and Games",
        "authors": "Aske Plaat",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-59238-7_2"
    },
    {
        "id": 25292,
        "title": "Reinforcement Learning: From TD(0) to Deep-Q-Learning",
        "authors": "Miroslav Kubat",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-81935-4_18"
    },
    {
        "id": 25293,
        "title": "$$H_\\infty $$ Control Using Reinforcement Learning",
        "authors": "Jinna Li, Frank L. Lewis, Jialu Fan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-28394-9_2"
    },
    {
        "id": 25294,
        "title": "Deep Reinforcement Learning",
        "authors": "Charu Aggarwal",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-29642-0_11"
    },
    {
        "id": 25295,
        "title": "Exploring Parity Challenges in Reinforcement Learning  Through Curriculum Learning with Noisy Labels",
        "authors": "Bei Zhou, Soren Riis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4693751"
    },
    {
        "id": 25296,
        "title": "The Partial Reinforcement Extinction Effect Depends on Learning about Non-Reinforced Trials Rather than Reinforcement Rate",
        "authors": "Justin Harris, Dorothy Kwok, Daniel Gottlieb",
        "published": "No Date",
        "citations": 0,
        "abstract": "Conditioned responding extinguishes more slowly after partial (inconsistent) reinforcement than after consistent reinforcement. This Partial Reinforcement Extinction Effect (PREE) is usually attributed to learning about nonreinforcement during the partial schedule. An alternative explanation attributes it to any difference in the rate of reinforcement, arguing that animals can detect the change to nonreinforcement more quickly after a denser schedule than a leaner schedule. Experiments 1a and 1b compared extinction of magazine responding to a conditioned stimulus (CS) reinforced with one food pellet per trial and a CS reinforced with two pellets per trial. Despite the difference in reinforcement rate, there was no reliable difference in extinction. Both experiments did demonstrate the conventional PREE comparing a partial CS (50% reinforced) with a consistent CS. Experiments 2 and 3 tested whether the PREE depends specifically on learning about nonreinforced trials during partial reinforcement. Rats were trained with two CS configurations, A and AX. One was partially reinforced, the other consistently reinforced. When AX was partial and A consistent, responding to AX extinguished more slowly than to A. When AX was consistent and A was partial, there was no difference in their extinction. Therefore, pairing X with partial reinforcement allowed rats to show a PREE to AX that did not generalise to A. Pairing A with partial reinforcement meant that rats showed a PREE to A that generalised to AX. Thus, the PREE depends on learning about nonreinforced trials during partial reinforcement and is not due to any difference in per-trial probability of reinforcement",
        "link": "http://dx.doi.org/10.31234/osf.io/9unpm"
    },
    {
        "id": 25297,
        "title": "Unsupervised learning and reinforcement learning",
        "authors": "Richard E. Neapolitan, Xia Jiang",
        "published": "2018-3-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b22400-12"
    },
    {
        "id": 25298,
        "title": "Instance-Based Reinforcement Learning",
        "authors": "William D. Smart",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_410"
    },
    {
        "id": 25299,
        "title": "Author response for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": " Kumar Chandrasekaran,  Prabaakaran Kandasamy,  Srividhya Ramanathan",
        "published": "2020-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v2/response1"
    },
    {
        "id": 25300,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2023-6-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119809180.ch11"
    },
    {
        "id": 25301,
        "title": "Author response for \"Deep learning and reinforcement learning approach on microgrid\"",
        "authors": " Kumar Chandrasekaran,  Prabaakaran Kandasamy,  Srividhya Ramanathan",
        "published": "2020-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531/v2/response1"
    },
    {
        "id": 25302,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2023-6-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119809180.ch11"
    },
    {
        "id": 25303,
        "title": "Learning Deception and Agent Assignment Using a Hierarchical Reinforcement Learning",
        "authors": "Amirhossein Asgharnia, Howard Schwartz, Mohamed Atia",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4685924"
    },
    {
        "id": 25304,
        "title": "Multi-agent Polygon Formation using Reinforcement Learning",
        "authors": "B. K. Swathi Prasad, Aditya G. Manjunath, Hariharan Ramasangu",
        "published": "2017",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006187001590165"
    },
    {
        "id": 25305,
        "title": "Deep Reinforcement Learning for Interference Alignment Wireless Networks",
        "authors": "F. Richard Yu, Ying He",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-10546-4_3"
    },
    {
        "id": 25306,
        "title": "Reinforcement Learning: From TD(0) to Deep-Q-Learning",
        "authors": "Miroslav Kubat",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-81935-4_18"
    },
    {
        "id": 25307,
        "title": "Deep Reinforcement Learning",
        "authors": "Charu Aggarwal",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-29642-0_11"
    },
    {
        "id": 25308,
        "title": "Exploring Parity Challenges in Reinforcement Learning  Through Curriculum Learning with Noisy Labels",
        "authors": "Bei Zhou, Soren Riis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4693751"
    },
    {
        "id": 25309,
        "title": "Inverse Reinforcement Learning",
        "authors": "Pieter Abbeel, Andrew Y. Ng",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_142"
    },
    {
        "id": 25310,
        "title": "Scenario-assisted Deep Reinforcement Learning",
        "authors": "Raz Yerushalmi, Guy Amir, Achiya Elyasaf, David Harel, Guy Katz, Assaf Marron",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010904700003119"
    },
    {
        "id": 25311,
        "title": "Assured Reinforcement Learning with Formally Verified Abstract Policies",
        "authors": "George Mason, Radu Calinescu, Daniel Kudenko, Alec Banks",
        "published": "2017",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006156001050117"
    },
    {
        "id": 25312,
        "title": "Introduction",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_1"
    },
    {
        "id": 25313,
        "title": "Developmental Modular Reinforcement Learning",
        "authors": "Jianyong Xue, Frédéric Alexandre",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2022.es2022-19"
    },
    {
        "id": 25314,
        "title": "Review for \"Connectivity conservation planning through deep reinforcement learning\"",
        "authors": "Richard Schuster",
        "published": "2024-1-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.14300/v2/review1"
    },
    {
        "id": 25315,
        "title": "Applications of Deep reinforcement learning in MEMS and nanotechnology",
        "authors": "Hrishitva Patel",
        "published": "No Date",
        "citations": 0,
        "abstract": "Deep reinforcement learning (DRL) is an artificial intelligence technique that allows agents to learn optimal behaviors through trial-and-error interactions with their environment. This paper reviews applications of DRL in the fields of micro-electro-mechanical systems (MEMS) and nanotechnology. DRL has been used to enhance the design, manufacturing, and control of micro- and nanoscale systems. Notable applications include optimizing MEMS device designs, controlling nanomaterial synthesis, enabling precise nanorobotic manipulation, automating nanofabrication, directing nanoparticle self-assembly, and optimizing MEMS/nanotechnology fabrication processes. DRL allows for greater precision, increased autonomy, and enhanced performance. However, challenges remain regarding computational complexity, data availability, and responsible AI adoption. Continued DRL research and development focused on micro- and nanoscale systems hold promise for transformative innovations in electronics, medicine, energy, and other domains.\n",
        "link": "http://dx.doi.org/10.32388/pombwn"
    },
    {
        "id": 25316,
        "title": "GAN-based Intrinsic Exploration for Sample Efficient Reinforcement Learning",
        "authors": "Doğay Kamar, Nazím Üre, Gözde Ünal",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010825500003116"
    },
    {
        "id": 25317,
        "title": "Experience Filtering for Robot Navigation using Deep Reinforcement Learning",
        "authors": "Phong Nguyen, Takayuki Akiyama, Hiroki Ohashi",
        "published": "2018",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006671802430249"
    },
    {
        "id": 25318,
        "title": "Fundamental Design Principles for Reinforcement Learning Algorithms",
        "authors": "Adithya M. Devraj, Ana Bušić, Sean Meyn",
        "published": "2021",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_4"
    },
    {
        "id": 25319,
        "title": "Wide and Deep Reinforcement Learning for Grid-based Action Games",
        "authors": "Juan Montoya, Christian Borgelt",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007313200500059"
    },
    {
        "id": 25320,
        "title": "Background on Reinforcement Learning and Optimal Control",
        "authors": "Jinna Li, Frank L. Lewis, Jialu Fan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-28394-9_1"
    },
    {
        "id": 25321,
        "title": "Deep Reinforcement Learning in Human Activity Recognition: A Survey",
        "authors": "Bahareh Nikpour",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Human activity recognition is a popular research field in computer vision that has already been widely studied. However, it is still an active research field since it plays an important role in many current and emerging real world intelligent systems, like visual surveillance and human-computer interaction. Deep Reinforcement Learning (DRL) has recently been employed to address the activity recognition problem with various purposes, such as finding attention in video data or obtaining the best network structure. DRL-based human activity recognition has only been around for a short time, and it is a challenging, novel field[ of study. Therefore, to facilitate further research in this field, we have constructed a comprehensive survey on activity recognition methods that incorporate deep reinforcement learning. Towards the end of this survey, we summarize key challenges and open problems in this area that can be addressed by researchers in the future.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19172369.v1"
    },
    {
        "id": 25322,
        "title": "Reinforcement learning and meta-decision making",
        "authors": "Pieter Verbeke, Tom Verguts",
        "published": "No Date",
        "citations": 0,
        "abstract": "A key aspect of cognitive flexibility is to efficiently make use of earlier experience to attain one’s goals. This requires learning, but also a modular, and more specifically hierarchical, structure. We hold that both are required but combining them leads to several computational challenges that brains and artificial agents (learn to) deal with. In a hierarchical structure, meta-decisions must be made, of which two types can be distinguished: First, a (meta-)decision may involve choosing which (lower-level) modules to select (module choice). Second, it may consist of choosing appropriate parameter settings within a module (parameter tuning). Further, prediction error monitoring may allow determining the right meta-decision (module choice or parameter tuning). We discuss computational challenges and empirical evidence relative to how these two meta-decisions may be implemented to support learning for cognitive flexibility.",
        "link": "http://dx.doi.org/10.31234/osf.io/uvfhe"
    },
    {
        "id": 25323,
        "title": "RLML: A Domain-specific Modelling Language for Reinforcement Learning",
        "authors": "Natalie Sinani",
        "published": "No Date",
        "citations": 0,
        "abstract": "In recent years, machine learning technologies have gained intense popularity and are being used in a wide range of domains. However, due to the complexity associated with machine learning algorithms, it is a challenge to make it user-friendly, easy to understand and implement. Machine learning applications are especially challenging for users who do not have proficiency in this area. In this work, we use model-driven engineering (MDE) methods and tools for developing a domain-specific modelling language (DSML) to contribute towards providing a solution for this problem. We targeted reinforcement learning domain from machine learning technologies, and evaluated the proposed language with multiple applications. We built a domain-specific modelling environment to support our reinforcement learning modelling language (RLML). The tool supports syntax-directed editing, constraint checking, and automatic generation of code from RLML models. With our proposed approach, we were able to move away from the complexity of implementing machine learning algorithms in general purpose languages and offer abstraction and simplicity for non-experts, which are a few of the characteristics and benefits of modelling languages.",
        "link": "http://dx.doi.org/10.32920/25412866.v1"
    },
    {
        "id": 25324,
        "title": "Reinforcement Learning for Limited Labeled Classification",
        "authors": "Xaolin Chun",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper explores the application of reinforcement learning techniques in the context of limited labeled classification tasks. Limited labeled data scenarios, where the availability of labeled samples is scarce or expensive, pose significant challenges for traditional supervised learning methods. Leveraging the power of reinforcement learning, this study proposes a novel framework that addresses the issue of limited labeled data by combining active exploration and uncertainty-based strategies. The framework utilizes an agent that interacts with the environment, actively selecting instances to label based on uncertainty estimates and exploration-exploitation trade-offs. Reinforcement learning algorithms, such as Q-learning or policy gradient methods, are employed to train the agent to make informed decisions on the most informative instances to label. Experimental evaluations on various benchmark datasets demonstrate the effectiveness of the proposed approach, showcasing improved classification performance compared to traditional supervised learning methods in limited labeled scenarios. The findings of this study provide insights into the potential of reinforcement learning for addressing the challenges posed by limited labeled classification tasks, offering a promising avenue for practical applications in domains with limited labeled data availability.",
        "link": "http://dx.doi.org/10.31219/osf.io/yjexd"
    },
    {
        "id": 25325,
        "title": "Introduction",
        "authors": "",
        "published": "2017-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439821091-6"
    },
    {
        "id": 25326,
        "title": "Reinforcement learning for control of valves",
        "authors": "Rajesh Siraskar",
        "published": "2021-6",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2021.100030"
    },
    {
        "id": 25327,
        "title": "Actor-Critic Reinforcement Learning with Neural Networks in Continuous Games",
        "authors": "Gabriel Leuenberger, Marco A. Wiering",
        "published": "2018",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006556500530060"
    },
    {
        "id": 25328,
        "title": "A Reinforcement Learning QoS Negotiation Model for IoT Middleware",
        "authors": "Itorobong Udoh, Gerald Kotonya",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009350102050212"
    },
    {
        "id": 25329,
        "title": "Einstieg in Deep Reinforcement Learning",
        "authors": "Alexander Zai, Brandon Brown",
        "published": "2020-10-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446466081.fm"
    },
    {
        "id": 25330,
        "title": "Intelligent Scheduling Based on Reinforcement Learning Approaches: Applying Advanced Q-Learning and State–Action–Reward–State–Action Reinforcement Learning Models for the Optimisation of Job Shop Scheduling Problems",
        "authors": "Atefeh Momenikorbekandi, Maysam Abbod",
        "published": "2023-11-23",
        "citations": 1,
        "abstract": "Flexible job shop scheduling problems (FJSPs) have attracted significant research interest because they can considerably increase production efficiency in terms of energy, cost and time; they are considered the main part of the manufacturing systems which frequently need to be resolved to manage the variations in production requirements. In this study, novel reinforcement learning (RL) models, including advanced Q-learning (QRL) and RL-based state–action–reward–state–action (SARSA) models, are proposed to enhance the scheduling performance of FJSPs, in order to reduce the total makespan. To more accurately depict the problem realities, two categories of simulated single-machine job shops and multi-machine job shops, as well as the scheduling of a furnace model, are used to compare the learning impact and performance of the novel RL models to other algorithms. FJSPs are challenging to resolve and are considered non-deterministic polynomial-time hardness (NP-hard) problems. Numerous algorithms have been used previously to solve FJSPs. However, because their key parameters cannot be effectively changed dynamically throughout the computation process, the effectiveness and quality of the solutions fail to meet production standards. Consequently, in this research, developed RL models are presented. The efficacy and benefits of the suggested SARSA method for solving FJSPs are shown by extensive computer testing and comparisons. As a result, this can be a competitive algorithm for FJSPs.",
        "link": "http://dx.doi.org/10.3390/electronics12234752"
    },
    {
        "id": 25331,
        "title": "Blending Learning and Planning",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-16"
    },
    {
        "id": 25332,
        "title": "Model-Based Reinforcement Learning",
        "authors": "Soumya Ray, Prasad Tadepalli",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_561"
    },
    {
        "id": 25333,
        "title": "Learning Efficient Coordination Strategy for Multi-step Tasks in Multi-agent Systems using Deep Reinforcement Learning",
        "authors": "Zean Zhu, Elhadji Diallo, Toshiharu Sugawara",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009160102870294"
    },
    {
        "id": 25334,
        "title": "Least-Squares Reinforcement Learning Methods",
        "authors": "Michail G. Lagoudakis",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_473"
    },
    {
        "id": 25335,
        "title": "Curriculum Learning in Reinforcement Learning",
        "authors": "Sanmit Narvekar",
        "published": "2017-8",
        "citations": 17,
        "abstract": "Transfer learning in reinforcement learning is an area of research that seeks to speed up or improve learning of a complex target task, by leveraging knowledge from one or more source tasks. This thesis will extend the concept of transfer learning to curriculum learning, where the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We discuss completed work on this topic, including methods for semi-automatically generating source tasks tailored to an agent and the characteristics of a target domain, and automatically sequencing such tasks into a curriculum. Finally, we also present ideas for future work.",
        "link": "http://dx.doi.org/10.24963/ijcai.2017/757"
    },
    {
        "id": 25336,
        "title": "Priority-Objective Reinforcement Learning",
        "authors": "Yusuf Al-Husaini, Matthias Rolf",
        "published": "2021-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl49984.2021.9515661"
    },
    {
        "id": 25337,
        "title": "Federated Reinforcement Learning at the Edge: Exploring the Learning-Communication Tradeoff",
        "authors": "Konstantinos Gatsis",
        "published": "2022-7-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc55457.2022.9837987"
    },
    {
        "id": 25338,
        "title": "Reinforcement Learning for Combinatorial Optimization",
        "authors": "Di Wang",
        "published": "2022-10-14",
        "citations": 0,
        "abstract": "Combinatorial optimization (CO) problems have many important application domains, including social networks, manufacturing, and transportation. However, as an NP-hard problem, the traditional CO problem-solvers require domain knowledge and hand-crafted heuristics. Facing big data challenges, can we solve these challenging problems with a learning structure within a short time? This article will demonstrate how to solve the combinatorial optimization problems with the deep reinforcement learning (DRL) method. Reinforcement learning (RL) is a subfield of machine learning (ML) that learns the optimal policy over time. Building on Markov decision process, RL has the solid theoretical foundation to obtain the optimal solution. Once parameters of DRL are trained, a new problem case can be solved quickly. Moreover, DRL learns the optimal solution without labels by maximizing the accumulative discounted reward received from the environment. This article will discuss three typical CO problems and present the advantages of DRL over other traditional methods.",
        "link": "http://dx.doi.org/10.4018/978-1-7998-9220-5.ch170"
    },
    {
        "id": 25339,
        "title": "Neural machine translation with reinforcement learning",
        "authors": "Masashi Sugiyama",
        "published": "No Date",
        "citations": 0,
        "abstract": "Deep learning has recently shown much progress for natural language processing problems and applications because of the vector space representation \\cite{pennington2014glove} of each word or document instead of discrete representation by a sparse bag-of-words and other deep learning structures, such as recurrent neural network and recursive neural network, etc.  Therefore, I really want to explore deep learning modules on natural language generation this summer.",
        "link": "http://dx.doi.org/10.31219/osf.io/drfjt"
    },
    {
        "id": 25340,
        "title": "Two Applications and a Conclusion",
        "authors": "",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.003.0013"
    },
    {
        "id": 25341,
        "title": "Reinforcement Learning: Discrete-Time Markov Decision Process",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0"
    },
    {
        "id": 25342,
        "title": "Deep Reinforcement Learning Controller Design for Unmanned Aerial Vehicles",
        "authors": "Ali Aboubih",
        "published": "No Date",
        "citations": 0,
        "abstract": "A Proximal Policy Optimization agent was trained to learn quadrotor dynamics, successfully selecting control outputs to stabilize the drone and track complex trajectories. The agent was trained to mimic a minimum snap trajectory. The UAV closely followed the path, maintaining desired speeds of 3.56 body lengths/second, and remaining within 0.5m of the path, in wind conditions up to 20 mph. The agent was also validated on other complex trajectories, still closely tracking them regardless of the path it was trained on. Compared to PID controllers, the RL controller had a faster response time, converging to the desired path quicker. PID tuning is high maintenance and is limited by linearization around hover state. This results in instabilities and overshoots not observed in the RL controller, as well as RL learning non-linear dynamics. However, the RL controller had noisy motor output, resulting in undesirable oscillatory behaviour not observed in PID.",
        "link": "http://dx.doi.org/10.32920/25412560.v1"
    },
    {
        "id": 25343,
        "title": "Deep Reinforcement Learning Controller Design for Unmanned Aerial Vehicles",
        "authors": "Ali Aboubih",
        "published": "No Date",
        "citations": 0,
        "abstract": "A Proximal Policy Optimization agent was trained to learn quadrotor dynamics, successfully selecting control outputs to stabilize the drone and track complex trajectories. The agent was trained to mimic a minimum snap trajectory. The UAV closely followed the path, maintaining desired speeds of 3.56 body lengths/second, and remaining within 0.5m of the path, in wind conditions up to 20 mph. The agent was also validated on other complex trajectories, still closely tracking them regardless of the path it was trained on. Compared to PID controllers, the RL controller had a faster response time, converging to the desired path quicker. PID tuning is high maintenance and is limited by linearization around hover state. This results in instabilities and overshoots not observed in the RL controller, as well as RL learning non-linear dynamics. However, the RL controller had noisy motor output, resulting in undesirable oscillatory behaviour not observed in PID.",
        "link": "http://dx.doi.org/10.32920/25412560"
    },
    {
        "id": 25344,
        "title": "RLML: A Domain-specific Modelling Language for Reinforcement Learning",
        "authors": "Natalie Sinani",
        "published": "No Date",
        "citations": 0,
        "abstract": "In recent years, machine learning technologies have gained intense popularity and are being used in a wide range of domains. However, due to the complexity associated with machine learning algorithms, it is a challenge to make it user-friendly, easy to understand and implement. Machine learning applications are especially challenging for users who do not have proficiency in this area. In this work, we use model-driven engineering (MDE) methods and tools for developing a domain-specific modelling language (DSML) to contribute towards providing a solution for this problem. We targeted reinforcement learning domain from machine learning technologies, and evaluated the proposed language with multiple applications. We built a domain-specific modelling environment to support our reinforcement learning modelling language (RLML). The tool supports syntax-directed editing, constraint checking, and automatic generation of code from RLML models. With our proposed approach, we were able to move away from the complexity of implementing machine learning algorithms in general purpose languages and offer abstraction and simplicity for non-experts, which are a few of the characteristics and benefits of modelling languages.",
        "link": "http://dx.doi.org/10.32920/25412866"
    },
    {
        "id": 25345,
        "title": "The Furtherance of Autonomous Engineering via Reinforcement Learning",
        "authors": "Doris Antensteiner, Vincent Dietrich, Michael Fiegert",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010544200002994"
    },
    {
        "id": 25346,
        "title": "Distribution Data Across Multiple Cloud Storage using Reinforcement Learning Method",
        "authors": "Abdullah Algarni, Daniel Kudenko",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006124804310438"
    },
    {
        "id": 25347,
        "title": "Review for \"Connectivity conservation planning through deep reinforcement learning\"",
        "authors": "Richard Schuster",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.14300/v1/review2"
    },
    {
        "id": 25348,
        "title": "From Reinforcement Learning to Optimal Control: A Unified Framework for Sequential Decisions",
        "authors": "Warren B. Powell",
        "published": "2021",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_3"
    },
    {
        "id": 25349,
        "title": "Reinforcement Learning for Distributed Control and Multi-player Games",
        "authors": "Bahare Kiumarsi, Hamidreza Modares, Frank Lewis",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_2"
    },
    {
        "id": 25350,
        "title": "Characterizing Speed Performance of Multi-Agent Reinforcement Learning",
        "authors": "Samuel Wiggins, Yuan Meng, Rajgopal Kannan, Viktor Prasanna",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012082200003541"
    },
    {
        "id": 25351,
        "title": "Dynamic Programming and Reinforcement Learning",
        "authors": "Ameet V Joshi",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-26622-6_9"
    },
    {
        "id": 25352,
        "title": "Dynamic Programming and Reinforcement Learning",
        "authors": "Ameet V. Joshi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-12282-8_10"
    },
    {
        "id": 25353,
        "title": "Reinforcement Learning for Online Learning Recommendation System",
        "authors": "Wacharawan Intayoad, Chayapol Kamyod, Punnarumol Temdee",
        "published": "2018-11",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gws.2018.8686513"
    },
    {
        "id": 25354,
        "title": "Deep Reinforcement Learning",
        "authors": "Charu C. Aggarwal",
        "published": "2018",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-94463-0_9"
    },
    {
        "id": 25355,
        "title": "Risk-Averse Reinforcement Learning for Portfolio Optimization",
        "authors": "Bayaraa Enkhsaikhan, Ohyun Jo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4502413"
    },
    {
        "id": 25356,
        "title": "Reinforcement Learning for Autonomous Vehicle Movements in Wireless Sensor Networks",
        "authors": "Haitham Afifi",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In this work we use autonomous vehicles to improve the performance of Wireless Sensor Networks (WSNs).</p>\n<p>In contrast to other autonomous vehicle applications, WSNs have two metrics for performance evaluation. First, quality of information (QoI) which is used to measure the quality of sensed data (e.g., measurement uncertainties or signal strength).</p>\n<p>Second, quality of service (QoS) which is used to measure the network’s performance for data forwarding (e.g., delay and packet losses). As a use case, we consider wireless acoustic sensor networks, where a group of speakers move inside a room and there are autonomous vehicles installed with microphones for streaming the audio data. We formulate the problem as a Markov decision problem (MDP) and solve it using Deep-QNetworks (DQN). Additionally, we compare the performance</p>\n<p>of DQN solution to two different real-world implementations: speakers holding/passing microphones and microphones being preinstalled in fixed positions.</p>\n<p>We show that the performance of autonomous vehicles in terms of QoI and QoS is better than the real-world implementation in some scenarios. Moreover, we study the impact of the vehicles speed on the learning process of the DQN solution and show how low speeds degrade the performance. Finally, we compare the DQN solution to a heuristic one and provide theoretical analysis of the performance with respect to dynamic WSNs.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14778252.v2"
    },
    {
        "id": 25357,
        "title": "Optimizing Dynamic Timing Analysis with Reinforcement Learning.",
        "authors": "James Obert, Angie Shia",
        "published": "2019-11-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1573933"
    },
    {
        "id": 25358,
        "title": "Asymmetric and adaptive reward coding via normalized reinforcement learning",
        "authors": "Kenway Louie",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractLearning is widely modeled in psychology, neuroscience, and computer science by prediction error-guided reinforcement learning (RL) algorithms. While standard RL assumes linear reward functions, reward-related neural activity is a saturating, nonlinear function of reward; however, the computational and behavioral implications of nonlinear RL are unknown. Here, we show that nonlinear RL incorporating the canonical divisive normalization computation introduces an intrinsic and tunable asymmetry in prediction error coding. At the behavioral level, this asymmetry explains empirical variability in risk preferences typically attributed to asymmetric learning rates. At the neural level, diversity in asymmetries provides a computational mechanism for recently proposed theories of distributional RL, allowing the brain to learn the full probability distribution of future rewards. This behavioral and computational flexibility argues for an incorporation of biologically valid value functions in computational models of learning and decision-making.",
        "link": "http://dx.doi.org/10.1101/2021.11.24.469880"
    },
    {
        "id": 25359,
        "title": "Decision letter for \"Connectivity conservation planning through deep reinforcement learning\"",
        "authors": "",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.14300/v1/decision1"
    },
    {
        "id": 25360,
        "title": "Reinforcement Learning for Systematic FX Trading",
        "authors": "Gabriel Borrageiro, Nick Firoozye, Paolo Barucca",
        "published": "No Date",
        "citations": 0,
        "abstract": "We conduct a detailed experiment on major cash fx pairs, accurately accounting for transaction and funding costs. These sources of profit and loss, including the price trends that occur in the currency markets, are made available to our recurrent reinforcement learner via a quadratic utility, which learns to target a position directly. We improve upon earlier work, by casting the problem of learning to target a risk position, in an online learning context. This online learning occurs sequentially in time, but also in the form of transfer learning. We transfer the output of radial basis function hidden processing units, whose means, covariances and overall size are determined by Gaussian mixture models, to the recurrent reinforcement learner and baseline momentum trader. Thus the intrinsic nature of the feature space is learnt and made available to the upstream models. The recurrent reinforcement learning trader achieves an annualised portfolio information ratio of 0.52 with compound return of 9.3\\%, net of execution and funding cost, over a 7 year test set. This is despite forcing the model to trade at the close of the trading day 5pm EST, when trading costs are statistically the most expensive. These results are comparable with the momentum baseline trader, reflecting the low interest differential environment since the the 2008 financial crisis, and very obvious currency trends since then. The recurrent reinforcement learner does nevertheless maintain an important advantage, in that the model's weights can be adapted to reflect the different sources of profit and loss variation. This is demonstrated visually by a USDRUB trading agent, who learns to target different positions, that reflect trading in the absence or presence of cost.<br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16778932.v1"
    },
    {
        "id": 25361,
        "title": "Reinforcement Learning for Systematic FX Trading",
        "authors": "Gabriel Borrageiro, Nick Firoozye, Paolo Barucca",
        "published": "No Date",
        "citations": 0,
        "abstract": "We explore online inductive transfer learning, with a feature representation transfer from a radial basis function network formed of Gaussian mixture model hidden processing units to a direct, recurrent reinforcement learning agent. This agent is put to work in an experiment, trading the major spot market currency pairs, where we accurately account for transaction and funding costs. These sources of profit and loss, including the price trends that occur in the currency markets, are made available to the agent via a quadratic utility, who learns to target a position directly. We improve upon earlier work by learning to target a risk position in an online transfer learning context. Our agent achieves an annualised portfolio information ratio of 0.52 with a compound return of 9.3%, net of execution and funding cost, over a 7-year test set; this is despite forcing the model to trade at the close of the trading day at 5 pm EST when trading costs are statistically the most expensive.<br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16778932.v4"
    },
    {
        "id": 25362,
        "title": "Contextual Action with Multiple Policies Inverse Reinforcement Learning for Behavior Simulation",
        "authors": "Nahum Alvarez, Itsuki Noda",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007684908870894"
    },
    {
        "id": 25363,
        "title": "Towards Multi-agent Reinforcement Learning using Quantum Boltzmann Machines",
        "authors": "Tobias Müller, Christoph Roch, Kyrill Schmid, Philipp Altmann",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010762100003116"
    },
    {
        "id": 25364,
        "title": "Reinforcement Learning",
        "authors": "Ke-Lin Du, M. N. S. Swamy",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4471-7452-3_17"
    },
    {
        "id": 25365,
        "title": "Benchmarking Offline Reinforcement Learning",
        "authors": "Andrew Tittaferrante, Abdulsalam Yassine",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00044"
    },
    {
        "id": 25366,
        "title": "Reinforcement Learning",
        "authors": "Reza Borhani, Soheila Borhani, Aggelos K. Katsaggelos",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-19502-0_8"
    },
    {
        "id": 25367,
        "title": "Multi-agent Coordination using Reinforcement Learning with a Relay Agent",
        "authors": "Wiem Zemzem, Moncef Tagina, Moncef Tagina",
        "published": "2017",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006327305370545"
    },
    {
        "id": 25368,
        "title": "Interactive Lungs Auscultation with Reinforcement Learning Agent",
        "authors": "Tomasz Grzywalski, Riccardo Belluzzo, Szymon Drgas, Agnieszka Cwalińska, Honorata Hafke-Dys",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007573608240832"
    },
    {
        "id": 25369,
        "title": "Multi-Environment Training Against Reward Poisoning Attacks on Deep Reinforcement Learning",
        "authors": "Myria Bouhaddi, Kamel Adi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012139900003555"
    },
    {
        "id": 25370,
        "title": "Reinforcement Learning of Robot Behavior based on a Digital Twin",
        "authors": "Tobias Hassel, Oliver Hofmann",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008880903810386"
    },
    {
        "id": 25371,
        "title": "Learning for a Robot: Deep Reinforcement Learning, Imitation Learning, Transfer Learning",
        "authors": "Jiang Hua, Liangcai Zeng, Gongfa Li, Zhaojie Ju",
        "published": "2021-2-11",
        "citations": 97,
        "abstract": "Dexterous manipulation of the robot is an important part of realizing intelligence, but manipulators can only perform simple tasks such as sorting and packing in a structured environment. In view of the existing problem, this paper presents a state-of-the-art survey on an intelligent robot with the capability of autonomous deciding and learning. The paper first reviews the main achievements and research of the robot, which were mainly based on the breakthrough of automatic control and hardware in mechanics. With the evolution of artificial intelligence, many pieces of research have made further progresses in adaptive and robust control. The survey reveals that the latest research in deep learning and reinforcement learning has paved the way for highly complex tasks to be performed by robots. Furthermore, deep reinforcement learning, imitation learning, and transfer learning in robot control are discussed in detail. Finally, major achievements based on these methods are summarized and analyzed thoroughly, and future research challenges are proposed.",
        "link": "http://dx.doi.org/10.3390/s21041278"
    },
    {
        "id": 25372,
        "title": "A Recent Publications Survey on Reinforcement Learning for Selecting Parameters of Meta-Heuristic and Machine Learning Algorithms",
        "authors": "Maria Chernigovskaya, Andrey Kharitonov, Klaus Turowski",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011954300003488"
    },
    {
        "id": 25373,
        "title": "Augmenting Reinforcement Learning to Enhance Cooperation in the Iterated Prisoner’s Dilemma",
        "authors": "Grace Feehan, Shaheen Fatima",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010787500003116"
    },
    {
        "id": 25374,
        "title": "Hierarchical Reinforcement Learning for Real-Time Strategy Games",
        "authors": "Remi Niel, Jasper Krebbers, Madalina M. Drugan, Marco A. Wiering",
        "published": "2018",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006593804700477"
    },
    {
        "id": 25375,
        "title": "No-regret Reinforcement Learning",
        "authors": "Aditya Gopalan",
        "published": "2019-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indiancc.2019.8715624"
    },
    {
        "id": 25376,
        "title": "Safe Driving Of Autonomous Vehicles Through Improved Deep Reinforcement Learning",
        "authors": "Abhishek Gupta",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this thesis, we propose an environment perception framework for autonomous driving\nusing deep reinforcement learning (DRL) that exhibits learning in autonomous vehicles under complex interactions with the environment, without being explicitly trained on driving\ndatasets. Unlike existing techniques, our proposed technique takes the learning loss into\naccount under deterministic as well as stochastic policy gradient. We apply DRL to object\ndetection and safe navigation while enhancing a self-driving vehicle’s ability to discern meaningful information from surrounding data. For efficient environmental perception and object\ndetection, various Q-learning based methods have been proposed in the literature. Unlike\nother works, this thesis proposes a collaborative deterministic as well as stochastic policy\ngradient based on DRL. Our technique is a combination of variational autoencoder (VAE),\ndeep deterministic policy gradient (DDPG), and soft actor-critic (SAC) that adequately\ntrains a self-driving vehicle. In this work, we focus on uninterrupted and reasonably safe\nautonomous driving without colliding with an obstacle or steering off the track. We propose\na collaborative framework that utilizes best features of VAE, DDPG, and SAC and models\nautonomous driving as partly stochastic and partly deterministic policy gradient problem in\ncontinuous action space, and continuous state space. To ensure that the vehicle traverses the\nroad over a considerable period of time, we employ a reward-penalty based system where a\nhigher negative penalty is associated with an unfavourable action and a comparatively lower\npositive reward is awarded for favourable actions. We also examine the variations in policy\nloss, value loss, reward function, and cumulative reward for ‘VAE+DDPG’ and ‘VAE+SAC’\nover the learning process.",
        "link": "http://dx.doi.org/10.32920/17313137"
    },
    {
        "id": 25377,
        "title": "Recent Advances in Reinforcement Learning",
        "authors": "M. Vidyasagar",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147512"
    },
    {
        "id": 25378,
        "title": "On CVaR-Based Reinforcement Learning in Quantitative Investment",
        "authors": "Ali Alameer, Khaled Alshehri",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We propose Conditional Value-at-Risk (CVaR) investment agents to solve the problems of single asset trading and assets allocation under the Direct Reinforcement Learning framework. We propose two convex CVaR-based agents, the CVaR-constrained and the unconstrained CVaR-sensitive. Convexity allows conveniently implementing incremental learning, leading to an adaptive investing agent at an efficient computational cost with the merit of guaranteed policy convergence. Our experiments with frictional investment under various markets reveal the CVaR-constrained potency in improving investment return per unit of risk. The unconstrained CVaR-sensitive agent, on the other hand, exhibits robustness in handling intense market pullbacks, with both CVaR-based agents showing superior risk management to a risk-insensitive one. Our approach also showed superiority over state-of-the-art methods, demonstrating the potential of CVaR-based RL investment models. We finally show how our agents are extendable to learn investing under the most general investment problem of optimizing a multi-asset portfolio.</p>\n<p><br></p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19961525"
    },
    {
        "id": 25379,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2019-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811200885_0010"
    },
    {
        "id": 25380,
        "title": "Reinforcement Learning (RL) Based SFC Request Scheduling in Computer Networks",
        "authors": "Eesha Nagireddy",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.58445/rars.409"
    },
    {
        "id": 25381,
        "title": "Distributional Reinforcement Learning",
        "authors": "Marc G. Bellemare, Will Dabney, Mark Rowland",
        "published": "2023-5-30",
        "citations": 9,
        "abstract": "The first comprehensive guide to distributional reinforcement learning, providing a new mathematical formalism for thinking about decisions from a probabilistic perspective.\nDistributional reinforcement learning is a new mathematical formalism for thinking about decisions. Going beyond the common approach to reinforcement learning and expected values, it focuses on the total reward or return obtained as a consequence of an agent's choices—specifically, how this return behaves from a probabilistic perspective. In this first comprehensive guide to distributional reinforcement learning, Marc G. Bellemare, Will Dabney, and Mark Rowland, who spearheaded development of the field, present its key concepts and review some of its many applications. They demonstrate its power to account for many complex, interesting phenomena that arise from interactions with one's environment.\nThe authors present core ideas from classical reinforcement learning to contextualize distributional topics and include mathematical proofs pertaining to major results discussed in the text. They guide the reader through a series of algorithmic and mathematical developments that, in turn, characterize, compute, estimate, and make decisions on the basis of the random return. Practitioners in disciplines as diverse as finance (risk management), computational neuroscience, computational psychiatry, psychology, macroeconomics, and robotics are already using distributional reinforcement learning, paving the way for its expanding applications in mathematical finance, engineering, and the life sciences. More than a mathematical approach, distributional reinforcement learning represents a new perspective on how intelligent agents make predictions and decisions.",
        "link": "http://dx.doi.org/10.7551/mitpress/14207.001.0001"
    },
    {
        "id": 25382,
        "title": "Definitions",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_4"
    },
    {
        "id": 25383,
        "title": "Definitions",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_8"
    },
    {
        "id": 25384,
        "title": "Definitions",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_12"
    },
    {
        "id": 25385,
        "title": "Reinforcement learning in biological systems for adaptive regulation",
        "authors": "Tomoyuki Yamaguchi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe adaptive control of complex biological systems remains unclear despite extensive research on their regulatory networks. We recently reported that epigenetic regulation of gene expression may be a learning process, in which amplification-and-decay cycles optimize expression patterns while basically maintaining current patterns. Here, we show that various biological processes, such as intestinal immunity, population dynamics, chemotaxis, and self-organization, are also characterized as reinforcement learning (RL) processes. An appropriate population balance is established autonomously through symmetric competitive amplification and decay, which is a biologically plausible RL process. Monte Carlo simulations of predator-prey numbers show that population dynamics based on this RL process enable the sustainability of predators and reproduce fluctuations with a phase delay when humans hunt prey more preferentially than predators. Another example is a random walk controlling step-length (s-rw), which allows the agent to approach the target position with a Levy walk trajectory. In addition, shortcut paths in a maze are autonomously generated by s-rw using a moving-direction policy or bias, which is optimized through another RL on a longer timescale. Furthermore, by applying s-rw to reaction-diffusion theory, Turing patterns can be self-organized. The RL process, expressed by a common mathematical equation, enables the adaptability of biological systems.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3571702/v1"
    },
    {
        "id": 25386,
        "title": "Crystallization Process Design by Model-Free Deep Reinforcement Learning",
        "authors": "Georgi Tancev",
        "published": "No Date",
        "citations": 0,
        "abstract": "Chemical process design is the search for an optimal manufacturing\nprotocol to perform chemical operations. For transient processes such as\ncrystallization, the optimal conditions can change over time, requiring\na dynamic strategy. Model-free deep reinforcement learning is an\napproach that can be used to identify the best sequence of states with\nrespect to a predefined reward function. In this work, proximal policy\noptimization is applied in a simulated environment to identify\noperational strategies that are optimal with respect to the desired\nparticle properties in unseeded batch cooling crystallization processes\nof paracetamol in ethanol. For this purpose, the corresponding Markov\ndecision process is formulated, and it is shown that the method is\npromising for the development of novel routes that allow the tuning of\nparticle size (623 μm) and provide high yields (96%) within a defined\nperiod of time (12 h).",
        "link": "http://dx.doi.org/10.36227/techrxiv.170792884.44909118/v2"
    },
    {
        "id": 25387,
        "title": "On CVaR-Based Reinforcement Learning in Quantitative Investment",
        "authors": "Ali Alameer, Khaled Alshehri",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We propose Conditional Value-at-Risk (CVaR) investment agents to solve the problems of single asset trading and assets allocation under the Direct Reinforcement Learning framework. We propose two convex CVaR-based agents, the CVaR-constrained and the unconstrained CVaR-sensitive. Convexity allows conveniently implementing incremental learning, leading to an adaptive investing agent at an efficient computational cost with the merit of guaranteed policy convergence. Our experiments with frictional investment under various markets reveal the CVaR-constrained potency in improving investment return per unit of risk. The unconstrained CVaR-sensitive agent, on the other hand, exhibits robustness in handling intense market pullbacks, with both CVaR-based agents showing superior risk management to a risk-insensitive one. Our approach also showed superiority over state-of-the-art methods, demonstrating the potential of CVaR-based RL investment models. We finally show how our agents are extendable to learn investing under the most general investment problem of optimizing a multi-asset portfolio.</p>\n<p><br></p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19961525.v1"
    },
    {
        "id": 25388,
        "title": "Decision letter for \"Connectivity conservation planning through deep reinforcement learning\"",
        "authors": "",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.14300/v2/decision1"
    },
    {
        "id": 25389,
        "title": "Deep Reinforcement Learning",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 66,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7"
    },
    {
        "id": 25390,
        "title": "Reinforcement Learning for Systematic FX Trading",
        "authors": "Gabriel Borrageiro, Nick Firoozye, Paolo Barucca",
        "published": "No Date",
        "citations": 0,
        "abstract": "We explore online inductive transfer learning, with a feature representation transfer from a radial basis function network formed of Gaussian mixture model hidden processing units to a direct, recurrent reinforcement learning agent. This agent is put to work in an experiment, trading the major spot market currency pairs, where we accurately account for transaction and funding costs. These sources of profit and loss, including the price trends that occur in the currency markets, are made available to the agent via a quadratic utility, who learns to target a position directly. We improve upon earlier work by learning to target a risk position in an online transfer learning context. Our agent achieves an annualised portfolio information ratio of 0.52 with a compound return of 9.3%, net of execution and funding cost, over a 7-year test set; this is despite forcing the model to trade at the close of the trading day at 5 pm EST when trading costs are statistically the most expensive.<br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16778932"
    },
    {
        "id": 25391,
        "title": "A Policy-Graph Approach to Explain Reinforcement Learning Agents: A Novel Policy-Graph Approach with Natural Language and Counterfactual Abstractions for Explaining Reinforcement Learning Agents",
        "authors": "Tongtong Liu, Joe McCalmon, Thai Le, Dongwon Lee, Sarra Alqahtani",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs reinforcement learning (RL) continues to improve and be appliedin situations alongside humans, the need to explain the learned behaviorsof RL agents to end-users becomes more important. Strategies forexplaining the reasoning behind an agent’s policy, called policy-levelexplanations, can lead to important insights about both the task and theagent’s behaviors. Following this line of research, in this work, we proposea novel approach, named as CAPS, that summarizes an agent’s policy inthe form of a directed graph with natural language descriptions. A decisiontree based clustering method is utilized to abstract the state space ofthe task into fewer, condensed states which makes the policy graphs moredigestible to end-users. We then use the user-defined predicates to enrich the abstract states with semantic meaning. To introduce counterfactual state explanations to the policy graph, wefirst identify the critical states in the graph then develop a novel counterfactualexplanation method based on action perturbation in those criticalstates.We generate explanation graphs using CAPS on 5 RL tasksfor deterministic and stochastic policies. We evaluate the effectivenessof CAPS on human participants who are not RL experts in twouser studies. When provided with our explanation graph, end-users are ableto accurately interpret policies of trained RL agents 80% of the time and 68.2%of users demonstrated an increase in their confidence in understandingan agent’s behavior.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2409910/v1"
    },
    {
        "id": 25392,
        "title": "Attractor Neural States: A Brain-Inspired Complementary Approach to Reinforcement Learning",
        "authors": "Oussama H. Hamid, Jochen Braun",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006580203850392"
    },
    {
        "id": 25393,
        "title": "Balancing Multiplayer Games across Player Skill Levels using Deep Reinforcement Learning",
        "authors": "Conor Stephens, Chris Exton",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010914200003116"
    },
    {
        "id": 25394,
        "title": "Reinforcement Learning for Modeling Large-Scale Cognitive Reasoning",
        "authors": "Ying Zhao, Emily Mooren, Nate Derbinsky",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006508702330238"
    },
    {
        "id": 25395,
        "title": "Reinventing Astronomical Survey Scheduling with Reinforcement Learning: Unveiling the Potential of Self-Driving Telescope",
        "authors": "Franco Terranova, Maggie Voetberg, Brian Nord, Eric Neilsen",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/2246942"
    },
    {
        "id": 25396,
        "title": "Reinforcement learning",
        "authors": "Yinhai Wang, Zhiyong Cui, Ruimin Ke",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-32-396126-4.00012-6"
    },
    {
        "id": 25397,
        "title": "Enhancing Hvac Control Systems Through Transfer Learning with Deep Reinforcement Learning Agents",
        "authors": "Kevlyn Kadamala, Des Chambers, Enda Barrett",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4581832"
    },
    {
        "id": 25398,
        "title": "Decision letter for \"Deep reinforcement learning for conservation decisions\"",
        "authors": "",
        "published": "2022-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13954/v2/decision1"
    },
    {
        "id": 25399,
        "title": "Glossary of Symbols and Acronyms",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.019"
    },
    {
        "id": 25400,
        "title": "Reinforcement Learning for Autonomous Vehicle Movements in Wireless Sensor Networks",
        "authors": "Haitham Afifi",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In this work we use autonomous vehicles to improve the performance of Wireless Sensor Networks (WSNs).</p>\n<p>In contrast to other autonomous vehicle applications, WSNs have two metrics for performance evaluation. First, quality of information (QoI) which is used to measure the quality of sensed data (e.g., measurement uncertainties or signal strength).</p>\n<p>Second, quality of service (QoS) which is used to measure the network’s performance for data forwarding (e.g., delay and packet losses). As a use case, we consider wireless acoustic sensor networks, where a group of speakers move inside a room and there are autonomous vehicles installed with microphones for streaming the audio data. We formulate the problem as a Markov decision problem (MDP) and solve it using Deep-QNetworks (DQN). Additionally, we compare the performance</p>\n<p>of DQN solution to two different real-world implementations: speakers holding/passing microphones and microphones being preinstalled in fixed positions.</p>\n<p>We show that the performance of autonomous vehicles in terms of QoI and QoS is better than the real-world implementation in some scenarios. Moreover, we study the impact of the vehicles speed on the learning process of the DQN solution and show how low speeds degrade the performance. Finally, we compare the DQN solution to a heuristic one and provide theoretical analysis of the performance with respect to dynamic WSNs.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14778252"
    },
    {
        "id": 25401,
        "title": "Reinforcement Learning Algorithms",
        "authors": "Gopinath Rebala, Ajay Ravi, Sanjay Churiwala",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-15729-6_17"
    },
    {
        "id": 25402,
        "title": "Reinforcement-learning-based wireless resource allocation",
        "authors": " Rui Wang",
        "published": "2019-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbte081e_ch11"
    },
    {
        "id": 25403,
        "title": "Unsupervised learning and clustered connectivity enhance reinforcement learning in spiking neural networks",
        "authors": "Philipp Weidel, Renato Duarte, Abigail Morrison",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTReinforcement learning is a learning paradigm that can account for how organisms learn to adapt their behavior in complex environments with sparse rewards. However, implementations in spiking neuronal networks typically rely on input architectures involving place cells or receptive fields. This is problematic, as such approaches either scale badly as the environment grows in size or complexity, or presuppose knowledge on how the environment should be partitioned. Here, we propose a learning architecture that combines unsupervised learning on the input projections with clustered connectivity within the representation layer. This combination allows input features to be mapped to clusters; thus the network self-organizes to produce task-relevant activity patterns that can serve as the basis for reinforcement learning on the output projections. On the basis of the MNIST and Mountain Car tasks, we show that our proposed model performs better than either a comparable unclustered network or a clustered network with static input projections. We conclude that the combination of unsupervised learning and clustered connectivity provides a generic representational substrate suitable for further computation.",
        "link": "http://dx.doi.org/10.1101/2020.03.17.995563"
    },
    {
        "id": 25404,
        "title": "A Policy-Graph Approach to Explain Reinforcement Learning Agents: A Novel Policy-Graph Approach with Natural Language and Counterfactual Abstractions for Explaining Reinforcement Learning Agents",
        "authors": "Tongtong Liu, Joe McCalmon, Thai Le, Dongwon Lee, Sarra Alqahtani",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs reinforcement learning (RL) continues to improve and be appliedin situations alongside humans, the need to explain the learned behaviorsof RL agents to end-users becomes more important. Strategies forexplaining the reasoning behind an agent’s policy, called policy-levelexplanations, can lead to important insights about both the task and theagent’s behaviors. Following this line of research, in this work, we proposea novel approach, named as CAPS, that summarizes an agent’s policy inthe form of a directed graph with natural language descriptions. A decisiontree based clustering method is utilized to abstract the state space ofthe task into fewer, condensed states which makes the policy graphs moredigestible to end-users. We then use the user-defined predicates to enrich the abstract states with semantic meaning. To introduce counterfactual state explanations to the policy graph, wefirst identify the critical states in the graph then develop a novel counterfactualexplanation method based on action perturbation in those criticalstates.We generate explanation graphs using CAPS on 5 RL tasksfor deterministic and stochastic policies. We evaluate the effectivenessof CAPS on human participants who are not RL experts in twouser studies. When provided with our explanation graph, end-users are ableto accurately interpret policies of trained RL agents 80% of the time and 68.2%of users demonstrated an increase in their confidence in understandingan agent’s behavior.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2409910/v1"
    },
    {
        "id": 25405,
        "title": "Reinforcement learning",
        "authors": "Yinhai Wang, Zhiyong Cui, Ruimin Ke",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-32-396126-4.00012-6"
    },
    {
        "id": 25406,
        "title": "Enhancing Hvac Control Systems Through Transfer Learning with Deep Reinforcement Learning Agents",
        "authors": "Kevlyn Kadamala, Des Chambers, Enda Barrett",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4581832"
    },
    {
        "id": 25407,
        "title": "Risk-Averse Reinforcement Learning for Portfolio Optimization",
        "authors": "Bayaraa Enkhsaikhan, Ohyun Jo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4502413"
    },
    {
        "id": 25408,
        "title": "Asymmetric and adaptive reward coding via normalized reinforcement learning",
        "authors": "Kenway Louie",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractLearning is widely modeled in psychology, neuroscience, and computer science by prediction error-guided reinforcement learning (RL) algorithms. While standard RL assumes linear reward functions, reward-related neural activity is a saturating, nonlinear function of reward; however, the computational and behavioral implications of nonlinear RL are unknown. Here, we show that nonlinear RL incorporating the canonical divisive normalization computation introduces an intrinsic and tunable asymmetry in prediction error coding. At the behavioral level, this asymmetry explains empirical variability in risk preferences typically attributed to asymmetric learning rates. At the neural level, diversity in asymmetries provides a computational mechanism for recently proposed theories of distributional RL, allowing the brain to learn the full probability distribution of future rewards. This behavioral and computational flexibility argues for an incorporation of biologically valid value functions in computational models of learning and decision-making.",
        "link": "http://dx.doi.org/10.1101/2021.11.24.469880"
    },
    {
        "id": 25409,
        "title": "Optimizing Dynamic Timing Analysis with Reinforcement Learning.",
        "authors": "James Obert, Angie Shia",
        "published": "2019-11-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1573933"
    },
    {
        "id": 25410,
        "title": "Reinforcement Learning for Systematic FX Trading",
        "authors": "Gabriel Borrageiro, Nick Firoozye, Paolo Barucca",
        "published": "No Date",
        "citations": 0,
        "abstract": "We conduct a detailed experiment on major cash fx pairs, accurately accounting for transaction and funding costs. These sources of profit and loss, including the price trends that occur in the currency markets, are made available to our recurrent reinforcement learner via a quadratic utility, which learns to target a position directly. We improve upon earlier work, by casting the problem of learning to target a risk position, in an online learning context. This online learning occurs sequentially in time, but also in the form of transfer learning. We transfer the output of radial basis function hidden processing units, whose means, covariances and overall size are determined by Gaussian mixture models, to the recurrent reinforcement learner and baseline momentum trader. Thus the intrinsic nature of the feature space is learnt and made available to the upstream models. The recurrent reinforcement learning trader achieves an annualised portfolio information ratio of 0.52 with compound return of 9.3\\%, net of execution and funding cost, over a 7 year test set. This is despite forcing the model to trade at the close of the trading day 5pm EST, when trading costs are statistically the most expensive. These results are comparable with the momentum baseline trader, reflecting the low interest differential environment since the the 2008 financial crisis, and very obvious currency trends since then. The recurrent reinforcement learner does nevertheless maintain an important advantage, in that the model's weights can be adapted to reflect the different sources of profit and loss variation. This is demonstrated visually by a USDRUB trading agent, who learns to target different positions, that reflect trading in the absence or presence of cost.<br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16778932.v1"
    },
    {
        "id": 25411,
        "title": "Reinforcement Learning for Autonomous Vehicle Movements in Wireless Sensor Networks",
        "authors": "Haitham Afifi",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In this work we use autonomous vehicles to improve the performance of Wireless Sensor Networks (WSNs).</p>\n<p>In contrast to other autonomous vehicle applications, WSNs have two metrics for performance evaluation. First, quality of information (QoI) which is used to measure the quality of sensed data (e.g., measurement uncertainties or signal strength).</p>\n<p>Second, quality of service (QoS) which is used to measure the network’s performance for data forwarding (e.g., delay and packet losses). As a use case, we consider wireless acoustic sensor networks, where a group of speakers move inside a room and there are autonomous vehicles installed with microphones for streaming the audio data. We formulate the problem as a Markov decision problem (MDP) and solve it using Deep-QNetworks (DQN). Additionally, we compare the performance</p>\n<p>of DQN solution to two different real-world implementations: speakers holding/passing microphones and microphones being preinstalled in fixed positions.</p>\n<p>We show that the performance of autonomous vehicles in terms of QoI and QoS is better than the real-world implementation in some scenarios. Moreover, we study the impact of the vehicles speed on the learning process of the DQN solution and show how low speeds degrade the performance. Finally, we compare the DQN solution to a heuristic one and provide theoretical analysis of the performance with respect to dynamic WSNs.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14778252.v2"
    },
    {
        "id": 25412,
        "title": "Reinforcement Learning for Systematic FX Trading",
        "authors": "Gabriel Borrageiro, Nick Firoozye, Paolo Barucca",
        "published": "No Date",
        "citations": 0,
        "abstract": "We explore online inductive transfer learning, with a feature representation transfer from a radial basis function network formed of Gaussian mixture model hidden processing units to a direct, recurrent reinforcement learning agent. This agent is put to work in an experiment, trading the major spot market currency pairs, where we accurately account for transaction and funding costs. These sources of profit and loss, including the price trends that occur in the currency markets, are made available to the agent via a quadratic utility, who learns to target a position directly. We improve upon earlier work by learning to target a risk position in an online transfer learning context. Our agent achieves an annualised portfolio information ratio of 0.52 with a compound return of 9.3%, net of execution and funding cost, over a 7-year test set; this is despite forcing the model to trade at the close of the trading day at 5 pm EST when trading costs are statistically the most expensive.<br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16778932.v4"
    },
    {
        "id": 25413,
        "title": "Decision letter for \"Connectivity conservation planning through deep reinforcement learning\"",
        "authors": "",
        "published": "2023-11-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.14300/v1/decision1"
    },
    {
        "id": 25414,
        "title": "Deep Reinforcement Learning based Active Queue Management for IoT Networks",
        "authors": "Minsu Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "Internet of Things (IoT) has pervaded most aspects of our life through the Fourth Industrial Revolution. It is expected that a typical family home could contain several hundreds of smart devices by 2022. Current network architecture has been moving to fog/edge architecture to have the capacity for IoT. However, in order to deal with the enormous amount of traffic generated by those devices and reduce queuing delay, novel self-learning network management algorithms are required on fog/edge nodes. For efficient network management, Active Queue Management (AQM) has been proposed which is the intelligent queuing discipline. In this paper, we propose a new AQM based on Deep Reinforcement Learning (DRL) to handle the latency as well as the trade-off between queuing delay and throughput. We choose Deep Q-Network (DQN) as a baseline of our scheme, and compare our approach with various AQM schemes by deploying them on the interface of fog/edge node in IoT infrastructure. We simulate the AQM schemes on the different bandwidth and round trip time (RTT) settings, and in the empirical results, our approach outperforms other AQM schemes in terms of delay and jitter maintaining above-average throughput and verifies that DRL applied AQM is an efficient network manager for congestion.",
        "link": "http://dx.doi.org/10.32920/ryerson.14649609.v1"
    },
    {
        "id": 25415,
        "title": "Risk-Averse Reinforcement Learning for Portfolio Optimization",
        "authors": "Bayaraa Enkhsaikhan, Ohyun Jo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4474208"
    },
    {
        "id": 25416,
        "title": "Distribution Network Reconfiguration Using Deep Reinforcement Learning",
        "authors": "Mukesh Gautam, Mohammed Ben-Idris",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2787"
    },
    {
        "id": 25417,
        "title": "Reinforcement Learning",
        "authors": "Jimut Bahan Pal, Debadri Chatterjee, Sounak Modak",
        "published": "No Date",
        "citations": 0,
        "abstract": "Reinforcement Learning (RL) is one of the model free machine learning algorithms where the agent learns its behaviours from the environment by actually interacting with it. This is better than the offline planner because the agent actually interacts with the environment to learn its behaviours because it is almost impossible to simulate a real world in a computer. By using the reinforcement learning, the agent learns those extra features which can only be learned in an real world environment hence giving it a learning capability like living organisms because in a real world there are certain parameters which cannot be simulated by a computer. Since the reinforcement learning agent gets its feedback from the environment, it allows the agent to automatically determine its behaviours that are considered ideal within a specified context. Reinforcement learning is deemed important in the field of artificial intelligence as it starts to make breakthrough and benchmarks in various industrial applications. Previously we have analysed the pacman game where the pacman agent is a reflex agent, here, we are trying to make the pacman agent more smarter by applying RL techniques, i.e, Q-learning successfully.",
        "link": "http://dx.doi.org/10.31219/osf.io/dz6sx"
    },
    {
        "id": 25418,
        "title": "Nutrient-sensitive reinforcement learning in monkeys",
        "authors": "Fei-Yang Huang, Fabian Grabenhorst",
        "published": "No Date",
        "citations": 0,
        "abstract": "ABSTRACTAnimals make adaptive food choices to acquire nutrients that are essential for survival. In reinforcement learning (RL), animals choose by assigning values to options and update these values with new experiences. This framework has been instrumental for identifying fundamental learning and decision variables, and their neural substrates. However, canonical RL models do not explain how learning depends on biologically critical intrinsic reward components, such as nutrients, and related homeostatic regulation. Here, we investigated this question in monkeys making choices for nutrient-defined food rewards under varying reward probabilities. We found that the nutrient composition of rewards strongly influenced monkeys’ choices and learning. The animals preferred rewards high in nutrient content and showed individual preferences for specific nutrients (sugar, fat). These nutrient preferences affected how the animals adapted to changing reward probabilities: the monkeys learned faster from preferred nutrient rewards and chose them frequently even when they were associated with lower reward probability. Although more recently experienced rewards generally had a stronger influence on monkeys’ choices, the impact of reward history depended on the rewards’ specific nutrient composition. A nutrient-sensitive RL model captured these processes. It updated the value of individual sugar and fat components of expected rewards from experience and integrated them into scalar values that explained the monkeys’ choices. Our findings indicate that nutrients constitute important reward components that influence subjective valuation, learning and choice. Incorporating nutrient-value functions into RL models may enhance their biological validity and help reveal unrecognized nutrient-specific learning and decision computations.",
        "link": "http://dx.doi.org/10.1101/2021.06.20.448600"
    },
    {
        "id": 25419,
        "title": "Contextual Action with Multiple Policies Inverse Reinforcement Learning for Behavior Simulation",
        "authors": "Nahum Alvarez, Itsuki Noda",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007684908870894"
    },
    {
        "id": 25420,
        "title": "Towards Multi-agent Reinforcement Learning using Quantum Boltzmann Machines",
        "authors": "Tobias Müller, Christoph Roch, Kyrill Schmid, Philipp Altmann",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010762100003116"
    },
    {
        "id": 25421,
        "title": "Reinforcement Learning",
        "authors": "Ke-Lin Du, M. N. S. Swamy",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4471-7452-3_17"
    },
    {
        "id": 25422,
        "title": "Benchmarking Offline Reinforcement Learning",
        "authors": "Andrew Tittaferrante, Abdulsalam Yassine",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00044"
    },
    {
        "id": 25423,
        "title": "Safe Driving Of Autonomous Vehicles Through Improved Deep Reinforcement Learning",
        "authors": "Abhishek Gupta",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this thesis, we propose an environment perception framework for autonomous driving\nusing deep reinforcement learning (DRL) that exhibits learning in autonomous vehicles under complex interactions with the environment, without being explicitly trained on driving\ndatasets. Unlike existing techniques, our proposed technique takes the learning loss into\naccount under deterministic as well as stochastic policy gradient. We apply DRL to object\ndetection and safe navigation while enhancing a self-driving vehicle’s ability to discern meaningful information from surrounding data. For efficient environmental perception and object\ndetection, various Q-learning based methods have been proposed in the literature. Unlike\nother works, this thesis proposes a collaborative deterministic as well as stochastic policy\ngradient based on DRL. Our technique is a combination of variational autoencoder (VAE),\ndeep deterministic policy gradient (DDPG), and soft actor-critic (SAC) that adequately\ntrains a self-driving vehicle. In this work, we focus on uninterrupted and reasonably safe\nautonomous driving without colliding with an obstacle or steering off the track. We propose\na collaborative framework that utilizes best features of VAE, DDPG, and SAC and models\nautonomous driving as partly stochastic and partly deterministic policy gradient problem in\ncontinuous action space, and continuous state space. To ensure that the vehicle traverses the\nroad over a considerable period of time, we employ a reward-penalty based system where a\nhigher negative penalty is associated with an unfavourable action and a comparatively lower\npositive reward is awarded for favourable actions. We also examine the variations in policy\nloss, value loss, reward function, and cumulative reward for ‘VAE+DDPG’ and ‘VAE+SAC’\nover the learning process.",
        "link": "http://dx.doi.org/10.32920/17313137.v1"
    },
    {
        "id": 25424,
        "title": "Deep Reinforcement Learning based Active Queue Management for IoT Networks",
        "authors": "Minsu Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "Internet of Things (IoT) has pervaded most aspects of our life through the Fourth Industrial Revolution. It is expected that a typical family home could contain several hundreds of smart devices by 2022. Current network architecture has been moving to fog/edge architecture to have the capacity for IoT. However, in order to deal with the enormous amount of traffic generated by those devices and reduce queuing delay, novel self-learning network management algorithms are required on fog/edge nodes. For efficient network management, Active Queue Management (AQM) has been proposed which is the intelligent queuing discipline. In this paper, we propose a new AQM based on Deep Reinforcement Learning (DRL) to handle the latency as well as the trade-off between queuing delay and throughput. We choose Deep Q-Network (DQN) as a baseline of our scheme, and compare our approach with various AQM schemes by deploying them on the interface of fog/edge node in IoT infrastructure. We simulate the AQM schemes on the different bandwidth and round trip time (RTT) settings, and in the empirical results, our approach outperforms other AQM schemes in terms of delay and jitter maintaining above-average throughput and verifies that DRL applied AQM is an efficient network manager for congestion.",
        "link": "http://dx.doi.org/10.32920/ryerson.14649609"
    },
    {
        "id": 25425,
        "title": "Reinforcement Learning and Affective Computing",
        "authors": "",
        "published": "2022-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3502398.3502405"
    },
    {
        "id": 25426,
        "title": "Enhanced Particle Swarm Optimization via Reinforcement Learning",
        "authors": "G. Gary Wang, Di Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.0002171v"
    },
    {
        "id": 25427,
        "title": "Enhancing Healthcare Quality with Reinforcement Learning Modeling",
        "authors": "Gaddi Blumrosen",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sensors43011.2019.8956903"
    },
    {
        "id": 25428,
        "title": "Mathematics of Reinforcement Learning",
        "authors": "",
        "published": "2021-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009070218.016"
    },
    {
        "id": 25429,
        "title": "Properties",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_13"
    },
    {
        "id": 25430,
        "title": "Expectation",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_11"
    },
    {
        "id": 25431,
        "title": "Summary",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_15"
    },
    {
        "id": 25432,
        "title": "Further Developments",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_10"
    },
    {
        "id": 25433,
        "title": "Decision letter for \"Deep reinforcement learning for conservation decisions\"",
        "authors": "",
        "published": "2021-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13954/v1/decision1"
    },
    {
        "id": 25434,
        "title": "Decision letter for \"Deep reinforcement learning for conservation decisions\"",
        "authors": "",
        "published": "2022-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13954/v3/decision1"
    },
    {
        "id": 25435,
        "title": "Reinforcement Learning for Systematic FX Trading",
        "authors": "Gabriel Borrageiro, Nick Firoozye, Paolo Barucca",
        "published": "No Date",
        "citations": 0,
        "abstract": "We conduct a detailed experiment on major cash fx pairs, accurately accounting for transaction and funding costs. These sources of profit and loss, including the price trends that occur in the currency markets, are made available to our recurrent reinforcement learner via a quadratic utility, which learns to target a position directly. We improve upon earlier work, by casting the problem of learning to target a risk position, in an online learning context. This online learning occurs sequentially in time, but also in the form of transfer learning. We transfer the output of radial basis function hidden processing units, whose means, covariances and overall size are determined by Gaussian mixture models, to the recurrent reinforcement learner and baseline momentum trader. Thus the intrinsic nature of the feature space is learnt and made available to the upstream models. The recurrent reinforcement learning trader achieves an annualised portfolio information ratio of 0.52 with compound return of 9.3\\%, net of execution and funding cost, over a 7 year test set. This is despite forcing the model to trade at the close of the trading day 5pm EST, when trading costs are statistically the most expensive. These results are comparable with the momentum baseline trader, reflecting the low interest differential environment since the the 2008 financial crisis, and very obvious currency trends since then. The recurrent reinforcement learner does nevertheless maintain an important advantage, in that the model's weights can be adapted to reflect the different sources of profit and loss variation. This is demonstrated visually by a USDRUB trading agent, who learns to target different positions, that reflect trading in the absence or presence of cost.<br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16778932.v3"
    },
    {
        "id": 25436,
        "title": "Reinforcement Learning (RL)",
        "authors": "",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811254185_0018"
    },
    {
        "id": 25437,
        "title": "Learning to Run",
        "authors": "Zihan Ding, Hao Dong",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_13"
    },
    {
        "id": 25438,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789813271234_0015"
    },
    {
        "id": 25439,
        "title": "Chapter 7 Natural Language Processing and Reinforcement Learning",
        "authors": "",
        "published": "2020-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683924937-008"
    },
    {
        "id": 25440,
        "title": "Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control",
        "authors": "Rohit Bokade, Xiaoning Jin, Christopher Amato",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3275883"
    },
    {
        "id": 25441,
        "title": "Attacks on Deep Reinforcement Learning Systems: A Tutorial",
        "authors": "Joseph Layton, Fei Hu",
        "published": "2023-3-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003187158-6"
    },
    {
        "id": 25442,
        "title": "Basics of Reinforcement Learning",
        "authors": "Thimira Amaratunga",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6431-7_12"
    },
    {
        "id": 25443,
        "title": "Learning for a Robot: Deep Reinforcement Learning, Imitation Learning, Transfer Learning",
        "authors": "Jiang Hua, Liangcai Zeng, Gongfa Li, Zhaojie Ju",
        "published": "2021-2-11",
        "citations": 97,
        "abstract": "Dexterous manipulation of the robot is an important part of realizing intelligence, but manipulators can only perform simple tasks such as sorting and packing in a structured environment. In view of the existing problem, this paper presents a state-of-the-art survey on an intelligent robot with the capability of autonomous deciding and learning. The paper first reviews the main achievements and research of the robot, which were mainly based on the breakthrough of automatic control and hardware in mechanics. With the evolution of artificial intelligence, many pieces of research have made further progresses in adaptive and robust control. The survey reveals that the latest research in deep learning and reinforcement learning has paved the way for highly complex tasks to be performed by robots. Furthermore, deep reinforcement learning, imitation learning, and transfer learning in robot control are discussed in detail. Finally, major achievements based on these methods are summarized and analyzed thoroughly, and future research challenges are proposed.",
        "link": "http://dx.doi.org/10.3390/s21041278"
    },
    {
        "id": 25444,
        "title": "Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms",
        "authors": "Kaiqing Zhang, Zhuoran Yang, Tamer Başar",
        "published": "2021",
        "citations": 239,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_12"
    },
    {
        "id": 25445,
        "title": "PenGym: Pentesting Training Framework for Reinforcement Learning Agents",
        "authors": "Thanh Nguyen, Zhi Chen, Kento Hasegawa, Kazuhide Fukushima, Razvan Beuran",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012367300003648"
    },
    {
        "id": 25446,
        "title": "Reinforcement Learning Background",
        "authors": "Alice Faisal, Ibrahim Al-Nahhal, Octavia A. Dobre, Telex M. N. Ngatched",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-52554-4_1"
    },
    {
        "id": 25447,
        "title": "BOOK: Storing Algorithm-Invariant Episodes for Deep Reinforcement Learning",
        "authors": "Simyung Chang, YoungJoon Yoo, Jaeseok Choi, Nojun Kwak",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007308000730082"
    },
    {
        "id": 25448,
        "title": "Run Time Assured Reinforcement Learning for Safe Satellite Docking",
        "authors": "",
        "published": "2022-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2022-1853.vid"
    },
    {
        "id": 25449,
        "title": "Intelligent Control of Construction Manufacturing Processes using Deep Reinforcement Learning",
        "authors": "Ian Flood, Paris Flood",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011309600003274"
    },
    {
        "id": 25450,
        "title": "A Reinforcement Learning Approach to Feature Model Maintainability Improvement",
        "authors": "Olfa Ferchichi, Raoudha Beltaifa, Lamia Jilani",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010480203890396"
    },
    {
        "id": 25451,
        "title": "Metaheuristics-based Exploration Strategies for Multi-Objective Reinforcement Learning",
        "authors": "Florian Felten, Grégoire Danoy, El-Ghazali Talbi, Pascal Bouvry",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010989100003116"
    },
    {
        "id": 25452,
        "title": "Reward Evokes Visual Perceptual Learning Following Reinforcement Learning Rules",
        "authors": "Zhiyan Wang, Dongho Kim, Giorgia Pedroncelli, Yuka Sasaki, Takeo Watanabe",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractVisual perceptual learning (VPL) is defined as a long-term performance enhancement as a result of visual experiences. A number of studies have demonstrated that reward can evoke VPL. However, the mechanisms of how reward evoke VPL remain unknown. One possible hypothesis is that VPL is obtained through reward related reinforcement processing. If this hypothesis is true, learning can only occur when reward follows the stimulus presentation. Another interpretation is that VPL is acquired through an enhancement of alertness in association with reward. If the alertness hypothesis is true, learning should occur when reward precedes the stimulus presentation. In our study, we tested the plausibility of the two hypotheses by manipulating the order of reward and stimulus presentation. In Experiment 1, we separated participants into two groups. During training, the ‘Before’ group received water reward 400ms prior to the onset of trained orientation stimulus while the ‘After’ group received water reward 400ms subsequent to the onset of trained orientation stimulus. Both groups were trained using the Continuous Flash Suppression paradigm to render the stimulus imperceptible to the participants by the presentation of dynamic noise in the untrained eye. We found training only in the ‘After’ group indicating that reward may evoke learning through reinforcement-like processing. In Experiment 2, we excluded the possibility that alertness may not be sufficient to elicit learning when presented before stimulus. We presented beep sound prior to the onset of stimulus to increase alertness. Our finding demonstrated that alertness is sufficient enough to evoke learning. In conclusion, our study provided evidence that reward can evoke VPL through reinforcement process.",
        "link": "http://dx.doi.org/10.1101/760017"
    },
    {
        "id": 25453,
        "title": "Bridging Reinforcement Learning and Iterative Learning Control: Autonomous Reference Tracking for Unknown, Nonlinear Dynamics",
        "authors": "Michael Meindl, Dustin Lehmann, Thomas Seel",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>This work addresses the problem of reference tracking in autonomously learning agents with unknown, nonlinear dynamics. Existing solutions require model information or extensive parameter tuning, and have rarely been validated in real-world experiments. We propose a learning control scheme that learns to approximate the unknown dynamics by a Gaussian Process (GP), which is used to optimize and apply a feedforward control input on each trial. Unlike existing approaches, the proposed method neither requires knowledge of the system states and their dynamics nor knowledge of an effective feedback control structure. All algorithm parameters are chosen automatically, i.e. the learning method works plug and play. The proposed method is validated in extensive simulations and real-world experiments. In contrast to most existing work, we study learning dynamics for more than one motion task as well as the robustness of performance across a large range of learning parameters. The method’s plug and play applicability is demonstrated by experiments with a balancing robot, in which the proposed method rapidly learns to track the desired output. Due to its model-agnostic and plug and play properties, the proposed method is expected to have high potential for application to a large class of reference tracking problems in systems with unknown, nonlinear dynamics.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.15022497.v1"
    },
    {
        "id": 25454,
        "title": "Learning to Close the Gap: Combining Task Frame Formalism and Reinforcement Learning for Compliant Vegetable Cutting",
        "authors": "Abhishek Padalkar, Matthias Nieuwenhuisen, Sven Schneider, Dirk Schulz",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009590602210231"
    },
    {
        "id": 25455,
        "title": "A nonlinear relationship between prediction errors and learning rates in human reinforcement-learning",
        "authors": "Boluwatife Ikwunne, Jolie Parham, Erdem Pulcu",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractReinforcement-learning (RL) models have been pivotal to our understanding of how agents perform learning-based adaptions in dynamically changing environments. However, the exact nature of the relationship (e.g. linear, logarithmic etc.) between key components of RL models such as prediction errors (PEs; the difference between the agent’s expectation and the actual outcome) and learning rates (LRs; a coefficient used by agents to update their beliefs about the environment) has not been studied in detail. Here, across (i) simulations, (ii) reanalyses of readily available datasets and (iii) a novel experiment, we demonstrate that the relationship between PEs and LRs is (i) nonlinear over the PE/LR space, and (ii) it can be better accounted for by an exponential-logarithmic function that can transform the magnitude of PEs instantaneously to LRs. In line with the temporal predictions of this model, we show that physiological correlates of LRs accumulate while learners observe the outcome of their choices and update their beliefs about the environment.",
        "link": "http://dx.doi.org/10.1101/751222"
    },
    {
        "id": 25456,
        "title": "Multi-agent Reinforcement Learning with Clipping Intrinsic Motivation",
        "authors": "",
        "published": "2022-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2022.12.5.1101"
    },
    {
        "id": 25457,
        "title": "Transfer Learning for Operator Selection: A Reinforcement Learning Approach",
        "authors": "Rafet Durgut, Mehmet Emin Aydin, Abdur Rakib",
        "published": "No Date",
        "citations": 0,
        "abstract": "In the past two decades, metaheuristic optimization algorithms (MOAs) have been increasingly popular, particularly in logistic, science, and engineering problems. The fundamental characteristics of such algorithms are that they are dependent on a parameter or a strategy. Some online and offline strategies are employed in order to obtain optimal configurations of the algorithms. Adaptive operator selection is one of them, and it determines whether or not to update a strategy from the strategy pool during the search process. In the filed of machine learning, Reinforcement Learning (RL) refers to goal-oriented algorithms, which learn from the environment how to achieve a goal. On MOAs, reinforcement learning has been utilised to control the operator selection process. Existing research, however, fails to show that learned information may be transferred from one problem-solving procedure to another. The primary goal of the proposed research is to determine the impact of transfer learning on RL and MOAs. As a test problem, a set union knapsack problem with 30 separate benchmark problem instances is used. The results are statistically compared in depth. The learning process, according to the findings, improved the convergence speed while significantly reducing the CPU time.",
        "link": "http://dx.doi.org/10.20944/preprints202112.0337.v1"
    },
    {
        "id": 25458,
        "title": "Research on Inertial Navigation Technology of Unmanned Aerial Vehicles with Integrated Reinforcement Learning Algorithm",
        "authors": "",
        "published": "2023-8-2",
        "citations": 0,
        "abstract": "With the continuous expansion of unmanned aerial vehicle (UAV) applications, traditional inertial navigation technology exhibits significant limitations in complex environments. In this study, we integrate improved reinforcement learning (RL) algorithms to enhance existing unmanned aerial vehicle inertial navigation technology and introduce a modulated mechanism (MM) for adjusting the state of the intelligent agent in an innovative manner [1,2]. Through interaction with the environment, the intelligent machine can learn more effective navigation strategies [3]. The ultimate goal is to provide a foundation for autonomous navigation of unmanned aerial vehicles during flight and improve navigation accuracy and robustness. We first define appropriate state representation and action space, and then design an adjustment mechanism based on the actions selected by the intelligent agent. The adjustment mechanism outputs the next state and reward value of the agent. Additionally, the adjustment mechanism calculates the error between the adjusted state and the unadjusted state. Furthermore, the intelligent agent stores the acquired experience samples containing states and reward values in a buffer and replays the experiences during each iteration to learn the dynamic characteristics of the environment. We name the improved algorithm as the DQM algorithm. Experimental results demonstrate that the intelligent agent using our proposed algorithm effectively reduces the accumulated errors of inertial navigation in dynamic environments. Although our research provides a basis for achieving autonomous navigation of unmanned aerial vehicles, there is still room for significant optimization. Further research can include testing unmanned aerial vehicles in simulated environments, testing unmanned aerial vehicles in realworld environments, optimizing the design of reward functions, improving the algorithm workflow to enhance convergence speed and performance, and enhancing the algorithm's generalization ability. It has been proven that by integrating reinforcement learning algorithms, unmanned aerial vehicles can achieve autonomous navigation, thereby improving navigation accuracy and robustness in dynamic and changing environments [4]. Therefore, this research plays an important role in promoting the development and application of unmanned aerial vehicle technology.",
        "link": "http://dx.doi.org/10.33140/jeee.02.03.06"
    },
    {
        "id": 25459,
        "title": "Continuous Parameter Control in Genetic Algorithms using Policy Gradient Reinforcement Learning",
        "authors": "Alejandro de Miguel Gomez, Farshad Toosi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010643500003063"
    },
    {
        "id": 25460,
        "title": "An overview of reinforcement learning and deep reinforcement learning for condition-based maintenance",
        "authors": "Zahra Dehghani Ghobadi, Firoozeh Haghighi, Abdollah safari",
        "published": "2021-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.30699/ijrrs.4.2.9"
    },
    {
        "id": 25461,
        "title": "Safe Reinforcement Learning Using Wasserstein Distributionally Robust MPC and Chance Constraint",
        "authors": "Arash Bahari Kordabad, Rafael Wisniewski, Sebastien Gros",
        "published": "2022",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3228922"
    },
    {
        "id": 25462,
        "title": "Reinforcement learning inspired forwarding strategy for information centric networks using Q-learning algorithm",
        "authors": "Krishna Samarth Delvadia, Nitul Dutta",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nContent interest forwarding is a prominent research area in Information Centric Network (ICN). An efficient forwarding strategy can significantly enhance the user level performance parameters such as data retrieval latency. It also helps to minimize the origin server load, network congestion and overhead. Reinforcement learning is widely accepted for taking efficient routing decisions in network. This paper introduces a Q-learning driven forwarding strategy in ICN for interest packets. We have investigated the feasibility of exploiting reinforcement learning mechanism named Q-Learning for Named Data Network (NDN) paradigm of ICN. By revising Q-Learning mechanism to address the inherent challenges related to overhead and latency for content retrieval, this paper introduces design and implementation of Q-Learning based forwarding mechanism. It aims to gain learning through historical events and selects best mechanism to forward interest. The performance investigation of proposed protocol is carried out using simulator named ndnSIM-2.0. Outcomes are compared by integrating proposed protocol with LCD (Leave Copy Down), LCE (Leave Copy Everywhere, CL4M (Cache Less for More) and ProbCache (Probability driven caching). Protocol behaviour is also compared against recent routing and forwarding mechanisms. The considered performance parameters are data retrieval delay, server hit rate, network overhead, network throughput and network load. The experimental outcomes conclude that integration of proposed forwarding strategy to state-of-the-art protocols lead to performance enhancement up to 10–35%. The integrated protocol variants are also sensitive to any changes in network compared to existing approaches.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1505504/v1"
    },
    {
        "id": 25463,
        "title": "Reinforcement Learning for Optimal Robot Control in Complex Environments",
        "authors": "",
        "published": "2022-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.59121/kjmlar2209330005"
    },
    {
        "id": 25464,
        "title": "Dissociation between task structure learning and performance in human model-based reinforcement learning",
        "authors": "Sabrine Hamroun, Maël Lebreton, Stefano Palminteri",
        "published": "No Date",
        "citations": 1,
        "abstract": "The multi-step learning paradigm has become the dominant paradigm to investigate the trade-off between model-free reinforcement learning – which only leverages state-action-reward associations – and model-based reinforcement learning – which additionally builds on an explicit representation of state-transitions. Experimentally, while reward values usually have to be learned by trial-and-errors, state-transitions are customarily provided by instructions and extensive training. Accordingly, little is known about the learning strategies that are implemented in the ecological situation in which action-state-transitions are not known ex-ante. To fill this gap, we administered a new version of the two-step tasks, in which action-state-transitions have to be learned in parallel of reward values, to two cohorts of participants tested in the lab (N=30) and online (N=200). While choice patterns in the learning phase showed little sign of model-based learning, participants still accurately retrieved action-state-transitions in post-learning assessments. Together our results reveal an intriguing dissociation between knowledge and performance in human reinforcement learning: while reward values and the action-state-transitions can be concomitantly learned in the two-step paradigm, choices do not seem to benefit from model-based computations.",
        "link": "http://dx.doi.org/10.31234/osf.io/2uw85"
    },
    {
        "id": 25465,
        "title": "Meta Reinforcement Learning with Hebbian Learning",
        "authors": "Di Wang",
        "published": "2022-10-26",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/uemcon54665.2022.9965711"
    },
    {
        "id": 25466,
        "title": "Safe Q-Learning Approaches for Human-in-Loop Reinforcement Learning",
        "authors": "Swathi Veerabathraswamy, Nirav Bhatt",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10442899"
    },
    {
        "id": 25467,
        "title": "Autonomous UAV Navigation Using Reinforcement Learning",
        "authors": "Mudassar Liaq,  , Yungcheol Byun",
        "published": "2019-12",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2019.9.6.869"
    },
    {
        "id": 25468,
        "title": "Multi-Agent Reinforcement Learning with Clipping Intrinsic Motivation",
        "authors": "",
        "published": "2022-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2022.12.3.1084"
    },
    {
        "id": 25469,
        "title": "A Survey on Reinforcement Learning and Deep Reinforcement Learning for Recommender Systems",
        "authors": "Mehrdad Rezaei, Nasseh Tabrizi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-39059-3_26"
    },
    {
        "id": 25470,
        "title": "Mixed Time-Frame Training for Reinforcement Learning",
        "authors": "Gautham Senthilnathan",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00042"
    },
    {
        "id": 25471,
        "title": "ADAPTIVE LEARNING THROUGH AI: REINFORCEMENT LEARNING IN TEACHING MULTIPLICATION TABLES",
        "authors": "Lara Drožđek, Igor Pesek",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/inted.2024.1186"
    },
    {
        "id": 25472,
        "title": "Multi-Agent Quantum Reinforcement Learning Using Evolutionary Optimization",
        "authors": "Michael Kölle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas Nüßlein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012382800003636"
    },
    {
        "id": 25473,
        "title": "Welcome to the Jungle: A Conceptual Comparison of Reinforcement Learning Algorithms",
        "authors": "Kenneth Schröder, Alexander Kastius, Rainer Schlosser",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011626700003396"
    },
    {
        "id": 25474,
        "title": "Production Scheduling based on Deep Reinforcement Learning using Graph Convolutional Neural Network",
        "authors": "Takanari Seito, Satoshi Munakata",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009095207660772"
    },
    {
        "id": 25475,
        "title": "Continuous Procedural Network of Roads Generation using L-Systems and Reinforcement Learning",
        "authors": "Ciprian Paduraru, Miruna Paduraru, Stefan Iordache",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011268300003266"
    },
    {
        "id": 25476,
        "title": "Unlocking Autonomous Telescopes through Reinforcement Learning: An Offline Framework and Insights from a Case Study",
        "authors": "Franco Terranova, Maggie Voetberg, Brian Nord, Eric Neilsen",
        "published": "2023-11-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/2217169"
    },
    {
        "id": 25477,
        "title": "Towards Self-Adaptive Resilient Swarms Using Multi-Agent Reinforcement Learning",
        "authors": "Rafael Pina, Varuna De Silva, Corentin Artaud",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012462800003654"
    },
    {
        "id": 25478,
        "title": "Hyperparameter Optimization for Deep Reinforcement Learning in Vehicle Energy Management",
        "authors": "Roman Liessner, Jakob Schmitt, Ansgar Dietermann, Bernard Bäker",
        "published": "2019",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007364701340144"
    },
    {
        "id": 25479,
        "title": "Approximate Dynamic Programming and Reinforcement Learning for Continuous States",
        "authors": "Paolo Brandimarte",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-61867-4_7"
    },
    {
        "id": 25480,
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
        "authors": "Michael Kölle, Mohamad Hgog, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012383900003636"
    },
    {
        "id": 25481,
        "title": "Bounded Rationality in Differential Games: A Reinforcement Learning-Based Approach",
        "authors": "Nick-Marios T. Kokolakis, Aris Kanellopoulos, Kyriakos G. Vamvoudakis",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_16"
    },
    {
        "id": 25482,
        "title": "Reinforcement Learning",
        "authors": "Haesik Kim",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-95041-5_5"
    },
    {
        "id": 25483,
        "title": "OpenAI Basics",
        "authors": "Abhishek Nandy, Manisha Biswas",
        "published": "2018",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-3285-9_3"
    },
    {
        "id": 25484,
        "title": "LiDAR-based drone navigation with reinforcement learning",
        "authors": "Pawel Miera, Hubert Szolc, Tomasz Kryjak",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning is of increasing importance in the field of robot control and simulation plays a~key role in this process. In the unmanned aerial vehicles (UAVs, drones), there is also an increase in the number of published scientific papers involving this approach. In this work, an autonomous drone control system was prepared to fly forward (according to its coordinates system) and pass the trees encountered in the forest based on the data from a rotating LiDAR sensor. The Proximal Policy Optimization (PPO) algorithm, an example of reinforcement learning (RL), was used to prepare it. A custom simulator in the Python language was developed for this purpose. The Gazebo environment, integrated with the Robot Operating System (ROS), was also used to test the resulting control algorithm. Finally, the prepared solution was implemented in the Nvidia Jetson Nano eGPU and verified in the real tests scenarios. During them, the drone successfully completed the set task and was able to repeatably avoid trees and fly through the forest.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23784246.v1"
    },
    {
        "id": 25485,
        "title": "Cloud Computing Based Demand Response Management Using Deep Reinforcement Learning; Review",
        "authors": "Gabriel Katiwa",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper concentrates on using electric water heaters with the aim of demanding response. This is so as this method is considered most efficient when it comes to ensuring the safety and stabilization of power grids through the maintenance of balance between the demand and supply of the power grid. For this reason, the article investigates the overshoot temperature and the impact it has on demand response as well as to ensure the comfort and pricing variables as it happened in previous publications. The demand response process utilizing electric water heaters is explored as well as the impact of the physical parameters and the settings of the water heaters. In addition, a model is developed which puts into consideration the demand response requirements, the comfort of these water heater equipment, the power supply price in a simultaneous manner. Also, the effect of these factors on the end results of demand response is addressed. Experimental data is further used to demonstrate the efficiency of the suggested strategy.",
        "link": "http://dx.doi.org/10.14293/s2199-1006.1.sor-.ppfi6ov.v1"
    },
    {
        "id": 25486,
        "title": "Discounted Returns",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_3"
    },
    {
        "id": 25487,
        "title": "Reinforcement Learning in Missile Guidance: A New Era of Tactical Precision",
        "authors": "Kartikeya Sethi",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper explores the groundbreaking potential of reinforcement learning (RL) in revolutionizing missile guidance systems. We delve into three distinct scenarios where RL’s innovative application is particularly promising: firstly, in enhancing a missile’s ability to evade interception during midcourse flight; secondly, in navigating complex terrains and avoiding maritime obstacles, crucial for anti-shipping purposes and evading air defense systems; and thirdly, in its capability to perform effectively in unknown environments, demonstrating improved guidance over traditional methods. Through these scenarios, we illustrate how RL can be a game changer in the field of missile guidance, offering advanced adaptability, precision, and effectiveness. The aim of this review is to highlight the potential of RL to transform missile guidance technologies, ushering in a new era of intelligent and autonomous missile systems.",
        "link": "http://dx.doi.org/10.14293/pr2199.000691.v1"
    },
    {
        "id": 25488,
        "title": "Reinforcement Learning Configuration Interaction",
        "authors": "Joshua Goings, Hang Hu, Chao Yang, Xiaosong Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "A reinforcement learning algorithm is developed for the selected configuration interaction problem. We explore how reinforcement learning can obtain compact wave functions at near full configuration interaction accuracy.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14342234.v2"
    },
    {
        "id": 25489,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Xiangnan Liu",
        "published": "2023-8-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/ixz56s"
    },
    {
        "id": 25490,
        "title": "Nonlinear Filtering and Reinforcement Learning-based Smart Autonomous Multi-agent Systems",
        "authors": "Kaustav Borah",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>There is growing importance in complex engineering systems to operate autonomously, especially given potential malfunctions that may occur in the system, such as sensors, actuators, components, communication networks, and controllers. Fault detection, isolation, and reconstruction (FDIR) are crucial for autonomous systems. There is significant demand to evolve efficient intelligent systems to detect faults, isolate fault locations and autonomously reconstruct any component of a complex dynamical system. Hence, it is essential to detect, isolate, and reconstruct the faults efficiently and on time when the systems are in operation. This dissertation presents a novel methodology for developing smart autonomous multiagent systems (SAMAS). The SAMAS comprises several agents; some are homogeneous while others are heterogeneous. The proposed method involves developing a multi-agent systems (MAS) model and designing a decentralized smart control system. The MAS model contains homogeneous and heterogeneous agents, communicates among agents through an undirected connected graph, external disturbances, goals, and constraints. The sensor, actuator, communication, and controller faults are also modeled in the MAS model. A decentralized smart control system is designed to create a SAMAS model in the presence of uncertainty in each agents’ dynamics and faults located in the sensors, actuators, communication networks, and controllers. The proposed control method is based on non-linear filtering techniques, deep reinforcement learning, and robust control techniques. A Chebyshev neural network (CNN) is incorporated to learn the uncertain nonlinear functions in the agent dynamics of MAS. Additionally, robust control term using the hyperbolic tangent function is applied to counteract the neural network approximation errors. Meanwhile, a novel algorithm has been proposed which is employed to estimate the uncertain states of the agent dynamics and to train the internal parameters of the neural network given a set of prior measurements. Moreover, an adaptive threshold method has been proposed to detect any kinds of faults present in the system followed by a likelihood-based isolation method to locate each faulty agent. The novel algorithm is known as a reinforced unscented Kalman filter (RUKF). The primary purpose of the RUKF is to detect and isolate the faults, and to adapt the process and measurement noise covariance matrices to reconstruct the faults. To assess the performance of the proposed methodology, we developed a SAMAS consisting of six heterogeneous uncertain agent dynamics. The SAMAS model and the proposed control methodology are numerically simulated using MATLAB. A Monte-Carlo (MC) simulation was carried out to assess the performance of the proposed control methodology in the presence of uncertain agent dynamics and with the sensor, actuator, communication, and controller faults. The fault isolation results are summarized in confusion matrices for each faulty case. The stability of the RUKF, which ran in conjunction with a robust control method, has been proven using the Lyapunov stability approach. Extensive simulations were conducted to evaluate the performance of the proposed method. In this study, the proposed method showed superior performance to the standard unscented Kalman filter (UKF) and adaptive UKF (AUKF). The proposed fault isolation scheme isolated the faulty agents with over 96.68% success rate at the system level. Hence, the results of the numerical simulations show the feasibility of the proposed approach. The proposed RUKF is also computationally less expensive than the standard UKF and AUKF. The proposed approach can be considered a promising tool to evaluate fault detection, isolation, and reconstruction in complex engineering systems. Furthermore, the proposed approach can be extended to other deep space complex systems. The proposed approach can also be used for MAS problems for the financial sector, such as stock market prediction, economic time series, and multi-arm bandit problems.</p>",
        "link": "http://dx.doi.org/10.32920/25412566.v1"
    },
    {
        "id": 25491,
        "title": "Review for \"EPPTA: Efficient partially observable reinforcement learning agent for penetration testing applications\"",
        "authors": "",
        "published": "2023-9-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/eng2.12818/v1/review1"
    },
    {
        "id": 25492,
        "title": "Reinforcement Learning in Contests",
        "authors": "Vikas Chaudhary",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3920906"
    },
    {
        "id": 25493,
        "title": "Reinforcement Learning in Railway Delay Management",
        "authors": "Yongqiu Zhu, Pengling Wang, Francesco Corman",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4436899"
    },
    {
        "id": 25494,
        "title": "Quantum Gates Implementation for Reinforcement Learning Decision- Making Process",
        "authors": "Aarash Maroufian",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper introduces a novel method to improve the decision-making process of reinforcement learning agents via quantum information technology methods. In this approach, states |s〉 of the system are replaced by quantum states (eigenfunctions) of the system |ψ〉, in which a system can be in a superposition of states, and rewards of each step are calculated based on the calculated eigenvalues of the previous step. The agent decides the next step of the system based on the result of the quantum gate’s effect on the available options. Due to the high-speed, escalated performance of quantum algorithms, this method will improve the performance of reinforcement learning agents in unknown environments.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2201946/v1"
    },
    {
        "id": 25495,
        "title": "Basic Concepts",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_1"
    },
    {
        "id": 25496,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Maryam Savari",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/37bjn5"
    },
    {
        "id": 25497,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Gang Chen",
        "published": "2023-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/l0hx49"
    },
    {
        "id": 25498,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Abdelhameed Ibrahim",
        "published": "2023-8-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/wi2j9v"
    },
    {
        "id": 25499,
        "title": "Multi-agent Reinforcement Learning for Autonomous Vehicles in Wireless Sensor Networks",
        "authors": "Haitham Afifi",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>We develop a Deep Reinforcement Learning (DeepRL) based multi-agent algorithm to efficiently control</div><div>autonomous vehicles in the context of Wireless Sensor Networks (WSNs). In contrast to other applications, WSNs</div><div>have two metrics for performance evaluation. First, quality of information (QoI) which is used to measure the</div><div>quality of sensed data. Second, quality of service (QoS) which is used to measure the network’s performance. As</div><div>a use case, we consider wireless acoustic sensor networks; a group of speakers move inside a room and there</div><div>are microphones installed on vehicles for streaming the audio data. We formulate an appropriate Markov Decision</div><div>Process (MDP) and present, besides a centralized solution, a multi-agent Deep Q-learning solution to control the vehicles. We compare the proposed solutions to a naive heuristic and two different real-world implementations: microphones being hold or preinstalled. We show using simulations that the performance of autonomous vehicles in terms of QoI and QoS is better than the real-world implementation and the proposed heuristic. Additionally, we provide theoretical analysis of the performance with respect to WSNs dynamics, such as speed, rooms dimensions and speaker’s talking time.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14778252.v1"
    },
    {
        "id": 25500,
        "title": "Schemas, reinforcement learning, and the medial prefrontal cortex",
        "authors": "Oded Bein, Yael Niv",
        "published": "No Date",
        "citations": 0,
        "abstract": "Schemas are rich and complex knowledge structures about the typical unfolding of events in a context. For example, a schema of a lovely dinner at a restaurant. Schemas are central in psychology and neuroscience. Here, we suggest that reinforcement learning (RL), a computational theory of learning the structure of the world and relevant goal-oriented behavior, underlies schema learning. We synthesize literature about schemas and RL to offer that three RL principles might govern the learning of schemas: learning via prediction errors, constructing hierarchical knowledge using hierarchical RL, and dimensionality reduction through learning a simplified and abstract representation of the world. We then suggest that the orbito-medial prefrontal cortex is involved in both schemas and RL due to its involvement in dimensionality reduction and in guiding memory reactivation through interactions with posterior brain regions. Finally, we hypothesize that the amount of dimensionality reduction might underlie gradients of involvement along the ventral-dorsal and posterior-anterior axes of the orbito-medial prefrontal cortex. More specific and detailed representations might engage the ventral and posterior parts, while abstraction might shift representations toward the dorsal and anterior parts of the medial prefrontal cortex.",
        "link": "http://dx.doi.org/10.31234/osf.io/spxq9"
    },
    {
        "id": 25501,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Abdelhameed Ibrahim",
        "published": "2023-8-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/wi2j9v"
    },
    {
        "id": 25502,
        "title": "Decision letter for \"A reinforcement‐learning approach for individual pitch control\"",
        "authors": "",
        "published": "2022-2-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2734/v1/decision1"
    },
    {
        "id": 25503,
        "title": "A survey of Reinforcement and Deep reinforcement learning for Coordination in Intelligent Traffic Light Control",
        "authors": "Aicha Saadi, Noureddine Abghour, Zouhair Chiba, Khalid Moussaid, Saadi Ali",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIntelligent traffic signal control is required for a transportation system to function properly. In contrast to existing traffic signals, where rules are typically developed manually, an intelligent traffic signal control system should dynamically adapt to real-time traffic. The use of reinforcement learning for intelligent traffic signal control is a growing trend, and recent studies have shown promising results. However, none of the current studies have tested actual traffic data yet. This paper presents the primary techniques learning and methods (RL, DL, DRL).The analysis of each technique, the learning of its strengths and limitations, in order to evaluate at which levels they satisfy the requirements of urban traffic. The paper also lines some of the simulators, which perform adaptive traffic.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3225879/v1"
    },
    {
        "id": 25504,
        "title": "Welcome to the Jungle: A Conceptual Comparison of Reinforcement Learning Algorithms",
        "authors": "Kenneth Schröder, Alexander Kastius, Rainer Schlosser",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011626700003396"
    },
    {
        "id": 25505,
        "title": "Production Scheduling based on Deep Reinforcement Learning using Graph Convolutional Neural Network",
        "authors": "Takanari Seito, Satoshi Munakata",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009095207660772"
    },
    {
        "id": 25506,
        "title": "Multi-Agent Quantum Reinforcement Learning Using Evolutionary Optimization",
        "authors": "Michael Kölle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas Nüßlein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012382800003636"
    },
    {
        "id": 25507,
        "title": "Multi-Agent Reinforcement Learning with Clipping Intrinsic Motivation",
        "authors": "",
        "published": "2022-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2022.12.3.1084"
    },
    {
        "id": 25508,
        "title": "Reinforcement Learning",
        "authors": "Haesik Kim",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-95041-5_5"
    },
    {
        "id": 25509,
        "title": "Cloud Computing Based Demand Response Management Using Deep Reinforcement Learning; Review",
        "authors": "Gabriel Katiwa",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper concentrates on using electric water heaters with the aim of demanding response. This is so as this method is considered most efficient when it comes to ensuring the safety and stabilization of power grids through the maintenance of balance between the demand and supply of the power grid. For this reason, the article investigates the overshoot temperature and the impact it has on demand response as well as to ensure the comfort and pricing variables as it happened in previous publications. The demand response process utilizing electric water heaters is explored as well as the impact of the physical parameters and the settings of the water heaters. In addition, a model is developed which puts into consideration the demand response requirements, the comfort of these water heater equipment, the power supply price in a simultaneous manner. Also, the effect of these factors on the end results of demand response is addressed. Experimental data is further used to demonstrate the efficiency of the suggested strategy.",
        "link": "http://dx.doi.org/10.14293/s2199-1006.1.sor-.ppfi6ov.v1"
    },
    {
        "id": 25510,
        "title": "OpenAI Basics",
        "authors": "Abhishek Nandy, Manisha Biswas",
        "published": "2018",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-3285-9_3"
    },
    {
        "id": 25511,
        "title": "Discounted Returns",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_3"
    },
    {
        "id": 25512,
        "title": "Reinforcement Learning in Missile Guidance: A New Era of Tactical Precision",
        "authors": "Kartikeya Sethi",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper explores the groundbreaking potential of reinforcement learning (RL) in revolutionizing missile guidance systems. We delve into three distinct scenarios where RL’s innovative application is particularly promising: firstly, in enhancing a missile’s ability to evade interception during midcourse flight; secondly, in navigating complex terrains and avoiding maritime obstacles, crucial for anti-shipping purposes and evading air defense systems; and thirdly, in its capability to perform effectively in unknown environments, demonstrating improved guidance over traditional methods. Through these scenarios, we illustrate how RL can be a game changer in the field of missile guidance, offering advanced adaptability, precision, and effectiveness. The aim of this review is to highlight the potential of RL to transform missile guidance technologies, ushering in a new era of intelligent and autonomous missile systems.",
        "link": "http://dx.doi.org/10.14293/pr2199.000691.v1"
    },
    {
        "id": 25513,
        "title": "LiDAR-based drone navigation with reinforcement learning",
        "authors": "Pawel Miera, Hubert Szolc, Tomasz Kryjak",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning is of increasing importance in the field of robot control and simulation plays a~key role in this process. In the unmanned aerial vehicles (UAVs, drones), there is also an increase in the number of published scientific papers involving this approach. In this work, an autonomous drone control system was prepared to fly forward (according to its coordinates system) and pass the trees encountered in the forest based on the data from a rotating LiDAR sensor. The Proximal Policy Optimization (PPO) algorithm, an example of reinforcement learning (RL), was used to prepare it. A custom simulator in the Python language was developed for this purpose. The Gazebo environment, integrated with the Robot Operating System (ROS), was also used to test the resulting control algorithm. Finally, the prepared solution was implemented in the Nvidia Jetson Nano eGPU and verified in the real tests scenarios. During them, the drone successfully completed the set task and was able to repeatably avoid trees and fly through the forest.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23784246.v1"
    },
    {
        "id": 25514,
        "title": "Reinforcement Learning Configuration Interaction",
        "authors": "Joshua Goings, Hang Hu, Chao Yang, Xiaosong Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "A reinforcement learning algorithm is developed for the selected configuration interaction problem. We explore how reinforcement learning can obtain compact wave functions at near full configuration interaction accuracy.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14342234.v2"
    },
    {
        "id": 25515,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Xiangnan Liu",
        "published": "2023-8-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/ixz56s"
    },
    {
        "id": 25516,
        "title": "Nonlinear Filtering and Reinforcement Learning-based Smart Autonomous Multi-agent Systems",
        "authors": "Kaustav Borah",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>There is growing importance in complex engineering systems to operate autonomously, especially given potential malfunctions that may occur in the system, such as sensors, actuators, components, communication networks, and controllers. Fault detection, isolation, and reconstruction (FDIR) are crucial for autonomous systems. There is significant demand to evolve efficient intelligent systems to detect faults, isolate fault locations and autonomously reconstruct any component of a complex dynamical system. Hence, it is essential to detect, isolate, and reconstruct the faults efficiently and on time when the systems are in operation. This dissertation presents a novel methodology for developing smart autonomous multiagent systems (SAMAS). The SAMAS comprises several agents; some are homogeneous while others are heterogeneous. The proposed method involves developing a multi-agent systems (MAS) model and designing a decentralized smart control system. The MAS model contains homogeneous and heterogeneous agents, communicates among agents through an undirected connected graph, external disturbances, goals, and constraints. The sensor, actuator, communication, and controller faults are also modeled in the MAS model. A decentralized smart control system is designed to create a SAMAS model in the presence of uncertainty in each agents’ dynamics and faults located in the sensors, actuators, communication networks, and controllers. The proposed control method is based on non-linear filtering techniques, deep reinforcement learning, and robust control techniques. A Chebyshev neural network (CNN) is incorporated to learn the uncertain nonlinear functions in the agent dynamics of MAS. Additionally, robust control term using the hyperbolic tangent function is applied to counteract the neural network approximation errors. Meanwhile, a novel algorithm has been proposed which is employed to estimate the uncertain states of the agent dynamics and to train the internal parameters of the neural network given a set of prior measurements. Moreover, an adaptive threshold method has been proposed to detect any kinds of faults present in the system followed by a likelihood-based isolation method to locate each faulty agent. The novel algorithm is known as a reinforced unscented Kalman filter (RUKF). The primary purpose of the RUKF is to detect and isolate the faults, and to adapt the process and measurement noise covariance matrices to reconstruct the faults. To assess the performance of the proposed methodology, we developed a SAMAS consisting of six heterogeneous uncertain agent dynamics. The SAMAS model and the proposed control methodology are numerically simulated using MATLAB. A Monte-Carlo (MC) simulation was carried out to assess the performance of the proposed control methodology in the presence of uncertain agent dynamics and with the sensor, actuator, communication, and controller faults. The fault isolation results are summarized in confusion matrices for each faulty case. The stability of the RUKF, which ran in conjunction with a robust control method, has been proven using the Lyapunov stability approach. Extensive simulations were conducted to evaluate the performance of the proposed method. In this study, the proposed method showed superior performance to the standard unscented Kalman filter (UKF) and adaptive UKF (AUKF). The proposed fault isolation scheme isolated the faulty agents with over 96.68% success rate at the system level. Hence, the results of the numerical simulations show the feasibility of the proposed approach. The proposed RUKF is also computationally less expensive than the standard UKF and AUKF. The proposed approach can be considered a promising tool to evaluate fault detection, isolation, and reconstruction in complex engineering systems. Furthermore, the proposed approach can be extended to other deep space complex systems. The proposed approach can also be used for MAS problems for the financial sector, such as stock market prediction, economic time series, and multi-arm bandit problems.</p>",
        "link": "http://dx.doi.org/10.32920/25412566.v1"
    },
    {
        "id": 25517,
        "title": "Learning to Teach Reinforcement Learning Agents",
        "authors": "Anestis Fachantidis, Matthew Taylor, Ioannis Vlahavas",
        "published": "2017-12-6",
        "citations": 33,
        "abstract": "In this article, we study the transfer learning model of action advice under a budget. We focus on reinforcement learning teachers providing action advice to heterogeneous students playing the game of Pac-Man under a limited advice budget. First, we examine several critical factors affecting advice quality in this setting, such as the average performance of the teacher, its variance and the importance of reward discounting in advising. The experiments show that the best performers are not always the best teachers and reveal the non-trivial importance of the coefficient of variation (CV) as a statistic for choosing policies that generate advice. The CV statistic relates variance to the corresponding mean. Second, the article studies policy learning for distributing advice under a budget. Whereas most methods in the relevant literature rely on heuristics for advice distribution, we formulate the problem as a learning one and propose a novel reinforcement learning algorithm capable of learning when to advise or not. The proposed algorithm is able to advise even when it does not have knowledge of the student’s intended action and needs significantly less training time compared to previous learning approaches. Finally, in this article, we argue that learning to advise under a budget is an instance of a more generic learning problem: Constrained Exploitation Reinforcement Learning.",
        "link": "http://dx.doi.org/10.3390/make1010002"
    },
    {
        "id": 25518,
        "title": "A Survey on Reinforcement Learning and Deep Reinforcement Learning for Recommender Systems",
        "authors": "Mehrdad Rezaei, Nasseh Tabrizi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-39059-3_26"
    },
    {
        "id": 25519,
        "title": "Optimization of a Deep Reinforcement Learning Policy for Construction Manufacturing Control",
        "authors": "Ian Flood, Xiaoyan Zhou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012091400003546"
    },
    {
        "id": 25520,
        "title": "Decoupling State Representation Methods from Reinforcement Learning in Car Racing",
        "authors": "Juan Montoya, Imant Daunhawer, Julia Vogt, Marco Wiering",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010237507520759"
    },
    {
        "id": 25521,
        "title": "Reinforcement Learning for Optimal Adaptive Control of Time Delay Systems",
        "authors": "Syed Ali Asad Rizvi, Yusheng Wei, Zongli Lin",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_8"
    },
    {
        "id": 25522,
        "title": "Approximate Dynamic Programming and Reinforcement Learning for Discrete States",
        "authors": "Paolo Brandimarte",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-61867-4_5"
    },
    {
        "id": 25523,
        "title": "Multi-agent Deep Reinforcement Learning for Task Allocation in Dynamic Environment",
        "authors": "Dhouha Ben Noureddine, Atef Gharbi, Samir Ben Ahmed",
        "published": "2017",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006393400170026"
    },
    {
        "id": 25524,
        "title": "Reinforcement Fuzzy Tree: A Method extracting Rules from Reinforcement Learning Models",
        "authors": "Wenda Wu, Mingxue Liao",
        "published": "2019-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icis46139.2019.8940165"
    },
    {
        "id": 25525,
        "title": "Multi-Modal Instruction based Reinforcement Learning using MoME Transformer",
        "authors": "Avish Kadakia, Sabah Mohammed",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> In this research, we present a model architecture for employing reinforcement learning with transformers for multimodal tasks. Using transformers allows us to take advantage of the transformer architecture’s simplicity and scalability, as well as developments in language and vision modelling such as ViT, GPT-x, and BERT. Specifically we have trained the model to recognize various digits from the MNIST dataset from their associated word labels and instructions provided. The approach is similar to how an infant would learn to associate pictorial representation of digits to their corresponding words. We have used a MoME transformer in conjunction with Deep Q Learning to train our model. The image inputs have been embedded using pre-trained ResNet18 and the instructions have been embedded using GLoVe before passing them to the model for prediction and training. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19566040"
    },
    {
        "id": 25526,
        "title": "LiDAR-based drone navigation with reinforcement learning",
        "authors": "Pawel Miera, Hubert Szolc, Tomasz Kryjak",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning is of increasing importance in the field of robot control and simulation plays a~key role in this process. In the unmanned aerial vehicles (UAVs, drones), there is also an increase in the number of published scientific papers involving this approach. In this work, an autonomous drone control system was prepared to fly forward (according to its coordinates system) and pass the trees encountered in the forest based on the data from a rotating LiDAR sensor. The Proximal Policy Optimization (PPO) algorithm, an example of reinforcement learning (RL), was used to prepare it. A custom simulator in the Python language was developed for this purpose. The Gazebo environment, integrated with the Robot Operating System (ROS), was also used to test the resulting control algorithm. Finally, the prepared solution was implemented in the Nvidia Jetson Nano eGPU and verified in the real tests scenarios. During them, the drone successfully completed the set task and was able to repeatably avoid trees and fly through the forest.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23784246"
    },
    {
        "id": 25527,
        "title": "Reinforcement Learning in Quantitative Trading: A Survey",
        "authors": "Ali Alameer, Haitham Saleh, Khaled Alshehri",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Quantitative trading through automated systems has been vastly growing in recent years. The advancement in machine learning algorithms has pushed that growth even further, where their capability in extracting high-level patterns within financial markets data is evident. Nonetheless, trading with supervised machine learning can be challenging since the system learns to predict the price to minimize the error rather than optimize a financial performance measure. Reinforcement Learning (RL), a machine learning paradigm that intersects with optimal control theory, could bridge that divide since it is a goal-oriented learning system that could perform the two main trading steps, market analysis and making decisions to optimize a financial measure, without explicitly predicting the future price movement. This survey reviews quantitative trading under the different main RL methods. We first begin by describing the trading process and how it suits the RL framework, and we briefly discuss the historical aspect of RL inception. We then abundantly discuss RL preliminaries, including the Markov Decision Process elements and the main approaches of extracting optimal policies under the RL framework. After that, we review the literature of QT under both tabular and function approximation RL. Finally, we propose directions for future research predominantly driven by the still open challenges in implementing RL on QT applications.</div><div><br></div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19303853"
    },
    {
        "id": 25528,
        "title": "Accelerated Reinforcement Learning",
        "authors": "K. Lakshmanan",
        "published": "2017-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indicon.2017.8487529"
    },
    {
        "id": 25529,
        "title": "Foundations of Reinforcement Learning with Applications in Finance",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193"
    },
    {
        "id": 25530,
        "title": "A Decision-Making Method for Connected Autonomous Driving Based on Reinforcement Learning",
        "authors": "",
        "published": "2020-12-30",
        "citations": 1,
        "abstract": "At present, with the development of Intelligent Vehicle Infrastructure Cooperative Systems (IVICS), the decision-making for automated vehicle based on connected environment conditions has attracted more attentions. Reliability, efficiency and generalization performance are the basic requirements for the vehicle decision-making system. Therefore, this paper proposed a decision-making method for connected autonomous driving based on Wasserstein Generative Adversarial Nets-Deep Deterministic Policy Gradient (WGAIL-DDPG) algorithm. In which, the key components for reinforcement learning (RL) model, reward function, is designed from the aspect of vehicle serviceability, such as safety, ride comfort and handling stability. To reduce the complexity of the proposed model, an imitation learning strategy is introduced to improve the RL training process. Meanwhile, the model training strategy based on cloud computing effectively solves the problem of insufficient computing resources of the vehicle-mounted system. Test results show that the proposed method can improve the efficiency for RL training process with reliable decision making performance and reveals excellent generalization capability.",
        "link": "http://dx.doi.org/10.4271/2020-01-5154"
    },
    {
        "id": 25531,
        "title": "Reinforcement Learning Configuration Interaction",
        "authors": "Joshua Goings, Hang Hu, Chao Yang, Xiaosong Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "A reinforcement learning algorithm is developed for the selected configuration interaction problem. We explore how reinforcement learning can obtain compact wave functions at near full configuration interaction accuracy.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14342234.v1"
    },
    {
        "id": 25532,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Yuegang Song",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/rdpno5"
    },
    {
        "id": 25533,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Maximiliano Rojas",
        "published": "2023-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/fdkwxi"
    },
    {
        "id": 25534,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Swarn Jha",
        "published": "2023-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/jw55kh"
    },
    {
        "id": 25535,
        "title": "Introduction to Deep Learning",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_6"
    },
    {
        "id": 25536,
        "title": "Learning to Close the Gap: Combining Task Frame Formalism and Reinforcement Learning for Compliant Vegetable Cutting",
        "authors": "Abhishek Padalkar, Matthias Nieuwenhuisen, Sven Schneider, Dirk Schulz",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009590602210231"
    },
    {
        "id": 25537,
        "title": "Quantum reinforcement learning",
        "authors": "Niels M. P. Neumann, Paolo B. U. L. de Heer, Frank Phillipson",
        "published": "2023-2-22",
        "citations": 1,
        "abstract": "AbstractIn this paper, we present implementations of an annealing-based and a gate-based quantum computing approach for finding the optimal policy to traverse a grid and compare them to a classical deep reinforcement learning approach. We extended these three approaches by allowing for stochastic actions instead of deterministic actions and by introducing a new learning technique called curriculum learning. With curriculum learning, we gradually increase the complexity of the environment and we find that it has a positive effect on the expected reward of a traversal. We see that the number of training steps needed for the two quantum approaches is lower than that needed for the classical approach.",
        "link": "http://dx.doi.org/10.1007/s11128-023-03867-9"
    },
    {
        "id": 25538,
        "title": "Quantum Amplitude Amplification for Reinforcement Learning",
        "authors": "K. Rajagopal, Q. Zhang, S. N. Balakrishnan, P. Fakhari, J. R. Busemeyer",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_26"
    },
    {
        "id": 25539,
        "title": "Dynamic Path Planning for Autonomous Vehicles Using Adaptive Reinforcement Learning",
        "authors": "Karim Wahdan, Nourhan Ehab, Yasmin Mansy, Amr El Mougy",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012363300003636"
    },
    {
        "id": 25540,
        "title": "Meta Reinforcement Learning with Hebbian Learning",
        "authors": "Di Wang",
        "published": "2022-10-26",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/uemcon54665.2022.9965711"
    },
    {
        "id": 25541,
        "title": "Reinforcement Learning for Optimal Robot Control in Complex Environments",
        "authors": "",
        "published": "2022-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.59121/kjmlar2209330005"
    },
    {
        "id": 25542,
        "title": "Reinforcement learning inspired forwarding strategy for information centric networks using Q-learning algorithm",
        "authors": "Krishna Samarth Delvadia, Nitul Dutta",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nContent interest forwarding is a prominent research area in Information Centric Network (ICN). An efficient forwarding strategy can significantly enhance the user level performance parameters such as data retrieval latency. It also helps to minimize the origin server load, network congestion and overhead. Reinforcement learning is widely accepted for taking efficient routing decisions in network. This paper introduces a Q-learning driven forwarding strategy in ICN for interest packets. We have investigated the feasibility of exploiting reinforcement learning mechanism named Q-Learning for Named Data Network (NDN) paradigm of ICN. By revising Q-Learning mechanism to address the inherent challenges related to overhead and latency for content retrieval, this paper introduces design and implementation of Q-Learning based forwarding mechanism. It aims to gain learning through historical events and selects best mechanism to forward interest. The performance investigation of proposed protocol is carried out using simulator named ndnSIM-2.0. Outcomes are compared by integrating proposed protocol with LCD (Leave Copy Down), LCE (Leave Copy Everywhere, CL4M (Cache Less for More) and ProbCache (Probability driven caching). Protocol behaviour is also compared against recent routing and forwarding mechanisms. The considered performance parameters are data retrieval delay, server hit rate, network overhead, network throughput and network load. The experimental outcomes conclude that integration of proposed forwarding strategy to state-of-the-art protocols lead to performance enhancement up to 10–35%. The integrated protocol variants are also sensitive to any changes in network compared to existing approaches.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1505504/v1"
    },
    {
        "id": 25543,
        "title": "Dissociation between task structure learning and performance in human model-based reinforcement learning",
        "authors": "Sabrine Hamroun, Maël Lebreton, Stefano Palminteri",
        "published": "No Date",
        "citations": 1,
        "abstract": "The multi-step learning paradigm has become the dominant paradigm to investigate the trade-off between model-free reinforcement learning – which only leverages state-action-reward associations – and model-based reinforcement learning – which additionally builds on an explicit representation of state-transitions. Experimentally, while reward values usually have to be learned by trial-and-errors, state-transitions are customarily provided by instructions and extensive training. Accordingly, little is known about the learning strategies that are implemented in the ecological situation in which action-state-transitions are not known ex-ante. To fill this gap, we administered a new version of the two-step tasks, in which action-state-transitions have to be learned in parallel of reward values, to two cohorts of participants tested in the lab (N=30) and online (N=200). While choice patterns in the learning phase showed little sign of model-based learning, participants still accurately retrieved action-state-transitions in post-learning assessments. Together our results reveal an intriguing dissociation between knowledge and performance in human reinforcement learning: while reward values and the action-state-transitions can be concomitantly learned in the two-step paradigm, choices do not seem to benefit from model-based computations.",
        "link": "http://dx.doi.org/10.31234/osf.io/2uw85"
    },
    {
        "id": 25544,
        "title": "Safe Q-Learning Approaches for Human-in-Loop Reinforcement Learning",
        "authors": "Swathi Veerabathraswamy, Nirav Bhatt",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10442899"
    },
    {
        "id": 25545,
        "title": "Reinforcement Learning",
        "authors": "Michael Paluszek, Stephanie Thomas, Eric Ham",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-7912-0_15"
    },
    {
        "id": 25546,
        "title": "Context-dependent outcome encoding in human reinforcement learning",
        "authors": "Stefano Palminteri, Maël Lebreton",
        "published": "No Date",
        "citations": 1,
        "abstract": "A wealth of evidence in perceptual and economic decision-making research suggests that the subjective value of one option is determined by other available options (i.e. the context). A series of studies provides evidence that the same coding principles apply to situations where decisions are shaped by past outcomes, i.e. in reinforcement-learning situations. In bandit tasks, human behavior is explained by models assuming that individuals do not learn the objective value of an outcome, but rather its subjective, context-dependent representation. We argue that, while such outcome context-dependence may be informationally or ecologically optimal, it concomitantly undermines the capacity to generalize value-based knowledge to new contexts – sometimes creating apparent decision paradoxes.",
        "link": "http://dx.doi.org/10.31234/osf.io/4qh2d"
    },
    {
        "id": 25547,
        "title": "A Lightweight Siamese Network-Driven Unsupervised Reinforcement Learning",
        "authors": "Zhenyu Ma, Yanjun Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4706109"
    },
    {
        "id": 25548,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Neha Sardana",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/ux1uil"
    },
    {
        "id": 25549,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Bhawesh Prasad",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/31cvbt"
    },
    {
        "id": 25550,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Michael Allen",
        "published": "2023-8-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/bar9lf"
    },
    {
        "id": 25551,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Shreyanth S",
        "published": "2023-8-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/6v9aw4"
    },
    {
        "id": 25552,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Mutsumi Kimura",
        "published": "2023-8-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/3ndnoe"
    },
    {
        "id": 25553,
        "title": "Reinforcement Learning in Quantitative Trading: A Survey",
        "authors": "Ali Alameer, Haitham Saleh, Khaled Alshehri",
        "published": "No Date",
        "citations": 1,
        "abstract": "<div>Quantitative trading through automated systems has been vastly growing in recent years. The advancement in machine learning algorithms has pushed that growth even further, where their capability in extracting high-level patterns within financial markets data is evident. Nonetheless, trading with supervised machine learning can be challenging since the system learns to predict the price to minimize the error rather than optimize a financial performance measure. Reinforcement Learning (RL), a machine learning paradigm that intersects with optimal control theory, could bridge that divide since it is a goal-oriented learning system that could perform the two main trading steps, market analysis and making decisions to optimize a financial measure, without explicitly predicting the future price movement. This survey reviews quantitative trading under the different main RL methods. We first begin by describing the trading process and how it suits the RL framework, and we briefly discuss the historical aspect of RL inception. We then abundantly discuss RL preliminaries, including the Markov Decision Process elements and the main approaches of extracting optimal policies under the RL framework. After that, we review the literature of QT under both tabular and function approximation RL. Finally, we propose directions for future research predominantly driven by the still open challenges in implementing RL on QT applications.</div><div><br></div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19303853.v1"
    },
    {
        "id": 25554,
        "title": "Approximate Dynamic Programming and Reinforcement Learning for Continuous States",
        "authors": "Paolo Brandimarte",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-61867-4_7"
    },
    {
        "id": 25555,
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
        "authors": "Michael Kölle, Mohamad Hgog, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012383900003636"
    },
    {
        "id": 25556,
        "title": "Bounded Rationality in Differential Games: A Reinforcement Learning-Based Approach",
        "authors": "Nick-Marios T. Kokolakis, Aris Kanellopoulos, Kyriakos G. Vamvoudakis",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_16"
    },
    {
        "id": 25557,
        "title": "Multidisciplinary Optimization in Decentralized Reinforcement Learning",
        "authors": "Thanh Nguyen, Snehasis Mukhopadhyay",
        "published": "2017-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2017.00-63"
    },
    {
        "id": 25558,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Sunita Varma",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/3dk4v0"
    },
    {
        "id": 25559,
        "title": "Search-Based Planning and Reinforcement Learning for Autonomous Systems and Robotics",
        "authors": "Than Le",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In this chapter,\nwe address the competent Autonomous Vehicles should have the ability to analyze\nthe structure and unstructured environments and then to localize itself\nrelative to surrounding things, where GPS, RFID or other similar means cannot\ngive enough information about the location. Reliable SLAM is the most basic\nprerequisite for any further artificial intelligent tasks of an autonomous\nmobile robots. The goal of this paper is to simulate a SLAM process on the\nadvanced software development. The model represents the system itself, whereas\nthe simulation represents the operation of the system over time. And the\nsoftware architecture will help us to focus our work to realize our wish with\nleast trivial work. It is an open-source meta-operating system, which provides\nus tremendous tools for robotics related problems.</p>\n\n<p>Specifically, we\naddress the advanced vehicles should have the ability to analyze the structured\nand unstructured environment based on solving the search-based planning and\nthen we move to discuss interested in reinforcement learning-based model to\noptimal trajectory in order to apply to autonomous systems.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.11607348"
    },
    {
        "id": 25560,
        "title": "A reinforcement-based mechanism for discontinuous learning",
        "authors": "Gautam Reddy",
        "published": "No Date",
        "citations": 0,
        "abstract": "Problem-solving and reasoning involve mental exploration and navigation in sparse relational spaces. A physical analogue is spatial navigation in structured environments such as a network of burrows. Recent experiments with mice navigating a labyrinth show a sharp discontinuity during learning, corresponding to a distinct moment of ‘sudden insight’ when mice figure out long, direct paths to the goal. This discontinuity is seemingly at odds with reinforcement learning (RL), which involves a gradual build-up of a value signal during learning. Here, we show that biologically-plausible RL rules combined with persistent exploration generically exhibit discontinuous learning. In tree-like structured environments, positive feedback from learning on behavior generates a ‘reinforcement wave’ with a steep profile. The discontinuity occurs when the wave reaches the starting point. By examining the nonlinear dynamics of reinforcement propagation, we establish a quantitative relationship between the learning rule, the agent’s exploration biases and learning speed. Predictions explain existing data and motivate specific experiments to isolate the phenomenon. Additionally, we characterize the exact learning dynamics of various RL rules for a complex sequential task.",
        "link": "http://dx.doi.org/10.1101/2022.05.06.490910"
    },
    {
        "id": 25561,
        "title": "Theory of reinforcement learning and motivation in the basal ganglia",
        "authors": "Rafal Bogacz",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractThis paper proposes how the neural circuits in vertebrates select actions on the basis of past experience and the current motivational state. According to the presented theory, the basal ganglia evaluate the utility of considered actions by combining the positive consequences (e.g. nutrition) scaled by the motivational state (e.g. hunger) with the negative consequences (e.g. effort). The theory suggests how the basal ganglia compute utility by combining the positive and negative consequences encoded in the synaptic weights of striatal Go and No-Go neurons, and the motivational state carried by neuromodulators including dopamine. Furthermore, the theory suggests how the striatal neurons to learn separately about consequences of actions, and how the dopaminergic neurons themselves learn what level of activity they need to produce to optimize behaviour. The theory accounts for the effects of dopaminergic modulation on behaviour, patterns of synaptic plasticity in striatum, and responses of dopaminergic neurons in diverse situations.",
        "link": "http://dx.doi.org/10.1101/174524"
    },
    {
        "id": 25562,
        "title": "Choice-confirmation bias and gradual perseveration in human reinforcement learning",
        "authors": "Stefano Palminteri",
        "published": "No Date",
        "citations": 2,
        "abstract": "Do we preferentially learn from outcomes that confirm our choices? This is one of the most basic, and yet consequence-bearing, questions concerning reinforcement learning. In recent years, we investigated this question in a series of studies implementing increasingly complex behavioral protocols. The learning rates fitted in experiments featuring partial or complete feedback, as well as free and forced choices, were systematically found to be consistent with a choice-confirmation bias. This result is robust across a broad range of outcome contingencies and response modalities. One of the prominent behavioral consequences of the confirmatory learning rate pattern is choice hysteresis: that is the tendency of repeating previous choices, despite contradictory evidence. As robust and replicable as they have proven to be, these findings were (legitimately) challenged by a couple of studies pointing out that a choice-confirmatory pattern of learning rates may spuriously arise from not taking into consideration an explicit choice autocorrelation term in the model. In the present study, we re-analyze data from four previously published papers (in total nine experiments; N=363), originally included in the studies demonstrating (or criticizing) the choice-confirmation bias in human participants. We fitted two models: one featured valence-specific updates (i.e., different learning rates for confirmatory and disconfirmatory outcomes) and one additionally including an explicit choice autocorrelation process (gradual perseveration). Our analysis confirms that the inclusion of the gradual perseveration process in the model significantly reduces the estimated choice-confirmation bias. However, in all considered experiments, the choice-confirmation bias remains present at the meta-analytical level, and significantly different from zero in most experiments. Our results demonstrate that the choice-confirmation bias resists the inclusion of an explicit choice autocorrelation term, thus proving to be a robust feature of human reinforcement learning.  We conclude by discussing the psychological plausibility of the gradual perseveration process in the context of these behavioral paradigms and by pointing to additional computational processes that may play an important role in estimating and interpreting the computational biases under scrutiny.",
        "link": "http://dx.doi.org/10.31234/osf.io/dpqj6"
    },
    {
        "id": 25563,
        "title": "Multi-Modal Instruction based Reinforcement Learning using MoME Transformer",
        "authors": "Avish Kadakia, Sabah Mohammed",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> In this research, we present a model architecture for employing reinforcement learning with transformers for multimodal tasks. Using transformers allows us to take advantage of the transformer architecture’s simplicity and scalability, as well as developments in language and vision modelling such as ViT, GPT-x, and BERT. Specifically we have trained the model to recognize various digits from the MNIST dataset from their associated word labels and instructions provided. The approach is similar to how an infant would learn to associate pictorial representation of digits to their corresponding words. We have used a MoME transformer in conjunction with Deep Q Learning to train our model. The image inputs have been embedded using pre-trained ResNet18 and the instructions have been embedded using GLoVe before passing them to the model for prediction and training. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19566040.v1"
    },
    {
        "id": 25564,
        "title": "Search-Based Planning and Reinforcement Learning for Autonomous Systems and Robotics",
        "authors": "Than Le",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In this chapter,\nwe address the competent Autonomous Vehicles should have the ability to analyze\nthe structure and unstructured environments and then to localize itself\nrelative to surrounding things, where GPS, RFID or other similar means cannot\ngive enough information about the location. Reliable SLAM is the most basic\nprerequisite for any further artificial intelligent tasks of an autonomous\nmobile robots. The goal of this paper is to simulate a SLAM process on the\nadvanced software development. The model represents the system itself, whereas\nthe simulation represents the operation of the system over time. And the\nsoftware architecture will help us to focus our work to realize our wish with\nleast trivial work. It is an open-source meta-operating system, which provides\nus tremendous tools for robotics related problems.</p>\n\n<p>Specifically, we\naddress the advanced vehicles should have the ability to analyze the structured\nand unstructured environment based on solving the search-based planning and\nthen we move to discuss interested in reinforcement learning-based model to\noptimal trajectory in order to apply to autonomous systems.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.11607348.v1"
    },
    {
        "id": 25565,
        "title": "Nonlinear Filtering and Reinforcement Learning-based Smart Autonomous Multi-agent Systems",
        "authors": "Kaustav Borah",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>There is growing importance in complex engineering systems to operate autonomously, especially given potential malfunctions that may occur in the system, such as sensors, actuators, components, communication networks, and controllers. Fault detection, isolation, and reconstruction (FDIR) are crucial for autonomous systems. There is significant demand to evolve efficient intelligent systems to detect faults, isolate fault locations and autonomously reconstruct any component of a complex dynamical system. Hence, it is essential to detect, isolate, and reconstruct the faults efficiently and on time when the systems are in operation. This dissertation presents a novel methodology for developing smart autonomous multiagent systems (SAMAS). The SAMAS comprises several agents; some are homogeneous while others are heterogeneous. The proposed method involves developing a multi-agent systems (MAS) model and designing a decentralized smart control system. The MAS model contains homogeneous and heterogeneous agents, communicates among agents through an undirected connected graph, external disturbances, goals, and constraints. The sensor, actuator, communication, and controller faults are also modeled in the MAS model. A decentralized smart control system is designed to create a SAMAS model in the presence of uncertainty in each agents’ dynamics and faults located in the sensors, actuators, communication networks, and controllers. The proposed control method is based on non-linear filtering techniques, deep reinforcement learning, and robust control techniques. A Chebyshev neural network (CNN) is incorporated to learn the uncertain nonlinear functions in the agent dynamics of MAS. Additionally, robust control term using the hyperbolic tangent function is applied to counteract the neural network approximation errors. Meanwhile, a novel algorithm has been proposed which is employed to estimate the uncertain states of the agent dynamics and to train the internal parameters of the neural network given a set of prior measurements. Moreover, an adaptive threshold method has been proposed to detect any kinds of faults present in the system followed by a likelihood-based isolation method to locate each faulty agent. The novel algorithm is known as a reinforced unscented Kalman filter (RUKF). The primary purpose of the RUKF is to detect and isolate the faults, and to adapt the process and measurement noise covariance matrices to reconstruct the faults. To assess the performance of the proposed methodology, we developed a SAMAS consisting of six heterogeneous uncertain agent dynamics. The SAMAS model and the proposed control methodology are numerically simulated using MATLAB. A Monte-Carlo (MC) simulation was carried out to assess the performance of the proposed control methodology in the presence of uncertain agent dynamics and with the sensor, actuator, communication, and controller faults. The fault isolation results are summarized in confusion matrices for each faulty case. The stability of the RUKF, which ran in conjunction with a robust control method, has been proven using the Lyapunov stability approach. Extensive simulations were conducted to evaluate the performance of the proposed method. In this study, the proposed method showed superior performance to the standard unscented Kalman filter (UKF) and adaptive UKF (AUKF). The proposed fault isolation scheme isolated the faulty agents with over 96.68% success rate at the system level. Hence, the results of the numerical simulations show the feasibility of the proposed approach. The proposed RUKF is also computationally less expensive than the standard UKF and AUKF. The proposed approach can be considered a promising tool to evaluate fault detection, isolation, and reconstruction in complex engineering systems. Furthermore, the proposed approach can be extended to other deep space complex systems. The proposed approach can also be used for MAS problems for the financial sector, such as stock market prediction, economic time series, and multi-arm bandit problems.</p>",
        "link": "http://dx.doi.org/10.32920/25412566"
    },
    {
        "id": 25566,
        "title": "Reinforcement Learning: Theory",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4135/9781529798524"
    },
    {
        "id": 25567,
        "title": "Survey on Practical Reinforcement Learning : from Imitation Learning to Offline Reinforcement Learning",
        "authors": "Dongsu Lee, Chanin Eom, Sungwoo Choi, Sungkwan Kim, Minhae Kwon",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7840/kics.2023.48.11.1405"
    },
    {
        "id": 25568,
        "title": "Reinforcement Learning for Routing",
        "authors": "Haiguang Liao, Levent Burak Kara",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-13074-8_11"
    },
    {
        "id": 25569,
        "title": "Hyperparameter Tuning in Offline Reinforcement Learning",
        "authors": "Andrew Tittaferrante, Abdulsalam Yassine",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00101"
    },
    {
        "id": 25570,
        "title": "A Proposal: Interactively Learning to Summarise Timelines by Reinforcement Learning",
        "authors": "Yuxuan Ye, Edwin Simpson",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.internlp-1.4"
    },
    {
        "id": 25571,
        "title": "Lifelong Reinforcement Learning",
        "authors": "Zhiyuan Chen, Bing Liu",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01575-5_6"
    },
    {
        "id": 25572,
        "title": "Deep Q-Learning",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_6"
    },
    {
        "id": 25573,
        "title": "Reinforcement Learning in Personalized Medicine",
        "authors": "Harry Yang, Haoda Fu",
        "published": "2022-8-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003150886-8"
    },
    {
        "id": 25574,
        "title": "Temporal Difference Learning",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_5"
    },
    {
        "id": 25575,
        "title": "Analyzing the Efficiency of Debt Collectors Using Reinforcement Learning Based Continual Learning",
        "authors": "Keerthana Sivamayilnathan, Elakkiya Rajasekar, Subramaniyaswamy V, Mayuri Mehta",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4516305"
    },
    {
        "id": 25576,
        "title": "Deep Learning and Reinforcement Learning",
        "authors": "",
        "published": "2023-11-15",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5772/intechopen.103984"
    },
    {
        "id": 25577,
        "title": "Optimization of Tsunami Evacuation with Reinforcement Learning",
        "authors": "Erick Mas, Luis Moya, Shunichi Koshimura",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4214384"
    },
    {
        "id": 25578,
        "title": "Review for \"The value–complexity trade-off for reinforcement learning based brain–computer interfaces\"",
        "authors": "",
        "published": "2020-5-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1741-2552/abc8d8/v1/review1"
    },
    {
        "id": 25579,
        "title": "Review for \"The value–complexity trade-off for reinforcement learning based brain–computer interfaces\"",
        "authors": "",
        "published": "2020-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1741-2552/abc8d8/v2/review2"
    },
    {
        "id": 25580,
        "title": "Nonlinear Control: Hamilton Jacobi Bellman (HJB) and Dynamic Programming",
        "authors": "Steven L. Brunton",
        "published": "No Date",
        "citations": 0,
        "abstract": "This video discusses optimal nonlinear control using the Hamilton Jacobi Bellman (HJB) equation, and how to solve this using dynamic programming.",
        "link": "http://dx.doi.org/10.52843/cassyni.4t5069"
    },
    {
        "id": 25581,
        "title": "Deep Reinforcement Learning For Trading - A Critical Survey",
        "authors": "Adrian Millea",
        "published": "No Date",
        "citations": 5,
        "abstract": "Deep reinforcement learning (DRL) has achieved significant results in many Machine Learning (ML) benchmarks. In this short survey we provide an overview of DRL applied to trading on financial markets, including a short meta-analysis using Google Scholar, with an emphasis on using hierarchy for dividing the problem space as well as using model-based RL to learn a world model of the trading environment which can be used for prediction. In addition, multiple risk measures are defined and discussed, which not only provide a way of quantifying the performance of various algorithms, but they can also act as (dense) reward-shaping mechanisms for the agent. We discuss in detail the various state representations used for financial markets, which we consider critical for the success and efficiency of such DRL agents. The market in focus for this survey is the cryptocurrency market.",
        "link": "http://dx.doi.org/10.20944/preprints202111.0044.v1"
    },
    {
        "id": 25582,
        "title": "Optimal Adaptive Control of Power Converters Using Reinforcement Learning",
        "authors": "Omid Zandi, Javad Poshtan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4758756"
    },
    {
        "id": 25583,
        "title": "LSTM, ConvLSTM, MDN-RNN and GridLSTM Memory-based Deep Reinforcement Learning",
        "authors": "Fernando Duarte, Nuno Lau, Artur Pereira, Luís Reis",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011664900003393"
    },
    {
        "id": 25584,
        "title": "Generalized Representation Learning Methods for Deep Reinforcement Learning",
        "authors": "Hanhua Zhu",
        "published": "2020-7",
        "citations": 0,
        "abstract": "Deep reinforcement learning (DRL) increases the successful applications of reinforcement learning (RL) techniques but also brings challenges such as low sample efficiency. In this work, I propose generalized representation learning methods to obtain compact state space suitable for RL from a raw observation state. I expect my new methods will increase sample efficiency of RL by understandable representations of state and therefore improve the performance of RL.",
        "link": "http://dx.doi.org/10.24963/ijcai.2020/748"
    },
    {
        "id": 25585,
        "title": "Step 6 – Deep and Reinforcement Learning",
        "authors": "Manohar Swamynathan",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-2866-1_6"
    },
    {
        "id": 25586,
        "title": "A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Energy Optimization for Smart Buildings",
        "authors": "Mikhail Genkin, J.  J. McArthur",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4538377"
    },
    {
        "id": 25587,
        "title": "Learning Robotic Skills through Reinforcement Learning",
        "authors": "Spinder Kaur, Guladab Bawa",
        "published": "2022-8-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icesc54411.2022.9885704"
    },
    {
        "id": 25588,
        "title": "Online Learning",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch3"
    },
    {
        "id": 25589,
        "title": "Networked Personalized Federated Learning Using Reinforcement Learning",
        "authors": "Francois Gauthier, Vinay Chakravarthi Gogineni, Stefan Werner",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279781"
    },
    {
        "id": 25590,
        "title": "A Study on Cooperative Action Selection Considering Unfairness in Decentralized Multiagent Reinforcement Learning",
        "authors": "Toshihiro Matsui, Hiroshi Matsuo",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006203800880095"
    },
    {
        "id": 25591,
        "title": "Reinforcement Learning Approach for Cooperative Control of  Multi-Agent Systems",
        "authors": "Valeria Javalera-Rincon, Vicenc Cayuela, Bernardo Seix, Fernando Orduña-Cabrera",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007349000800091"
    },
    {
        "id": 25592,
        "title": "Stabilized Control of a Drone with Deep Reinforcement Learning",
        "authors": "Tomoki OTSUKI, Akiya KAMIMURA",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1299/jsmermd.2019.1p2-n04"
    },
    {
        "id": 25593,
        "title": "A Reinforcement Learning Environment for Directed Quantum Circuit Synthesis",
        "authors": "Michael Kölle, Tom Schubert, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012383200003636"
    },
    {
        "id": 25594,
        "title": "Properties and Calculation",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_5"
    },
    {
        "id": 25595,
        "title": "Deep Reinforcement Learning in Unity",
        "authors": "Abhilash Majumder",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6503-1"
    },
    {
        "id": 25596,
        "title": "Reinforcement learning for Traffic Signal Control: Comparison with commercial systems",
        "authors": "Álvaro Cabrejas Egea",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36443/10259/7002"
    },
    {
        "id": 25597,
        "title": "Motion planning using reinforcement learning",
        "authors": "Rahul Kala",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-443-18908-1.00016-9"
    },
    {
        "id": 25598,
        "title": "Reinforcement Learning",
        "authors": "Kenji Doya",
        "published": "2023-5-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108755610.013"
    },
    {
        "id": 25599,
        "title": "Adaptive Coding is Optimal in Reinforcement Learning",
        "authors": "Aldo Rustichini, Stefano Palminteri, Magdalena Soukupova",
        "published": "No Date",
        "citations": 0,
        "abstract": "Adaptive coding is defined as the adjustment of parameters of a cognitive process as a function of the task's statistical properties. Our aim is to demonstrate that adaptive coding maximizes performance in situations where decision-making follows a reinforcement learning process (RLM), and test empirically test the predictions of our model. We first provide a detailed model of adaptive coding of reward in a RLM and study its performance, measured in expected value obtained from chosen options. In our model, the parameters modulating learning adjust optimally according to the information on outcomes. Their values are modulated by the trade-off between exploration and exploitation. When feedback is complete, exploration is redundant, and parameters adjust to maximize exploitation. When feedback is partial (only provided on the chosen option) an optimal compromise is reached between exploration and exploitation. We test these predictions in a set of experiments with complete and partial feedback. We find support for the hypothesis that coding is adaptive, and has the linear form we hypothesize. The design allows us to discriminate between two possible formulations, finding support for the hypothesis that adaptation is specific to reward encoding.  We also find that the representational bias introduced in choice by the adaptive coding is transitory and is corrected when appropriate information is provided to participants.",
        "link": "http://dx.doi.org/10.31234/osf.io/7ghx6"
    },
    {
        "id": 25600,
        "title": "A Scalable Derivative-free Exploration Approach for Reinforcement Learning",
        "authors": "Xiong-Hui Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nExploration in complex environments remains a challenge for deep reinforcement learning (RL). Derivative-free optimization (DFO), which provides efficient black-box solution sampling mechanisms, is a potential way to address this issue. Recent studies which inject these mechanisms into RL have shown better exploration abilities than derivative-based policy optimization methods. However, we found that these methods suffer from low sample efficiency in high-dimensional policy parameter space, which limits the adaptability of these methods in complex tasks requiring large neural networks to represent well-performed policies. In this paper, we propose a scalable exploration algorithm based on derivative-free optimization, named Scalable Derivative-free Exploration (SDFE), which improves exploration in complex RL problems. SDFE handles the high-dimensional policy searching problem by optimizing policy in a transformed small parameter space of policy, rather than the original parameter space of the policy neural network. SDFE is a general framework that is compatible with derivative-free optimization methods and off-policy policy-based algorithms. In experiments, we instantiate SDFE with two derivative-free optimization algorithms (SRACOS and CMAES) and three off-policy actor-critic algorithms (SAC, ACER, and DDPG), to show its efficiency and adaptability. We conduct experiments on MuJoCo tasks and 42 pixel-based games in Atari, empirically verifying that SDFE can reach better performance with a competitive number of samples on both tasks.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1973394/v1"
    },
    {
        "id": 25601,
        "title": "A Reinforcement Learning Method to Select Ad Networks in Waterfall Strategy",
        "authors": "Reza Afshar, Yingqian Zhang, Murat Firat, Uzay Kaymak",
        "published": "2019",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007395502560265"
    },
    {
        "id": 25602,
        "title": "Reinforcement learning",
        "authors": "Donald J. Norris",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-5174-4_9"
    },
    {
        "id": 25603,
        "title": "Interpretable Reinforcement Learning with Multilevel Subgoal Discovery",
        "authors": "Alexander Demin, Denis Ponomaryov",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00043"
    },
    {
        "id": 25604,
        "title": "Prior-Information Enhanced Reinforcement Learning for Energy Management Systems",
        "authors": "Th´eo Zangato, Aomar Osmani, Pegah Alizadeh",
        "published": "2024-1-27",
        "citations": 0,
        "abstract": "Amidst increasing energy demands and growing environmental concerns, the promotion of sustainable and energy-efficient practices has become imperative. This paper introduces a reinforcement learning-based technique for optimizing energy consumption and its associated costs, with a focus on energy management systems. A three-step approach for the efficient management of charging cycles in energy storage units within buildings is presented combining RL with prior knowledge. A unique strategy is adopted: clustering building load curves to discern typical energy consumption patterns, embedding domain knowledge into the learning algorithm to refine the agent’s action space and predicting of future observations to make real-time decisions. We showcase the effectiveness of our method using real-world data. It enables controlled exploration and efficient training of Energy Management System (EMS) agents. When compared to the benchmark, our model reduces energy costs by up to 15%, cutting down consumption during peak periods, and demonstrating adaptability across various building consumption profiles.",
        "link": "http://dx.doi.org/10.5121/csit.2024.140207"
    },
    {
        "id": 25605,
        "title": "Reinforcement Learning for Algorithmic Trading",
        "authors": "",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009028943.014"
    },
    {
        "id": 25606,
        "title": "Toward ensuring better learning performance in reinforcement learning",
        "authors": "Menglei Zhang, Yukikazu Nakamoto",
        "published": "2022-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/candarw57323.2022.00037"
    },
    {
        "id": 25607,
        "title": "Quality Metrics for Reinforcement Learning for Edge Cloud and Internet-of-Things Systems",
        "authors": "Claus Pahl, Hamid Barzegar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012194800003584"
    },
    {
        "id": 25608,
        "title": "Uncertainty-based Out-of-Distribution Classification in Deep Reinforcement Learning",
        "authors": "Andreas Sedlmeier, Thomas Gabor, Thomy Phan, Lenz Belzner, Claudia Linnhoff-Popien",
        "published": "2020",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008949905220529"
    },
    {
        "id": 25609,
        "title": "Integral Reinforcement Learning for Optimal Regulation",
        "authors": "Bosen Lian, Wenqian Xue, Frank L. Lewis, Hamidreza Modares, Bahare Kiumarsi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45252-9_3"
    },
    {
        "id": 25610,
        "title": "Reinforcement Learning with Quantitative Verification for Assured Multi-Agent Policies",
        "authors": "Joshua Riley, Radu Calinescu, Colin Paterson, Daniel Kudenko, Alec Banks",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010258102370245"
    },
    {
        "id": 25611,
        "title": "Reinforcement Learning vs Genetic Algorithms in Game-Theoretic Cyber-Security",
        "authors": "Stefan Niculae",
        "published": "No Date",
        "citations": 3,
        "abstract": "Penetration testing is the practice of performing a simulated attack on a computer system in order to reveal its vulnerabilities. The most common approach is to gain information and then plan and execute the attack manually, by a security expert. This manual method cannot meet the speed and frequency required for efficient, large-scale secu- rity solutions development. To address this, we formalize penetration testing as a security game between an attacker who tries to compro- mise a network and a defending adversary actively protecting it. We compare multiple algorithms for finding the attacker’s strategy, from fixed-strategy to Reinforcement Learning, namely Q-Learning (QL), Extended Classifier Systems (XCS) and Deep Q-Networks (DQN). The attacker’s strength is measured in terms of speed and stealthi- ness, in the specific environment used in our simulations. The results show that QL surpasses human performance, XCS yields worse than human performance but is more stable, and the slow convergence of DQN keeps it from achieving exceptional performance, in addition, we find that all of these Machine Learning approaches outperform fixed-strategy attackers.",
        "link": "http://dx.doi.org/10.31237/osf.io/nxzep"
    },
    {
        "id": 25612,
        "title": "Reinforcement Learning for a system-of-systems approach in water management",
        "authors": "Lan Hoang",
        "published": "No Date",
        "citations": 0,
        "abstract": "&lt;p&gt;The water cycle connects many essential parts of the environment and is a key process supporting life on Earth. Amid climate change impacts and competing water consumptions from a growing population, there is a need for better management of this scarce resource. Yet, water management is complex. As a resource, water exists under various forms, from water droplets in the atmosphere to embodied water in consumer products. Its flows and existence transcend national and geographic borders; its management, however, are limited by boundaries. To date, machine learning has shown potentials in applications across domains, from showing skills in game plays to improving efficiencies and operation of real-life processes. The system-of-systems perspective has emerged in many fields as an attempt to capture the complexity arising from individual components. Within a system, the interactions and interdependencies across components can produce unintended consequences. Moreover, their effects that are not explainable just from studying a component on its own. Its concept intertwines with Complexity Science, and points to Wicked Problems, solutions of which are difficult to find and achieve. Climate change itself has been recognised as a &amp;#8216;Super Wicked&amp;#8217; problem, for which deadlines are approaching but for which there are no clear solutions. Yet, there is often a lack of understanding of the interactions and dependencies, even from a physical modelling perspective. A comprehensive approach to capturing these interactions is through physical modelling of water processes, such as hydraulics and hydrological modelling. The structure and data pipelines of such an approach, nevertheless, is static and does not evolve unless reconfigured by model experts.&amp;#160;&lt;/p&gt;&lt;p&gt;We propose that a form of machine learning, Deep Reinforcement Learning, can be used to better capture the complex whole system interactions of components in the water cycle and assist in their management. This approach capitalises the rapid advances of Machine Learning in environmental applications and differ to traditional optimisation techniques in that it provides distributed learning, consistent models for components that can evolve to connect and continuously adapt to the operating environment. This is key in capturing the changes brought about by climate change and the subsequent environmental and human change in response.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. Reinforcement Learning for improving process modelling &lt;/strong&gt;to produce a spectrum of fully physical models for hybrid physical-neural networks to full Deep Learning models that can mimic the natural processes of interest, such as streamflows or rainfall-runoff. An example case study could be a hydrological model of a river catchment and its upstream-downstream dam operation. The components in this case can be individual reservoir models, neural network-based emulators, or differential equation models.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. Reinforcement Learning for holistic modelling of physical processes in water managemen &lt;/strong&gt;to capture the whole system. Since each component is modelled as a full or hybrid physical-neural network model, the components could be integrated to provide a whole system approach. Within this, Reinforcement Learning can act as the constructor or go beyond this to provide solutions for targeted problems.&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&amp;#160;&lt;/p&gt;",
        "link": "http://dx.doi.org/10.5194/egusphere-egu21-7484"
    },
    {
        "id": 25613,
        "title": "Reinforcement learning during locomotion",
        "authors": "Jonathan M Wood, Hyosub E Kim, Susanne M Morton",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractWhen learning a new motor skill, people often must use trial and error to discover which movement is best. In the reinforcement learning framework, this concept is known as exploration and has been observed as increased movement variability in motor tasks. For locomotor tasks, however, increased variability decreases upright stability. As such, exploration during gait may jeopardize balance and safety, making reinforcement learning less effective. Therefore, we set out to determine if humans could acquire and retain a novel locomotor pattern using reinforcement learning alone. Young healthy male and female humans walked on a treadmill and were provided with binary reward feedback (success or failure only) to learn a novel stepping pattern. We also recruited a comparison group who walked with the same novel stepping pattern but did so by correcting for target error, induced by providing real time veridical visual feedback of steps and a target. In two experiments, we compared learning, motor variability, and two forms of motor memories between the groups. We found that individuals in the binary reward group did, in fact, acquire the new walking pattern by exploring (increased variability). Additionally, while reinforcement learning did not increase implicit motor memories, it resulted in more accurate explicit motor memories compared to the target error group. Overall, these results demonstrate that humans can acquire new walking patterns with reinforcement learning and retain much of the learning over 24 hours.Significance StatementHumans can learn some novel movements by independently discovering the actions that lead to success. This discovery process, exploration, requires increased motor variability to determine the best movement. However, in bipedal locomotion especially, increasing motor variability decreases stability, heightening the risk of negative outcomes such as a trip, injury, or fall. Despite this stability constraint, the current study shows that individuals do use exploration to find the most rewarding walking patterns. This form of learning led to improved explicit retention but not implicit aftereffects. Thus, the reinforcement learning framework can explain findings across a wide range of motor and cognitive tasks, including locomotion.",
        "link": "http://dx.doi.org/10.1101/2023.09.13.557581"
    },
    {
        "id": 25614,
        "title": "Optimal Highway Ramp Speed Control with Deep Reinforcement Learning",
        "authors": "Zhou (Eric) Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nHighway ramp speed control plays a critical role in optimizing traffic flow and improving transportation system efficiency. In this paper, we propose an optimal highway ramp speed control method using deep reinforcement learning. By leveraging the power of deep neural networks and reinforcement learning, our method, named DeepRampControl, learns to dynamically adjust vehicle speeds at ramps to minimize disruptions to the main traffic flow. We compare DeepRampControl with traditional rule-based approaches and other machine learning-based methods through comprehensive experiments in a simulated environment. The results demonstrate that DeepRampControl achieves higher traffic flow efficiency, lower average merging delay, and reduced energy consumption. These findings highlight the potential of deep reinforcement learning in optimizing highway ramp speed control and its ability to adapt to dynamic traffic conditions. The proposed method contributes to the development of intelligent and adaptive traffic control systems, paving the way for more efficient and sustainable transportation networks.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3162082/v1"
    },
    {
        "id": 25615,
        "title": "Detection of Man-in-the-Middle Attacks in Model-Free Reinforcement Learning",
        "authors": "Rishi Rani",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper proposes a Bellman Deviation algorithm for the detection of man-in-the-middle (MITM) attacks occurring when an agent controls a Markov Decision Process (MDP) system using  model-free reinforcement learning. We show an intuitive necessary and sufficient ``informational advantage\" condition  for  the proposed algorithm to guarantee the detection of attacks  with high probability, while  also avoiding false alarms.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22357600"
    },
    {
        "id": 25616,
        "title": "Properties and Calculation",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_9"
    },
    {
        "id": 25617,
        "title": "Multi-Fidelity Reinforcement Learning with Control Variates",
        "authors": "Sami Khairy, Prasanna Balaprakash",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-181"
    },
    {
        "id": 25618,
        "title": "Detection of Man-in-the-Middle Attacks in Model-Free Reinforcement Learning",
        "authors": "Rishi Rani",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper proposes a Bellman Deviation algorithm for the detection of man-in-the-middle (MITM) attacks occurring when an agent controls a Markov Decision Process (MDP) system using  model-free reinforcement learning. We show an intuitive necessary and sufficient ``informational advantage\" condition  for  the proposed algorithm to guarantee the detection of attacks  with high probability, while  also avoiding false alarms.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22357600.v1"
    },
    {
        "id": 25619,
        "title": "Understanding Adaptive Immune System as Reinforcement Learning",
        "authors": "Takuya Kato, Tetsuya J. Kobayashi",
        "published": "No Date",
        "citations": 1,
        "abstract": "The adaptive immune system of vertebrates can detect, respond to, and memorize diverse pathogens from past experience. While the selection of T helper (Th) clones is the simple and established mechanism to recognize and memorize new pathogens, the question that still remains unexplored is how the Th cells can acquire better ways to bias the responses of immune cells for eliminating pathogens more efficiently by translating the recognized antigen information into regulatory signals. In this work, we address this problem by associating the adaptive immune network organized by the Th cells with reinforcement learning (RL). By employing recent advancements of network-based RL, we show that the Th immune network can acquire the association between antigen patterns of and the effective responses to pathogens. Moreover, the clonal selection as well as other inter-cellular interactions are derived as a learning rule of this network. We also demonstrate that the stationary clone-size distribution after learning shares characteristic features with those observed experimentally. Our theoretical framework may contribute to revising and renewing our understanding of adaptive immunity as a learning system.",
        "link": "http://dx.doi.org/10.1101/2020.01.31.929620"
    },
    {
        "id": 25620,
        "title": "Learning to Drive via Apprenticeship Learning and Deep Reinforcement Learning",
        "authors": "Wenhui Huang, Francesco Braghin, Zhuo Wang",
        "published": "2019-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai.2019.00220"
    },
    {
        "id": 25621,
        "title": "A Study on Cooperative Action Selection Considering Unfairness in Decentralized Multiagent Reinforcement Learning",
        "authors": "Toshihiro Matsui, Hiroshi Matsuo",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006203800880095"
    },
    {
        "id": 25622,
        "title": "Reinforcement Learning Approach for Cooperative Control of  Multi-Agent Systems",
        "authors": "Valeria Javalera-Rincon, Vicenc Cayuela, Bernardo Seix, Fernando Orduña-Cabrera",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007349000800091"
    },
    {
        "id": 25623,
        "title": "Stabilized Control of a Drone with Deep Reinforcement Learning",
        "authors": "Tomoki OTSUKI, Akiya KAMIMURA",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1299/jsmermd.2019.1p2-n04"
    },
    {
        "id": 25624,
        "title": "A Reinforcement Learning Environment for Directed Quantum Circuit Synthesis",
        "authors": "Michael Kölle, Tom Schubert, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012383200003636"
    },
    {
        "id": 25625,
        "title": "Setting the Stage, Return of the Actors",
        "authors": "",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009051873.013"
    },
    {
        "id": 25626,
        "title": "Reinforcement Schedules",
        "authors": "James E. Mazur, Amy L. Odum",
        "published": "2023-3-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003215950-6"
    },
    {
        "id": 25627,
        "title": "Reinforcement Learning and Causal Inference",
        "authors": "Momiao Xiong",
        "published": "2022-2-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003028543-8"
    },
    {
        "id": 25628,
        "title": "Exploring Reinforcement Learning Environment for User-centric Applications in VANET",
        "authors": "",
        "published": "2022-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.55162/mcet.03.092"
    },
    {
        "id": 25629,
        "title": "Learning Structures Through Reinforcement",
        "authors": "Anne G.E. Collins",
        "published": "2018",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-812098-9.00005-x"
    },
    {
        "id": 25630,
        "title": "Reinforcement Learning for Coherent Beam Combining",
        "authors": "Henrik Tünnermann, Akira Shirakawa",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1364/cleopr.2018.w1a.2"
    },
    {
        "id": 25631,
        "title": "Two-Agent Self-Play",
        "authors": "Aske Plaat",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0638-1_6"
    },
    {
        "id": 25632,
        "title": "Cost Effective Transfer of Reinforcement Learning Policies",
        "authors": "Orel Lavie, Gilad Katz, Asaf Shabtai",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4341615"
    },
    {
        "id": 25633,
        "title": "Design of a Matter-Wave Gyroscope with Reinforcement Learning",
        "authors": "Liang-Ying Chih, Murray Holland",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/m.6275705d66d5dcf63a311655"
    },
    {
        "id": 25634,
        "title": "Area Coverage for Swarm Robots Via Inverse Reinforcement Learning",
        "authors": "Mingxuan Chen, Ping Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4592186"
    },
    {
        "id": 25635,
        "title": "Size Scaling in Self-Play Reinforcement Learning",
        "authors": "Oren Neumann, Claudius Gros",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2022.es2022-53"
    },
    {
        "id": 25636,
        "title": "Peer Review #2 of \"SANgo: a storage infrastructure simulator with reinforcement learning support (v0.1)\"",
        "authors": "",
        "published": "2020-5-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.271v0.1/reviews/2"
    },
    {
        "id": 25637,
        "title": "Peer Review #2 of \"SANgo: a storage infrastructure simulator with reinforcement learning support (v0.2)\"",
        "authors": "",
        "published": "2020-5-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.271v0.2/reviews/2"
    },
    {
        "id": 25638,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "J. C. Cuevas",
        "published": "2023-8-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/w91n9r"
    },
    {
        "id": 25639,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Matteo Lo Preti",
        "published": "2023-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/vnz66g"
    },
    {
        "id": 25640,
        "title": "Workflow scheduling strategy based on deep reinforcement learning",
        "authors": "Shuo ZHANG, Zhuofeng Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWith the increase of Internet of Things devices, the data intensive workflow has emerged. Because the data-intensive workflow has the characteristics of scattered data sources, large data scale and collaborative distributed execution at the cloud edge. It brings many challenges to the execution of workflow, such as data flow control management, data transmission scheduling, etc. Aiming at the execution constraints and data transmission optimization of data-intensive workflow, this paper proposes a workflow scheduling method based on deep reinforcement learning. First, the execution constraints, edge node load and data transmission volume of IoT data workflow are modeled; Then the data - intensive workflow is segmented with the optimization goal of data transmission; Besides, taking the workflow execution time and average load balancing as the optimization goal, the improved DQN algorithm is used to schedule the workflow. Based on the DQN algorithm, the model reward function and action selection are redesigned and improved. The simulation results based on WorkflowSim show that, compared with MOPSO, NSGA-II and GTBGA, the algorithm proposed in this paper can effectively reduce the execution time of IoT data workflow under the condition of ensuring the execution constraints and load balancing of edge nodes.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2431749/v1"
    },
    {
        "id": 25641,
        "title": "Ensemble Consensus Representation Deep Reinforcement Learning for Hybrid FSO/RF Communication Systems",
        "authors": "Shagufta Henna",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Hybrid FSO/RF system requires an efficient FSO and RF link switching mechanism to improve the system capacity by realizing the complementary benefits of both the links. The dynamics of network conditions, such as fog, dust, and sand storms compound the link switching problem and control complexity. To address this problem, we initiate the study of deep reinforcement learning (DRL) for link switching of hybrid FSO/RF systems. Specifically, in this work, we focus on actor-critic called Actor/Critic-FSO/RF and Deep-Q network (DQN) called DQN-FSO/RF for FSO/RF link switching under atmospheric turbulences. To formulate the problem, we define the state, action, and reward function of a hybrid FSO/RF system. DQN-FSO/RF frequently updates the deployed policy that interacts with the environment in a hybrid FSO/RF system, resulting in high switching costs. To overcome this, we lift this problem to ensemble consensus-based representation learning for deep reinforcement called DQNEnsemble-FSO/RF. The proposed novel DQNEnsemble-FSO/RF DRL approach uses consensus learned features representations based on an ensemble of asynchronous threads to update the deployed policy. Experimental results corroborate that the proposed DQNEnsemble-FSO/RF’s consensus learned features switching achieves better performance than Actor/Critic-FSO/RF, DQN-FSO/RF, and MyOpic for FSO/RF link switching while keeping the switching cost significantly low.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.15109434"
    },
    {
        "id": 25642,
        "title": "Photonic Reinforcement Learning Based on Optoelectronic Reservoir Computing",
        "authors": "Kazutaka Kanno, Atsushi Uchida",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nReinforcement learning has been intensively investigated and developed in artificial intelligence in the absence of training data, such as autonomous driving vehicles, robot control, and internet advertising. However, the computational cost of reinforcement learning with deep neural networks is extremely high, and reducing the learning cost is a challenging issue. We propose a photonic on-line implementation of reinforcement learning using optoelectronic delay-based reservoir computing, both experimentally and numerically. In the proposed scheme, we accelerate reinforcement learning at a rate of several megahertz because there is no required learning process for the internal connection weights in reservoir computing. We perform two benchmark tasks, CartPole-v0 and MountanCar-v0 tasks, to evaluate the proposed scheme. Our results represent the first hardware implementation of reinforcement learning based on photonic reservoir computing and paves the way for fast and efficient reinforcement learning as a novel photonic accelerator.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-988124/v1"
    },
    {
        "id": 25643,
        "title": "Evaluating deep reinforcement learning algorithms for quadrupedal slope handling",
        "authors": "",
        "published": "2020-8-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.13180/clawar.2020.24-26.08.58"
    },
    {
        "id": 25644,
        "title": "Editor's evaluation: The functional form of value normalization in human reinforcement learning",
        "authors": "Thorsten Kahnt",
        "published": "2022-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.83891.sa0"
    },
    {
        "id": 25645,
        "title": "Properties and Calculation",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_5"
    },
    {
        "id": 25646,
        "title": "Reinforcement Learning",
        "authors": "Charu C. Aggarwal",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-72357-6_10"
    },
    {
        "id": 25647,
        "title": "Reinforcement Learning",
        "authors": "Kenji Doya",
        "published": "2023-5-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108755610.013"
    },
    {
        "id": 25648,
        "title": "Adaptive Coding is Optimal in Reinforcement Learning",
        "authors": "Aldo Rustichini, Stefano Palminteri, Magdalena Soukupova",
        "published": "No Date",
        "citations": 0,
        "abstract": "Adaptive coding is defined as the adjustment of parameters of a cognitive process as a function of the task's statistical properties. Our aim is to demonstrate that adaptive coding maximizes performance in situations where decision-making follows a reinforcement learning process (RLM), and test empirically test the predictions of our model. We first provide a detailed model of adaptive coding of reward in a RLM and study its performance, measured in expected value obtained from chosen options. In our model, the parameters modulating learning adjust optimally according to the information on outcomes. Their values are modulated by the trade-off between exploration and exploitation. When feedback is complete, exploration is redundant, and parameters adjust to maximize exploitation. When feedback is partial (only provided on the chosen option) an optimal compromise is reached between exploration and exploitation. We test these predictions in a set of experiments with complete and partial feedback. We find support for the hypothesis that coding is adaptive, and has the linear form we hypothesize. The design allows us to discriminate between two possible formulations, finding support for the hypothesis that adaptation is specific to reward encoding.  We also find that the representational bias introduced in choice by the adaptive coding is transitory and is corrected when appropriate information is provided to participants.",
        "link": "http://dx.doi.org/10.31234/osf.io/7ghx6"
    },
    {
        "id": 25649,
        "title": "The evolution of social dominance through reinforcement learning",
        "authors": "Olof Leimar",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractGroups of social animals are often organised into dominance hierarchies that are formed through pairwise interactions. There is much experimental data on hierarchies, examining such things as winner, loser, and bystander effects, as well as the linearity and replicability of hierarchies, but there is a lack evolutionary analyses of these basic observations. Here I present a game-theory model of hierarchy formation in which individuals adjust their aggressive behaviour towards other group members through reinforcement learning. Individual traits such as the tendency to generalise learning between interactions with different individuals, the rate of learning, and the initial tendency to be aggressive are genetically determined and can be tuned by evolution. I find that evolution favours individuals with high social competence, making use of individual recognition, bystander learning and, to a limited extent, generalising learned behaviour between opponents when adjusting their behaviour towards other group members. The results are in good agreement with experimental data, for instance in finding weaker winner effects compared to loser effects.",
        "link": "http://dx.doi.org/10.1101/2020.07.23.218040"
    },
    {
        "id": 25650,
        "title": "Review for \"The value–complexity trade-off for reinforcement learning based brain–computer interfaces\"",
        "authors": "",
        "published": "2020-10-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1741-2552/abc8d8/v2/review1"
    },
    {
        "id": 25651,
        "title": "A Scalable Derivative-free Exploration Approach for Reinforcement Learning",
        "authors": "Xiong-Hui Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nExploration in complex environments remains a challenge for deep reinforcement learning (RL). Derivative-free optimization (DFO), which provides efficient black-box solution sampling mechanisms, is a potential way to address this issue. Recent studies which inject these mechanisms into RL have shown better exploration abilities than derivative-based policy optimization methods. However, we found that these methods suffer from low sample efficiency in high-dimensional policy parameter space, which limits the adaptability of these methods in complex tasks requiring large neural networks to represent well-performed policies. In this paper, we propose a scalable exploration algorithm based on derivative-free optimization, named Scalable Derivative-free Exploration (SDFE), which improves exploration in complex RL problems. SDFE handles the high-dimensional policy searching problem by optimizing policy in a transformed small parameter space of policy, rather than the original parameter space of the policy neural network. SDFE is a general framework that is compatible with derivative-free optimization methods and off-policy policy-based algorithms. In experiments, we instantiate SDFE with two derivative-free optimization algorithms (SRACOS and CMAES) and three off-policy actor-critic algorithms (SAC, ACER, and DDPG), to show its efficiency and adaptability. We conduct experiments on MuJoCo tasks and 42 pixel-based games in Atari, empirically verifying that SDFE can reach better performance with a competitive number of samples on both tasks.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1973394/v1"
    },
    {
        "id": 25652,
        "title": "Reinforcement-Learning-Based IDS for 6LoWPAN",
        "authors": "Aryan Mohammadi Pasikhani, Andrew John Clark, Prosanta Gope",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>The Routing Protocol for low power Lossy networks (RPL) is a\ncritical operational component of low power wireless personal area networks\nusing IPv6 (6LoWPANs). In this paper we propose a Reinforcement Learning (RL)\nbased IDS to detect various attacks on RPL in 6LoWPANs, including several\nunaddressed by current research. The proposed scheme can also detect previously\nunseen attacks and the presence of mobile intruders. The scheme is well suited\nto the resource constrained environments of our target networks.</p><br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16645000.v1"
    },
    {
        "id": 25653,
        "title": "Reinforcement learning for Traffic Signal Control: Comparison with commercial systems",
        "authors": "Álvaro Cabrejas Egea",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36443/10259/7002"
    },
    {
        "id": 25654,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "K. S. Kavitha",
        "published": "2023-8-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/rc0ei6"
    },
    {
        "id": 25655,
        "title": "Deep Reinforcement Learning in Unity",
        "authors": "Abhilash Majumder",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6503-1"
    },
    {
        "id": 25656,
        "title": "Motion planning using reinforcement learning",
        "authors": "Rahul Kala",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-443-18908-1.00016-9"
    },
    {
        "id": 25657,
        "title": "Combining Geophysical Inversion with Reinforcement Learning",
        "authors": "P. Dell’Aversana",
        "published": "2022",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202210229"
    },
    {
        "id": 25658,
        "title": "Inverse Reinforcement Learning and Imitation Learning",
        "authors": "Matthew F. Dixon, Igor Halperin, Paul Bilokon",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41068-1_11"
    },
    {
        "id": 25659,
        "title": "TOWARDS AN ADAPTIVE LEARNING SYSTEM BASED ON LEARNING BY REINFORCEMENT AND AUTOMATIC LEARNING",
        "authors": "Mustapha Riad, Mohammed Qbadou, Es-Saadia Aoula",
        "published": "2020-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/iceri.2020.0345"
    },
    {
        "id": 25660,
        "title": "Reinforcement Twinning: From Digital Twins to Model-Based Reinforcement Learning",
        "authors": "Lorenzo Schena, Pedro Afonso  Duque Morgado Marques, Romain Poletti, Samuel Ahizi, Jan Van den Berghe, Miguel Alfonso Mendez",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4761240"
    },
    {
        "id": 25661,
        "title": "Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning",
        "authors": "Swaroop Nath, Pushpak Bhattacharyya, Harshad Khadilkar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.977"
    },
    {
        "id": 25662,
        "title": "Optimization of Deep Reinforcement Learning with Hybrid Multi-Task Learning",
        "authors": "Nelson Vithayathil Varghese, Qusay H. Mahmoud",
        "published": "2021-4-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/syscon48628.2021.9447080"
    },
    {
        "id": 25663,
        "title": "Learning to Navigate for Mobile Robot with Continual Reinforcement Learning",
        "authors": "Ning Wang, Dingyuan Zhang, Yong Wang",
        "published": "2020-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9188558"
    },
    {
        "id": 25664,
        "title": "Longer duration intertrial intervals without visual stimuli have reinforcement value and increase the rate of reinforcement and punishment learning in computer-based discriminations in humans",
        "authors": "Xiaojin Ma, Blair Bracciano, Nicole Hoppas, Sydney Zimmerman, Charles L. Pickens",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.lmot.2022.101867"
    },
    {
        "id": 25665,
        "title": "Lifelong Reinforcement Learning",
        "authors": "Zhiyuan Chen, Bing Liu",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01581-6_9"
    },
    {
        "id": 25666,
        "title": "Integrating Learning and Planning",
        "authors": "Huaqing Zhang, Ruitong Huang, Shanghang Zhang",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_9"
    },
    {
        "id": 25667,
        "title": "Learning a Belief Representation for Delayed Reinforcement Learning",
        "authors": "Pierre Liotet, Erick Venneri, Marcello Restelli",
        "published": "2021-7-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9534358"
    },
    {
        "id": 25668,
        "title": "Deep Reinforcement Learning",
        "authors": "",
        "published": "2022-12-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119910695.ch4"
    },
    {
        "id": 25669,
        "title": "Learning Urban Driving Policies using Deep Reinforcement Learning",
        "authors": "Tanmay Agarwal, Hitesh Arora, Jeff Schneider",
        "published": "2021-9-19",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9564412"
    },
    {
        "id": 25670,
        "title": "Optimization of Multilayer Optical Films with Unsupervised Learning, reinforcement learning and genetic algorithm",
        "authors": "Jiang Anqing, Osamu Yoshie",
        "published": "2020",
        "citations": 0,
        "abstract": "We proposed an optical film optimal algorithm using a three-step machine learning algorithm. which applies to design layered thin-film materials. As a verification, the absorption of the designed solar selective absorption film is 91%.",
        "link": "http://dx.doi.org/10.1364/fio.2020.jm6a.5"
    },
    {
        "id": 25671,
        "title": "Learning to Tune a Class of Controllers with Deep Reinforcement Learning",
        "authors": "William John Shipman",
        "published": "2021-9-9",
        "citations": 0,
        "abstract": "Control systems require maintenance in the form of tuning their parameters in order to maximize their performance in the face of process changes in minerals processing circuits. This work focuses on using deep reinforcement learning to train an agent to perform this maintenance continuously. A generic simulation of a first-order process with a time delay, controlled by a proportional-integral controller, was used as the training environment. Domain randomization in this environment was used to aid in generalizing the agent to unseen conditions on a physical circuit. Proximal policy optimization was used to train the agent, and hyper-parameter optimization was performed to select the optimal agent neural network size and training algorithm parameters. Two agents were tested, examining the impact of the observation space used by the agent and concluding that the best observation consists of the parameters of an auto-regressive with exogenous input model fitted to the measurements of the controlled variable. The best trained agent was deployed at an industrial comminution circuit where it was tested on two flow rate control loops. This agent improved the performance of one of these control loops but decreased the performance of the other control loop. While deep reinforcement learning does show promise in controller tuning, several challenges and directions for further study have been identified.",
        "link": "http://dx.doi.org/10.3390/min11090989"
    },
    {
        "id": 25672,
        "title": "Reinforcement Learning Aided Performance Optimization of Feedback Control Systems",
        "authors": "Changsheng Hua",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-33034-7"
    },
    {
        "id": 25673,
        "title": "3D Based Generative PROTAC Linker Design with Reinforcement Learning",
        "authors": "baiqing li, Hongming Chen",
        "published": "No Date",
        "citations": 1,
        "abstract": "Proteolysis targeting chimeras (PROTACs), have emerged as an effective therapeutic modality by harnessing the ubiquitin-proteasome system to selectively induce targeted protein degradation, with the potential to modulate traditional undruggable targets. Due to its hetero-bifunctional characteristics, in which a linker joins warhead binding to a protein of interest, conferring specificity, and E3-ligand binding to an E3 ubiquitin ligase, a PROTAC molecule can form a PROTAC ternary structure for bring the protein of interest to the vicinity of the E3 ligase. The rational PROTAC linker design is challenging due to its relatively large molecular weight and the complexity of maintaining the binding mode of warhead and E3-ligand in the binding pockets of counterpart. Conventional linker generation method can only generate linkers in either 1D SMILES or 2D graph, without taking into account the information of ternary structures. Here we propose a novel 3D linker generative model PROTAC-INVENT which can not only generate SMILES of PROTAC but also its 3D putative binding conformation coupled with the target protein and the E3 ligase. The model is trained jointly with the RL approach to bias the generation of PROTAC structures toward pre-defined 2D and 3D based properties. Examples were provided to demonstrate the utility of the model for generating reasonable 3D conformation of PROTACs. On the other hand, our results show that the associated workflow for 3D PROTAC conformation generation can also be used as an efficient docking protocol for PROTACs.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2023-j740w"
    },
    {
        "id": 25674,
        "title": "Reinforcement Learning for Control of Human Locomotion in Simulation",
        "authors": "Andrii Dashkovets, Brokoslaw Laschowski",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractControl of robotic leg prostheses and exoskeletons is an open challenge. Computer modeling and simulation can be used to study the dynamics and control of human walking and extract principles that can be programmed into robotic legs to behave similar to biological legs. In this study, we present the development of an efficient two-layer Q-learning algorithm, with k-d trees, that operates over continuous action spaces and a reward model that estimates the degree of muscle activation similarity between the agent and human state-to-action pairs and state-to-action sequences. We used a human musculoskeletal model acting in a high-dimensional, physics-based simulation environment to train and evaluate our algorithm to simulate biomimetic walking. We used imitation learning and artificial bio-mechanics data to accelerate training via expert demonstrations and used experimental human data to compare and validate our predictive simulations, achieving 79% accuracy. Also, when compared to the previous state-of-the-art that used deep deterministic policy gradient, our algorithm was significantly more efficient with lower computational and memory storage requirements (i.e., requiring 7 times less RAM and 87 times less CPU compute), which can benefit real-time embedded computing. Overall, our new two-layer Q-learning algorithm using sequential data for continuous imitation of human locomotion serves as a first step towards the development of bioinspired controllers for robotic prosthetic legs and exoskeletons. Future work will focus on improving the prediction accuracy compared to experimental data and expanding our simulations to other locomotor activities.",
        "link": "http://dx.doi.org/10.1101/2023.12.19.572447"
    },
    {
        "id": 25675,
        "title": "Environment and Policy",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_2"
    },
    {
        "id": 25676,
        "title": "Mood modelling within reinforcement learning",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1162/isal_a_021"
    },
    {
        "id": 25677,
        "title": "Contextual reinforcement learning",
        "authors": "John Langford",
        "published": "2017-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata.2017.8257902"
    },
    {
        "id": 25678,
        "title": "Ensemble Consensus Representation Deep Reinforcement Learning for Hybrid FSO/RF Communication Systems",
        "authors": "Shagufta Henna",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Hybrid FSO/RF system requires an efficient FSO and RF link switching mechanism to improve the system capacity by realizing the complementary benefits of both the links. The dynamics of network conditions, such as fog, dust, and sand storms compound the link switching problem and control complexity. To address this problem, we initiate the study of deep reinforcement learning (DRL) for link switching of hybrid FSO/RF systems. Specifically, in this work, we focus on actor-critic called Actor/Critic-FSO/RF and Deep-Q network (DQN) called DQN-FSO/RF for FSO/RF link switching under atmospheric turbulences. To formulate the problem, we define the state, action, and reward function of a hybrid FSO/RF system. DQN-FSO/RF frequently updates the deployed policy that interacts with the environment in a hybrid FSO/RF system, resulting in high switching costs. To overcome this, we lift this problem to ensemble consensus-based representation learning for deep reinforcement called DQNEnsemble-FSO/RF. The proposed novel DQNEnsemble-FSO/RF DRL approach uses consensus learned features representations based on an ensemble of asynchronous threads to update the deployed policy. Experimental results corroborate that the proposed DQNEnsemble-FSO/RF’s consensus learned features switching achieves better performance than Actor/Critic-FSO/RF, DQN-FSO/RF, and MyOpic for FSO/RF link switching while keeping the switching cost significantly low.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.15109434.v2"
    },
    {
        "id": 25679,
        "title": "Reinforcement Learning for Sequential Decision and Optimal Control",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8"
    },
    {
        "id": 25680,
        "title": "Decision letter: Neural computations underlying inverse reinforcement learning in the human brain",
        "authors": "",
        "published": "2017-8-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.29718.016"
    },
    {
        "id": 25681,
        "title": "Reinforcement Learning Jazz Improvisation: When Music Meets Game Theory",
        "authors": "Feng Fu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Live performances of music are always charming, with the unpredictability of improvisation due to the dynamic between musicians and interactions with the audience. Jazz improvisation is a particularly noteworthy example for further investigation from a theoretical perspective. Here, we introduce a novel mathematical game theory model for jazz improvisation, providing a framework for studying music theory and improvisational methodologies. We use computational modeling, mainly reinforcement learning,  to explore diverse stochastic improvisational strategies and their paired performance on improvisation. We find that the most effective strategy pair is a strategy that reacts to the most recent payoff (Stepwise Changes) with a reinforcement learning strategy limited to notes in the given chord (Chord-Following Reinforcement Learning). Conversely, a strategy that reacts to the partner's last note and attempts to harmonize with it (Harmony Prediction) strategy pair yields the lowest non-control payoff and highest standard deviation, indicating that picking notes based on immediate reactions to the partner player can yield inconsistent outcomes. On average, the Chord-Following Reinforcement Learning strategy demonstrates the highest mean payoff, while Harmony Prediction exhibits the lowest. Our work lays the foundation for promising applications beyond jazz: including the use of artificial intelligence (AI) models to extract data from audio clips to refine musical reward systems, and training machine learning (ML) models on existing jazz solos to further refine strategies within the game.",
        "link": "http://dx.doi.org/10.31219/osf.io/5khm2"
    },
    {
        "id": 25682,
        "title": "Continual adaptation in deep reinforcement learning-based control applied to non-stationary building environments",
        "authors": "Avisek Naug, Marcos Quiñones-Grueiro, Gautam Biswas",
        "published": "2020-11-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3427773.3427867"
    },
    {
        "id": 25683,
        "title": "Subgoal Reachability in Goal Conditioned Hierarchical Reinforcement Learning",
        "authors": "Michał Bortkiewicz, Jakub Łyskawa, Paweł Wawrzyński, Mateusz Ostaszewski, Artur Grudkowski, Bartłomiej Sobieski, Tomasz Trzciński",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012326200003636"
    },
    {
        "id": 25684,
        "title": "Deep W-Networks: Solving Multi-Objective Optimisation Problems with Deep Reinforcement Learning",
        "authors": "Jernej Hribar, Luke Hackett, Ivana Dusparic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011610300003393"
    },
    {
        "id": 25685,
        "title": "A History-based Framework for Online Continuous Action Ensembles in Deep Reinforcement Learning",
        "authors": "Renata Oliveira, Wouter Caarls",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010199005800588"
    },
    {
        "id": 25686,
        "title": "Reinforcement Learning-Based Model Reduction for Partial Differential Equations: Application to the Burgers Equation",
        "authors": "Mouhacine Benosman, Ankush Chakrabarty, Jeff Borggaard",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_11"
    },
    {
        "id": 25687,
        "title": "Targeted Adversarial Attacks on Deep Reinforcement Learning Policies via Model Checking",
        "authors": "Dennis Gross, Thiago Simão, Nils Jansen, Guillermo Pérez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011693200003393"
    },
    {
        "id": 25688,
        "title": "Opponent Modelling in the Game of Tron using Reinforcement Learning",
        "authors": "Stefan J. L. Knegt, Madalina M. Drugan, Marco A. Wiering",
        "published": "2018",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006536300290040"
    },
    {
        "id": 25689,
        "title": "Adversarial Reinforcement Learning in a Cyber Security Simulation",
        "authors": "Richard Elderman, Leon J. J. Pater, Albert S. Thie, Madalina M. Drugan, Marco M. Wiering",
        "published": "2017",
        "citations": 34,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006197105590566"
    },
    {
        "id": 25690,
        "title": "Deep Reinforcement Learning for Advanced Energy Management of Hybrid Electric Vehicles",
        "authors": "Roman Liessner, Christian Schroer, Ansgar Dietermann, Bernard Bäker",
        "published": "2018",
        "citations": 33,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006573000610072"
    },
    {
        "id": 25691,
        "title": "Applying Reinforcement Learning and Supervised Learning Techniques to Play Hearthstone",
        "authors": "Ilya Kachalsky, Ilya Zakirzyanov, Vladimir Ulyantsev",
        "published": "2017-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2017.00016"
    },
    {
        "id": 25692,
        "title": "Multi-UAV Navigation for Partially Observable Communication Coverage by Graph Reinforcement Learning",
        "authors": "Zhenhui Ye",
        "published": "No Date",
        "citations": 1,
        "abstract": "<div>In this paper, we aim to design a deep reinforcement learning(DRL) based control solution to navigate a swarm of unmanned aerial vehicles (UAVs) to fly around an unexplored target area under  provide optimal communication coverage for the ground mobile users. Compared with existing DRL-based solutions that mainly solve the problem with global observation and centralized training, a practical and efficient Decentralized Training and Decentralized Execution(DTDE) framework is desirable to train and deploy each UAV in a distributed manner. To this end, we propose a novel DRL approach named Deep Recurrent Graph Network(DRGN) that makes use of Graph Attention Network-based Flying Ad-hoc Network(GAT-FANET) to achieve inter-UAV communications and Gated Recurrent Unit (GRU) to record historical information. We conducted extensive experiments to define an appropriate structure for GAT-FANET and examine the performance of DRGN. The simulation results show that the proposed model outperforms four state-of-the-art DRL-based approaches and four heuristic baselines, and demonstrate the scalability, transferability, robustness, and interpretability of DRGN.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.15048273.v1"
    },
    {
        "id": 25693,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Jairam Jairam Naik",
        "published": "2023-9-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/zc264z"
    },
    {
        "id": 25694,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Mahendra Bhatu Gawali",
        "published": "2023-8-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/vzt201"
    },
    {
        "id": 25695,
        "title": "Review of: \"Applications of Deep reinforcement learning in MEMS and nanotechnology\"",
        "authors": "Safa Bhar Layeb",
        "published": "2023-8-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/q3d84t"
    },
    {
        "id": 25696,
        "title": "Reinforcement-Learning-Based IDS for 6LoWPAN",
        "authors": "Aryan Mohammadi Pasikhani, Andrew John Clark, Prosanta Gope",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The Routing Protocol for low power Lossy networks (RPL) is a\ncritical operational component of low power wireless personal area networks\nusing IPv6 (6LoWPANs). In this paper we propose a Reinforcement Learning (RL)\nbased IDS to detect various attacks on RPL in 6LoWPANs, including several\nunaddressed by current research. The proposed scheme can also detect previously\nunseen attacks and the presence of mobile intruders. The scheme is well suited\nto the resource constrained environments of our target networks.</p><br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16645000"
    },
    {
        "id": 25697,
        "title": "Inverse Reinforcement Learning for Marketing",
        "authors": "Igor Halperin",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3087057"
    },
    {
        "id": 25698,
        "title": "A Scalable Derivative-free Exploration Approach for Reinforcement Learning",
        "authors": "Xiong-Hui Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nExploration in complex environments remains a challenge for deep reinforcement learning (RL). Derivative-free optimization (DFO), which provides efficient black-box solution sampling mechanisms, is a potential way to address this issue. Recent studies which inject these mechanisms into RL have shown better exploration abilities than derivative-based policy optimization methods. However, we found that these methods suffer from low sample efficiency in high-dimensional policy parameter space, which limits the adaptability of these methods in complex tasks requiring large neural networks to represent well-performed policies. In this paper, we propose a scalable exploration algorithm based on derivative-free optimization, named Scalable Derivative-free Exploration (SDFE), which improves exploration in complex RL problems. SDFE handles the high-dimensional policy searching problem by optimizing policy in a transformed small parameter space of policy, rather than the original parameter space of policy neural network. SDFE is a general framework that is compatible with derivative-free optimization methods and off-policy policy-based algorithms. In experiments, we instantiate SDFE with two derivative-free optimization algorithms (SRACOS and CMAES) and three off-policy actor-critic algorithms (SAC, ACER, and DDPG), to show its efficiency and adaptability. We conduct experiments on MuJoCo tasks and 42 pixel-based games in Atari, empirically verifying that SDFE can reach better performance with a competitive number of samples on both tasks.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1973394/v2"
    },
    {
        "id": 25699,
        "title": "Development Tools",
        "authors": "Rafael Ris-Ala",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-37345-9_4"
    },
    {
        "id": 25700,
        "title": "What is dopamine doing in model-based reinforcement learning?",
        "authors": "Thomas Akam, Mark Walton",
        "published": "No Date",
        "citations": 0,
        "abstract": "Experiments have implicated dopamine in model-based reinforcement learning (RL). These findings are unexpected as dopamine is thought to encode a reward prediction error (RPE), which is the key teaching signal in model-free RL. Here we examine two possible accounts for dopamine’s involvement in model-based RL: the first that dopamine neurons carry a prediction error used to update a type of predictive state representation called a successor representation, the second that two well established aspects of dopaminergic activity, RPEs and surprise signals, can together explain dopamine’s involvement in model-based RL.",
        "link": "http://dx.doi.org/10.31234/osf.io/z2fmw"
    },
    {
        "id": 25701,
        "title": "Deep Learning for Natural Language Processing",
        "authors": "Yuan Wang, Zekun Li, Zhenyu Deng, Huiling Song, Jucheng Yang",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "With the constantly growing number of topical or sentiment-bearing texts and dialogs on the Web, the demand for automatic language or text analysis algorithms continues to expand. This chapter discusses about advanced deep learning techniques for classical and hot research directions in the field of natural language processing, including text classification, sentiment analysis, and task-oriented dialog systems. In text classification, we focus on tasks of multi-label text classification and extreme multi-label text classification, which allow for automatically annotates the texts with the most relevant labels. In sentiment analysis, we look into aspect-based sentiment analysis that makes automatic extraction of fine-grained sentiment information from texts, and multimodal sentiment analysis that classifies people’s opinions or attitudes from multimedia data through fusion techniques. In dialog system, we introduce how deep learning techniques work in pipeline mode and end-to-end mode for task-oriented dialog system. In this chapter, the rapidly evolving state of the research on the three topics is reviewed. Furthermore, trends in the research on deep learning for natural language processing are identified, and a discussion about future advances is provided.",
        "link": "http://dx.doi.org/10.5772/intechopen.112550"
    },
    {
        "id": 25702,
        "title": "Defensive Schemes for Cyber Security of Deep Reinforcement Learning",
        "authors": "Jiamiao Zhao, Fei Hu, Xiali Hei",
        "published": "2023-3-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003187158-12"
    },
    {
        "id": 25703,
        "title": "Fast Learning Cognitive Radios in Underlay Dynamic Spectrum Access: Integration of Transfer Learning into Deep Reinforcement Learning",
        "authors": "Fatemeh Shah-Mohammadi, Andres Kwasinski",
        "published": "2020-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wts48268.2020.9198732"
    },
    {
        "id": 25704,
        "title": "Learning Urban Driving Policies using Deep Reinforcement Learning",
        "authors": "Tanmay Agarwal, Hitesh Arora, Jeff Schneider",
        "published": "2021-9-19",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9564412"
    },
    {
        "id": 25705,
        "title": "Deep Reinforcement Learning",
        "authors": "",
        "published": "2022-12-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119910695.ch4"
    },
    {
        "id": 25706,
        "title": "Integrating Learning and Planning",
        "authors": "Huaqing Zhang, Ruitong Huang, Shanghang Zhang",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_9"
    },
    {
        "id": 25707,
        "title": "Learning a Belief Representation for Delayed Reinforcement Learning",
        "authors": "Pierre Liotet, Erick Venneri, Marcello Restelli",
        "published": "2021-7-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9534358"
    },
    {
        "id": 25708,
        "title": "Reinforcement learning",
        "authors": "Donald J. Norris",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-5174-4_9"
    },
    {
        "id": 25709,
        "title": "Interpretable Reinforcement Learning with Multilevel Subgoal Discovery",
        "authors": "Alexander Demin, Denis Ponomaryov",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00043"
    },
    {
        "id": 25710,
        "title": "Toward ensuring better learning performance in reinforcement learning",
        "authors": "Menglei Zhang, Yukikazu Nakamoto",
        "published": "2022-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/candarw57323.2022.00037"
    },
    {
        "id": 25711,
        "title": "Reinforcement Learning for Algorithmic Trading",
        "authors": "",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009028943.014"
    },
    {
        "id": 25712,
        "title": "Prior-Information Enhanced Reinforcement Learning for Energy Management Systems",
        "authors": "Th´eo Zangato, Aomar Osmani, Pegah Alizadeh",
        "published": "2024-1-27",
        "citations": 0,
        "abstract": "Amidst increasing energy demands and growing environmental concerns, the promotion of sustainable and energy-efficient practices has become imperative. This paper introduces a reinforcement learning-based technique for optimizing energy consumption and its associated costs, with a focus on energy management systems. A three-step approach for the efficient management of charging cycles in energy storage units within buildings is presented combining RL with prior knowledge. A unique strategy is adopted: clustering building load curves to discern typical energy consumption patterns, embedding domain knowledge into the learning algorithm to refine the agent’s action space and predicting of future observations to make real-time decisions. We showcase the effectiveness of our method using real-world data. It enables controlled exploration and efficient training of Energy Management System (EMS) agents. When compared to the benchmark, our model reduces energy costs by up to 15%, cutting down consumption during peak periods, and demonstrating adaptability across various building consumption profiles.",
        "link": "http://dx.doi.org/10.5121/csit.2024.140207"
    },
    {
        "id": 25713,
        "title": "Learning to Drive via Apprenticeship Learning and Deep Reinforcement Learning",
        "authors": "Wenhui Huang, Francesco Braghin, Zhuo Wang",
        "published": "2019-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai.2019.00220"
    },
    {
        "id": 25714,
        "title": "Reinforcement Twinning: From Digital Twins to Model-Based Reinforcement Learning",
        "authors": "Lorenzo Schena, Pedro Afonso  Duque Morgado Marques, Romain Poletti, Samuel Ahizi, Jan Van den Berghe, Miguel Alfonso Mendez",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4761240"
    },
    {
        "id": 25715,
        "title": "Trust and Security of Deep Reinforcement Learning",
        "authors": "Yen-Hung Chen, Mu-Tien Huang, Yuh-Jong Hu",
        "published": "2023-3-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003187158-7"
    },
    {
        "id": 25716,
        "title": "Delaying feedback compensates for impaired reinforcement learning in developmental dyslexia",
        "authors": "Yafit Gabay",
        "published": "2021-11",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.nlm.2021.107518"
    },
    {
        "id": 25717,
        "title": "Longer duration intertrial intervals without visual stimuli have reinforcement value and increase the rate of reinforcement and punishment learning in computer-based discriminations in humans",
        "authors": "Xiaojin Ma, Blair Bracciano, Nicole Hoppas, Sydney Zimmerman, Charles L. Pickens",
        "published": "2023-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.lmot.2022.101867"
    },
    {
        "id": 25718,
        "title": "A Reinforcement Learning System with Multi-Layered Fuzzy Neural Network",
        "authors": "Takashi Kuremoto, Hiroki Matsusaka, Masanao Obayashi, Shingo Mabu, Kunikazu Kobayashi",
        "published": "2017",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12792/icisip2017.081"
    },
    {
        "id": 25719,
        "title": "A Recommendation Module based on Reinforcement Learning to an Intelligent Tutoring System for Software Maintenance",
        "authors": "Rodrigo Francisco, Flávio Silva",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011083900003182"
    },
    {
        "id": 25720,
        "title": "Inverse Reinforcement Learning for Multiplayer Non-Zero-Sum Games",
        "authors": "Bosen Lian, Wenqian Xue, Frank L. Lewis, Hamidreza Modares, Bahare Kiumarsi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45252-9_8"
    },
    {
        "id": 25721,
        "title": "A Hybrid Dynamical Systems Perspective on Reinforcement Learning for Cyber-Physical Systems: Vistas, Open Problems, and Challenges",
        "authors": "Jorge I. Poveda, Andrew R. Teel",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_24"
    },
    {
        "id": 25722,
        "title": "Multi-agent Reinforcement Learning for Bargaining under Risk and Asymmetric Information",
        "authors": "Kyrill Schmid, Lenz Belzner, Thomy Phan, Thomas Gabor, Claudia Linnhoff-Popien",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008913901440151"
    },
    {
        "id": 25723,
        "title": "Transferable Reinforcement Learning for Smart Homes",
        "authors": "Xiangyu Zhang, Xin Jin, Charles Tripp, David J. Biagioni, Peter Graf, Huaiguang Jiang",
        "published": "2020-11-17",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3427773.3427865"
    },
    {
        "id": 25724,
        "title": "Human-Aligned Trading by Imitative Multi-Loss Reinforcement Learning",
        "authors": "zhengxin ye, Björn  W. Schuller",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4352099"
    },
    {
        "id": 25725,
        "title": "A web-based implementation of the two-step human reinforcement learning task",
        "authors": "Isaac Kinley",
        "published": "No Date",
        "citations": 0,
        "abstract": "I present a web-based implementation of the two-step human reinforcement learning task for the jsPsych library. The two-step task allows researchers to estimate the contribution of model-based and model-free reinforcement learning strategies to human decision making. The current tool enables this task to be easily incorporated into a jsPsych experiment and includes a set of interactive instructions for participants. Moreover, the tool is distributed with example R code for analyzing the data that it produces.",
        "link": "http://dx.doi.org/10.31234/osf.io/b5x3w"
    },
    {
        "id": 25726,
        "title": "Robust Well-Production Control Using Surrogate Assisted Reinforcement Learning",
        "authors": "A. Dixit, A. Elsheikh",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202244101"
    },
    {
        "id": 25727,
        "title": "Assessing the Performance of Reinforcement Learning on Passive RRAM Crossbar Array",
        "authors": "Arjun Tyagi, Shubham Sahay",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning is a promising approach that can allow machines to acquire knowledge and solve problems without the intervention of humans. However, the current implementation of reinforcement learning algorithms on standard complementary metal-oxide-semiconductor based platform constraints the performance due to von Neumann architecture, which leads to increased energy consumption and latency. To this end, in this work, we propose an extremely area- and energy-efficient implementation of Monte Carlo learning on passive resistive random access memory (RRAM) crossbar array considering the non-ideal hardware artifacts such as device-to-device variation, noise and endurance failure. To illustrate the capabilities of our implementation, we considered the classical control problem of cart-pole. Our results indicate that the proposed passive RRAM crossbar-based implementation of Monte Carlo learning not only outperforms prior digital and active 1 Transistor - 1 RRAM (1T1R) crossbar-based implementation by more than five orders of magnitude in terms of area but is also robust against spatial and temporal variations and endurance failure of RRAM devices.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22347277"
    },
    {
        "id": 25728,
        "title": "Reinforcement Learning in Noise Enabled Simulated Environment",
        "authors": "Tiju George Varghese, Bismin V Sherif",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nReinforcement learning (RL) is a subfield of Artificial intelligence concerned with how agents are trained to become intelligent, so as to take optimum actions in a stochastic environment, in order to maximize the cumulative reward in the long term. The main objective in this field is to find a balance between exploration and exploitation based search strategy. In this context a control problem refers to an objective function of environment state at different points in time and control variables. Optimal control deals with the problem of finding a set of control variable that helps in maximizing the cumulative reward under certain constraints. In case of CartPole, also known as inverted pendulum, the goal here is to move the cart to the left or to the right, so that the pole (pendulum) can stand within a certain angle as long as possible. This work aims to design and develop an intelligent agent for CartPole balancing task using reinforcement learning technique. The actor-critic based reinforcement algorithm was used for training the proposed agent. The results of the proposed agent is compared with a genetic algorithm (GA) based agent. The proposed agent was found to score higher rewards than GA based agent and the training process was significantly faster compared to GA based agent.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2478826/v1"
    },
    {
        "id": 25729,
        "title": "Deep Reinforcement Learning in Human Activity Recognition: A Survey",
        "authors": "Bahareh Nikpour, Dimitrios Sinodinos, Narges Armanfard",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>  Human activity recognition is a popular research field in computer vision that has already been widely studied. However, it is still an active research field since it plays an important role in many current and emerging real-world intelligent systems, like visual surveillance and human-computer interaction. Deep Reinforcement Learning (DRL) has recently been employed to address the activity recognition problem with various purposes, such as finding attention in video data or obtaining the best network structure. DRL-based human activity recognition has only been around for a short time, and it is a challenging, novel field of study. Therefore, to facilitate further research in this field, we have constructed a comprehensive survey on activity recognition methods that incorporate deep reinforcement learning. Towards the end of this survey, we summarize key challenges and open problems in this area that can be addressed by researchers in the future.  </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19172369"
    },
    {
        "id": 25730,
        "title": "The mid-lateral cerebellum is necessary for reinforcement learning",
        "authors": "Naveen Sendhilnathan, Michael E. Goldberg",
        "published": "No Date",
        "citations": 5,
        "abstract": "SummaryThe cerebellum has long been considered crucial for supervised motor learning and its optimization1-3. However, new evidence has also implicated the cerebellum in reward based learning4-8, executive function9-12, and frontal-like clinical deficits13. We recently showed that the simple spikes of Purkinje cells (P-cells) in the mid-lateral cerebellar hemisphere (Crus I and II) encode a reinforcement error signal when monkeys learn to associate arbitrary symbols with hand movements4. However, it is unclear if the cerebellum is necessary for any process beyond motor learning. To investigate if the mid-lateral cerebellum is actually necessary for learning visuomotor associations, we reversibly inactivated the mid-lateral cerebellum of two primates with muscimol while they learned to associate arbitrary symbols with hand movements. Here we show that cerebellar inactivation impaired the monkey’s ability to learn new associations, although it had no effect on the monkeys’ performance on a task with overtrained symbols. A computational model corroborates our results. Cerebellar inactivation increased the reaction time, but there were no deficits in any motor kinematics such as the hand movement, licking or eye movement. There was no loss of function when we inactivated a more anterior region of the cerebellum that is implicated in motor control. We suggest that the mid-lateral cerebellum, which provides a reinforcement learning error signal4, is necessary for visuomotor association learning. Our results have implications for the involvement of cerebellum in cognitive control, and add critical constraints to brain models of non-motor learning14,15.",
        "link": "http://dx.doi.org/10.1101/2020.03.20.000190"
    },
    {
        "id": 25731,
        "title": "Multi-agent  Reinforcement  Learning-based Capacity  Planning  for On-demand  Vehicular  Fog  Computing",
        "authors": "Wencan Mao",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Fog computing reduces network latency by moving computational resources close to where the data is generated. Vehicular fog computing (VFC) is an emerging computing paradigm where fog nodes deployed on moving vehicles (i.e., vehicular fog nodes (VFNs)) complement stationary fog nodes (e.g., the ones co-located with cellular base stations) to satisfy the spatiotemporally varying demand for computing resources in a cost-efficient manner. On-demand VFC (ODVFC) supports dynamic routing of VFNs, with the aim of fulfilling the spatiotemporally varying demand for computational resources in a cost-efficient manner. Different from previous works on capacity planning and vehicle routing that utilize compute-intensive optimization methods such as integer linear programming (ILP), this paper explores the feasibility of applying reinforcement learning to dynamic capacity planning in a time-efficient manner. Specifically, we propose to apply multi-agent reinforcement learning (MARL) with actor-critic methods to train the VFN routing policies. This approach allows distributed VFNs to cooperatively maximize the techno-economic performance of ODVFC. For evaluation, we built an open-source VFC simulation platform that integrates vehicular traffic simulation with 5G NR V2X and MARL environment. Compared with decentralized learning (i.e., each VFN independently learns its routing policy), centralized learning (i.e., using a global agent for VFN routing), and ILP methods, our proposal proves to achieve 8.3% higher revenue and 13.2% higher number of served tasks than decentralized training; and it has 40.6% and 83% lower execution time than centralized learning and ILP, respectively, with only 14% lower revenue than both. It is also scalable to real-life scenarios with a great number of users and VFNs.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24184047.v1"
    },
    {
        "id": 25732,
        "title": "Efficient Hierarchical Storage Management Framework Empowered By Reinforcement Learning",
        "authors": "Tianru Zhang, Andreas Hellander, Salman Toor",
        "published": "No Date",
        "citations": 0,
        "abstract": "With the rapid development of big data and cloud computing, data management has become increasingly challenging. Over the years, a number of frameworks for data management and storage with various characteristics and features have become available. Most of these are highly efficient, but ultimately create data silos. It becomes difficult to move and work coherently with data as new requirements emerge as no single framework can efficiently fulfill the data management needs of diverse applications. A possible solution is to design smart and efficient hierarchical (multi-tier) storage solutions. A hierarchical storage system (HSS) is a meta solution that consists of different storage frameworks organized as a jointly constructed large storage pool. It brings a number of benefits including better utilization of the storage, cost-efficiency, and use of different features provided by the underlying storage frameworks. In order to maximize the gains of hierarchical storage solutions, it is important that they include intelligent and autonomous mechanisms for data management grounded in the features of the different underlying frameworks. These decisions should be made according to the characteristics of the dataset, tier status, and access patterns. These are highly dynamic parameters and defining a policy based on the mentioned parameters is a non-trivial task. This paper presents an open-source hierarchical storage framework with a dynamic migration policy based on reinforcement learning (RL). We present a mathematical model, a software architecture, and an implementation based on both simulations and a live cloud-based environment. We compare the proposed RL-based strategy to a baseline of three rule-based policies, showing that the RL-based policy achieves significantly higher efficiency and optimal data distribution in different scenarios compared to the dynamic rule-based policies.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19181666.v1"
    },
    {
        "id": 25733,
        "title": "Reinforcement Learning for Cyber-Physical Systems",
        "authors": "Chong Li, Meikang Qiu",
        "published": "2019-2-22",
        "citations": 50,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620"
    },
    {
        "id": 25734,
        "title": "A Study on Multi-agent Reinforcement Learning for Autonomous Distribution Vehicles",
        "authors": "Serap Ergün",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nA self-driving car, also called an autonomous vehicle, is capable of sensing the environment and driving without any human intervention, utilizing software algorithms and a fully automated driving system. This allows the vehicle to manage external conditions similar to a human. Safe and efficient delivery services require road controls and road restrictions to prevent accidents and reduce damage from unexpected technical failures. This study formulates the Autonomous Delivery Vehicles optimization problem and proposes a multi-agent reinforcement learning method using the analytically calculated shortest path information.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2180296/v1"
    },
    {
        "id": 25735,
        "title": "Review for \"Mental health analysis for college students based on pattern recognition and reinforcement learning\"",
        "authors": "",
        "published": "2023-6-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/itl2.453/v2/review1"
    },
    {
        "id": 25736,
        "title": "Reinforcement Learning Based Decision Support Tool For  Epidemic Control",
        "authors": "Mohamed-Amine CHADI, Hajar MOUSANNIF",
        "published": "No Date",
        "citations": 0,
        "abstract": "Rationale: Covid-19 Is Certainly One Of The Worst Pandemics Ever. In The Absence Of A Vaccine, Classical Epidemiological Measures Such As Testing In Order To Isolate The Infected People, Quarantine And Social Distancing Are Ways To Reduce The Growing Speed Of New Infections As Much As Possible And As Soon As Possible, But With A Cost To Economic And Social Disruption. It Is Therefore A Challenge To Implement Timely And Appropriate Public Health Interventions. Objective: This Study Investigates A Reinforcement Learning Based Approach To Incrementally Learn How Much Intensity Of Each Public Health Intervention Should Be Applied At Each Period In A Given Region. Methods: First We Define The Basic Components Of A Reinforcement Learning (Rl) Set Up (I.E., States, Reward, Actions, And Transition Function), This Represents The Learning Environment For The Agent (I.E., An Ai-Model). Then We Train Our Agent Using Rl In An Online Fashion, Using A Reinforcement Learning Algorithm Known As Reinforce. Finally, A Developed Flow Network, Serving As An Epidemiological Model Is Used To Visualize The Results Of The Decisions Taken By The Agent Given Different Epidemic And Demographic State Scenarios. Main Results: After A Relatively Short Period Of Training, The Agent Starts Taking Reasonable Actions Allowing A Balance Between The Public Health And Economic Considerations. In Order To Test The Developed Tool, We Ran The Rl-Agent On Different Regions (Demographic Scale) And Recorded The Output Policy Which Was Still Consistent With The Training Performance. The Flow Network Used To Visualize The Results Of The Simulation Is Considerably Useful Since It Shows A High Correlation Between The Simulated Results And The Real Case Scenarios. Conclusion: This Work Shows That Reinforcement Learning Paradigm Can Be Used To Learn Public Health Policies In Complex Epidemiological Models. Moreover, Through This Experiment, We Demonstrate That The Developed Model Can Be Very Useful If Fed In With Real Data. Future Work: When Treating Trade-Off Problems (Balance Between Two Goals) Like Here, Engineering A Good Reward (That Encapsulates All Goals) Can Be Difficult, Therefore Future Work Might Tackle This Problem By Investigating Other Techniques Such As Inverse Reinforcement Learning And Human-In-The-Loop. Also, Regarding The Developed Epidemiological Model, We Aim To Gather Proper Real Data That Can Be Used To Make The Training Environment More Realistic, As Well As To Apply It For Network Of Regions Instead Of A Single Region.",
        "link": "http://dx.doi.org/10.31234/osf.io/tcr8s"
    },
    {
        "id": 25737,
        "title": "Reinforcement Learning Agent Design and Optimization with Bandwidth Allocation Model",
        "authors": "Rafael F. Reale, Joberto Martins",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning (RL) is currently used in various real-life applications. RL-based solutions have the potential to generically address problems, including the ones that are difficult to solve with heuristics and meta-heuristics and, in addition, the set of problems and issues where some intelligent or cognitive approach is required. However, reinforcement learning agents require a not straightforward design and have important design issues. RL agent design issues include the target problem modeling, state-space explosion, the training process, and agent efficiency. Research currently addresses these issues aiming to foster RL dissemination. A BAM model, in summary, allocates and shares resources with users. There are three basic BAM models and several hybrids that differ in how they allocate and share resources among users. This paper addresses the issue of an RL agent design and efficiency. The RL agent's objective is to allocate and share resources among users. The paper investigates how a BAM model can contribute to the RL agent design and efficiency. The AllocTC-Sharing (ATCS) model is analytically described and simulated to evaluate how it mimics the RL agent operation and how the ATCS can offload computational tasks from the RL agent. The essential argument researched is whether algorithms integrated with the RL agent design and operation have the potential to facilitate agent design and optimize its execution. The ATCS analytical model and simulation presented demonstrate that a BAM model offloads agent tasks and assists the agent's design and optimization.  </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22118471.v1"
    },
    {
        "id": 25738,
        "title": "Decision letter: Offline replay supports planning in human reinforcement learning",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.32548.024"
    },
    {
        "id": 25739,
        "title": "Peer Review #2 of \"LORM: a novel reinforcement learning framework for biped gait control (v0.2)\"",
        "authors": "",
        "published": "2022-3-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.927v0.2/reviews/2"
    },
    {
        "id": 25740,
        "title": "Similarities Between Policy Gradient Methods (PGM) in Reinforcement Learning (RL) and Supervised Learning (SL)",
        "authors": "Eric Benhamou",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3391216"
    },
    {
        "id": 25741,
        "title": "The Application of Reinforcement Learning in Amazons",
        "authors": "Hebin Huang, Shuqin Li",
        "published": "2019-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlbdbi48998.2019.00083"
    },
    {
        "id": 25742,
        "title": "Can a reinforcement learning agent practice before it starts learning?",
        "authors": "Minwoo Lee, Charles W. Anderson",
        "published": "2017-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7966361"
    },
    {
        "id": 25743,
        "title": "Learning Policies for Neural Network Architecture Optimization Using Reinforcement Learning",
        "authors": "Raghav Vadhera, Manfred Huber",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "Deep learning systems tend to be very sensitive to the specific network architecture both in terms of learning ability and performance of the learned solution. This, together with the difficulty of tuning neural network architectures leads to a need for automatic network optimization. Previous work largely optimizes a network for one specific problem using architecture search, requiring significant amounts of time training different architectures during optimization. To address this and to open up the potential for transfer across tasks, this paper presents a novel approach that uses Reinforcement Learning to learn a policy for network optimization in a derived architecture embedding space that incrementally optimizes the network for the given problem. By utilizing policy learning and an abstract problem embedding, this approach brings the promise of transfer of the policy across problems and thus the potential optimization of networks for new problems without the need for excessive additional training. For an initial evaluation of the base capabilities, experiments for a standard classification problem are performed in this paper, showing the ability of the approach to optimize the architecture for a specific problem within a given rang of fully connected networks, and indicating its potential for learning effective policies to automatically improve network architectures.",
        "link": "http://dx.doi.org/10.32473/flairs.36.133380"
    },
    {
        "id": 25744,
        "title": "Optimal Mobile Relay Beamforming Via Reinforcement Learning",
        "authors": "Konstantinos Diamantaras, Athina Petropulu",
        "published": "2019-10",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp.2019.8918745"
    },
    {
        "id": 25745,
        "title": "Learning to Adapt - Deep Reinforcement Learning in Treatment-Resistant Prostate Cancer",
        "authors": "Kit Gallagher, Maximillian Strobl, Robert Gatenby, Philip Maini, Alexander Anderson",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractStandard-of-care treatment regimes have long been designed to for maximal cell kill, yet these strategies often fail when applied to treatment–resistant tumors, resulting in patient relapse. Adaptive treatment strategies have been developed as an alternative approach, harnessing intra-tumoral competition to suppress the growth of treatment resistant populations, to delay or even prevent tumor progression. Following recent clinical implementations of adaptive therapy, it is of significant interest to optimise adaptive treatment protocols. We propose the application of deep reinforcement learning models to provide generalised solutions within adaptive drug scheduling, and demonstrate this framework can outperform the current adaptive protocols, extending time to progression by up to a quarter. This strategy is robust to varying model parameterisations, and the underlying tumor model. We demonstrate the deep learning framework can produce interpretable, adaptive strategies based on a single tumor burden threshold, replicating and informing a novel, analytically–derived optimal treatment strategy with no knowledge of the underlying mathematical tumor model. This approach is highly relevant beyond the simple, analytically–tractable tumor model considered here, demonstrating the capability of deep learning frameworks to help inform and develop treatment strategies in complex settings. Finally, we propose a pathway to integrate mechanistic modelling with DRL to tailor generalist treatment strategies to individual patients in the clinic, generating personalised treatment schedules that consistently outperform clinical standard-of-care protocols.",
        "link": "http://dx.doi.org/10.1101/2023.04.28.538766"
    },
    {
        "id": 25746,
        "title": "A reinforcement learning approach to network routing based on adaptive learning rates and route memory",
        "authors": "Maksim Kavalerov, Yuliya Likhacheva, Yuliya Shilova",
        "published": "2017-3",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/secon.2017.7925316"
    },
    {
        "id": 25747,
        "title": "Knowledge Distillation and Reward Shaping of Deep Reinforcement Learning",
        "authors": "Yue Mei",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424889"
    },
    {
        "id": 25748,
        "title": "Learning at variable attentional load requires cooperation between working memory, meta-learning and attention-augmented reinforcement learning",
        "authors": "Thilo Womelsdorf, Marcus R. Watson, Paul Tiesinga",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractFlexible learning of changing reward contingencies can be realized with different strategies. A fast learning strategy involves using working memory of recently rewarded objects to guide choices. A slower learning strategy uses prediction errors to gradually update value expectations to improve choices. How the fast and slow strategies work together in scenarios with real-world stimulus complexity is not well known. Here, we disentangle their relative contributions in rhesus monkeys while they learned the relevance of object features at variable attentional load. We found that learning behavior across six subjects is consistently best predicted with a model combining (i) fast working memory (ii) slower reinforcement learning from differently weighted positive and negative prediction errors, as well as (iii) selective suppression of non-chosen feature values and (iv) a meta-learning mechanism adjusting exploration rates based on a memory trace of recent errors. These mechanisms cooperate differently at low and high attentional loads. While working memory was essential for efficient learning at lower attentional loads, enhanced weighting of negative prediction errors and meta-learning were essential for efficient learning at higher attentional loads. Together, these findings pinpoint a canonical set of learning mechanisms and demonstrate how they cooperate when subjects flexibly adjust to environments with variable real-world attentional demands.Significance statementLearning which visual features are relevant for achieving our goals is challenging in real-world scenarios with multiple distracting features and feature dimensions. It is known that in such scenarios learning benefits significantly from attentional prioritization. Here we show that beyond attention, flexible learning uses a working memory system, a separate learning gain for avoiding negative outcomes, and a meta-learning process that adaptively increases exploration rates whenever errors accumulate. These subcomponent processes of cognitive flexibility depend on distinct learning signals that operate at varying timescales, including the most recent reward outcome (for working memory), memories of recent outcomes (for adjusting exploration), and reward prediction errors (for attention augmented reinforcement learning). These results illustrate the specific mechanisms that cooperate during cognitive flexibility.",
        "link": "http://dx.doi.org/10.1101/2020.09.27.315432"
    },
    {
        "id": 25749,
        "title": "Inverse Reinforcement Learning for Two-Player Zero-Sum Games",
        "authors": "Bosen Lian, Wenqian Xue, Frank L. Lewis, Hamidreza Modares, Bahare Kiumarsi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45252-9_7"
    },
    {
        "id": 25750,
        "title": "Service Selection for Service-Oriented Architecture using Off-line Reinforcement Learning in Dynamic Environments",
        "authors": "Yuya Kondo, Ahmed Moustafa",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010872400003116"
    },
    {
        "id": 25751,
        "title": "Exploring the Synergy of Prompt Engineering and Reinforcement Learning for Enhanced Control and Responsiveness in Chat GPT",
        "authors": "",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "Conversational AI systems, such as Chat GPT, have exhibited remarkable performance in generating human-like responses. However, achieving consistent control and responsiveness remains a challenge. This research paper explores the combined effects of prompt engineering and reinforcement learning techniques in enhancing control and responsiveness in Chat GPT. Our experiments demonstrate significant improvements in the model’s performance across diverse domains and tasks. We discuss the implications of these findings for various real-world applications, such as customer support, virtual assistants, content generation, and education, and provide insights into future research directions and ethical considerations in the development of more reliable, controllable, and effective conversational AI systems.",
        "link": "http://dx.doi.org/10.33140/jeee.02.03.02"
    },
    {
        "id": 25752,
        "title": "Design of PID Controller using Reinforcement Learning",
        "authors": "Ashis De, Barun Mazumdar, Aritra Dhabal, Saikat Bhattacharjee, Aridip Maity, Sourav Bandopadhyay",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.55248/gengpi.4.1123.113004"
    },
    {
        "id": 25753,
        "title": "DRL4HFC: Deep Reinforcement Learning for Container-Based Scheduling in Hybrid Fog/Cloud System",
        "authors": "Ameni Kallel, Molka Rekik, Mahdi Khemakhem",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012356800003636"
    },
    {
        "id": 25754,
        "title": "Reinforcement Learning Patents: A Transatlantic Review",
        "authors": "Brian Haney",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3694680"
    },
    {
        "id": 25755,
        "title": "Design of Control Systems Using Active Uncertainty Reduction-Based Reinforcement Learning",
        "authors": "Zequn Wang, Narendra Patwardhan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.0001738v"
    },
    {
        "id": 25756,
        "title": "Efficient Hierarchical Storage Management Framework Empowered By Reinforcement Learning",
        "authors": "Tianru Zhang, Andreas Hellander, Salman Toor",
        "published": "No Date",
        "citations": 0,
        "abstract": "With the rapid development of big data and cloud computing, data management has become increasingly challenging. Over the years, a number of frameworks for data management and storage with various characteristics and features have become available. Most of these are highly efficient, but ultimately create data silos. It becomes difficult to move and work coherently with data as new requirements emerge as no single framework can efficiently fulfill the data management needs of diverse applications. A possible solution is to design smart and efficient hierarchical (multi-tier) storage solutions. A hierarchical storage system (HSS) is a meta solution that consists of different storage frameworks organized as a jointly constructed large storage pool. It brings a number of benefits including better utilization of the storage, cost-efficiency, and use of different features provided by the underlying storage frameworks. In order to maximize the gains of hierarchical storage solutions, it is important that they include intelligent and autonomous mechanisms for data management grounded in the features of the different underlying frameworks. These decisions should be made according to the characteristics of the dataset, tier status, and access patterns. These are highly dynamic parameters and defining a policy based on the mentioned parameters is a non-trivial task. This paper presents an open-source hierarchical storage framework with a dynamic migration policy based on reinforcement learning (RL). We present a mathematical model, a software architecture, and an implementation based on both simulations and a live cloud-based environment. We compare the proposed RL-based strategy to a baseline of three rule-based policies, showing that the RL-based policy achieves significantly higher efficiency and optimal data distribution in different scenarios compared to the dynamic rule-based policies.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19181666"
    },
    {
        "id": 25757,
        "title": "Equivariant Reinforcement Learning for Quadrotor UAV",
        "authors": "Beomyeol Yu, Taeyoung Lee",
        "published": "2023-5-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156379"
    },
    {
        "id": 25758,
        "title": "Controlling the Propagation of Mechanical Discontinuity using Reinforcement Learning",
        "authors": "Yuteng Jin, Siddharth Misra",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/essoar.10508031.1"
    },
    {
        "id": 25759,
        "title": "Deep Reinforcement Learning in Human Activity Recognition: A Survey",
        "authors": "Bahareh Nikpour, Dimitrios Sinodinos, Narges Armanfard",
        "published": "No Date",
        "citations": 1,
        "abstract": "<div>Human activity recognition is a popular research field in computer vision that has already been widely studied. However, it is still an active research field since it plays an important role in many current and emerging real world intelligent systems, like visual surveillance and human-computer interaction. Deep Reinforcement Learning (DRL) has recently been employed to address the activity recognition problem with various purposes, such as finding attention in video data or obtaining the best network structure. DRL-based human activity recognition has only been around for a short time, and it is a challenging, novel field[ of study. Therefore, to facilitate further research in this field, we have constructed a comprehensive survey on activity recognition methods that incorporate deep reinforcement learning. Towards the end of this survey, we summarize key challenges and open problems in this area that can be addressed by researchers in the future.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19172369.v2"
    },
    {
        "id": 25760,
        "title": "A New Unsupervised/Reinforcement Learning Method In Spiking Pattern Classification Networks",
        "authors": "Soheila Nazari",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nComputations adapted from the interactions of neurons in the nervous system may be a capable platform that can create powerful machines in terms of cognitive abilities such as real-time learning, decision-making and generalization. In this regard, here an intelligent machine based on the basic and approved mechanisms of the nervous system has been proposed. Therefore, the input layer of the presented machine is adapted from the retinal model and the middle layer and the output layer is composed of population of pyramidal neurons/ interneurons, AMPA/GABA receptors, and excitatory/inhibitory neurotransmitters. A machine that has a bio-adapted structure requires a learning based on biological evidence. Similarly, a new learning mechanism based on unsupervised (Power-STDP) and reinforcement learning procedure (Actor-Critic algorithm) was proposed which was called PSAC learning algorithm. Three challenging datasets MNIST, EMNIST, and CIFAR10 were used to confirm the performance of the proposed learning algorithm compared to deep and spiking networks, and respectively accuracies of 97.7%, 97.95% (digits) and 93.73% (letters), and 93.6% have been obtained, which shows an improvement in accuracy compared to previous spiking networks. In addition to being more accurate than the previous spike-based learning methods, the proposed learning approach shows a higher convergence speed in the training process. Although the obtained classification accuracies are slightly lower than deep networks, but higher training speed, low power consumption if implemented on neuromorphic platforms, and unsupervised learning are the advantages of the proposed network.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3560563/v1"
    },
    {
        "id": 25761,
        "title": "Environment Adversarial Reinforcement Learning",
        "authors": "John R. Cooper",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2024-2405"
    },
    {
        "id": 25762,
        "title": "Peer Review #1 of \"Architecture design of a reinforcement environment for learning sign languages (v0.1)\"",
        "authors": "",
        "published": "2021-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.740v0.1/reviews/1"
    },
    {
        "id": 25763,
        "title": "Review for \"Mental health analysis for college students based on pattern recognition and reinforcement learning\"",
        "authors": "",
        "published": "2023-5-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/itl2.453/v1/review2"
    },
    {
        "id": 25764,
        "title": "Autonomous PEV Charging Scheduling Using Deep-Q Network and Dyna-Q Reinforcement Learning",
        "authors": "Fan Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper proposes a demand response method that aims to reduce the long-term charging cost of a plug-in electric vehicle (PEV) while overcoming obstacles such as the stochastic nature of the user’s driving be- haviour, traffic condition, energy usage, and energy price. The problem is formulated as a Markov Decision Process (MDP) with unknown transition probabilities and solved using deep reinforcement learning (RL) techniques. Existing methods using machine learning either requires initial user behaviour data, or converges far too slowly. This method does not require any initial data on the PEV owner’s driving behaviour and shows improvement on learning speed. A combination of both model-based and model-free learning called Dyna-Q algorithm is utilized. Every time a real experience is obtained, the model is updated and the RL agent will learn from both real data set and “imagined” experience from the model. Due to the vast amount of state space, a table-look up method is impractical and a value approximation method using deep neural networks is employed for estimating the long-term expected reward of all state-action pairs. An average of historical price is used to predict future price. Three different user behaviour without any initial PEV owner behaviour data are simulated. A purely model-free DQN method is shown to run out of battery during trips very often, and is impractical for real life charging scenarios. Simulation results demonstrate the effectiveness of the proposed approach and its ability to reach an optimal policy quicker while avoiding state of charge (SOC) depleting during trips when compared to existing PEV charging schemes for all three different users profiles.</p>",
        "link": "http://dx.doi.org/10.32920/ryerson.14661024"
    },
    {
        "id": 25765,
        "title": "Learning to Drive Using Sparse Imitation Reinforcement Learning",
        "authors": "Yuci Han, Alper Yilmaz",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956121"
    },
    {
        "id": 25766,
        "title": "Pseudo-learning effects in reinforcement learning model-based analysis: A problem of misspecification of initial preference",
        "authors": "Kentaro Katahira, Bai Yu, Takashi Nakao",
        "published": "No Date",
        "citations": 4,
        "abstract": "In this study, we investigate a methodological problem of reinforcement-learning (RL) model-based analysis of choice behavior. We show that misspecification of the initial preference of subjects can significantly affect the parameter estimates, model selection, and conclusions of an analysis. This problem can be considered to be an extension of the methodological flaw in the free-choice paradigm (FCP), which has been controversial in studies of decision making. To illustrate the problem, we conducted simulations of a hypothetical reward-based choice experiment. The simulation shows that the RL model-based analysis reports an apparent preference change if hypothetical subjects prefer one option from the beginning, even when they do not change their preferences (i.e., via learning). We discuss possible solutions for this problem.",
        "link": "http://dx.doi.org/10.31234/osf.io/a6hzq"
    },
    {
        "id": 25767,
        "title": "A Deep Reinforcement Learning Based Emotional State Analysis Method for Online Learning",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23977/jaip.2022.050210"
    },
    {
        "id": 25768,
        "title": "A Pareto Fronts Learning Evolutionary Algorithm Based on Reinforcement Learning for Constrained Multiobjective Optimization",
        "authors": "Yuhang Hu, Yuelin Qu, Ying Huang, Wei Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4706049"
    },
    {
        "id": 25769,
        "title": "Aircraft Control Method Based on Deep Reinforcement Learning",
        "authors": "Yan Zhen, Mingrui Hao",
        "published": "2020-11-20",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls49620.2020.9275205"
    },
    {
        "id": 25770,
        "title": "Differentiating effort-related aspects of motivation from reinforcement learning: commentary on Soder et al. “Dose–response effects of d-amphetamine on effort-based decision-making and reinforcement learning”",
        "authors": "John D. Salamone",
        "published": "2021-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1038/s41386-020-00930-2"
    },
    {
        "id": 25771,
        "title": "A Study Toward Multi-Objective Multiagent Reinforcement Learning Considering Worst Case and Fairness Among Agents",
        "authors": "Toshihiro Matsui",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011687100003393"
    },
    {
        "id": 25772,
        "title": "Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms on a Building Energy Demand Coordination Task",
        "authors": "Gauraang Dhamankar, Jose R. Vazquez-Canteli, Zoltan Nagy",
        "published": "2020-11-17",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3427773.3427870"
    },
    {
        "id": 25773,
        "title": "Flexible Exploration Strategies in Multi-Agent Reinforcement Learning for Instability by Mutual Learning",
        "authors": "Yuki Miyashita, Toshiharu Sugawara",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00100"
    },
    {
        "id": 25774,
        "title": "Deep Reinforcement Learning for Resource Management in Network Function Virtualization",
        "authors": "Bheema Shanker Neyigapula",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNetwork Function Virtualization (NFV) has revolutionized the networking land- scape by enabling the flexible deployment of network services through software. However, efficient resource management remains a critical challenge in NFV envi- ronments due to the dynamic nature of network conditions and workloads. This research paper proposes a novel approach to address this challenge by employing deep reinforcement learning techniques for resource management in NFV. The paper presents a comprehensive framework that combines deep reinforcement learning principles with NFV’s resource allocation problem. By leveraging this approach, intelligent and adaptive resource allocation strategies are developed, enhancing NFV performance, reducing operational costs, and optimizing resource utilization. Through rigorous experimentation and analysis, the paper evaluates the proposed framework’s effectiveness, comparing it with existing methods and showcasing its potential to transform NFV resource management.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3239087/v1"
    },
    {
        "id": 25775,
        "title": "Attention and reinforcement learning in Parkinson’s disease",
        "authors": "Brónagh McCoy, Rebecca P. Lawson, Jan Theeuwes",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTDopamine is known to be involved in several important cognitive processes, most notably in learning from rewards and in the ability to attend to task-relevant aspects of the environment. Both of these features of dopaminergic signalling have been studied separately in research involving Parkinson’s disease (PD) patients, who exhibit diminished levels of dopamine. Here, we tie together some of the commonalities in the effects of dopamine on these aspects of cognition by having PD patients (ON and OFF dopaminergic medication) and healthy controls (HCs) perform two tasks that probe these processes. Within-patient behavioural measures of distractibility, from an attentional capture task, and learning performance, from a probabilistic classification reinforcement learning task, were included in one model to assess the role of distractibility during learning. Dopamine medication state and distractibility level were found to have an interactive effect on learning performance; less distractibility in PD ON was associated with higher accuracy during learning, and this was altered in PD OFF. Functional magnetic resonance imaging (fMRI) data acquired during the learning task furthermore allowed us to assess multivariate patterns of positive and negative outcomes in fronto-striatal and visual brain regions involved in both learning processes and the executive control of attention. Here, we demonstrate that while PD ON show a clearer distinction between outcomes than OFF in dorsolateral prefrontal cortex (DLPFC) and putamen, PD OFF show better distinction of activation patterns in visual regions that respond to the stimuli presented during the task. These results demonstrate that dopamine plays a key role in modulating the interaction between attention and learning at the level of both behaviour and activation patterns in the brain.",
        "link": "http://dx.doi.org/10.1101/2020.09.12.294702"
    },
    {
        "id": 25776,
        "title": "Reinforcement Learning Agent Design and Optimization with Bandwidth Allocation Model",
        "authors": "Rafael F. Reale, Joberto Martins",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning (RL) is currently used in various real-life applications. RL-based solutions have the potential to generically address problems, including the ones that are difficult to solve with heuristics and meta-heuristics and, in addition, the set of problems and issues where some intelligent or cognitive approach is required. However, reinforcement learning agents require a not straightforward design and have important design issues. RL agent design issues include the target problem modeling, state-space explosion, the training process, and agent efficiency. Research currently addresses these issues aiming to foster RL dissemination. A BAM model, in summary, allocates and shares resources with users. There are three basic BAM models and several hybrids that differ in how they allocate and share resources among users. This paper addresses the issue of an RL agent design and efficiency. The RL agent's objective is to allocate and share resources among users. The paper investigates how a BAM model can contribute to the RL agent design and efficiency. The AllocTC-Sharing (ATCS) model is analytically described and simulated to evaluate how it mimics the RL agent operation and how the ATCS can offload computational tasks from the RL agent. The essential argument researched is whether algorithms integrated with the RL agent design and operation have the potential to facilitate agent design and optimize its execution. The ATCS analytical model and simulation presented demonstrate that a BAM model offloads agent tasks and assists the agent's design and optimization.  </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22118471"
    },
    {
        "id": 25777,
        "title": "Reinforcement Learning Induced Non-Neutrality of Monetary Policy in Computational Economic Simulation",
        "authors": "Bořivoj Vlk",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4325511"
    },
    {
        "id": 25778,
        "title": "Stochastic Calculus-Guided Reinforcement Learning: A Probabilistic Framework for Optimal Decision-Making",
        "authors": "Raghavendra Devadas",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4743717"
    },
    {
        "id": 25779,
        "title": "RL Theory and Algorithms",
        "authors": "Abhishek Nandy, Manisha Biswas",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-3285-9_2"
    },
    {
        "id": 25780,
        "title": "Reinforcement Learning from Simulated Environments: An Encoder Decoder Approach",
        "authors": "",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22360/springsim.2020.ais.001"
    },
    {
        "id": 25781,
        "title": "Solving Channel Allocation by Reinforcement Learning in Cognitive Enabled Vehicular Ad Hoc Networks",
        "authors": "Yunfan Su",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Vehicular ad hoc network (VANET) is a promising technique that improves traffic safety and transportation efficiency and provides a comfortable driving experience. However, due to the rapid growth of applications that demand channel resources, efficient channel allocation schemes are required to utilize the performance of the vehicular networks. In this thesis, two Reinforcement learning (RL)-based channel allocation methods are proposed for a cognitive enabled VANET environment to maximize a long-term average system reward. First, we present a model-based dynamic programming method, which requires the calculations of the transition probabilities and time intervals between decision epochs. After obtaining the transition probabilities and time intervals, a relative value iteration (RVI) algorithm is used to find the asymptotically optimal policy. Then, we propose a model-free reinforcement learning method, in which we employ an agent to interact with the environment iteratively and learn from the feedback to approximate the optimal policy. Simulation results show that our reinforcement learning method can acquire a similar performance to that of the dynamic programming while both outperform the greedy method.</p>",
        "link": "http://dx.doi.org/10.32920/ryerson.14652336"
    },
    {
        "id": 25782,
        "title": "Monetary Policy Quality and Institutional Central Bank Structure: A Reinforcement Learning Approach",
        "authors": "Zachary Kessler",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4596355"
    },
    {
        "id": 25783,
        "title": "Assessing the Performance of Reinforcement Learning on Passive RRAM Crossbar Array",
        "authors": "Arjun Tyagi, Shubham Sahay",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning is a promising approach that can allow machines to acquire knowledge and solve problems without the intervention of humans. However, the current implementation of reinforcement learning algorithms on standard complementary metal-oxide-semiconductor based platform constraints the performance due to von Neumann architecture, which leads to increased energy consumption and latency. To this end, in this work, we propose an extremely area- and energy-efficient implementation of Monte Carlo learning on passive resistive random access memory (RRAM) crossbar array considering the non-ideal hardware artifacts such as device-to-device variation, noise and endurance failure. To illustrate the capabilities of our implementation, we considered the classical control problem of cart-pole. Our results indicate that the proposed passive RRAM crossbar-based implementation of Monte Carlo learning not only outperforms prior digital and active 1 Transistor - 1 RRAM (1T1R) crossbar-based implementation by more than five orders of magnitude in terms of area but is also robust against spatial and temporal variations and endurance failure of RRAM devices.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22347277.v1"
    },
    {
        "id": 25784,
        "title": "Reinforcement Learning with Hybrid Quantum Approximation in the NISQ Context",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1"
    },
    {
        "id": 25785,
        "title": "Review for \"Discovering mechanisms for materials microstructure optimization via reinforcement learning of a generative model\"",
        "authors": "",
        "published": "2022-6-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/aca004/v1/review1"
    },
    {
        "id": 25786,
        "title": "Dynamic selective auditory attention detection using RNN and reinforcement learning",
        "authors": "Masoud Geravanchizadeh, Hossein Roushan",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe cocktail party phenomenon describes the ability of the human brain to focus auditory attention on a particular stimulus while ignoring other acoustic events. Selective auditory attention detection (SAAD) is an important issue in the development of brain-computer interface systems and cocktail party processors. This paper proposes a new dynamic attention detection system to process the temporal evolution of the input signal. The proposed dynamic SAAD is modeled as a sequential decision-making problem, which is solved by recurrent neural network (RNN) and reinforcement learning methods of Q-learning and deep Q-learning. Among different dynamic learning approaches, the evaluation results show that the deep Q-learning approach with RNN as agent provides the highest classification accuracy (94.2%) with the least detection delay. The proposed SAAD system is advantageous, in the sense that the detection of attention is performed dynamically for the sequential inputs. Also, the system has the potential to be used in scenarios, where the attention of the listener might be switched in time in the presence of various acoustic events.",
        "link": "http://dx.doi.org/10.1101/2021.02.18.431748"
    },
    {
        "id": 25787,
        "title": "Safe Deployment of a Reinforcement Learning Robot Using Self Stabilization",
        "authors": "Nanda Kishore Sreenivas, Shrisha Rao",
        "published": "No Date",
        "citations": 0,
        "abstract": "In toy environments like video games, a reinforcement learning agent is deployed and operates within the same state space in which it was trained.  However, in robotics applications such as industrial systems or autonomous vehicles, this cannot be guaranteed.  A robot can be pushed out of its training space by some unforeseen perturbation, which may cause it to go into an unknown state from which it has not been trained to move towards its goal.  While most prior work in the area of RL safety focuses on ensuring safety in the training phase, this paper focuses on ensuring the safe deployment of a robot that has already been trained to operate within a safe space.  This work defines a condition on the state and action spaces, that if satisfied, guarantees the robot's recovery to safety independently.  We also propose a strategy and design that facilitate this recovery within a finite number of steps after perturbation.  This is implemented and tested against a standard RL model, and the results indicate a much-improved performance.",
        "link": "http://dx.doi.org/10.36227/techrxiv.14842245.v2"
    },
    {
        "id": 25788,
        "title": "Hippocampal Pattern Separation Supports Reinforcement Learning",
        "authors": "Ian Ballard, Anthony D. Wagner, Samuel M. McClure",
        "published": "No Date",
        "citations": 1,
        "abstract": "1ABSTRACTAnimals rely on learned associations to make decisions. Associations can be based on relationships between object features (e.g., the three-leaflets of poison ivy leaves) and outcomes (e.g., rash). More often, outcomes are linked to multidimensional states (e.g., poison ivy is green in summer but red in spring). Feature-based reinforcement learning fails when the values of individual features depend on the other features present. One solution is to assign value to multifeatural conjunctive representations. We tested if the hippocampus formed separable conjunctive representations that enabled learning of response contingencies for stimuli of the form: AB+, B-, AC-, C+. Pattern analyses on functional MRI data showed the hippocampus formed conjunctive representations that were dissociable from feature components and that these representations influenced striatal PEs. Our results establish a novel role for hippocampal pattern separation and conjunctive representation in reinforcement learning.",
        "link": "http://dx.doi.org/10.1101/293332"
    },
    {
        "id": 25789,
        "title": "Joint Selection using Deep Reinforcement Learning for Skeleton-based Activity Recognition",
        "authors": "Bahareh Nikpour, Narges Armanfard",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Skeleton based human activity recognition has attracted lots of attention due to its wide range of applications. Skeleton data includes two or three dimensional coordinates of body joints. All of the body joints are not effective in recognizing different activities, so finding key joints within a video and across different activities has a significant role in improving the performance. In this paper we propose a novel framework that performs joint selection in skeleton video frames for the purpose of human activity recognition. To this end, we formulate the joint selection problem as a Markov Decision Process (MDP) where we employ deep reinforcement learning to find the most informative joints per frame. The proposed joint selection method is a general framework that can be employed to improve human activity classification methods. Experimental results on two benchmark activity recognition data sets using three different classifiers demonstrate effectiveness of the proposed joint selection method.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14887869"
    },
    {
        "id": 25790,
        "title": "Transfer Reinforcement Learning for Cooling Water System Optimization",
        "authors": "Zhechao Wang, Zhengwei Li, Wenxia Cai",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4409765"
    },
    {
        "id": 25791,
        "title": "Reinforcement Learning",
        "authors": "Shin-ichi Maeda",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-03243-2_859-2"
    },
    {
        "id": 25792,
        "title": "Intelligent Operation and Control of Microgrids Using Multiple Reinforcement Learning Agents",
        "authors": "Raja Suryadevara, Peter Idowu",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The penetration of weather dependent renewable energy sources which are highly stochastic in nature create new challenges with system security, reliability, flexibility, and sustainability. This research focuses on the development of an artificial intelligence-based control method for a laboratory-scale hardware-in-the-loop microgrid setup. This setup features a variety of loads, battery banks, protection relays, renewable and nonrenewable sources of energy, and advanced metering devices interfaced through standard communication protocols. A resilient, cost-effective distributed control strategy allows flexible and reliable autonomous operation and control of the microgrid. Thus, a deep reinforcement learning based multi-agent system is the primary focus for grid control in this research. These intelligent agents learn from the microgrid environment and take actions to maximize their cumulative rewards based on prior experiences. The agents receive rewards for taking good actions and are penalized for bad actions. The results obtained in this research prove the feasibility of this approach.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21197626.v1"
    },
    {
        "id": 25793,
        "title": "Joint Selection using Deep Reinforcement Learning for Skeleton-based Activity Recognition",
        "authors": "Bahareh Nikpour, Narges Armanfard",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Skeleton based human activity recognition has attracted lots of attention due to its wide range of applications. Skeleton data includes two or three dimensional coordinates of body joints. All of the body joints are not effective in recognizing different activities, so finding key joints within a video and across different activities has a significant role in improving the performance. In this paper we propose a novel framework that performs joint selection in skeleton video frames for the purpose of human activity recognition. To this end, we formulate the joint selection problem as a Markov Decision Process (MDP) where we employ deep reinforcement learning to find the most informative joints per frame. The proposed joint selection method is a general framework that can be employed to improve human activity classification methods. Experimental results on two benchmark activity recognition data sets using three different classifiers demonstrate effectiveness of the proposed joint selection method.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14887869.v1"
    },
    {
        "id": 25794,
        "title": "Overview",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-1"
    },
    {
        "id": 25795,
        "title": "Solving Channel Allocation by Reinforcement Learning in Cognitive Enabled Vehicular Ad Hoc Networks",
        "authors": "Yunfan Su",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Vehicular ad hoc network (VANET) is a promising technique that improves traffic safety and transportation efficiency and provides a comfortable driving experience. However, due to the rapid growth of applications that demand channel resources, efficient channel allocation schemes are required to utilize the performance of the vehicular networks. In this thesis, two Reinforcement learning (RL)-based channel allocation methods are proposed for a cognitive enabled VANET environment to maximize a long-term average system reward. First, we present a model-based dynamic programming method, which requires the calculations of the transition probabilities and time intervals between decision epochs. After obtaining the transition probabilities and time intervals, a relative value iteration (RVI) algorithm is used to find the asymptotically optimal policy. Then, we propose a model-free reinforcement learning method, in which we employ an agent to interact with the environment iteratively and learn from the feedback to approximate the optimal policy. Simulation results show that our reinforcement learning method can acquire a similar performance to that of the dynamic programming while both outperform the greedy method.</p>",
        "link": "http://dx.doi.org/10.32920/ryerson.14652336.v1"
    },
    {
        "id": 25796,
        "title": "Autonomous PEV Charging Scheduling Using Deep-Q Network and Dyna-Q Reinforcement Learning",
        "authors": "Fan Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper proposes a demand response method that aims to reduce the long-term charging cost of a plug-in electric vehicle (PEV) while overcoming obstacles such as the stochastic nature of the user’s driving be- haviour, traffic condition, energy usage, and energy price. The problem is formulated as a Markov Decision Process (MDP) with unknown transition probabilities and solved using deep reinforcement learning (RL) techniques. Existing methods using machine learning either requires initial user behaviour data, or converges far too slowly. This method does not require any initial data on the PEV owner’s driving behaviour and shows improvement on learning speed. A combination of both model-based and model-free learning called Dyna-Q algorithm is utilized. Every time a real experience is obtained, the model is updated and the RL agent will learn from both real data set and “imagined” experience from the model. Due to the vast amount of state space, a table-look up method is impractical and a value approximation method using deep neural networks is employed for estimating the long-term expected reward of all state-action pairs. An average of historical price is used to predict future price. Three different user behaviour without any initial PEV owner behaviour data are simulated. A purely model-free DQN method is shown to run out of battery during trips very often, and is impractical for real life charging scenarios. Simulation results demonstrate the effectiveness of the proposed approach and its ability to reach an optimal policy quicker while avoiding state of charge (SOC) depleting during trips when compared to existing PEV charging schemes for all three different users profiles.</p>",
        "link": "http://dx.doi.org/10.32920/ryerson.14661024.v1"
    },
    {
        "id": 25797,
        "title": "Recurrent Attentional Reinforcement Learning for Machinery Fault Diagnosis",
        "authors": "Zhenhui tang, Jingcheng wang, Shunyu Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4720939"
    },
    {
        "id": 25798,
        "title": "Reinforcement Learning for Multi-Scale Molecular Modeling",
        "authors": "Jun Zhang, Yaokun Lei, Yi Isaac Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Molecular simulations are widely applied in the study of chemical and bio-physical systems of interest. However, the accessible timescales of atomistic simulations are limited, and extracting equilibrium properties of systems containing rare events remains challenging. Two distinct strategies are usually adopted in this regard: either sticking to the atom level and performing enhanced sampling, or trading details for speed by leveraging coarse-grained models. Although both strategies are promising, either of them, if adopted individually, exhibits severe limitations. In this paper we propose a machine-learning approach to ally these two worlds. In our approach, simulations on different scales are executed simultaneously and benefit mutually from their cross-talks: Accurate coarse-grained (CG) models can be inferred from the fine-grained (FG) simulations; In turn, FG simulations can be boosted by the guidance of CG models. Our method grounds on unsupervised and reinforcement learning, defined by a variational and adaptive training objective, and allows end-to-end and online training of parametric models. Through multiple experiments, we show that our method is efficient and flexible, and performs well on challenging chemical and bio-molecular systems.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.9640814.v1"
    },
    {
        "id": 25799,
        "title": "Peer Review #1 of \"LORM: a novel reinforcement learning framework for biped gait control (v0.1)\"",
        "authors": "",
        "published": "2022-3-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.927v0.1/reviews/1"
    },
    {
        "id": 25800,
        "title": "Ensemble Consensus-based Representation Deep Reinforcement Learning for Hybrid FSO/RF Communication Systems",
        "authors": "Shagufta Henna",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Hybrid FSO/RF system requires an efficient FSO and RF link switching mechanism to improve the system capacity by realizing the complementary benefits of both the links. The dynamics of network conditions, such as fog, dust, and sand storms compound the link switching problem and control complexity. To address this problem, we initiate the study of deep reinforcement learning (DRL) for link switching of hybrid FSO/RF systems. Specifically, in this work, we focus on actor-critic called Actor/Critic-FSO/RF and Deep-Q network (DQN) called DQN-FSO/RF for FSO/RF link switching under atmospheric turbulences. To formulate the problem, we define the state, action, and reward function of a hybrid FSO/RF system. DQN-FSO/RF frequently updates the deployed policy that interacts with the environment in a hybrid FSO/RF system, resulting in high switching costs. To overcome this, we lift this problem to ensemble consensus-based representation learning for deep reinforcement called DQNEnsemble-FSO/RF. The proposed novel DQNEnsemble-FSO/RF DRL approach uses consensus learned features representations based on an ensemble of asynchronous threads to update the deployed policy. Experimental results corroborate that the proposed DQNEnsemble-FSO/RF’s consensus learned features switching achieves better performance than Actor/Critic-FSO/RF, DQN-FSO/RF, and MyOpic for FSO/RF link switching while keeping the switching cost significantly low.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.15109434.v1"
    },
    {
        "id": 25801,
        "title": "Review for \"EPPTA: Efficient partially observable reinforcement learning agent for penetration testing applications\"",
        "authors": " Jingheng Xu",
        "published": "2023-9-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/eng2.12818/v1/review2"
    },
    {
        "id": 25802,
        "title": "Review for \"Discovering mechanisms for materials microstructure optimization via reinforcement learning of a generative model\"",
        "authors": "",
        "published": "2022-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/aca004/v2/review1"
    },
    {
        "id": 25803,
        "title": "Velocity estimation in reinforcement learning",
        "authors": "Carlos Velazquez, Manuel Villarreal, Arturo Bouzas",
        "published": "No Date",
        "citations": 0,
        "abstract": "The current work aims to study how people make predictions, under a reinforcement learning framework, in an environment that fluctuates from trial to trial and is corrupted with Gaussian noise. A computer-based experiment was developed where subjects were required to predict the future location of a spaceship that orbited around planet Earth. Its position was sampled from a Gaussian distribution with the mean changing at a variable velocity and four different values of variance that defined our signal-to-noise conditions. Three error-driven algorithms using a Bayesian approach were proposed as candidates to describe our data. The first is the standard delta-rule. The second and third models are delta rules incorporating a velocity component which is updated using prediction errors. The third model additionally assumes a hierarchical structure where individual learning rates for velocity and decision noise come from Gaussian distributions with means following a hyperbolic function. We used leave-one-out cross-validation and the Widely Applicable Information Criterion to compare the predictive accuracy of these models. In general, our results provided evidence in favor of the hierarchical model and highlight two main conclusions. First, when facing an environment that fluctuates from trial to trial, people can learn to estimate its velocity to make predictions. Second,  learning rates for velocity and decision noise are influenced by uncertainty constraints represented by the signal-to-noise ratio. This higher order control was modeled using a hierarchical structure, which qualitatively accounts for individual variability and is able to generalize and make predictions about new subjects on each experimental condition.",
        "link": "http://dx.doi.org/10.1101/432492"
    },
    {
        "id": 25804,
        "title": "Peer Review #1 of \"LORM: a novel reinforcement learning framework for biped gait control (v0.2)\"",
        "authors": "",
        "published": "2022-3-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.927v0.2/reviews/1"
    },
    {
        "id": 25805,
        "title": "Peer Review #3 of \"Architecture design of a reinforcement environment for learning sign languages (v0.1)\"",
        "authors": "",
        "published": "2021-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.740v0.1/reviews/3"
    },
    {
        "id": 25806,
        "title": "Peer Review #1 of \"Architecture design of a reinforcement environment for learning sign languages (v0.2)\"",
        "authors": "",
        "published": "2021-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.740v0.2/reviews/1"
    },
    {
        "id": 25807,
        "title": "Optimizing Portfolio Selection Through Stock Ranking and Matching: A Reinforcement Learning Approach",
        "authors": "Chaher Alzaman",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4742704"
    },
    {
        "id": 25808,
        "title": "Risk-sensitive Reinforcement Learning and Robust Learning for Control",
        "authors": "Erfaun Noorani, John S. Baras",
        "published": "2021-12-14",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683113"
    },
    {
        "id": 25809,
        "title": "Adaptive FPGA Placement Optimization via Reinforcement Learning",
        "authors": "Kevin E. Murray, Vaughn Betz",
        "published": "2019-9",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlcad48534.2019.9142079"
    },
    {
        "id": 25810,
        "title": "Learning Locomotion For Legged Robots Based on Reinforcement Learning: A Survey",
        "authors": "Jinghong Yue",
        "published": "2020-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ceect50755.2020.9298680"
    },
    {
        "id": 25811,
        "title": "Controlling the Propagation of Mechanical Discontinuity using Reinforcement Learning",
        "authors": "Yuteng Jin, Siddharth Misra",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/essoar.10508031.2"
    },
    {
        "id": 25812,
        "title": "Controlling the Propagation of Mechanical Discontinuity using Reinforcement Learning",
        "authors": "Yuteng Jin, Siddharth Misra",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/essoar.10508031.3"
    },
    {
        "id": 25813,
        "title": "Reinforcement Learning",
        "authors": "Shin-ichi Maeda",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-03243-2_859-1"
    },
    {
        "id": 25814,
        "title": "How hospitals response to disasters; a conceptual deep reinforcement learning approach",
        "authors": "Ardeshir Mirbakhsh",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDuring a disaster the requests for using ambulance services increases. Efficient assignment of the ambulances leads to lowering the patients' travel time. Simulating these environments is very complex and needs a solid framework. This paper uses a Deep Reinforcement Learning approach to better schedule ambulance dispatch problem during those disasters. The concept of a call and assignment of ambulances are illustrated and the elements of states, rewards, and actions in the formulations are described. The algorithm steps for solving this problem are also presented. This paper can help disaster planners to have a better idea for better scheduling ambulances.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2715241/v1"
    },
    {
        "id": 25815,
        "title": "The role of explicit strategies during reinforcement-based motor learning",
        "authors": "Peter Holland, Olivier Codol",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractDespite increasing interest in the role of reward in motor learning, the underlying mechanisms remain ill-defined. In particular, the relevance of explicit strategies to reward-based motor learning is unclear. To address this, we examined subject’s (n=30) ability to learn to compensate for a gradually introduced 25° visuomotor rotation with only reward-based feedback (binary success/failure). Only two-thirds of subjects (n=20) were successful at the maximum angle. The remaining subjects initially follow the rotation but after a variable number of trials begin to reach at an insufficiently large angle and subsequently return to near baseline performance (n=10). Furthermore, those that were successful accomplished this largely via the use of strategies, evidenced by a large reduction in reach angle when asked to remove any strategy they employed. However, both groups display a small degree of remaining retention even after the removal of strategies. All subjects made greater and more variable changes in reach angle following incorrect (unrewarded) trials. However, subjects who failed to learn showed decreased sensitivity to errors, even in the initial period in which they followed the rotation, a pattern previously found in Parkinsonian patients. In a second experiment, the addition of a secondary mental rotation task completely abolished learning (n=10), whilst a control group replicated the results of the first experiment (n=10). These results emphasize a pivotal role of strategy-use during reinforcement-based motor learning and the susceptibility of this form of learning to disruption has important implications for its potential therapeutic benefits.",
        "link": "http://dx.doi.org/10.1101/234534"
    },
    {
        "id": 25816,
        "title": "Multi-Task Reinforcement Learning in Humans",
        "authors": "Momchil S. Tomov, Eric Schulz, Samuel J. Gershman",
        "published": "No Date",
        "citations": 3,
        "abstract": "ABSTRACTThe ability to transfer knowledge across tasks and generalize to novel ones is an important hallmark of human intelligence. Yet not much is known about human multi-task reinforcement learning. We study participants’ behavior in a novel two-step decision making task with multiple features and changing reward functions. We compare their behavior to two state-of-the-art algorithms for multi-task reinforcement learning, one that maps previous policies and encountered features to new reward functions and one that approximates value functions across tasks, as well as to standard model-based and model-free algorithms. Across three exploratory experiments and a large preregistered experiment, our results provide strong evidence for a strategy that maps previously learned policies to novel scenarios. These results enrich our understanding of human reinforcement learning in complex environments with changing task demands.",
        "link": "http://dx.doi.org/10.1101/815332"
    },
    {
        "id": 25817,
        "title": "Practice with Code",
        "authors": "Rafael Ris-Ala",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-37345-9_5"
    },
    {
        "id": 25818,
        "title": "Community Battery Management using Reinforcement Learning in a residential scenario",
        "authors": "José Pedro Cruz, Sérgio Guerreiro",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The increasing interest in the residential renewable energy sources leveraged the development of the Battery Energy Storage Systems (BESS) ecosystems that allow the losses mitigation caused by the lack of intelligent storage of unused energy. BESS's have reduced user's costs by storing the overproduced energy by the renewable sources and using it when the demand was bigger than the production. Despite that, cost minimizing can yet be improved by applying a Energy Management System (EMS) which takes in consideration several factors and draws a policy to reduce costs. Model-based approaches have been thought as a solution but the increasing complexity of the models turned the researchers into a model-free direction using Reinforcement Learning. This paper creates an innovative agent, trained by Deep Reinforcement Learning algorithms, capable of reducing the costs in a building. A literature review of other approaches, a solution formulation to solve the problem, and evaluation using synthetic data and data supplied by an energy company are contained. Furthermore, evaluation scenarios and techniques that can lead to agent' instability are mentioned. Results indicate that the agent has better performance in terms of revenue when compared with the same BESS operation without the agent. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21407106"
    },
    {
        "id": 25819,
        "title": "A Study on Multi-agent Reinforcement Learning for Autonomous Distribution Vehicles",
        "authors": "Serap Ergün",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nA self-driving car, also called an autonomous vehicle, is capable of sensing the environment and driving without any human intervention, utilizing software algorithms and a fully automated driving system. This allows the vehicle to manage external conditions similar to a human. Safe and efficient delivery services require road controls and road restrictions to prevent accidents and reduce damage from unexpected technical failures. This study formulates the Autonomous Delivery Vehicles optimization problem and proposes a multi-agent reinforcement learning method using the analytically calculated shortest path information.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2180296/v2"
    },
    {
        "id": 25820,
        "title": "A Coupled Reinforcement Learning Mechanism for Concurrent Restoration of Interdependent Critical Infrastructures",
        "authors": "Shabnam Rezapour",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4752489"
    },
    {
        "id": 25821,
        "title": "Deep Reinforcement Learning in Human Activity Recognition: A Survey",
        "authors": "Bahareh Nikpour, Dimitrios Sinodinos, Narges Armanfard",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>  Human activity recognition is a popular research field in computer vision that has already been widely studied. However, it is still an active research field since it plays an important role in many current and emerging real-world intelligent systems, like visual surveillance and human-computer interaction. Deep Reinforcement Learning (DRL) has recently been employed to address the activity recognition problem with various purposes, such as finding attention in video data or obtaining the best network structure. DRL-based human activity recognition has only been around for a short time, and it is a challenging, novel field of study. Therefore, to facilitate further research in this field, we have constructed a comprehensive survey on activity recognition methods that incorporate deep reinforcement learning. Towards the end of this survey, we summarize key challenges and open problems in this area that can be addressed by researchers in the future.  </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19172369.v3"
    },
    {
        "id": 25822,
        "title": "Peer Review #3 of \"Testosterone and estradiol affect adolescent reinforcement learning (v0.2)\"",
        "authors": "IF Mohamed Suffian",
        "published": "2022-2-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12653v0.2/reviews/3"
    },
    {
        "id": 25823,
        "title": "Peer Review #2 of \"LORM: a novel reinforcement learning framework for biped gait control (v0.1)\"",
        "authors": "",
        "published": "2022-3-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.927v0.1/reviews/2"
    },
    {
        "id": 25824,
        "title": "Let’s Do the Time Warp Again: Human Action Assistance for Reinforcement Learning Agents",
        "authors": "Carter Burn, Frederick Crabbe, Rebecca Hwa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010258700920100"
    },
    {
        "id": 25825,
        "title": "LSTM-based Abstraction of Hetero Observation and Transition in Non-Communicative Multi-Agent Reinforcement Learning",
        "authors": "Fumito Uwano",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010795700003116"
    },
    {
        "id": 25826,
        "title": "Autonomous Braking and Throttle System: A Deep Reinforcement Learning Approach for Naturalistic Driving",
        "authors": "Varshit Dubey, Ruhshad Kasad, Karan Agrawal",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010157401730180"
    },
    {
        "id": 25827,
        "title": "Variational Quantum Circuit Design for Quantum Reinforcement Learning on Continuous Environments",
        "authors": "Georg Kruse, Theodora-Augustina Drăgan, Robert Wille, Jeanette Miriam Lorenz",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012353100003636"
    },
    {
        "id": 25828,
        "title": "Real-time Active Vision for a Humanoid Soccer Robot using Deep Reinforcement Learning",
        "authors": "Soheil Khatibi, Meisam Teimouri, Mahdi Rezaei",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010237307420751"
    },
    {
        "id": 25829,
        "title": "Outperformance of Mall-Receptionist Android as Inverse Reinforcement Learning is Transitioned to Reinforcement Learning",
        "authors": "Zhichao Chen, Yutaka Nakamura, Hiroshi Ishiguro",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lra.2023.3267385"
    },
    {
        "id": 25830,
        "title": "The Nature of Reinforcement",
        "authors": "James Allison",
        "published": "2019-2-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315788982-4"
    },
    {
        "id": 25831,
        "title": "Learning to Code: Coded Caching via Deep Reinforcement Learning",
        "authors": "Navid Naderializadeh, Seyed Mohammad Asghari",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf44664.2019.9048907"
    },
    {
        "id": 25832,
        "title": "Safe Reinforcement Learning: Learning with Supervision Using a Constraint-Admissible Set",
        "authors": "Zhaojian Li, Uros Kalabic, Tianshu Chu",
        "published": "2018-6",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2018.8430770"
    },
    {
        "id": 25833,
        "title": "Differentiating effort-related aspects of motivation from reinforcement learning: commentary on Soder et al. “Dose–response effects of d-amphetamine on effort-based decision-making and reinforcement learning”",
        "authors": "John D. Salamone",
        "published": "2021-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1038/s41386-020-00930-2"
    },
    {
        "id": 25834,
        "title": "A Study Toward Multi-Objective Multiagent Reinforcement Learning Considering Worst Case and Fairness Among Agents",
        "authors": "Toshihiro Matsui",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011687100003393"
    },
    {
        "id": 25835,
        "title": "Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms on a Building Energy Demand Coordination Task",
        "authors": "Gauraang Dhamankar, Jose R. Vazquez-Canteli, Zoltan Nagy",
        "published": "2020-11-17",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3427773.3427870"
    },
    {
        "id": 25836,
        "title": "Safe Deployment of a Reinforcement Learning Robot Using Self Stabilization",
        "authors": "Nanda Kishore Sreenivas, Shrisha Rao",
        "published": "No Date",
        "citations": 0,
        "abstract": "In toy environments like video games, a reinforcement learning agent is deployed and operates within the same state space in which it was trained.  However, in robotics applications such as industrial systems or autonomous vehicles, this cannot be guaranteed.  A robot can be pushed out of its training space by some unforeseen perturbation, which may cause it to go into an unknown state from which it has not been trained to move towards its goal.  While most prior work in the area of RL safety focuses on ensuring safety in the training phase, this paper focuses on ensuring the safe deployment of a robot that has already been trained to operate within a safe space.  This work defines a condition on the state and action spaces, that if satisfied, guarantees the robot's recovery to safety independently.  We also propose a strategy and design that facilitate this recovery within a finite number of steps after perturbation.  This is implemented and tested against a standard RL model, and the results indicate a much-improved performance.",
        "link": "http://dx.doi.org/10.36227/techrxiv.14842245.v2"
    },
    {
        "id": 25837,
        "title": "Deep Reinforcement Learning for Resource Management in Network Function Virtualization",
        "authors": "Bheema Shanker Neyigapula",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNetwork Function Virtualization (NFV) has revolutionized the networking land- scape by enabling the flexible deployment of network services through software. However, efficient resource management remains a critical challenge in NFV envi- ronments due to the dynamic nature of network conditions and workloads. This research paper proposes a novel approach to address this challenge by employing deep reinforcement learning techniques for resource management in NFV. The paper presents a comprehensive framework that combines deep reinforcement learning principles with NFV’s resource allocation problem. By leveraging this approach, intelligent and adaptive resource allocation strategies are developed, enhancing NFV performance, reducing operational costs, and optimizing resource utilization. Through rigorous experimentation and analysis, the paper evaluates the proposed framework’s effectiveness, comparing it with existing methods and showcasing its potential to transform NFV resource management.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3239087/v1"
    },
    {
        "id": 25838,
        "title": "Attention and reinforcement learning in Parkinson’s disease",
        "authors": "Brónagh McCoy, Rebecca P. Lawson, Jan Theeuwes",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTDopamine is known to be involved in several important cognitive processes, most notably in learning from rewards and in the ability to attend to task-relevant aspects of the environment. Both of these features of dopaminergic signalling have been studied separately in research involving Parkinson’s disease (PD) patients, who exhibit diminished levels of dopamine. Here, we tie together some of the commonalities in the effects of dopamine on these aspects of cognition by having PD patients (ON and OFF dopaminergic medication) and healthy controls (HCs) perform two tasks that probe these processes. Within-patient behavioural measures of distractibility, from an attentional capture task, and learning performance, from a probabilistic classification reinforcement learning task, were included in one model to assess the role of distractibility during learning. Dopamine medication state and distractibility level were found to have an interactive effect on learning performance; less distractibility in PD ON was associated with higher accuracy during learning, and this was altered in PD OFF. Functional magnetic resonance imaging (fMRI) data acquired during the learning task furthermore allowed us to assess multivariate patterns of positive and negative outcomes in fronto-striatal and visual brain regions involved in both learning processes and the executive control of attention. Here, we demonstrate that while PD ON show a clearer distinction between outcomes than OFF in dorsolateral prefrontal cortex (DLPFC) and putamen, PD OFF show better distinction of activation patterns in visual regions that respond to the stimuli presented during the task. These results demonstrate that dopamine plays a key role in modulating the interaction between attention and learning at the level of both behaviour and activation patterns in the brain.",
        "link": "http://dx.doi.org/10.1101/2020.09.12.294702"
    },
    {
        "id": 25839,
        "title": "Reinforcement Learning Induced Non-Neutrality of Monetary Policy in Computational Economic Simulation",
        "authors": "Bořivoj Vlk",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4325511"
    },
    {
        "id": 25840,
        "title": "RL Theory and Algorithms",
        "authors": "Abhishek Nandy, Manisha Biswas",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-3285-9_2"
    },
    {
        "id": 25841,
        "title": "Reinforcement Learning from Simulated Environments: An Encoder Decoder Approach",
        "authors": "",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22360/springsim.2020.ais.001"
    },
    {
        "id": 25842,
        "title": "Reinforcement Learning with Hybrid Quantum Approximation in the NISQ Context",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1"
    },
    {
        "id": 25843,
        "title": "Solving Channel Allocation by Reinforcement Learning in Cognitive Enabled Vehicular Ad Hoc Networks",
        "authors": "Yunfan Su",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Vehicular ad hoc network (VANET) is a promising technique that improves traffic safety and transportation efficiency and provides a comfortable driving experience. However, due to the rapid growth of applications that demand channel resources, efficient channel allocation schemes are required to utilize the performance of the vehicular networks. In this thesis, two Reinforcement learning (RL)-based channel allocation methods are proposed for a cognitive enabled VANET environment to maximize a long-term average system reward. First, we present a model-based dynamic programming method, which requires the calculations of the transition probabilities and time intervals between decision epochs. After obtaining the transition probabilities and time intervals, a relative value iteration (RVI) algorithm is used to find the asymptotically optimal policy. Then, we propose a model-free reinforcement learning method, in which we employ an agent to interact with the environment iteratively and learn from the feedback to approximate the optimal policy. Simulation results show that our reinforcement learning method can acquire a similar performance to that of the dynamic programming while both outperform the greedy method.</p>",
        "link": "http://dx.doi.org/10.32920/ryerson.14652336"
    },
    {
        "id": 25844,
        "title": "Reinforcement Learning Agent Design and Optimization with Bandwidth Allocation Model",
        "authors": "Rafael F. Reale, Joberto Martins",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning (RL) is currently used in various real-life applications. RL-based solutions have the potential to generically address problems, including the ones that are difficult to solve with heuristics and meta-heuristics and, in addition, the set of problems and issues where some intelligent or cognitive approach is required. However, reinforcement learning agents require a not straightforward design and have important design issues. RL agent design issues include the target problem modeling, state-space explosion, the training process, and agent efficiency. Research currently addresses these issues aiming to foster RL dissemination. A BAM model, in summary, allocates and shares resources with users. There are three basic BAM models and several hybrids that differ in how they allocate and share resources among users. This paper addresses the issue of an RL agent design and efficiency. The RL agent's objective is to allocate and share resources among users. The paper investigates how a BAM model can contribute to the RL agent design and efficiency. The AllocTC-Sharing (ATCS) model is analytically described and simulated to evaluate how it mimics the RL agent operation and how the ATCS can offload computational tasks from the RL agent. The essential argument researched is whether algorithms integrated with the RL agent design and operation have the potential to facilitate agent design and optimize its execution. The ATCS analytical model and simulation presented demonstrate that a BAM model offloads agent tasks and assists the agent's design and optimization.  </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22118471"
    },
    {
        "id": 25845,
        "title": "Assessing the Performance of Reinforcement Learning on Passive RRAM Crossbar Array",
        "authors": "Arjun Tyagi, Shubham Sahay",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning is a promising approach that can allow machines to acquire knowledge and solve problems without the intervention of humans. However, the current implementation of reinforcement learning algorithms on standard complementary metal-oxide-semiconductor based platform constraints the performance due to von Neumann architecture, which leads to increased energy consumption and latency. To this end, in this work, we propose an extremely area- and energy-efficient implementation of Monte Carlo learning on passive resistive random access memory (RRAM) crossbar array considering the non-ideal hardware artifacts such as device-to-device variation, noise and endurance failure. To illustrate the capabilities of our implementation, we considered the classical control problem of cart-pole. Our results indicate that the proposed passive RRAM crossbar-based implementation of Monte Carlo learning not only outperforms prior digital and active 1 Transistor - 1 RRAM (1T1R) crossbar-based implementation by more than five orders of magnitude in terms of area but is also robust against spatial and temporal variations and endurance failure of RRAM devices.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22347277.v1"
    },
    {
        "id": 25846,
        "title": "Monetary Policy Quality and Institutional Central Bank Structure: A Reinforcement Learning Approach",
        "authors": "Zachary Kessler",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4596355"
    },
    {
        "id": 25847,
        "title": "Dynamic selective auditory attention detection using RNN and reinforcement learning",
        "authors": "Masoud Geravanchizadeh, Hossein Roushan",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe cocktail party phenomenon describes the ability of the human brain to focus auditory attention on a particular stimulus while ignoring other acoustic events. Selective auditory attention detection (SAAD) is an important issue in the development of brain-computer interface systems and cocktail party processors. This paper proposes a new dynamic attention detection system to process the temporal evolution of the input signal. The proposed dynamic SAAD is modeled as a sequential decision-making problem, which is solved by recurrent neural network (RNN) and reinforcement learning methods of Q-learning and deep Q-learning. Among different dynamic learning approaches, the evaluation results show that the deep Q-learning approach with RNN as agent provides the highest classification accuracy (94.2%) with the least detection delay. The proposed SAAD system is advantageous, in the sense that the detection of attention is performed dynamically for the sequential inputs. Also, the system has the potential to be used in scenarios, where the attention of the listener might be switched in time in the presence of various acoustic events.",
        "link": "http://dx.doi.org/10.1101/2021.02.18.431748"
    },
    {
        "id": 25848,
        "title": "Review for \"Discovering mechanisms for materials microstructure optimization via reinforcement learning of a generative model\"",
        "authors": "",
        "published": "2022-6-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/aca004/v1/review1"
    },
    {
        "id": 25849,
        "title": "Stochastic Calculus-Guided Reinforcement Learning: A Probabilistic Framework for Optimal Decision-Making",
        "authors": "Raghavendra Devadas",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4743717"
    },
    {
        "id": 25850,
        "title": "Using Deep Learning tools for fitting Reinforcement Learning Models",
        "authors": "Milena Rmus, Jimmy Xia, Jasmine Collins, Anne Collins",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2022.1020-0"
    },
    {
        "id": 25851,
        "title": "Learning-Process Evaluation to Select Spatial Abstractions in Reinforcement Learning",
        "authors": "Cleiton Alves da Silva, Valdinei Freire",
        "published": "2017-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bracis.2017.83"
    },
    {
        "id": 25852,
        "title": "Reinforcement Learning Methods in Algorithmic Trading",
        "authors": "",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009028943.012"
    },
    {
        "id": 25853,
        "title": "Modeling Decisions in Games Using Reinforcement Learning",
        "authors": "Himanshu Singal, Palvi Aggarwal, Varun Dutt",
        "published": "2017-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlds.2017.13"
    },
    {
        "id": 25854,
        "title": "Combining Deep Learning on Order Books with Reinforcement Learning for Profitable Trading",
        "authors": "Koti Jaddu, Paul Bilokon",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4611708"
    },
    {
        "id": 25855,
        "title": "Machine Learning for Big Data Analytics, Interactive and Reinforcement",
        "authors": "Ritwik Raj, Anjana Mishra",
        "published": "2020-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367854737-13"
    },
    {
        "id": 25856,
        "title": "An Introduction to Reinforcement Learning",
        "authors": "David Paper",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-7341-8_14"
    },
    {
        "id": 25857,
        "title": "Combining Policy Gradient and Q-Learning",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_8"
    },
    {
        "id": 25858,
        "title": "Contrastive Visual Representation Learning Enhanced with Knowledge Embedding for Reinforcement Learning",
        "authors": "Zhuoan Ma",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cecit53797.2021.00079"
    },
    {
        "id": 25859,
        "title": "Multi-Domain Active Learning for Multi-Agent Reinforcement Learning",
        "authors": "Devarani Devi Ningombam",
        "published": "2023-9-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icidea59866.2023.10295208"
    },
    {
        "id": 25860,
        "title": "Learning to Suppress Tremors: A Deep Reinforcement Learning-Enabled Soft Exoskeleton for Parkinson's Patients",
        "authors": "Endrei Tamás, Sándor Földi, Ádám Makk, György Cserey",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNeurological tremors, prevalent among a large population, are one of the most rampant movement disorders. While biomechanical loading and exoskeletons show promise in enhancing patient well-being, traditional control algorithms limit their efficacy in dynamic movements and personalized interventions. Furthermore, there exists a pressing need for more comprehensive and robust validation methods to ensure the effectiveness and generalizability of proposed solutions. This paper proposes a physical simulation approach modelling multiple arm joints and tremor propagation. This study also introduces a novel adaptable reinforcement learning environment tailored for disorders with tremors. Furthermore we present a deep reinforcement learning based encoder-actor controller for Parkinson's tremor present in various shoulder and elbow joint axes, displayed in dynamic movements. We found that the controller is capable of suppressing tremors in all recorded dynamic movements. The control achieves an average tremor of over 57.43% for three out of the four distinct movement trajectories.  Notably, the controller attains a maximum accuracy of 99.49%. Our findings suggest that a deep reinforcement learning based control strategy offers a viable solution for tremor suppression in real-world scenarios. By overcoming the limitations of traditional control algorithms, this work takes a new step in the adaptation of biomechanical loading into the everyday life of patients. This work also opens avenues for more adaptive and personalized interventions in managing movement disorders.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3824299/v2"
    },
    {
        "id": 25861,
        "title": "Intrinsically Motivated Self-supervised Learning in Reinforcement Learning",
        "authors": "Yue Zhao, Chenzhuang Du, Hang Zhao, Tiejun Li",
        "published": "2022-5-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9812213"
    },
    {
        "id": 25862,
        "title": "Machining sequence learning via inverse reinforcement learning",
        "authors": "Yasutomo Sugisawa, Keigo Takasugi, Naoki Asakawa",
        "published": "2022-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.precisioneng.2021.09.017"
    },
    {
        "id": 25863,
        "title": "Personalized Learning Path Generation in E-Learning Systems using Reinforcement Learning and Generative Adversarial Networks",
        "authors": "Subharag Sarkar, Manfred Huber",
        "published": "2021-10-17",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc52423.2021.9658967"
    },
    {
        "id": 25864,
        "title": "Probabilistic Multi-knowledge Transfer in Reinforcement Learning",
        "authors": "Daniel Fernandez, Fernando Fernandez, Javier Garcia",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00079"
    },
    {
        "id": 25865,
        "title": "Learning How to Soar: Steady State Autonomous Dynamic Soaring Through Reinforcement Learning",
        "authors": "Corey Montella",
        "published": "2020-1-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2020-1848"
    },
    {
        "id": 25866,
        "title": "Deep Reinforcement Learning Monitor for Snapshot Recording",
        "authors": "Giang Dao, Indrajeet Mishra, Minwoo Lee",
        "published": "2018-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00095"
    },
    {
        "id": 25867,
        "title": "An Industrial Use-Case for Reinforcement Learning: Optimizing a Production Planning in Stochastic Conditions",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijml.2024.14.1.1152"
    },
    {
        "id": 25868,
        "title": "Asynchronous Multitask Reinforcement Learning with Dropout for Continuous Control",
        "authors": "Zilong Jiao, Jae Oh",
        "published": "2019-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2019.00099"
    },
    {
        "id": 25869,
        "title": "Deep Reinforcement Learning for Dynamic Power Allocation in Cell-free mmWave Massive MIMO",
        "authors": "Yu Zhao, Ignas Niemegeers, Sonia Heemstra de Groot",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010617300002999"
    },
    {
        "id": 25870,
        "title": "Micro Junction Agent: A Scalable Multi-agent Reinforcement Learning Method for Traffic Control",
        "authors": "BumKyu Choi, Jean Seong Choe, Jong-kook Kim",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010849600003116"
    },
    {
        "id": 25871,
        "title": "Deep vs. Deep Bayesian: Faster Reinforcement Learning on a Multi-robot Competitive Experiment",
        "authors": "Jingyi Huang, Fabio Giardina, Andre Rosendo",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010601905010506"
    },
    {
        "id": 25872,
        "title": "Deep Reinforcement Learning for Dynamic Power Allocation in Cell-free mmWave Massive MIMO",
        "authors": "Yu Zhao, Ignas Niemegeers, Sonia Heemstra de Groot",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010617300330045"
    },
    {
        "id": 25873,
        "title": "Viewpoint-independent Single-view 3D Object Reconstruction using Reinforcement Learning",
        "authors": "Seiya Ito, Byeongjun Ju, Naoshi Kaneko, Kazuhiko Sumi",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010825900003124"
    },
    {
        "id": 25874,
        "title": "Integration of Efficient Deep Q-Network Techniques Into QT-Opt Reinforcement Learning Structure",
        "authors": "Shudao Wei, Chenxing Li, Jan Seyler, Shahram Eivazi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011715000003393"
    },
    {
        "id": 25875,
        "title": "Multi‐agent reinforcement learning for process control: Exploring the intersection between fields of reinforcement learning, control theory, and game theory",
        "authors": "Yue Yifei, Samavedham Lakshminarayanan",
        "published": "2023-11",
        "citations": 3,
        "abstract": "AbstractThe application of reinforcement learning (RL) in process control has garnered increasing research attention. However, much of the current literature is focused on training and deploying a single RL agent. The application of multi‐agent reinforcement learning (MARL) has not been fully explored in process control. This work aims to: (i) develop a unique RL agent configuration that is suitable in a MARL control system for multiloop control, (ii) demonstrate the efficacy of MARL systems in controlling multiloop process that even exhibit strong interactions, and (iii) conduct a comparative study of the performance of MARL systems trained with different game‐theoretic strategies. First, we propose a design of an RL agent configuration that combines the functionalities of a feedback controller and a decoupler in a control loop. Thereafter, we deploy two such agents to form a MARL system that learns how to control a two‐input, two‐output system that exhibits strong interactions. After training, the MARL system shows effective control performance on the process. With further simulations, we examine how the MARL control system performs with increasing levels of process interaction and when trained with reward function configurations based on different game‐theoretic strategies (i.e., pure cooperation and mixed strategies). The results show that the performance of the MARL system is weakly dependent on the reward function configuration for systems with weak to moderate loop interactions. The MARL system with mixed strategies appears to perform marginally better than MARL under pure cooperation in systems with very strong loop interactions.",
        "link": "http://dx.doi.org/10.1002/cjce.24878"
    },
    {
        "id": 25876,
        "title": "Generic Multi-Agent Reinforcement Learning Approach for Flexible Job-Shop Scheduling",
        "authors": "Schirin Bär",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-39179-9"
    },
    {
        "id": 25877,
        "title": "Utility Theory",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-7"
    },
    {
        "id": 25878,
        "title": "Enhancing metacognitive reinforcement learning using reward structures and feedback",
        "authors": "Paul Krueger, Falk Lieder, Tom Griffiths",
        "published": "No Date",
        "citations": 0,
        "abstract": "One of the most remarkable aspects of the human mind is its ability to improve itself based on experience. Such learning occurs in a range of domains, from simple stimulus-response mappings, motor skills, and perceptual abilities, to problem-solving, cognitive control, and learning itself. Demonstrations of cognitive and brain plasticity have inspired cognitive training programs. The success of cognitive training has been mixed and the underlying learning mechanisms are not well understood. Feedback is an important component of many effective cognitive training programs, but it remains unclear what makes some feedback structures more effective than others. To address these problems, we model cognitive plasticity as metacognitive reinforcement learning. Here, we develop a metacognitive reinforcement learning model of how people learn how many steps to plan ahead in sequential decision problems, and test its predictions experimentally.The results of our first experiment suggested that our model can discern which reward structures are more conducive to metacognitive learning. This suggests that our model could be used to design feedback structures that make existing environments more conducive to cognitive growth. A follow-up experiment confirmed that feedback structures designed according to our model can indeed accelerate learning to plan. These results suggest that modeling metacognitive learning is a promising step towards building a theoretical foundation for promoting cognitive growth through cognitive training and other interventions.",
        "link": "http://dx.doi.org/10.31234/osf.io/3tgvb"
    },
    {
        "id": 25879,
        "title": "Good-for-MDPs Automata for Probabilistic Analysis and Reinforcement Learning",
        "authors": "Ernst Moritz Hahn, Ernst Moritz Hahn",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.604907f51a80aac83ca25d98"
    },
    {
        "id": 25880,
        "title": "Modeling and Predicting Cancer Clonal Evolution with Reinforcement Learning",
        "authors": "Stefan Ivanovic, Mohammed El-Kebir",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractCancer results from an evolutionary process that typically yields multiple clones with varying sets of mutations within the same tumor. Accurately modeling this process is key to understanding and predicting cancer evolution. Here, we introduce CloMu (Clone ToMutation), a flexible and low-parameter tree-generative model of cancer evolution. CloMu uses a two-layer neural network trained via reinforcement learning to determine the probability of new mutations based on the existing mutations on a clone. CloMu supports several prediction tasks, including the determination of evolutionary trajectories, tree selection, causality and interchangeability between mutations, and mutation fitness. Importantly, previous methods support only some of these tasks, and many suffer from overfitting on datasets with a large number of mutations. Using simulations, we demonstrate that CloMu either matches or outperforms current methods on a wide variety of prediction tasks. In particular, for simulated data with interchangeable mutations, current methods are unable to uncover causal relationships as effectively as CloMu. On breast cancer and leukemia cohorts, we show that CloMu determines similarities and causal relationships between mutations as well as the fitness of mutations. We validate CloMu’s inferred mutation fitness values for the leukemia cohort by comparing them to clonal proportion data not used during training, showing high concordance. In summary, CloMu’s low-parameter model facilitates a wide range of prediction tasks regarding cancer evolution on increasingly available cohort-level datasets.",
        "link": "http://dx.doi.org/10.1101/2022.12.11.519917"
    },
    {
        "id": 25881,
        "title": "A multiagent reinforcement learning control approach to environment exploration",
        "authors": "Mohammad S. Imtiaz, Jing Wang",
        "published": "2017-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/secon.2017.7925381"
    },
    {
        "id": 25882,
        "title": "Algorithm Cheatsheet",
        "authors": "Zihan Ding",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_20"
    },
    {
        "id": 25883,
        "title": "Mfrlmo: Model-Free Reinforcement Learning for Multi-Objective Optimization of Apache Spark",
        "authors": "Muhammed  Maruf öztürk",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4358735"
    },
    {
        "id": 25884,
        "title": "Iacppo: A Deep Reinforcement Learning-Based Model for Warehouse Inventory Replenishment",
        "authors": "Ran Tian, Haopeng Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4459934"
    },
    {
        "id": 25885,
        "title": "Deep Reinforcement Learning and Convex Mean-Variance Optimisation for Portfolio Management",
        "authors": "Ruan Pretorius, Terence van Zyl",
        "published": "No Date",
        "citations": 2,
        "abstract": "<div>Traditional portfolio management methods can incorporate specific investor preferences but rely on accurate forecasts of asset returns and covariances. Reinforcement learning (RL) methods do not rely on these explicit forecasts and are better suited for multi-stage decision processes. To address limitations of the evaluated research, experiments were conducted on three markets in different economies with different overall trends. By incorporating specific investor preferences into our RL models’ reward functions, a more comprehensive comparison could be made to traditional methods in risk-return space. Transaction costs were also modelled more realistically by including nonlinear changes introduced by market volatility and trading volume. The results of this study suggest that there can be an advantage to using RL methods compared to traditional convex mean-variance optimisation methods under certain market conditions. Our RL models could significantly outperform traditional single-period optimisation (SPO) and multi-period optimisation (MPO) models in upward trending markets, but only up to specific risk limits. In sideways trending markets, the performance of SPO and MPO models can be closely matched by our RL models for the majority of the excess risk range tested. The specific market conditions under which these models could outperform each other highlight the importance of a more comprehensive comparison of Pareto optimal frontiers in risk-return space. These frontiers give investors a more granular view of which models might provide better performance for their specific risk tolerance or return targets.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19165745.v1"
    },
    {
        "id": 25886,
        "title": "Using Reinforcement Learning to Optimize Quantum Circuits in thePresence of Noise",
        "authors": "Khalil Guy, Gabriel Perdue",
        "published": "2020-8-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1661681"
    },
    {
        "id": 25887,
        "title": "Nucleus Basalis links sensory stimuli with delayed reinforcement to support learning",
        "authors": "W. Guo, D.B. Polley",
        "published": "No Date",
        "citations": 0,
        "abstract": "SummaryLinking stimuli with delayed reinforcement requires neural circuits that can bridge extended temporal gaps. Auditory cortex (ACx) circuits reorganize to support auditory fear learning, but only when afferent sensory inputs temporally overlap with cholinergic reinforcement signals. Here we show that mouse ACx neurons rapidly reorganize to support learning, even when sensory and reinforcement cues are separated by a long gap. We found that cholinergic basal forebrain neurons bypass the temporal delay through multiplexed, short-latency encoding of sensory and reinforcement cues. At the initiation of learning, cholinergic neurons in Nucleus Basalis increase responses to conditioned sound frequencies and increase functional connectivity with ACx. By rapidly scaling up responses to sounds that predict reinforcement, cholinergic inputs jump the gap to align with bottom-up sensory traces and support associative cortical plasticity.",
        "link": "http://dx.doi.org/10.1101/663211"
    },
    {
        "id": 25888,
        "title": "Avoiding Jammers: A Reinforcement Learning Approach",
        "authors": "Serkan Ak, Stefan Bruggenwirth",
        "published": "2020-4",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radar42522.2020.9114797"
    },
    {
        "id": 25889,
        "title": "Review for \"High-dimensional reinforcement learning for optimization and control of ultracold quantum gases\"",
        "authors": " Xiaolong Zhao",
        "published": "2023-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad1437/v1/review1"
    },
    {
        "id": 25890,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v3/review1"
    },
    {
        "id": 25891,
        "title": "Learning without gradients: multi-agent reinforcement learning approach to optimization",
        "authors": "Amir Morcos, Hong Man, Aaron West, Brian Maguire",
        "published": "2022-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2636231"
    },
    {
        "id": 25892,
        "title": "Multi-Agent Deep Reinforcement Learning for Walker Systems",
        "authors": "Inhee Park, Teng-Sheng Moh",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00082"
    },
    {
        "id": 25893,
        "title": "Getting Priorities Right: Intrinsic Motivation with Multi-Objective Reinforcement Learning",
        "authors": "Yusuf Al-Husaini, Matthias Rolf",
        "published": "2022-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl53763.2022.9962187"
    },
    {
        "id": 25894,
        "title": "Iterative Learning Control Guided Reinforcement Learning Control Scheme for Batch Processes",
        "authors": "Xinghai Xu, Huimin Xie, Kechao Wen, Runze He, Wenjing Hong, Jia Shi",
        "published": "No Date",
        "citations": 1,
        "abstract": "Iterative learning control (ILC) offers an effective learning control\nscheme to solve the control problems of the batch processes. Although\nthe control performances of ILC systems can be improved batch-by-batch,\nthe convergence still strongly depends on the repeatability of the\nprocess and thus lack of robustness. Meanwhile, the data-driven-based\ndeep reinforcement learning (DRL) algorithms have good robustness due to\nthe generalization of the neural network, but it has lower data\nefficiency in training. In this paper, we propose a complementary\ncontrol scheme for the batch processes by employing a DRL guided by a\nclassical ILC, termed as the IL-RLC scheme. This scheme has higher data\nefficiency than the DRL without guidance and better robustness than the\nILC, which are demonstrated by the numerical simulations on a linear\nbatch process and a nonlinear batch reactor. This work provides a way\nfor the application of DRL algorithm in the batch process control.",
        "link": "http://dx.doi.org/10.22541/au.160583942.24407351/v1"
    },
    {
        "id": 25895,
        "title": "Grundlagen des Deep Learning",
        "authors": "Andreas Folkers",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-28886-0_2"
    },
    {
        "id": 25896,
        "title": "Model-free Predictive Optimal Iterative Learning Control using Reinforcement Learning",
        "authors": "Yueqing Zhang, Bing Chu, Zhan Shu",
        "published": "2022-6-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867561"
    },
    {
        "id": 25897,
        "title": "Transfer Learning for Operator Selection: A Reinforcement Learning Approach",
        "authors": "Rafet Durgut, Mehmet Emin Aydin, Abdur Rakib",
        "published": "2022-1-17",
        "citations": 5,
        "abstract": "In the past two decades, metaheuristic optimisation algorithms (MOAs) have been increasingly popular, particularly in logistic, science, and engineering problems. The fundamental characteristics of such algorithms are that they are dependent on a parameter or a strategy. Some online and offline strategies are employed in order to obtain optimal configurations of the algorithms. Adaptive operator selection is one of them, and it determines whether or not to update a strategy from the strategy pool during the search process. In the field of machine learning, Reinforcement Learning (RL) refers to goal-oriented algorithms, which learn from the environment how to achieve a goal. On MOAs, reinforcement learning has been utilised to control the operator selection process. However, existing research fails to show that learned information may be transferred from one problem-solving procedure to another. The primary goal of the proposed research is to determine the impact of transfer learning on RL and MOAs. As a test problem, a set union knapsack problem with 30 separate benchmark problem instances is used. The results are statistically compared in depth. The learning process, according to the findings, improved the convergence speed while significantly reducing the CPU time.",
        "link": "http://dx.doi.org/10.3390/a15010024"
    },
    {
        "id": 25898,
        "title": "Learning to Walk Via Deep Reinforcement Learning",
        "authors": "Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, Sergey Levine",
        "published": "2019-6-22",
        "citations": 149,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2019.xv.011"
    },
    {
        "id": 25899,
        "title": "Improving Intrusion Detection Systems with Multi-Agent Deep Reinforcement Learning: Enhanced Centralized and Decentralized Approaches",
        "authors": "Amani Bacha, Farah Ktata, Faten Louati",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012124600003555"
    },
    {
        "id": 25900,
        "title": "Distributed Service Area Control for Ride Sharing by using Multi-Agent Deep Reinforcement Learning",
        "authors": "Naoki Yoshida, Itsuki Noda, Toshiharu Sugawara",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010310901010112"
    },
    {
        "id": 25901,
        "title": "Multi-Agent Deep Reinforcement Learning for Walker Systems",
        "authors": "Inhee Park, Teng-Sheng Moh",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00082"
    },
    {
        "id": 25902,
        "title": "Getting Priorities Right: Intrinsic Motivation with Multi-Objective Reinforcement Learning",
        "authors": "Yusuf Al-Husaini, Matthias Rolf",
        "published": "2022-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl53763.2022.9962187"
    },
    {
        "id": 25903,
        "title": "Iterative Learning Control Guided Reinforcement Learning Control Scheme for Batch Processes",
        "authors": "Xinghai Xu, Huimin Xie, Kechao Wen, Runze He, Wenjing Hong, Jia Shi",
        "published": "No Date",
        "citations": 1,
        "abstract": "Iterative learning control (ILC) offers an effective learning control\nscheme to solve the control problems of the batch processes. Although\nthe control performances of ILC systems can be improved batch-by-batch,\nthe convergence still strongly depends on the repeatability of the\nprocess and thus lack of robustness. Meanwhile, the data-driven-based\ndeep reinforcement learning (DRL) algorithms have good robustness due to\nthe generalization of the neural network, but it has lower data\nefficiency in training. In this paper, we propose a complementary\ncontrol scheme for the batch processes by employing a DRL guided by a\nclassical ILC, termed as the IL-RLC scheme. This scheme has higher data\nefficiency than the DRL without guidance and better robustness than the\nILC, which are demonstrated by the numerical simulations on a linear\nbatch process and a nonlinear batch reactor. This work provides a way\nfor the application of DRL algorithm in the batch process control.",
        "link": "http://dx.doi.org/10.22541/au.160583942.24407351/v1"
    },
    {
        "id": 25904,
        "title": "Grundlagen des Deep Learning",
        "authors": "Andreas Folkers",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-28886-0_2"
    },
    {
        "id": 25905,
        "title": "Model-free Predictive Optimal Iterative Learning Control using Reinforcement Learning",
        "authors": "Yueqing Zhang, Bing Chu, Zhan Shu",
        "published": "2022-6-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867561"
    },
    {
        "id": 25906,
        "title": "Transfer Learning for Operator Selection: A Reinforcement Learning Approach",
        "authors": "Rafet Durgut, Mehmet Emin Aydin, Abdur Rakib",
        "published": "2022-1-17",
        "citations": 5,
        "abstract": "In the past two decades, metaheuristic optimisation algorithms (MOAs) have been increasingly popular, particularly in logistic, science, and engineering problems. The fundamental characteristics of such algorithms are that they are dependent on a parameter or a strategy. Some online and offline strategies are employed in order to obtain optimal configurations of the algorithms. Adaptive operator selection is one of them, and it determines whether or not to update a strategy from the strategy pool during the search process. In the field of machine learning, Reinforcement Learning (RL) refers to goal-oriented algorithms, which learn from the environment how to achieve a goal. On MOAs, reinforcement learning has been utilised to control the operator selection process. However, existing research fails to show that learned information may be transferred from one problem-solving procedure to another. The primary goal of the proposed research is to determine the impact of transfer learning on RL and MOAs. As a test problem, a set union knapsack problem with 30 separate benchmark problem instances is used. The results are statistically compared in depth. The learning process, according to the findings, improved the convergence speed while significantly reducing the CPU time.",
        "link": "http://dx.doi.org/10.3390/a15010024"
    },
    {
        "id": 25907,
        "title": "Learning to Walk Via Deep Reinforcement Learning",
        "authors": "Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, Sergey Levine",
        "published": "2019-6-22",
        "citations": 149,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2019.xv.011"
    },
    {
        "id": 25908,
        "title": "Improving Intrusion Detection Systems with Multi-Agent Deep Reinforcement Learning: Enhanced Centralized and Decentralized Approaches",
        "authors": "Amani Bacha, Farah Ktata, Faten Louati",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012124600003555"
    },
    {
        "id": 25909,
        "title": "Distributed Service Area Control for Ride Sharing by using Multi-Agent Deep Reinforcement Learning",
        "authors": "Naoki Yoshida, Itsuki Noda, Toshiharu Sugawara",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010310901010112"
    },
    {
        "id": 25910,
        "title": "Traffic Light Control using Reinforcement Learning: A Survey and an Open Source Implementation",
        "authors": "Ciprian Paduraru, Miruna Paduraru, Alin Stefanescu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011040300003191"
    },
    {
        "id": 25911,
        "title": "Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning",
        "authors": "Changxi You, Jianbo Lu, Dimitar Filev, Panagiotis Tsiotras",
        "published": "2019-4",
        "citations": 163,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.robot.2019.01.003"
    },
    {
        "id": 25912,
        "title": "Trading Utility and Uncertainty: Applying the Value of Information to Resolve the Exploration–Exploitation Dilemma in Reinforcement Learning",
        "authors": "Isaac J. Sledge, José C. Príncipe",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_19"
    },
    {
        "id": 25913,
        "title": "Background on Integral and Inverse Reinforcement Learning for Feedback Control",
        "authors": "Bosen Lian, Wenqian Xue, Frank L. Lewis, Hamidreza Modares, Bahare Kiumarsi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-45252-9_2"
    },
    {
        "id": 25914,
        "title": "Markov Processes",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-3"
    },
    {
        "id": 25915,
        "title": "Reinforcement Learning For Waveform Design",
        "authors": "Graeme E. Smith, Taylor J. Reininger",
        "published": "2021-5-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2147009.2021.9455187"
    },
    {
        "id": 25916,
        "title": "Controlling Mixed-Mode Fatigue Crack Growth using Deep Reinforcement Learning",
        "authors": "Yuteng Jin, Siddharth Misra",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/essoar.10508686.1"
    },
    {
        "id": 25917,
        "title": "Deep Reinforcement Learning for Composite Material Optimization",
        "authors": "HAOTIAN FENG, PAVANA PRABHAKAR",
        "published": "2020-9-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12783/asc35/34901"
    },
    {
        "id": 25918,
        "title": "Premium Control With Reinforcement Learning",
        "authors": "Lina Palmborg, Filip Lindskog",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4156450"
    },
    {
        "id": 25919,
        "title": "Deep Reinforcement Learning with Python",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4"
    },
    {
        "id": 25920,
        "title": "Harnessing Reinforcement Learning for Neural Motion Planning",
        "authors": "Tom Jurgenson, Aviv Tamar",
        "published": "2019-6-22",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2019.xv.026"
    },
    {
        "id": 25921,
        "title": "Simulating bout-and-pause patterns with reinforcement learning",
        "authors": "Kota Yamada, Atsunori Kanemura",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractAnimal responses occur according to a specific temporal structure composed of two states, where a bout is followed by a long pause until the next bout. Such about-and-pause pattern has three components: the bout length, the within-bout response rate, and the bout initiation rate. Previous studies have investigated how these three components are affected by experimental manipulations. However, it remains unknown what underlying mechanisms cause bout-and-pause patterns. In this article, we propose two mechanisms and examine computational models developed based on reinforcement learning. The model is characterized by two mechanisms. The first mechanism is choice—an agent makes a choice between operant and other behaviors. The second mechanism is cost—a cost is associated with the changeover of behaviors. These two mechanisms are extracted from past experimental findings. Simulation results suggested that both the choice and cost mechanisms are required to generate bout-and-pause patterns and if either of them is knocked out, the model does not generate bout-and-pause patterns. We further analyzed the proposed model and found that it reproduced the relationships between experimental manipulations and the three components that have been reported by previous studies. In addition, we showed alternative models can generate bout-and-pause patterns as long as they implement the two mechanisms.",
        "link": "http://dx.doi.org/10.1101/632745"
    },
    {
        "id": 25922,
        "title": "Reinforcement Learning: Theory and Applications in HEMS",
        "authors": "Omar Al-Ani, Sanjoy Das",
        "published": "No Date",
        "citations": 0,
        "abstract": "The steep rise in reinforcement learning (RL) in various applications in energy as well as the penetration of home automation in recent years are the motivation for this article. It surveys the use of RL in various home energy management system (HEMS) applications. There is a focus on deep neural network (DNN) models in RL. The article provides an overview of reinforcement learning. This is followed with discussions on state-of-the-art methods for value, policy, and actor&ndash;critic methods in deep reinforcement learning (DRL). In order to make the published literature in reinforcement learning more accessible to the HEMS community, verbal descriptions are accompanied with explanatory figures as well as mathematical expressions using standard machine learning terminology. Next, a detailed survey of how reinforcement learning is used in different HEMS domains is described. The survey also considers what kind of reinforcement learning algorithms are used in each HEMS application. It suggests that research in this direction is still in its infancy. Lastly, the article proposes four performance metrics to evaluate RL methods.",
        "link": "http://dx.doi.org/10.20944/preprints202208.0104.v2"
    },
    {
        "id": 25923,
        "title": "Deep Reinforcement Learning for Dynamic Power Allocation in Cell-free mmWave Massive MIMO",
        "authors": "Yu Zhao, Ignas Niemegeers, Sonia Heemstra de Groot",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010617300330045"
    },
    {
        "id": 25924,
        "title": "Deep Reinforcement Learning for Dynamic Power Allocation in Cell-free mmWave Massive MIMO",
        "authors": "Yu Zhao, Ignas Niemegeers, Sonia Heemstra de Groot",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010617300002999"
    },
    {
        "id": 25925,
        "title": "Micro Junction Agent: A Scalable Multi-agent Reinforcement Learning Method for Traffic Control",
        "authors": "BumKyu Choi, Jean Seong Choe, Jong-kook Kim",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010849600003116"
    },
    {
        "id": 25926,
        "title": "Deep vs. Deep Bayesian: Faster Reinforcement Learning on a Multi-robot Competitive Experiment",
        "authors": "Jingyi Huang, Fabio Giardina, Andre Rosendo",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010601905010506"
    },
    {
        "id": 25927,
        "title": "Integration of Efficient Deep Q-Network Techniques Into QT-Opt Reinforcement Learning Structure",
        "authors": "Shudao Wei, Chenxing Li, Jan Seyler, Shahram Eivazi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011715000003393"
    },
    {
        "id": 25928,
        "title": "Viewpoint-independent Single-view 3D Object Reconstruction using Reinforcement Learning",
        "authors": "Seiya Ito, Byeongjun Ju, Naoshi Kaneko, Kazuhiko Sumi",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010825900003124"
    },
    {
        "id": 25929,
        "title": "Multi‐agent reinforcement learning for process control: Exploring the intersection between fields of reinforcement learning, control theory, and game theory",
        "authors": "Yue Yifei, Samavedham Lakshminarayanan",
        "published": "2023-11",
        "citations": 3,
        "abstract": "AbstractThe application of reinforcement learning (RL) in process control has garnered increasing research attention. However, much of the current literature is focused on training and deploying a single RL agent. The application of multi‐agent reinforcement learning (MARL) has not been fully explored in process control. This work aims to: (i) develop a unique RL agent configuration that is suitable in a MARL control system for multiloop control, (ii) demonstrate the efficacy of MARL systems in controlling multiloop process that even exhibit strong interactions, and (iii) conduct a comparative study of the performance of MARL systems trained with different game‐theoretic strategies. First, we propose a design of an RL agent configuration that combines the functionalities of a feedback controller and a decoupler in a control loop. Thereafter, we deploy two such agents to form a MARL system that learns how to control a two‐input, two‐output system that exhibits strong interactions. After training, the MARL system shows effective control performance on the process. With further simulations, we examine how the MARL control system performs with increasing levels of process interaction and when trained with reward function configurations based on different game‐theoretic strategies (i.e., pure cooperation and mixed strategies). The results show that the performance of the MARL system is weakly dependent on the reward function configuration for systems with weak to moderate loop interactions. The MARL system with mixed strategies appears to perform marginally better than MARL under pure cooperation in systems with very strong loop interactions.",
        "link": "http://dx.doi.org/10.1002/cjce.24878"
    },
    {
        "id": 25930,
        "title": "Multi-Layer Defense Algorithm Against Deep Reinforcement Learning-based Intruders in Smart Grids",
        "authors": "Hossein Mohammadi Rouzbahani",
        "published": "No Date",
        "citations": 0,
        "abstract": "All experiments are performed on a subset of the Pecan Street dataset, which is available in the Non-Intrusive Load Monitoring Toolkit (NILMTK) format",
        "link": "http://dx.doi.org/10.36227/techrxiv.19398449"
    },
    {
        "id": 25931,
        "title": "Episodic Control as Meta-Reinforcement Learning",
        "authors": "S Ritter, JX Wang, Z Kurth-Nelson, M Botvinick",
        "published": "No Date",
        "citations": 4,
        "abstract": "AbstractRecent research has placed episodic reinforcement learning (RL) alongside model-free and model-based RL on the list of processes centrally involved in human reward-based learning. In the present work, we extend the unified account of model-free and model-based RL developed by Wang et al. (2018) to further integrate episodic learning. In this account, a generic model-free “meta-learner” learns to deploy and coordinate among all of these learning algorithms. The meta-learner learns through brief encounters with many novel tasks, so that it learns to learn about new tasks. We show that when equipped with an episodic memory system inspired by theories of reinstatement and gating, the meta-learner learns to use the episodic and model-based learning algorithms observed in humans in a task designed to dissociate among the influences of various learning strategies. We discuss implications and predictions of the model.",
        "link": "http://dx.doi.org/10.1101/360537"
    },
    {
        "id": 25932,
        "title": "Reinforcement Learning Game Training: A Brief Intuitive",
        "authors": "Chetanya Mahajan",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3666956"
    },
    {
        "id": 25933,
        "title": "Rriot: Recurrent Reinforcement Learning for Cyber Threat Detection on Iot Devices",
        "authors": "Curtis Rookard, Anahita Khojandi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4597143"
    },
    {
        "id": 25934,
        "title": "Algorithm Table",
        "authors": "Zihan Ding",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_19"
    },
    {
        "id": 25935,
        "title": "Enhancing Content Recommendation in Real-Time: A Live Recommender System Leveraging Deep Reinforcement Learning",
        "authors": "Andras Kovecs",
        "published": "No Date",
        "citations": 0,
        "abstract": "In light of the rapid emergence of deep reinforcement learning (DRL) in the field of recommender systems, this paper aims to provide a comprehensive overview of recent trends in this area. We begin by highlighting the motivation behind applying DRL in recommender systems, followed by a samples of current DRL-based approaches and existing methods. We also discuss emerging topics, open issues, and provide our perspective on advancing the domain. This survey serves as introductory material for researchers from academia and industry interested in DRL-based recommender systems, while identifying noteworthy opportunities for further research in this exciting field.",
        "link": "http://dx.doi.org/10.14293/pr2199.000069.v1"
    },
    {
        "id": 25936,
        "title": "Decision letter: Effects of dopamine on reinforcement learning and consolidation in Parkinson’s disease",
        "authors": "",
        "published": "2017-4-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.26801.026"
    },
    {
        "id": 25937,
        "title": "Explore High Thermal Conductivity Amorphous Polymers using Reinforcement Learning",
        "authors": "RUIMIN MA, Hanfeng Zhang, Tengfei Luo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Developing amorphous polymers with desirable thermal conductivity has significant implications, as they are ubiquitous in applications where thermal transport is critical. Conventional Edisonian approaches are slow and without guarantee of success in material development. In this work, using a reinforcement learning scheme, we design polymers with thermal conductivity above 0.4 W/m- K. We leverage a machine learning model trained against 469 thermal conductivity data calculated from high-throughput molecular dynamics (MD) simulations as the surrogate for thermal conductivity prediction, and we use a recurrent neural network trained with around one million virtual polymer structures as a polymer generator. For all newly generated polymers with thermal conductivity > 0.400 W/m-K, we have evaluated their synthesizability by calculating the synthesis accessibility score and validated the thermal conductivity of selected polymers using MD simulations. The best thermally conductive polymer designed has a MD-calculated thermal conductivity of 0.693 W/m-K, which is also estimated to be easily synthesizable. Our demonstrated inverse design scheme based on reinforcement learning may advance polymer development with target properties, and the scheme can also be generalized to other materials development tasks for different applications.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-6jftj-v2"
    },
    {
        "id": 25938,
        "title": "Reinforcement Learning: Playing Tic-Tac-Toe",
        "authors": "Jocelyn Ho, Jeffrey Huang, Benjamin Chang, Allison Liu, Zoe Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Machine learning constructs computer systems that develop through experience. Applications surround disciplines in daily life ranging from malware filtering to image recognition. Recent research has shifted towards maximizing efficiency in decision-making, creating algorithms that quickly and accurately process patterns to generate insight. This research focuses on reinforcement learning, a paradigm of machine learning that makes decisions through maximizing reward. Specifically, we use Q-learning – a model-free reinforcement learning algorithm – to assign scores for different decisions given the unique states of the problem. Widyantoro et al. (2009) has studied the effect of Q-learning on learning to play Tic-Tac-Toe. However, the study yielded a win/tie rate of less than 50 percent. We believe that does not represent an effective algorithm to fully exploit the benefits of Q-learning. In the same environment, this research aims to close the gaps in the effectiveness of Q-learning while minimizing human input. Data were processed by setting the epsilon value as 0.9 to ensure randomness, then consecutively decrease with a constant rate as possible states increase. The program played 300,000 games against its previous version, eventually securing a win/tie rate of approximately 90 percent. Future directions include improving the efficiency of Q-learning algorithms and applying the research in practical fields.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20407575.v1"
    },
    {
        "id": 25939,
        "title": "An Adaptive Network Signal Controller with Reinforcement Learning under Dynamic Traffic Fluctuations",
        "authors": "Wang Texas, Chaolun Ma",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/m.63283144f30377bc3baf7cc6"
    },
    {
        "id": 25940,
        "title": "Deep Reinforcement Learning Algorithms in Intelligent Infrastructure",
        "authors": " Serrano",
        "published": "2019-8-16",
        "citations": 14,
        "abstract": "Intelligent infrastructure, including smart cities and intelligent buildings, must learn and adapt to the variable needs and requirements of users, owners and operators in order to be future proof and to provide a return on investment based on Operational Expenditure (OPEX) and Capital Expenditure (CAPEX). To address this challenge, this article presents a biological algorithm based on neural networks and deep reinforcement learning that enables infrastructure to be intelligent by making predictions about its different variables. In addition, the proposed method makes decisions based on real time data. Intelligent infrastructure must be able to proactively monitor, protect and repair itself: this includes independent components and assets working the same way any autonomous biological organisms would. Neurons of artificial neural networks are associated with a prediction or decision layer based on a deep reinforcement learning algorithm that takes into consideration all of its previous learning. The proposed method was validated against an intelligent infrastructure dataset with outstanding results: the intelligent infrastructure was able to learn, predict and adapt to its variables, and components could make relevant decisions autonomously, emulating a living biological organism in which data flow exhaustively.",
        "link": "http://dx.doi.org/10.3390/infrastructures4030052"
    },
    {
        "id": 25941,
        "title": "Human‐Robot Interaction Control Using Reinforcement Learning",
        "authors": "Wen Yu, Adolfo Perrusquía",
        "published": "2021-10-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773"
    },
    {
        "id": 25942,
        "title": "The tortoise and the hare: interactions between reinforcement learning and working memory",
        "authors": "Anne G.E. Collins",
        "published": "No Date",
        "citations": 4,
        "abstract": "AbstractLearning to make rewarding choices in response to stimuli depends on a slow but steady process, reinforcement learning, and a fast and flexible, but capacity limited process, working memory. Using both systems in parallel, with their contributions weighted based on performance, should allow us to leverage the best of each system: rapid early learning, supplemented by long term robust acquisition. However, this assumes that using one process does not interfere with the other. We use computational modeling to investigate the interactions between the two processes in a behavioral experiment, and show that working memory interferes with reinforcement learning. Previous research showed that neural representations of reward prediction errors, a key marker of reinforcement learning, were blunted when working memory was used for learning. We thus predicted that arbitrating in favor of working memory to learn faster in simple problems would weaken the reinforcement learning process. We tested this by measuring performance in a delayed testing phase where the use of working memory was impossible, and thus subject choices depended on reinforcement learning. Counter-intuitively, but confirming our predictions, we observed that associations learned most easily were retained worse than associations learned slower: using working memory to learn quickly came at the cost of long-term retention. Computational modeling confirmed that this could only be accounted for by working memory interference in reinforcement learning computations. These results further our understanding of how multiple systems contribute in parallel to human learning, and may have important applications for education and computational psychiatry.",
        "link": "http://dx.doi.org/10.1101/234724"
    },
    {
        "id": 25943,
        "title": "Deep Reinforcement Learning For Visual Navigation of Wheeled Mobile Robots",
        "authors": "Ezebuugo Nwaonumah, Biswanath Samanta",
        "published": "2020-3-28",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/southeastcon44009.2020.9249654"
    },
    {
        "id": 25944,
        "title": "Deep Reinforcement Learning",
        "authors": "",
        "published": "2020",
        "citations": 85,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0"
    },
    {
        "id": 25945,
        "title": "Hybrid Intellectual Scheme for Scheduling of Heterogeneous Workflows based on Evolutionary Approach and Reinforcement Learning",
        "authors": "Mikhail Melnik, Ivan Dolgov, Denis Nasonov",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010112802000211"
    },
    {
        "id": 25946,
        "title": "Reinforcement Learning and Advanced Reinforcement Learning to Improve Autonomous Vehicle Planning",
        "authors": "Avinash. J. Agrawal, Rashmi R. Welekar, Namita Parati, Pravin R. Satav, Uma Patel Thakur, Archana V. Potnurwar",
        "published": "2023-7-25",
        "citations": 0,
        "abstract": "Planning for autonomous vehicles is a challenging process that involves navigating through dynamic and unpredictable surroundings while making judgments in real-time. Traditional planning methods sometimes rely on predetermined rules or customized heuristics, which could not generalize well to various driving conditions. In this article, we provide a unique framework to enhance autonomous vehicle planning by fusing conventional RL methods with cutting-edge reinforcement learning techniques. To handle many elements of planning issues, our system integrates cutting-edge algorithms including deep reinforcement learning, hierarchical reinforcement learning, and meta-learning. Our framework helps autonomous vehicles make decisions that are more reliable and effective by utilizing the advantages of these cutting-edge strategies.With the use of the RLTT technique, an autonomous vehicle can learn about the intentions and preferences of human drivers by inferring the underlying reward function from expert behaviour that has been seen. The autonomous car can make safer and more human-like decisions by learning from expert demonstrations about the fundamental goals and limitations of driving. Large-scale simulations and practical experiments can be carried out to gauge the effectiveness of the suggested approach. On the basis of parameters like safety, effectiveness, and human likeness, the autonomous vehicle planning system's performance can be assessed. The outcomes of these assessments can help to inform future developments and offer insightful information about the strengths and weaknesses of the strategy.",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i7s.7526"
    },
    {
        "id": 25947,
        "title": "Flexible Reinforcement Learning Framework for Building Control using EnergyPlus-Modelica Energy Models",
        "authors": "Joon-Yong Lee, Sen Huang, Aowabin Rahman, Amanda D. Smith, Srinivas Katipamula",
        "published": "2020-11-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3427773.3427873"
    },
    {
        "id": 25948,
        "title": "Manufacturing Control in Job Shop Environments with Reinforcement Learning",
        "authors": "Vladimir Samsonov, Marco Kemmerling, Maren Paegert, Daniel Lütticke, Frederick Sauermann, Andreas Gützlaff, Günther Schuh, Tobias Meisen",
        "published": "2021",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010202405890597"
    },
    {
        "id": 25949,
        "title": "Reward Design for Deep Reinforcement Learning Towards Imparting Commonsense Knowledge in Text-Based Scenario",
        "authors": "Ryota Kubo, Fumito Uwano, Manabu Ohta",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012456900003636"
    },
    {
        "id": 25950,
        "title": "Electricity Pricing aware Deep Reinforcement Learning based Intelligent HVAC Control",
        "authors": "Kuldeep Kurte, Jeffrey Munk, Kadir Amasyali, Olivera Kotevska, Borui Cui, Teja Kuruganti, Helia Zandi",
        "published": "2020-11-17",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3427773.3427866"
    },
    {
        "id": 25951,
        "title": "Good-for-MDPs Automata for Probabilistic Analysis and Reinforcement Learning",
        "authors": "Ernst Moritz Hahn, Ernst Moritz Hahn",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.604907f51a80aac83ca25d98"
    },
    {
        "id": 25952,
        "title": "Enhancing metacognitive reinforcement learning using reward structures and feedback",
        "authors": "Paul Krueger, Falk Lieder, Tom Griffiths",
        "published": "No Date",
        "citations": 0,
        "abstract": "One of the most remarkable aspects of the human mind is its ability to improve itself based on experience. Such learning occurs in a range of domains, from simple stimulus-response mappings, motor skills, and perceptual abilities, to problem-solving, cognitive control, and learning itself. Demonstrations of cognitive and brain plasticity have inspired cognitive training programs. The success of cognitive training has been mixed and the underlying learning mechanisms are not well understood. Feedback is an important component of many effective cognitive training programs, but it remains unclear what makes some feedback structures more effective than others. To address these problems, we model cognitive plasticity as metacognitive reinforcement learning. Here, we develop a metacognitive reinforcement learning model of how people learn how many steps to plan ahead in sequential decision problems, and test its predictions experimentally.The results of our first experiment suggested that our model can discern which reward structures are more conducive to metacognitive learning. This suggests that our model could be used to design feedback structures that make existing environments more conducive to cognitive growth. A follow-up experiment confirmed that feedback structures designed according to our model can indeed accelerate learning to plan. These results suggest that modeling metacognitive learning is a promising step towards building a theoretical foundation for promoting cognitive growth through cognitive training and other interventions.",
        "link": "http://dx.doi.org/10.31234/osf.io/3tgvb"
    },
    {
        "id": 25953,
        "title": "Modeling and Predicting Cancer Clonal Evolution with Reinforcement Learning",
        "authors": "Stefan Ivanovic, Mohammed El-Kebir",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractCancer results from an evolutionary process that typically yields multiple clones with varying sets of mutations within the same tumor. Accurately modeling this process is key to understanding and predicting cancer evolution. Here, we introduce CloMu (Clone ToMutation), a flexible and low-parameter tree-generative model of cancer evolution. CloMu uses a two-layer neural network trained via reinforcement learning to determine the probability of new mutations based on the existing mutations on a clone. CloMu supports several prediction tasks, including the determination of evolutionary trajectories, tree selection, causality and interchangeability between mutations, and mutation fitness. Importantly, previous methods support only some of these tasks, and many suffer from overfitting on datasets with a large number of mutations. Using simulations, we demonstrate that CloMu either matches or outperforms current methods on a wide variety of prediction tasks. In particular, for simulated data with interchangeable mutations, current methods are unable to uncover causal relationships as effectively as CloMu. On breast cancer and leukemia cohorts, we show that CloMu determines similarities and causal relationships between mutations as well as the fitness of mutations. We validate CloMu’s inferred mutation fitness values for the leukemia cohort by comparing them to clonal proportion data not used during training, showing high concordance. In summary, CloMu’s low-parameter model facilitates a wide range of prediction tasks regarding cancer evolution on increasingly available cohort-level datasets.",
        "link": "http://dx.doi.org/10.1101/2022.12.11.519917"
    },
    {
        "id": 25954,
        "title": "A multiagent reinforcement learning control approach to environment exploration",
        "authors": "Mohammad S. Imtiaz, Jing Wang",
        "published": "2017-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/secon.2017.7925381"
    },
    {
        "id": 25955,
        "title": "Algorithm Cheatsheet",
        "authors": "Zihan Ding",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_20"
    },
    {
        "id": 25956,
        "title": "Mfrlmo: Model-Free Reinforcement Learning for Multi-Objective Optimization of Apache Spark",
        "authors": "Muhammed  Maruf öztürk",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4358735"
    },
    {
        "id": 25957,
        "title": "Nucleus Basalis links sensory stimuli with delayed reinforcement to support learning",
        "authors": "W. Guo, D.B. Polley",
        "published": "No Date",
        "citations": 0,
        "abstract": "SummaryLinking stimuli with delayed reinforcement requires neural circuits that can bridge extended temporal gaps. Auditory cortex (ACx) circuits reorganize to support auditory fear learning, but only when afferent sensory inputs temporally overlap with cholinergic reinforcement signals. Here we show that mouse ACx neurons rapidly reorganize to support learning, even when sensory and reinforcement cues are separated by a long gap. We found that cholinergic basal forebrain neurons bypass the temporal delay through multiplexed, short-latency encoding of sensory and reinforcement cues. At the initiation of learning, cholinergic neurons in Nucleus Basalis increase responses to conditioned sound frequencies and increase functional connectivity with ACx. By rapidly scaling up responses to sounds that predict reinforcement, cholinergic inputs jump the gap to align with bottom-up sensory traces and support associative cortical plasticity.",
        "link": "http://dx.doi.org/10.1101/663211"
    },
    {
        "id": 25958,
        "title": "Review for \"High-dimensional reinforcement learning for optimization and control of ultracold quantum gases\"",
        "authors": " Xiaolong Zhao",
        "published": "2023-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad1437/v1/review1"
    },
    {
        "id": 25959,
        "title": "Deep Reinforcement Learning and Convex Mean-Variance Optimisation for Portfolio Management",
        "authors": "Ruan Pretorius, Terence van Zyl",
        "published": "No Date",
        "citations": 2,
        "abstract": "<div>Traditional portfolio management methods can incorporate specific investor preferences but rely on accurate forecasts of asset returns and covariances. Reinforcement learning (RL) methods do not rely on these explicit forecasts and are better suited for multi-stage decision processes. To address limitations of the evaluated research, experiments were conducted on three markets in different economies with different overall trends. By incorporating specific investor preferences into our RL models’ reward functions, a more comprehensive comparison could be made to traditional methods in risk-return space. Transaction costs were also modelled more realistically by including nonlinear changes introduced by market volatility and trading volume. The results of this study suggest that there can be an advantage to using RL methods compared to traditional convex mean-variance optimisation methods under certain market conditions. Our RL models could significantly outperform traditional single-period optimisation (SPO) and multi-period optimisation (MPO) models in upward trending markets, but only up to specific risk limits. In sideways trending markets, the performance of SPO and MPO models can be closely matched by our RL models for the majority of the excess risk range tested. The specific market conditions under which these models could outperform each other highlight the importance of a more comprehensive comparison of Pareto optimal frontiers in risk-return space. These frontiers give investors a more granular view of which models might provide better performance for their specific risk tolerance or return targets.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19165745.v1"
    },
    {
        "id": 25960,
        "title": "Generic Multi-Agent Reinforcement Learning Approach for Flexible Job-Shop Scheduling",
        "authors": "Schirin Bär",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-39179-9"
    },
    {
        "id": 25961,
        "title": "Utility Theory",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-7"
    },
    {
        "id": 25962,
        "title": "Iacppo: A Deep Reinforcement Learning-Based Model for Warehouse Inventory Replenishment",
        "authors": "Ran Tian, Haopeng Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4459934"
    },
    {
        "id": 25963,
        "title": "Using Reinforcement Learning to Optimize Quantum Circuits in thePresence of Noise",
        "authors": "Khalil Guy, Gabriel Perdue",
        "published": "2020-8-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1661681"
    },
    {
        "id": 25964,
        "title": "Avoiding Jammers: A Reinforcement Learning Approach",
        "authors": "Serkan Ak, Stefan Bruggenwirth",
        "published": "2020-4",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radar42522.2020.9114797"
    },
    {
        "id": 25965,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v3/review1"
    },
    {
        "id": 25966,
        "title": "Time Series Compression Based on Reinforcement Learning",
        "authors": "Nan Jiang, Qingping Xiang, Hongzhi Wang, Bo Zheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4410766"
    },
    {
        "id": 25967,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2022-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v1/review1"
    },
    {
        "id": 25968,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2023-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v2/review1"
    },
    {
        "id": 25969,
        "title": "Financial Planning via Deep Reinforcement Learning AI",
        "authors": "Gordon Irlam",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3201703"
    },
    {
        "id": 25970,
        "title": "HINRL4Rec: A Novel Integrated Heterogeneous Network Embedding with Reinforcement Learning for Recommendation",
        "authors": "Tham Vo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nRecent KG-oriented recommendation techniques mainly focus on the direct interaction between entities in the given KGs as the rich information sources for leveraging the quality of recommendation outputs. However, they are still hindered by the heterogeneity, type-varied entities and their relationships in knowledge graphs (KG) as the heterogeneous information networks (HIN). This limitation seems challenging to build up an effective approach for the KG-based recommendation system in both semantic path-based exploitation and heterogeneous information extraction. To meet these challenges, we proposed a novel integrated HIN embedding with reinforcement learning (RL)-based feature engineering for recommendation, called as: HINRL4Rec. First of all, we apply the combined textual meta-path-based embedding approach for learning multiple-rich-schematic representations of user/item and their associated entities. Then, these extracted multi-typed embeddings of user and item entities are fused into the unified embedding spaces during the KG embedding process. Finally, the unified representations of users and items are then used to facilitate the RL-based policy-driven searching process in the next steps for performing the recommendation task. Extensive experiments in real-world datasets demonstrate the effectiveness of our proposed model in comparing with recent state-of-the-art recommendation baselines.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-610194/v1"
    },
    {
        "id": 25971,
        "title": "Reinforcement Learning",
        "authors": "Mark Chang",
        "published": "2020-5-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429345159-11"
    },
    {
        "id": 25972,
        "title": "Self-Imitation Guided High-Efficient Goal-Conditioned Reinforcement Learning",
        "authors": "Yao LI, YuHui Wang, XiaoYang Tan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4419852"
    },
    {
        "id": 25973,
        "title": "Hierarchical Model-Based Deep Reinforcement Learning For Single-Asset Trading",
        "authors": "Adrian Millea",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present a hierarchical reinforcement learning (RL) architecture that employs various low-level agents to act in the trading environment, i.e. the market. The highest level agent selects among a group of specialised agents, and then the selected agent decides when to sell or buy a single asset for some period. This period can be variable according to a termination function. We hypothesized that due to different market regimes, more than one single agent is needed when trying to learn from such heterogeneous data, and instead, multiple agents will perform better, with each one specialising in a subset of the data. We use $k-means$ clustering to partition the data and train each agent with a different cluster. Partitioning the input data also helps model-based RL (MBRL), where models can be heterogeneous. We also add two simple decision-making models to the set of low-level agents, diversifying the pool of available agents and thus increasing overall behaviour flexibility. We perform multiple experiments showing the strengths of a hierarchical approach and test various prediction models at both levels. We also use a risk-based reward at the high level, which transforms the overall problem into a risk-return optimization. This type of reward shows a significant reduction in risk while minimally reducing profits. Overall, the hierarchical approach shows significant promise, especially when the pool of low-level agents is highly diverse.",
        "link": "http://dx.doi.org/10.20944/preprints202305.1532.v1"
    },
    {
        "id": 25974,
        "title": "Deep Reinforcement Learning and Convex Mean-Variance Optimisation for Portfolio Management",
        "authors": "Ruan Pretorius, Terence van Zyl",
        "published": "No Date",
        "citations": 1,
        "abstract": "<div>Traditional portfolio management methods can incorporate specific investor preferences but rely on accurate forecasts of asset returns and covariances. Reinforcement learning (RL) methods do not rely on these explicit forecasts and are better suited for multi-stage decision processes. To address limitations of the evaluated research, experiments were conducted on three markets in different economies with different overall trends. By incorporating specific investor preferences into our RL models’ reward functions, a more comprehensive comparison could be made to traditional methods in risk-return space. Transaction costs were also modelled more realistically by including nonlinear changes introduced by market volatility and trading volume. The results of this study suggest that there can be an advantage to using RL methods compared to traditional convex mean-variance optimisation methods under certain market conditions. Our RL models could significantly outperform traditional single-period optimisation (SPO) and multi-period optimisation (MPO) models in upward trending markets, but only up to specific risk limits. In sideways trending markets, the performance of SPO and MPO models can be closely matched by our RL models for the majority of the excess risk range tested. The specific market conditions under which these models could outperform each other highlight the importance of a more comprehensive comparison of Pareto optimal frontiers in risk-return space. These frontiers give investors a more granular view of which models might provide better performance for their specific risk tolerance or return targets.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19165745"
    },
    {
        "id": 25975,
        "title": "Reinforcement learning increasingly shapes memory specificity from childhood to adulthood",
        "authors": "Kate Nussenbaum, Catherine A. Hartley",
        "published": "No Date",
        "citations": 0,
        "abstract": "In some contexts, abstract stimulus representations can effectively promote the pursuit of reward, whereas in others, more detailed representations are needed to guide choice. Here, using a novel reinforcement-learning task, we asked how children, adolescents, and adults flexibly adjust the specificity of the representations used for learning based on experienced reward statistics, as well as how the specificity of these learning representations influences subsequent memory. Across two experiments (total N = 224), we found that children, adolescents, and adults flexibly up- and down-weighted more detailed versus broader stimulus representations, depending on the reward structure of the environment. The representations used for learning shaped mnemonic specificity; placing greater weight on detailed representations during value-guided learning enhanced subsequent memory for stimulus details, while placing greater weight on broader, categorical representations enhanced memory only for categorical information. Moreover, the relation between learning and memory strengthened with age; relative to adults, children demonstrated reduced coupling between the specificity of the representations used for value-based choice and the specificity of their subsequent memories. Our work demonstrates that from early in life, reward shapes the granularity with which the world is partitioned, which in turn exerts an increasing influence on how experiences are remembered into adulthood.",
        "link": "http://dx.doi.org/10.31234/osf.io/utsxn"
    },
    {
        "id": 25976,
        "title": "FleetRL: Realistic Reinforcement Learning&amp;nbsp;Environments for Commercial Vehicle Fleets",
        "authors": "Enzo Cording, Jagruti Thakur",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4618687"
    },
    {
        "id": 25977,
        "title": "Two types of locus coeruleus norepinephrine neurons drive reinforcement learning",
        "authors": "Zhixiao Su, Jeremiah Y. Cohen",
        "published": "No Date",
        "citations": 12,
        "abstract": "The cerebral cortex generates flexible behavior by learning. Reinforcement learning is thought to be driven by error signals in midbrain dopamine neurons. However, they project more densely to basal ganglia than cortex, leaving open the possibility of another source of learning signals for cortex. The locus coeruleus (LC) contains most of the brain’s norepinephrine (NE) neurons and project broadly to cortex. We measured activity from identified mouse LC-NE neurons during a behavioral task requiring ongoing learning from reward prediction errors (RPEs). We found two types of LC-NE neurons: neurons with wide action potentials (type I) were excited by positive RPE and showed an increasing relationship with change of choice likelihood. Neurons with thin action potentials (type II) were excited by lack of reward and showed a decreasing relationship with change of choice likelihood. Silencing LC-NE neurons changed future choices, as predicted from the electrophysiological recordings and a model of how RPEs are used to guide learning. We reveal functional heterogeneity of a neuromodulatory system in the brain and show that NE inputs to cortex act as a quantitative learning signal for flexible behavior.",
        "link": "http://dx.doi.org/10.1101/2022.12.08.519670"
    },
    {
        "id": 25978,
        "title": "Top-down design of protein architectures with reinforcement learning",
        "authors": "A.J. Borst, D. Baker",
        "published": "2023-5-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2210/pdb8f54/pdb"
    },
    {
        "id": 25979,
        "title": "Attributation Analysis of Reinforcement Learning Based Highway Driver",
        "authors": "Nikodem Pankiewicz, Paweł Kowalczyk",
        "published": "No Date",
        "citations": 0,
        "abstract": "While machine learning models are powering more and more everyday devices, there is a growing need for explaining them. This especially applies to the use of Deep Reinforcement Learning in solutions that require security, such as vehicle motion planning. In this paper, we propose a method of understanding what the RL agent&rsquo;s decision is based on. The method relies on conducting statistical analysis on a massive set of state-decisions samples. It indicates which input features have an impact on the agent&rsquo;s decision and the relationships between decisions, the significance of the input features, and their values. The method allows us for determining whether the process of making a decision by the agent is coherent with human intuition and what contradicts it. We applied the proposed method to the RL motion planning agent which is supposed to drive a vehicle safely and efficiently on a highway. We find out that making such analysis allows for a better understanding agent&rsquo;s decisions, inspecting its behavior, debugging the ANN model, and verifying the correctness of input values, which increases its credibility.",
        "link": "http://dx.doi.org/10.20944/preprints202209.0196.v1"
    },
    {
        "id": 25980,
        "title": "DVNE-DRL: Dynamic virtual network embedding algorithm based on deep reinforcement learning",
        "authors": "Xiancui Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nVirtual Network Embedding (VNE), as the key challenge of network resource management technology, lies in the contradiction between online embedding decision and pursuing long-term average revenue goals. Most of the previous work ignored the dynamics in Virtual Network (VN) modeling, or could not automatically detect the complex and time-varying network state to provide a reasonable network embedding scheme. In view of this, we model a network embedding framework where the topology and resource allocation change dynamically with the number of network users and workload, and then introduce a deep reinforcement learning method to solve the VNE problem. Further, a dynamic virtual network embedding algorithm based on Deep Reinforcement Learning (DRL), named DVNE-DRL, is proposed. In DVNE-DRL, VNE is modeled as Markov Decision Process (MDP), and Reinforcement Learning Agent (RLA) is used to periodically detect the state of network environment and provide a more reasonable network embedding scheme in real time. Different from the previous methods based on DRL algorithm to solve VNE problem, this paper improves the method of feature extraction and matrix optimization, and considers the characteristics of virtual network and physical network together to alleviate the problem of redundancy and slow convergence. The simulation results show that compared with the existing advanced algorithms, the acceptance rate and average revenue of DVNE-DRL are increased by about 25% and 35%, respectively.\n K E Y W O R D S\nVNE, DRL, embedding framework, DRL-DVNE，MDP，feature extraction",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2659912/v1"
    },
    {
        "id": 25981,
        "title": "Deep Reinforcement Learning",
        "authors": "Moez Krichen",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10306453"
    },
    {
        "id": 25982,
        "title": "Deep Reinforcement Learning for Building Honeypots against Runtime DOS Attack",
        "authors": "Selvakumar Veluchamy, RubaSoundar Kathavarayan",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nHoneypot is a network environment used to protect the legitimate network resources from attacks. Honeypot creates an environment that impresses attackers to inject their activities to steal resources. This is a way to detect the attacks by doing attack detection procedures. In this work, Denial of Service (DoS) attacks are effectively detected by proposed honeypot system. Machine Learning (ML) and Deep Learning (DL) methods evolve in many areas to build intelligent decision making systems. This work uses DL approaches and secures event validation procedures for finding predicting DoS attacks. The proposed system called Deep Adaptive Reinforcement Learning for Honeypots (DARLH) is implemented to monitor internal and external DoS attacks. In the honeypot environment, the proposed DARLH system implements DARL based IDS (Intrusion Detection System) agents and Deep Recurrent Neural Network (DRNN) based IDS agents for monitoring multiple runtime DoS attacks. These techniques support for dynamic IDS against DoS attack. In addition, the DARLH creates protected poison distribution and server side supervision system for keeping the monitoring events legitimate. This work is implemented and performance is evaluated. The results are compared with existing systems like GNBH, BCH and RNSG. In this comparison, the proposed system provides 5–10% better results than other systems.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-207770/v1"
    },
    {
        "id": 25983,
        "title": "Explore High Thermal Conductivity Amorphous Polymers using Reinforcement Learning",
        "authors": "RUIMIN MA, Hanfeng Zhang, Tengfei Luo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Developing amorphous polymers with desirable thermal conductivity has significant implications, as they are ubiquitous in applications where thermal transport is critical. Conventional Edisonian approaches are slow and without guarantee of success in material development. In this work, using a reinforcement learning scheme, we design polymers with thermal conductivity above 0.4 W/m- K. We leverage a machine learning model trained against 469 thermal conductivity data calculated from high-throughput molecular dynamics (MD) simulations as the surrogate for thermal conductivity prediction, and we use a recurrent neural network trained with around one million virtual polymer structures as a polymer generator. For all newly generated polymers with thermal conductivity > 0.400 W/m-K, we have evaluated their synthesizability by calculating the synthesis accessibility score and validated the thermal conductivity of selected polymers using MD simulations. The best thermally conductive polymer designed has a MD-calculated thermal conductivity of 0.693 W/m-K, which is also estimated to be easily synthesizable. Our demonstrated inverse design scheme based on reinforcement learning may advance polymer development with target properties, and the scheme can also be generalized to other materials development tasks for different applications.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-6jftj"
    },
    {
        "id": 25984,
        "title": "Deep Reinforcement Learning of Dialogue Policies with Less Weight Updates",
        "authors": "Heriberto Cuayáhuitl, Seunghak Yu",
        "published": "2017-8-20",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2017-1060"
    },
    {
        "id": 25985,
        "title": "Training Agents to Play 2D Games Using Reinforcement Learning",
        "authors": "Harshil Jhaveri, Nishay Madhani, Narendra M. Shekokar",
        "published": "2021-7-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003133681-21"
    },
    {
        "id": 25986,
        "title": "Reinforcement Learning in Sports",
        "authors": "Kevin Ashley",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-5772-2_10"
    },
    {
        "id": 25987,
        "title": "Research on Image Recognition based on Reinforcement Learning",
        "authors": "Jiabin Luo, Rongzhen Luo",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvidl58838.2023.10166036"
    },
    {
        "id": 25988,
        "title": "Amygdala and ventral striatum population codes implement multiple learning rates for reinforcement learning",
        "authors": "Bruno B. Averbeck",
        "published": "2017-11",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci.2017.8285354"
    },
    {
        "id": 25989,
        "title": "Applications of Reinforcement Learning",
        "authors": "Matthew F. Dixon, Igor Halperin, Paul Bilokon",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41068-1_10"
    },
    {
        "id": 25990,
        "title": "Optimal Tracking Control of Vehicle Cooperative Platoon Based on Reinforcement Learning",
        "authors": "ChangCheng Li",
        "published": "2022-8-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls55054.2022.9858414"
    },
    {
        "id": 25991,
        "title": "Learning Highway Ramp Merging Via Reinforcement Learning with Temporally-Extended Actions",
        "authors": "Samuel Triest, Adam Villaflor, John M. Dolan",
        "published": "2020-10-19",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv47402.2020.9304841"
    },
    {
        "id": 25992,
        "title": "SparseIDS: Learning Packet Sampling with Reinforcement Learning",
        "authors": "Maximilian Bachl, Fares Meghdouri, Joachim Fabini, Tanja Zseby",
        "published": "2020-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cns48642.2020.9162253"
    },
    {
        "id": 25993,
        "title": "Sample Efficient Reinforcement Learning with Active Learning for Molecular Design",
        "authors": "Michael Dodds, Jeff Guo, Thomas Löhr, Alessandro Tibo, Ola Engkvist, Jon Paul Janet",
        "published": "No Date",
        "citations": 2,
        "abstract": "Reinforcement learning (RL) is a powerful and flexible paradigm for searching for solutions in high-dimensional action spaces. However, bridging the gap between playing computer games with thousands of simulated episodes and solving real scientific problems with complex and involved environments (up to actual laboratory experiments) requires improvements in terms of sample efficiency to make the most of expensive information. The discovery of new drugs is a major commercial application of RL, motivated by the very large nature of the chemical space and the need to perform multiparameter optimization (MPO) across different properties. In silico methods, such as virtual library screening (VS) and de-novo molecular generation with RL, show great promise in accelerating this search. However, incorporation of increasingly complex computational models in these workflows requires increasing sample efficiency. Here, we introduce an active learning system linked with an RL model (RL-AL) for molecular design, which aims to improve the sample-efficiency of the optimization process. We identity and characterize unique challenges combining RL and AL, investigate the interplay between the systems, and develop a novel AL approach to solve the MPO problem. Our approach greatly expedites the search for novel solutions relative to baseline-RL for simple ligand- and structure-based oracle functions, with a 1000-75 000%-increase in hits generated for a fixed oracle budget and a 14-65-fold reduction in computational time to find a specific number of hits. Furthermore, compounds discovered through RL-AL display substantial enrichment of a multi-parameter scoring objective, indicating superior efficacy in curating high-scoring compounds, without a reduction in output diversity. This significant acceleration improves the feasibility of oracle functions that have largely been overlooked in RL due to high computational costs, for example free energy perturbation methods, and in principle is applicable to any RL domain.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2023-j88dg"
    },
    {
        "id": 25994,
        "title": "Deep Reinforcement Learning",
        "authors": "Chen Lei",
        "published": "2021",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-2233-5_10"
    },
    {
        "id": 25995,
        "title": "Learning to Paint With Model-Based Deep Reinforcement Learning",
        "authors": "Zhewei Huang, Shuchang Zhou, Wen Heng",
        "published": "2019-10",
        "citations": 69,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2019.00880"
    },
    {
        "id": 25996,
        "title": "The partial reinforcement extinction effect depends on learning about nonreinforced trials rather than reinforcement rate.",
        "authors": "Justin A. Harris, Dorothy W. S. Kwok, Daniel A. Gottlieb",
        "published": "2019-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1037/xan0000220"
    },
    {
        "id": 25997,
        "title": "A survey on deep learning and deep reinforcement learning in robotics with a tutorial on deep reinforcement learning",
        "authors": "Eduardo F. Morales, Rafael Murrieta-Cid, Israel Becerra, Marco A. Esquivel-Basaldua",
        "published": "2021-11",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11370-021-00398-z"
    },
    {
        "id": 25998,
        "title": "Learning Battles in ViZDoom via Deep Reinforcement Learning",
        "authors": "Kun Shao, Dongbin Zhao, Nannan Li, Yuanheng Zhu",
        "published": "2018-8",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2018.8490423"
    },
    {
        "id": 25999,
        "title": "Distributed learning of energy contracts negotiation strategies with collaborative reinforcement learning",
        "authors": "Tiago Pinto, Zita Vale",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eem.2019.8916342"
    },
    {
        "id": 26000,
        "title": "Active Learning for Image Classification: A Deep Reinforcement Learning Approach",
        "authors": "Le Sun, Yihong Gong",
        "published": "2019-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cchi.2019.8901911"
    },
    {
        "id": 26001,
        "title": "Multi-Agent Exploration for Faster and Reliable Deep Q-Learning Convergence in Reinforcement Learning",
        "authors": "Abhijit Majumdar, Patrick Benavidez, Mo Jamshidi",
        "published": "2018-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/wac.2018.8430409"
    },
    {
        "id": 26002,
        "title": "Exponential TD Learning: A Risk-Sensitive Actor-Critic Reinforcement Learning Algorithm",
        "authors": "Erfaun Noorani, Christos N. Mavridis, John S. Baras",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156626"
    },
    {
        "id": 26003,
        "title": "Learning and Adapting Behavior of Autonomous Vehicles through Inverse Reinforcement Learning",
        "authors": "Rainer Trauth, Marc Kaufeld, Maximilian Geisslinger, Johannes Betz",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv55152.2023.10186668"
    },
    {
        "id": 26004,
        "title": "Towards Explainable Deep Reinforcement Learning for Traffic Signal Control",
        "authors": "Lincoln Schreiber, Gabriel Ramos, Ana Bazzan",
        "published": "2021-7-24",
        "citations": 1,
        "abstract": "Deep reinforcement learning has shown potential for traffic signal control. However, the lack of explainability has limited its use in real-world conditions. In this work, we present a Deep Q-learning approach, with the SHAP framework, able to explain its policy. Our approach can explain the impact of features on each action, which promotes the understanding of how the agent behaves in the face of different traffic conditions. Furthermore, our approach improved travel time, waiting time, and speed by 21.49%, 27.97%, 20.87%, compared to fixed-time traffic signal controllers.",
        "link": "http://dx.doi.org/10.52591/202107249"
    },
    {
        "id": 26005,
        "title": "Resource Rationing for Federated Learning with Reinforcement Learning",
        "authors": "Mingqi Han, Xinghua Sun, Sihui Zheng, Xijun Wang, Hongzhou Tan",
        "published": "2021-11-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/comcomap53641.2021.9653111"
    },
    {
        "id": 26006,
        "title": "Learning Crowd-Aware Robot Navigation from Challenging Environments via Distributed Deep Reinforcement Learning",
        "authors": "Sango Matsuzaki, Yuji Hasegawa",
        "published": "2022-5-23",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9812011"
    },
    {
        "id": 26007,
        "title": "Deep Reinforcement Learning for Exact Combinatorial Optimization: Learning to Branch",
        "authors": "Tianyu Zhang, Amin Banitalebi-Dehkordi, Yong Zhang",
        "published": "2022-8-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956256"
    },
    {
        "id": 26008,
        "title": "A reinforcement learning algorithm for scheduling parallel processors with identical speedup functions",
        "authors": "Farid Ziaei, Mohammad Ranjbar",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2023.100485"
    },
    {
        "id": 26009,
        "title": "Learning to Coordinate with Deep Reinforcement Learning in Doubles Pong Game",
        "authors": "Elhadji Amadou Oury Diallo, Ayumi Sugiyama, Toshiharu Sugawara",
        "published": "2017-12",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2017.0-184"
    },
    {
        "id": 26010,
        "title": "Towards Off-policy Evaluation as a Prerequisite for Real-world Reinforcement Learning in Building Control",
        "authors": "Bingqing Chen, Ming Jin, Zhe Wang, Tianzhen Hong, Mario Bergés",
        "published": "2020-11-17",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3427773.3427871"
    },
    {
        "id": 26011,
        "title": "Indoor Point-to-Point Navigation with Deep Reinforcement Learning and Ultra-Wideband",
        "authors": "Enrico Sutera, Vittorio Mazzia, Francesco Salvetti, Giovanni Fantin, Marcello Chiaberge",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010202600380047"
    },
    {
        "id": 26012,
        "title": "The partial reinforcement extinction effect depends on learning about nonreinforced trials rather than reinforcement rate.",
        "authors": "Justin A. Harris, Dorothy W. S. Kwok, Daniel A. Gottlieb",
        "published": "2019-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1037/xan0000220"
    },
    {
        "id": 26013,
        "title": "A survey on deep learning and deep reinforcement learning in robotics with a tutorial on deep reinforcement learning",
        "authors": "Eduardo F. Morales, Rafael Murrieta-Cid, Israel Becerra, Marco A. Esquivel-Basaldua",
        "published": "2021-11",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11370-021-00398-z"
    },
    {
        "id": 26014,
        "title": "Reinforcement Learning: Playing Tic-Tac-Toe",
        "authors": "Jocelyn Ho, Jeffrey Huang, Benjamin Chang, Allison Liu, Zoe Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Machine learning constructs computer systems that develop through experience. Applications surround disciplines in daily life ranging from malware filtering to image recognition. Recent research has shifted towards maximizing efficiency in decision-making, creating algorithms that quickly and accurately process patterns to generate insight. This research focuses on reinforcement learning, a paradigm of machine learning that makes decisions through maximizing reward. Specifically, we use Q-learning – a model-free reinforcement learning algorithm – to assign scores for different decisions given the unique states of the problem. Widyantoro et al. (2009) has studied the effect of Q-learning on learning to play Tic-Tac-Toe. However, the study yielded a win/tie rate of less than 50 percent. We believe that does not represent an effective algorithm to fully exploit the benefits of Q-learning. In the same environment, this research aims to close the gaps in the effectiveness of Q-learning while minimizing human input. Data were processed by setting the epsilon value as 0.9 to ensure randomness, then consecutively decrease with a constant rate as possible states increase. The program played 300,000 games against its previous version, eventually securing a win/tie rate of approximately 90 percent. Future directions include improving the efficiency of Q-learning algorithms and applying the research in practical fields.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20407575"
    },
    {
        "id": 26015,
        "title": "Emril:Ensemble Method Based on Reinforcement Learning for Imbalanced Drifting Data Streams",
        "authors": "Muhammad Usman, Huanhuan Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4545034"
    },
    {
        "id": 26016,
        "title": "Reflected Beam Metasurface Controlled by Reinforcement Learning",
        "authors": "Yilin Jiang, Yuxuan Pan, Chengyue Yan, Jinxin Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Metasurface is a new type of electromagnetic material based on the\ngeneralized Snell theorem . It can achieve different reflection and\nincidence effects from natural materials, which has attracted extensive\nattention in the field of electromagnetics. In this paper, a\none-dimensional metasurface is designed and simulated which can change\nthe reflected electromagnetic wave beam. The additional phase difference\non the metasurface unit is controlled by 1 bit coding to control the\ndirection of the reflected wave beam. Combined with the Deep Q Network\nalgorithm in the reinforcement learning tool, the coding corresponding\nto the beam meeting the target direction is found.",
        "link": "http://dx.doi.org/10.22541/au.166176002.26324949/v1"
    },
    {
        "id": 26017,
        "title": "Direct Portfolio Selection Using Recurrent Reinforcement Learning",
        "authors": "Lin Li",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3479973"
    },
    {
        "id": 26018,
        "title": "Author response for \"Connectivity conservation planning through deep reinforcement learning\"",
        "authors": " Julián Equihua,  Michael Beckmann,  Ralf Seppelt",
        "published": "2023-12-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.14300/v2/response1"
    },
    {
        "id": 26019,
        "title": "Off-Policy Safe Reinforcement Learning for Nonlinear Discrete-Time Systems",
        "authors": "Mayank Shekhar JHA, Bahare Kiumarsi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4559729"
    },
    {
        "id": 26020,
        "title": "Reinforcement Learning-Based Load Balancing for Heavy Traffic Internet of Things",
        "authors": "jianjun lei, Jie Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4442739"
    },
    {
        "id": 26021,
        "title": "Real-Time Traffic Prediction with Deep Reinforcement Learning and Meteorology Data",
        "authors": "Zhou (Eric) Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nReal-time traffic prediction plays a crucial role in optimizing transportation efficiency and mitigating congestion in urban road networks. In this paper, we propose a novel approach that combines deep reinforcement learning (DRL) and meteorology data for real-time traffic prediction. By integrating DRL algorithms, our approach captures the dynamic decision-making processes involved in traffic prediction, while the inclusion of meteorology data allows for a comprehensive understanding of the impact of weather conditions on traffic patterns. We conduct extensive experiments on a real-world traffic dataset and compare our method with state-of-the-art approaches. The results demonstrate the superiority of our proposed method in terms of prediction accuracy, outperforming traditional statistical models, machine learning-based approaches, and deep learning techniques. Our approach provides valuable insights for traffic management, route planning, and resource allocation in urban road networks.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3155165/v1"
    },
    {
        "id": 26022,
        "title": "The Art of Reinforcement Learning",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6"
    },
    {
        "id": 26023,
        "title": "Deep Reinforcement Learning Processor Design for Mobile Applications",
        "authors": "Juhyoung Lee, Hoi-Jun Yoo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-36793-9"
    },
    {
        "id": 26024,
        "title": "Decision letter for \"EPPTA: Efficient partially observable reinforcement learning agent for penetration testing applications\"",
        "authors": "",
        "published": "2023-9-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/eng2.12818/v1/decision1"
    },
    {
        "id": 26025,
        "title": "Deep Reinforcement Learning for Capacity Allocation with Fare-Locking",
        "authors": "bi wenjie, Chunjuan Fu, Haiying Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4622858"
    },
    {
        "id": 26026,
        "title": "Algorithmic trading on financial time series using Deep Reinforcement Learning",
        "authors": "Alireza Asghari, Nasser Mozayani",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe use of technology in financial markets has led to extensive changes in conventional trading structures.Today, most orders that reach exchanges are created by algorithmic trading agents.\nToday, machine learning-based methods play an important role in building automated trading systems. The increasing complexity and dynamism of financial markets are among the key challenges of these methods. The most widely used machine learning approach is supervised learning, but in interactive environments, the use of supervised learning alone has limitations such as difficulty in defining appropriate labels and lack of modeling of the dynamic nature of the market. Due to the good performance of deep reinforcement learning-based approaches, we will use these approaches to solve the mentioned problems.\nIn this paper, we presented a deep reinforcement learning framework for trading in the financial market, a set of input features and indicators selected and tailored to the purpose of the problem, reward function, appropriate models based on fully connected, convolutional and hybrid networks. The proposed top models traded under real market conditions such as transaction costs and then were evaluated. In addition to outperforming the buy and hold strategy, these models achieved excellent cumulative returns while having appropriate risk metrics.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3910354/v1"
    },
    {
        "id": 26027,
        "title": "Review for \"High-dimensional reinforcement learning for optimization and control of ultracold quantum gases\"",
        "authors": " Xiaolong Zhao",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad1437/v2/review2"
    },
    {
        "id": 26028,
        "title": "Review for \"The value–complexity trade-off for reinforcement learning based brain–computer interfaces\"",
        "authors": "Christopher wirth",
        "published": "2020-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1741-2552/abc8d8/v1/review2"
    },
    {
        "id": 26029,
        "title": "Quantum Reinforcement Learning with Quantum Photonics",
        "authors": "Lucas Lamata",
        "published": "2021-1-28",
        "citations": 12,
        "abstract": "Quantum machine learning has emerged as a promising paradigm that could accelerate machine learning calculations. Inside this field, quantum reinforcement learning aims at designing and building quantum agents that may exchange information with their environment and adapt to it, with the aim of achieving some goal. Different quantum platforms have been considered for quantum machine learning and specifically for quantum reinforcement learning. Here, we review the field of quantum reinforcement learning and its implementation with quantum photonics. This quantum technology may enhance quantum computation and communication, as well as machine learning, via the fruitful marriage between these previously unrelated fields.",
        "link": "http://dx.doi.org/10.3390/photonics8020033"
    },
    {
        "id": 26030,
        "title": "Peer Review #1 of \"SANgo: a storage infrastructure simulator with reinforcement learning support (v0.1)\"",
        "authors": "R Goswami",
        "published": "2020-5-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.271v0.1/reviews/1"
    },
    {
        "id": 26031,
        "title": "Author response: Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.3.sa4"
    },
    {
        "id": 26032,
        "title": "Top-down design of protein architectures with reinforcement learning",
        "authors": "A.J. Borst, D. Baker",
        "published": "2023-5-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2210/pdb8f4x/pdb"
    },
    {
        "id": 26033,
        "title": "Top-down design of protein architectures with reinforcement learning",
        "authors": "A.J. Borst, D. Baker",
        "published": "2023-5-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2210/pdb8f53/pdb"
    },
    {
        "id": 26034,
        "title": "Cryptocurrency portfolio management with deep reinforcement learning",
        "authors": "Zhengyao Jiang, Jinjun Liang",
        "published": "2017-9",
        "citations": 99,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/intellisys.2017.8324237"
    },
    {
        "id": 26035,
        "title": "Learning Transferable Policies for Autonomous Planetary Landing via Deep Reinforcement Learning",
        "authors": "Giulia Ciabatti, Shreyansh Daftry, Roberto Capobianco",
        "published": "2021-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2021-4006"
    },
    {
        "id": 26036,
        "title": "Deep Learning for Control: a non-Reinforcement Learning View",
        "authors": "Ion Matei, Raj Minhas, Maksym Zhenirovskyy, Johan de Kleer, Rahul Rai",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147287"
    },
    {
        "id": 26037,
        "title": "Learning More Complex Actions with Deep Reinforcement Learning",
        "authors": "Chenxi Wang, Youtian Du, Shengyuan Xie, Yongdi Lu",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/irc52146.2021.00028"
    },
    {
        "id": 26038,
        "title": "Reinforcement Learning Based Communication Security for Unmanned Aerial Vehicles",
        "authors": "Liang Xiao, Donghua Jiang, Sicong Liu",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-6726-5_3"
    },
    {
        "id": 26039,
        "title": "Reinforcement Learning Using Bayesian Algorithms with Applications",
        "authors": "H. Raghupathi, G Ravi, Rajan Maduri",
        "published": "2022-3-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003164265-5"
    },
    {
        "id": 26040,
        "title": "Fault Diagnosis for A Class of Nonlinear Systems Based on Reinforcement Learning and Deterministic Learning",
        "authors": "Zejian Zhu, Weiming Wu, Tianrui Chen, Cong Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this paper a novel fault diagnosis (FD) approach combing the rein-forcement learning (RL) and the deterministic learning theory (DLT)is proposed for a class of discrete-time nonlinear system with unknowndynamics. First, a bank of DLT-based dynamical neural network (NN)identifiers are utilized to achieve locally-accurate approximations ofthe unknown system dynamics along the normal and fault trajecto-ries. Based on this, a novel feature learning method combing the RLwith DLT is proposed to further adapt the NN weights to extract dis-criminative features. The extracted features are represented by constant NNs. Finally, constant NN-based dynamical estimators are constructedto achieve rapid FD. The novelties of the proposed methods are: 1)according to the DLT, the exponential convergence of the NN weightscan be rigorously analysed based on the Lyapunov stability theory;2) a new class of strategic utility function is designed based on theconcept of the synchronization error in DLT-based dynamical pat-tern recognition approach, which is different from other RL-based FDtechniques that penalise the future wrong FD decisions. Simulationresults shows the practical significance of the proposed FD method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1930103/v1"
    },
    {
        "id": 26041,
        "title": "Effective Reinforcement Learning using Transfer Learning",
        "authors": "N Sandeep Varma, Pradyumna Rahul K, Vaishnavi Sinha",
        "published": "2022-7-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdsis55133.2022.9915962"
    },
    {
        "id": 26042,
        "title": "Inverse reinforcement learning for dexterous hand manipulation",
        "authors": "Jedrzej Orbik, Alejandro Agostini, Dongheui Lee",
        "published": "2021-8-23",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl49984.2021.9515637"
    },
    {
        "id": 26043,
        "title": "Optimal Trajectory Learning for UAV-BS Video Provisioning System: A Deep Reinforcement Learning Approach",
        "authors": "Dohyun Kwon, Joongheon Kim",
        "published": "2019-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin.2019.8718194"
    },
    {
        "id": 26044,
        "title": "Offline reinforcement learning with task hierarchies",
        "authors": "Devin Schwab, Soumya Ray",
        "published": "2017-10",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-017-5650-8"
    },
    {
        "id": 26045,
        "title": "Contrastive Learning Methods for Deep Reinforcement Learning",
        "authors": "Di Wang, Mengqi Hu",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3312383"
    },
    {
        "id": 26046,
        "title": "Reinforcement Learning with Policy Gradients",
        "authors": "Ryan G. McClarren",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-70388-2_9"
    },
    {
        "id": 26047,
        "title": "Multiagent Modeling and Learning",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch20"
    },
    {
        "id": 26048,
        "title": "Can Meta-Interpretive Learning outperform Deep Reinforcement Learning of Evaluable Game strategies?",
        "authors": "Céline Hocquette",
        "published": "2019-8",
        "citations": 1,
        "abstract": "World-class human players have been outperformed in a number of complex two person games such as Go by Deep Reinforcement Learning systems GO. However, several drawbacks can be identified for these systems: 1) The data efficiency is unclear given they appear to require far more training games to achieve such performance than any human player might experience in a lifetime. 2) These systems are not easily interpretable as they provide limited explanation about how decisions are made. 3) These systems do not provide transferability of the learned strategies to other games. We study in this work how an explicit logical representation can overcome these limitations and introduce a new logical system called MIGO designed for learning two player game optimal strategies. It benefits from a strong inductive bias which provides the capability to learn efficiently from a few examples of games played. Additionally, MIGO's learned rules are relatively easy to comprehend, and are demonstrated to achieve significant transfer learning.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/909"
    },
    {
        "id": 26049,
        "title": "Scaling Deep Reinforcement Learning for Autonomous Mobility on Demand Systems: A Curriculum Learning Approach",
        "authors": "Leandro Parada, Jose Escribano, Yuxiang Feng, Renos Karamanis, Panagiotis Angeloudis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4450260"
    },
    {
        "id": 26050,
        "title": "End-to-End Reinforcement Learning for Multi-agent Continuous Control",
        "authors": "Zilong Jiao, Jae Oh",
        "published": "2019-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2019.00100"
    },
    {
        "id": 26051,
        "title": "Training Unity Machine Learning Agents using reinforcement learning method",
        "authors": "Marat Urmanov, Madina Alimanova, Askar Nurkey",
        "published": "2019-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecco48375.2019.9043194"
    },
    {
        "id": 26052,
        "title": "Deep Reinforcement Learning for Text and Speech",
        "authors": "Uday Kamath, John Liu, James Whitaker",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-14596-5_13"
    },
    {
        "id": 26053,
        "title": "Consensus Q‐Learning for Multi‐agent Cooperative Planning",
        "authors": "",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119699057.ch3"
    },
    {
        "id": 26054,
        "title": "Bootstrap Advantage Estimation for Policy Optimization in Reinforcement Learning",
        "authors": "Md Masudur Rahman, Yexiang Xue",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00041"
    },
    {
        "id": 26055,
        "title": "Punisher: A Deep Reinforcement Learning Model Trained by Correcting Bad Actions",
        "authors": "Jianyi Yang",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424763"
    },
    {
        "id": 26056,
        "title": "Improving reinforcement learning output feedback control for unknown nonlinear pure feedback system",
        "authors": "Dazi Li, Wei Wang",
        "published": "2017-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls.2017.8067720"
    },
    {
        "id": 26057,
        "title": "Nicotine-free electronic-cigarettes for smoking cessation: Occasional reinforcement during extinction",
        "authors": "Mina Fukuda",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.lmot.2021.101766"
    },
    {
        "id": 26058,
        "title": "Node Correlation Effects on Learning Dynamics in Networked Multiagent Reinforcement Learning",
        "authors": "Valentina Guleva",
        "published": "2022-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dcna56428.2022.9923243"
    },
    {
        "id": 26059,
        "title": "Learning to Score: Tuning Cluster Schedulers through Reinforcement Learning",
        "authors": "Martin Asenov, Qiwen Deng, Gingfung Yeung, Adam Barker",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ic2e59103.2023.00021"
    },
    {
        "id": 26060,
        "title": "Learning from Different Perspectives for Regret Reduction in Reinforcement Learning: A Free Energy Approach",
        "authors": "Milad Ghorbani, Reshad Hosseini, Seyed Pooya Shariatpanahi, Majid Nili Ahmadabadi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4431992"
    },
    {
        "id": 26061,
        "title": "Reinforcement learning methods and role of internet of things in civil engineering applications",
        "authors": "Kundan Meshram",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-443-15364-8.00009-3"
    },
    {
        "id": 26062,
        "title": "An Incentive Mechanism Design for Efficient Edge Learning by Deep Reinforcement Learning Approach",
        "authors": "Yufeng Zhan, Jiang Zhang",
        "published": "2020-7",
        "citations": 51,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infocom41043.2020.9155268"
    },
    {
        "id": 26063,
        "title": "Stability-certified reinforcement learning control via spectral normalization",
        "authors": "Ryoichi Takase, Nobuyuki Yoshikawa, Toshisada Mariyama, Takeshi Tsuchiya",
        "published": "2022-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2022.100409"
    },
    {
        "id": 26064,
        "title": "Transfer Learning for Multiagent Reinforcement Learning Systems",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5"
    },
    {
        "id": 26065,
        "title": "Learning Task-independent Joint Control for Robotic Manipulators with Reinforcement Learning and Curriculum Learning",
        "authors": "Lars Væhrens, Daniel Díez Álvarez, Ulrich Berger, Simon Bøgh",
        "published": "2022-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00201"
    },
    {
        "id": 26066,
        "title": "Network Structure Engineering for Dissemination Amplification in Social Networks using Reinforcement Learning",
        "authors": "Abdorasoul Ghasemi, Mohammad Ali Karami",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Network structure engineering aims to intervene in the network structure to improve a specific underlying process and has applications in many research fields like biology, transportation, and network design. Here we address the problem of online social networks (OSNs) engineering by link addition through the friend suggestion for an efficient dissemination process. We propose an end-to-end machine learning framework called <em>GCQL</em> for optimizing or generating an effective network structure to improve the dissemination process in OSNs. We first propose a dissemination model called <em>Complex-SII</em>, combining two widely used dissemination models in OSNs, SIR and IC, and complex contagion. The <em>GCQL</em> next uses a reinforcement learning algorithm called Neural Fitted <em>Q</em>-Iteration for optimizing a network structure for the dissemination process and uses a graph convolution network called <em>GCN</em> for graph embedding and learning the important patterns and characteristics of network structure for efficient spreading. The <em>GCQL</em> is trained with small-sized graphs and uses the learning outcomes for medium/large-sized graphs. Our simulation results show that <em>GCQL</em> can generate relatively more efficient network structures than the greedy approach for small-sized networks. <em>GCQL</em> also uses these results to develop relatively more efficient networks than the greedy approach for medium/large-sized network structures at a much higher speed. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21640700"
    },
    {
        "id": 26067,
        "title": "Front Matter",
        "authors": "",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119699057.fmatter"
    },
    {
        "id": 26068,
        "title": "Multi-Agent Reinforcement Learning Based Joint Resource Allocation for Ultra-Reliable Low-Latency Vehicular Communication",
        "authors": "Nasir Khan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Future vehicular networks must ensure ultra-reliable low-latency communication (URLLC) for the timely delivery of safety-critical information. Previously proposed resource allocation schemes for URLLC mostly rely on centralized optimization-based algorithms and cannot guarantee the reliability and latency requirements of vehicle-to-vehicle (V2V) communications. This paper investigates the joint power and blocklength allocation to minimize the worst-case decoding-error probability in the finite blocklength (FBL) regime for a URLLC-based V2V communication network. We formulate the problem as a non-convex mixed-integer nonlinear programming problem (MINLP). We first develop a centralized optimization theory-based algorithm based on the derivation of the joint convexity of the decoding error probability in the blocklength and transmit power variables within the region of interest. Next, we propose a two-layered multi-agent deep reinforcement learning based centrally trained and distributively executed framework. The first layer involves establishing multiple deep Q-networks (DQNs) at the central trainer to train the local DQNs for block length optimization. The second layer involves an actor-critic network and utilizes the deep deterministic policy-gradient (DDPG)-based algorithm to train the local actor network for each V2V link. Simulation results demonstrate that the proposed distributed scheme can achieve close-to-optimal solutions with a much lower computational complexity than the centralized optimization based solution. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21685046"
    },
    {
        "id": 26069,
        "title": "Reinforcement Learning for Optimal Public Watershed and Aquifer Distribution and Management Actions",
        "authors": "Roberto Ortega, Dana Carciumaru",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4643886"
    },
    {
        "id": 26070,
        "title": "A Micro Reinforcement Learning Architecture for Intrusion Detection Systems",
        "authors": "Boshra Darabi, Mozafar Bag-Mohammadi, Mojtaba Karami",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4701125"
    },
    {
        "id": 26071,
        "title": "Integrating Deep Reinforcement Learning Networks with Health System Simulations.",
        "authors": "Michael Allen, Thomas Monks",
        "published": "No Date",
        "citations": 2,
        "abstract": "Background and motivation: Combining Deep Reinforcement Learning (Deep RL) and Health Systems Simulations has significant potential, for both research into improving Deep RL performance and safety, and in operational practice. While individual toolkits exist for Deep RL and Health Systems Simulations, no framework to integrate the two has been established. Aim: Provide a framework for integrating Deep RL Networks with Health System Simulations, and to ensure this framework is compatible with Deep RL agents that have been developed and tested using OpenAI Gym. Methods: We developed our framework based on the OpenAI Gym framework, and demonstrate its use on a simple hospital bed capacity model. We built the Deep RL agents using PyTorch, and the Hospital Simulation using SimPy. Results: We demonstrate example models using a Double Deep Q Network or a Duelling Double Deep Q Network as the Deep RL agent. Conclusion: SimPy may be used to create Health System Simulations that are compatible with agents developed and tested on OpenAI Gym environments. GitHub repository of code: https://github.com/MichaelAllen1966/learninghospital",
        "link": "http://dx.doi.org/10.20944/preprints202007.0598.v1"
    },
    {
        "id": 26072,
        "title": "Multi-Agent Reinforcement Learning for Network Routing in Integrated Access Backhaul Networks",
        "authors": "Shahaf Yamin, Haim Permuter",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4414648"
    },
    {
        "id": 26073,
        "title": "Neuroevolution-based Inverse Reinforcement Learning",
        "authors": "Karan K. Budhraja, Tim Oates",
        "published": "2017-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec.2017.7969297"
    },
    {
        "id": 26074,
        "title": "Multi-Agent Deep Reinforcement Learning to Improve Efficiency of Autonomous Mining Transportation",
        "authors": "Shuwei Pei, Jue Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4604945"
    },
    {
        "id": 26075,
        "title": "Laser Engraver Control System based on Reinforcement Adversarial Learning",
        "authors": "Evgeny Nikolaev",
        "published": "2019-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rusautocon.2019.8867762"
    },
    {
        "id": 26076,
        "title": "Application of Deep Reinforcement Learning in Asset Liability Management",
        "authors": "Takura Wekwete, Rodwell Kufakunesu, Gusti van Zyl",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4474207"
    },
    {
        "id": 26077,
        "title": "A Review on Derivative Hedging using Reinforcement Learning",
        "authors": "Peng Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4217989"
    },
    {
        "id": 26078,
        "title": "Tiled Aperture Beam Combining with Reinforcement Learning",
        "authors": "Henrik Tunnermann, Akira Shirakawa",
        "published": "2020",
        "citations": 1,
        "abstract": "We show deep reinforcement learning for phase stabilization in tiled aperture coherent beam combining. The trained agent was able to stabilize the phase using only a far-field image as input.",
        "link": "http://dx.doi.org/10.1364/cleo_si.2020.sm1l.2"
    },
    {
        "id": 26079,
        "title": "Reinforcement learning influences memory specificity across development",
        "authors": "Kate Nussenbaum, Catherine Hartley",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1096-0"
    },
    {
        "id": 26080,
        "title": "Application of Deep Reinforcement Learning in Asset Liability Management",
        "authors": "Takura Wekwete, Rodwell Kufakunesu, Gusti van Zyl",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4417872"
    },
    {
        "id": 26081,
        "title": "Review for \"Prefrontal transcranial magnetic stimulation boosts response vigor during reinforcement learning in healthy adults\"",
        "authors": " Lieke Hofmans",
        "published": "2022-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/ejn.15905/v2/review1"
    },
    {
        "id": 26082,
        "title": "Decision letter for \"The value–complexity trade-off for reinforcement learning based brain–computer interfaces\"",
        "authors": "",
        "published": "2020-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1741-2552/abc8d8/v1/decision1"
    },
    {
        "id": 26083,
        "title": "Decision letter for \"The value–complexity trade-off for reinforcement learning based brain–computer interfaces\"",
        "authors": "",
        "published": "2020-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1741-2552/abc8d8/v2/decision1"
    },
    {
        "id": 26084,
        "title": "Decision letter for \"High-dimensional reinforcement learning for optimization and control of ultracold quantum gases\"",
        "authors": "",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad1437/v1/decision1"
    },
    {
        "id": 26085,
        "title": "Decision letter for \"High-dimensional reinforcement learning for optimization and control of ultracold quantum gases\"",
        "authors": "",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad1437/v2/decision1"
    },
    {
        "id": 26086,
        "title": "Explaining Voltage Control Decisions: A Scenario-Based Approach in Deep Reinforcement Learning",
        "authors": "Blaz Dobravec, Jure Žabkar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4535502"
    },
    {
        "id": 26087,
        "title": "Comparing Deep Reinforcement Learning Architectures for Autonomous Racing",
        "authors": "Benjamin Evans, Hendrik  Willem Jordaan, Herman  Arnold Engelbrecht",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4496980"
    },
    {
        "id": 26088,
        "title": "A Decentralized Path Planning Model Based on Deep Reinforcement Learning",
        "authors": "dong guo, Shouwen Ji, yanke yao",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4411759"
    },
    {
        "id": 26089,
        "title": "Review for \"Mental health analysis for college students based on pattern recognition and reinforcement learning\"",
        "authors": " Jianhui Lv",
        "published": "2023-6-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/itl2.453/v2/review2"
    },
    {
        "id": 26090,
        "title": "Shared Autonomy via Deep Reinforcement Learning",
        "authors": "Siddharth Reddy, Anca Dragan, Sergey Levine",
        "published": "2018-6-26",
        "citations": 73,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2018.xiv.005"
    },
    {
        "id": 26091,
        "title": "Autonomous Driving using Deep Reinforcement Learning in Urban Environment",
        "authors": "Hashim Shakil Ansari, Goutam R,  ",
        "published": "2019-4-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd23442"
    },
    {
        "id": 26092,
        "title": "PARL: A Dialog System Framework with Prompts as Actions for Reinforcement Learning",
        "authors": "Tao Xiang, Yangzhe Li, Monika Wintergerst, Ana Pecini, Dominika Młynarczyk, Georg Groh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011725200003393"
    },
    {
        "id": 26093,
        "title": "Accelerated Variant of Reinforcement Learning Algorithms for Light Control with Non-stationary User Behaviour",
        "authors": "Nassim Haddam, Benjamin Boulakia, Dominique Barth",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010987900003203"
    },
    {
        "id": 26094,
        "title": "Adaptive Action Supervision in Reinforcement Learning from Real-World Multi-Agent Demonstrations",
        "authors": "Keisuke Fujii, Kazushi Tsutsui, Atom Scott, Hiroshi Nakahara, Naoya Takeishi, Yoshinobu Kawahara",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012261100003636"
    },
    {
        "id": 26095,
        "title": "Reinforcement Learning als eine Grundlage für die Cross Domain Fusion heterogener Daten",
        "authors": "Sören Christensen, Sven Tomforde",
        "published": "2022-8",
        "citations": 0,
        "abstract": "AbstractWe propose to establish a research direction based on Reinforcement Learning in the scope of Cross Domain Fusion. More precisely, we combine the algorithmic approach of evolutionary rule-based Reinforcement Learning with the efficiency and performance of Deep Reinforcement Learning, while simultaneously developing a sound mathematical foundation. A possible scenario is traffic control in urban regions.",
        "link": "http://dx.doi.org/10.1007/s00287-022-01468-x"
    },
    {
        "id": 26096,
        "title": "A Deep Reinforcement Learning-Based Approach in Porker Game",
        "authors": "Yan Kong Yan Kong, Yefeng Rui Yan Kong, Chih-Hsien Hsia Yefeng Rui",
        "published": "2023-4",
        "citations": 1,
        "abstract": "\n                        <p>Recent years have witnessed the big success deep reinforcement learning achieved in the domain of card and board games, such as Go, chess and Texas Hold&rsquo;em poker. However, Dou Di Zhu, a traditional Chinese card game, is still a challenging task for deep reinforcement learning methods due to the enormous action space and the sparse and delayed reward of each action from the environment. Basic reinforcement learning algorithms are more effective in the simple environments which have small action spaces and valuable and concrete reward functions, and unfortunately, are shown not be able to deal with Dou Di Zhu satisfactorily. This work introduces an approach named Two-steps Q-Network based on DQN to playing Dou Di Zhu, which compresses the huge action space through dividing it into two parts according to the rules of Dou Di Zhu and fills in the sparse rewards using inverse reinforcement learning (IRL) through abstracting the reward function from experts&rsquo; demonstrations. It is illustrated by the experiments that two-steps Q-network gains great advancements compared with DQN used in Dou Di Zhu.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/199115992023043402004"
    },
    {
        "id": 26097,
        "title": "Reinforcement Learning, Unsupervised Methods, and Concept Drift in Stream Learning",
        "authors": "András A. Benczúr, Levente Kocsis, Róbert Pálovics",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-77525-8_327"
    },
    {
        "id": 26098,
        "title": "Grounding Hindsight Instructions in Multi-Goal Reinforcement Learning for Robotics",
        "authors": "Frank Roder, Manfred Eppe, Stefan Wermter",
        "published": "2022-9-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl53763.2022.9962207"
    },
    {
        "id": 26099,
        "title": "Towards Explainable Deep Reinforcement Learning for Traffic Signal Control",
        "authors": "Lincoln Schreiber, Gabriel Ramos, Ana Bazzan",
        "published": "2021-7-24",
        "citations": 0,
        "abstract": "Deep reinforcement learning has shown potential for traffic signal control. However, the lack of explainability has limited its use in real-world conditions. In this work, we present a Deep Q learning approach, with the SHAP framework, able to explain its policy. Our approach can explain the impact of features on each action, which promotes the understanding of how the agent behaves in the face of different traffic conditions. Furthermore, our approach improved travel time, waiting time, and speed by 21.49%, 27.97%, 20.87%, compared to fixed-time traffic signal controllers.",
        "link": "http://dx.doi.org/10.52591/lxai2021072414"
    },
    {
        "id": 26100,
        "title": "SCHED²: Scheduling Deep Learning Training via Deep Reinforcement Learning",
        "authors": "Yunteng Luan, Xukun Chen, Hanyu Zhao, Zhi Yang, Yafei Dai",
        "published": "2019-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9014110"
    },
    {
        "id": 26101,
        "title": "Asset Allocation: From Markowitz to Deep Reinforcement Learning",
        "authors": "Ricard Durall",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4148379"
    },
    {
        "id": 26102,
        "title": "Optimal Taxation with Incomplete Markets–An Exploration Via Reinforcement Learning",
        "authors": "Zhigang Feng, Jiequn Han, Shenghao Zhu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4758552"
    },
    {
        "id": 26103,
        "title": "Safe Reinforcement Learning with Nonlinear Dynamics via Model Predictive Shielding",
        "authors": "Osbert Bastani",
        "published": "2021-5-25",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9483182"
    },
    {
        "id": 26104,
        "title": "Deep Reinforcement Learning-Based Multirestricted Dynamic-Request Transportation Framework",
        "authors": "Erdal Akin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3341471"
    },
    {
        "id": 26105,
        "title": "IVDR: Imitation learning with Variational inference and Distributional Reinforcement learning to find Optimal Driving Strategy",
        "authors": "Kihyung Joo, Simon S. Woo",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00047"
    },
    {
        "id": 26106,
        "title": "Comparing deep reinforcement learning architectures for autonomous racing",
        "authors": "Benjamin David Evans, Hendrik Willem Jordaan, Herman Arnold Engelbrecht",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2023.100496"
    },
    {
        "id": 26107,
        "title": "Improving reinforcement learning output feedback control for unknown nonlinear pure feedback system",
        "authors": "Dazi Li, Wei Wang",
        "published": "2017-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls.2017.8067720"
    },
    {
        "id": 26108,
        "title": "Nicotine-free electronic-cigarettes for smoking cessation: Occasional reinforcement during extinction",
        "authors": "Mina Fukuda",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.lmot.2021.101766"
    },
    {
        "id": 26109,
        "title": "Node Correlation Effects on Learning Dynamics in Networked Multiagent Reinforcement Learning",
        "authors": "Valentina Guleva",
        "published": "2022-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dcna56428.2022.9923243"
    },
    {
        "id": 26110,
        "title": "Reinforcement learning methods and role of internet of things in civil engineering applications",
        "authors": "Kundan Meshram",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-443-15364-8.00009-3"
    },
    {
        "id": 26111,
        "title": "Learning from Different Perspectives for Regret Reduction in Reinforcement Learning: A Free Energy Approach",
        "authors": "Milad Ghorbani, Reshad Hosseini, Seyed Pooya Shariatpanahi, Majid Nili Ahmadabadi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4431992"
    },
    {
        "id": 26112,
        "title": "Learning to Score: Tuning Cluster Schedulers through Reinforcement Learning",
        "authors": "Martin Asenov, Qiwen Deng, Gingfung Yeung, Adam Barker",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ic2e59103.2023.00021"
    },
    {
        "id": 26113,
        "title": "An Incentive Mechanism Design for Efficient Edge Learning by Deep Reinforcement Learning Approach",
        "authors": "Yufeng Zhan, Jiang Zhang",
        "published": "2020-7",
        "citations": 51,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infocom41043.2020.9155268"
    },
    {
        "id": 26114,
        "title": "On Normative Reinforcement Learning via Safe Reinforcement Learning",
        "authors": "Emery A. Neufeld, Ezio Bartocci, Agata Ciabattoni",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-21203-1_5"
    },
    {
        "id": 26115,
        "title": "AI’S Contribution to Ubiquitous Systems and Pervasive Networks Security – Reinforcement Learning vs Recurrent Networks",
        "authors": "Christophe Feltus",
        "published": "2021-3-1",
        "citations": 2,
        "abstract": "Reinforcement learning and recurrent networks are two emerging machine-learning paradigms. The first learns the best actions an agent needs to perform to maximize its rewards in a particular environment and the second has the specificity to use an internal state to remember previous analysis results and consider them for the current one. Research into RL and recurrent network has been proven to have made a real contribution to the protection of ubiquitous systems and pervasive networks against intrusions and malwares. In this paper, a systematic review of this research was performed in regard to various attacks and an analysis of the trends and future fields of interest for the RL and recurrent network-based research in network security was complete.",
        "link": "http://dx.doi.org/10.5383/juspn.15.02.001"
    },
    {
        "id": 26116,
        "title": "Proposal of a Signal Control Method Using Deep Reinforcement Learning with Pedestrian Traffic Flow",
        "authors": "Akimasa Murata, Yuichi Sei, Yasuyuki Tahara, Akihiko Ohsuga",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011665000003393"
    },
    {
        "id": 26117,
        "title": "Augmenting Reinforcement Learning with a Planning Model for Optimizing Energy Demand Response",
        "authors": "Lucas Spangher, Akash Gokul, Manan Khattar, Joseph Palakapilly, Utkarsha Agwan, Akaash Tawade, Costas Spanos",
        "published": "2020-11-17",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3427773.3427863"
    },
    {
        "id": 26118,
        "title": "Reinforcement Learning based Video Summarization with Combination of ResNet and Gated Recurrent Unit",
        "authors": "Muhammad Afzal, Muhammad Tahir",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010197402610268"
    },
    {
        "id": 26119,
        "title": "Optimal Monetary Policy Using Reinforcement Learning",
        "authors": "Natascha Hinterlang, Alina Tänzer",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4013384"
    },
    {
        "id": 26120,
        "title": "A Model of Adaptive Reinforcement Learning",
        "authors": "Julian Romero, Yaroslav Rosokha",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3350711"
    },
    {
        "id": 26121,
        "title": "Reinforcement Learning for Warfarin Dosing",
        "authors": " ",
        "published": "2019-5-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31525/ct1-nct03962400"
    },
    {
        "id": 26122,
        "title": "Safe Reinforcement Learning for High-Speed Autonomous Racing",
        "authors": "Benjamin Evans, Hendrik  Willem Jordaan, Herman  Arnold Engelbrecht",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4349187"
    },
    {
        "id": 26123,
        "title": "Equivalence between Visitation Frequency and Policy",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_10"
    },
    {
        "id": 26124,
        "title": "Projection Exploration for Multi-agent Reinforcement Learning",
        "authors": "Hainan Tang, Juntao Liu, Zhenjie Wang, Ziwen Gao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn multi-agent reinforcement learning (MARL), complete exploration is difficult to achieve because of the curse of dimensionality and sparse rewards. Existing methods improve the exploration to some extent, such as noise-based exploration methods and count-based exploration methods. But they don’t evaluate the state exploration value, suffering low data efficiency. In this paper, a projection exploration algorithm (PERL) is designed for MARL. In this method, states with high exploration value are identified in the projected state-action space with maximum distribution entropy through the count-based method. Then the agents are trained to reach these states in a coordinated manner. Meanwhile, comparison experiments are conducted in multi-agent environments of Multi-Particle environments (Pass, Secret-Room) and StarCraftII (3m,2s3z and 3s_vs_5z). Comparison results suggest that PERL consistently outperforms the state-of-the-art baselines in environments with high-dimensional state-action space and sparse rewards.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2759603/v1"
    },
    {
        "id": 26125,
        "title": "Reinforcement Learning-Guided Crack Propagation",
        "authors": "T. Venkatesh Varma, Raghav Ramani, Rajdeep Dutta, Senthilnath Jayavelu, Saikat Sarkar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4532642"
    },
    {
        "id": 26126,
        "title": "Reinforcement Learning for Electric Vehicle Charging using Dueling Neural Networks",
        "authors": "Gargya Gokhale, Bert Claessens, Chris Develder",
        "published": "No Date",
        "citations": 1,
        "abstract": "We consider the problem of coordinating the charging of an entire fleet of electric vehicles (EV), using a model-free approach, i.e. purely data-driven reinforcement learning (RL). The objective of the RL-based control is to optimize charging actions, while fulfilling all EV charging constraints (e.g. timely completion of the charging). In particular, we focus on batch-mode learning and adopt fitted Q-iteration (FQI). A core component in FQI is approximating the Q-function using a regression technique, from which the policy is derived. Recently, a dueling neural networks architecture was proposed and shown to lead to better policy evaluation in the presence of many similar-valued actions, as applied in a computer game context. The main research contributions of the current paper are that (i)we develop a dueling neural networks approach for the setting of joint coordination of an entire EV fleet, and (ii)we evaluate its performance and compare it to an all-knowing benchmark and an FQI approach using EXTRA trees regression technique, a popular approach currently discussed in EV related works. We present a case study where RL agents are trained with an epsilon-greedy approach for different objectives, (a)cost minimization, and (b)maximization of self-consumption of local renewable energy sources. Our results indicate that RL agents achieve significant cost reductions (70--80%) compared to a business-as-usual scenario without smart charging. Comparing the dueling neural networks regression to EXTRA trees indicates that for our case study's EV fleet parameters and training scenario, the EXTRA trees-based agents achieve higher performance in terms of both lower costs (or higher self-consumption) and stronger robustness, i.e. less variation among trained agents. This suggests that adopting dueling neural networks in this EV setting is not particularly beneficial as opposed to the Atari game context from where this idea originated.",
        "link": "http://dx.doi.org/10.20944/preprints202103.0592.v1"
    },
    {
        "id": 26127,
        "title": "PROXIMAL BELLMAN MAPPINGS FOR REINFORCEMENT LEARNING AND THEIR APPLICATION TO ROBUST ADAPTIVE FILTERING",
        "authors": "Yuki Akiyama, Konstantinos Slavakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper aims at the algorithmic/theoretical core of reinforcement learning (RL) by introducing the novel class of proximal Bellman mappings. These mappings are defined in reproducing kernel Hilbert spaces (RKHSs), to benefit from the rich approximation properties and inner product of RKHSs, they are shown to belong to the powerful Hilbertian family of (firmly) nonexpansive mappings, regardless of the values of their discount factors, and possess ample degrees of design freedom to even reproduce attributes of the classical Bellman mappings and to pave the way for novel RL designs. An approximate policy-iteration scheme is built on the proposed class of mappings to solve the problem of selecting online, at every time instance, the “optimal” exponent p in a p-norm loss to combat outliers in linear adaptive filtering, without training data and any knowledge on the statistical properties of the outliers. Numerical tests on synthetic data showcase the superior performance of the proposed framework over several non-RL and kernel-based RL schemes.</p>\n<p><br></p>\n<p> </p>\n<p>----</p>\n<p> © 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24123240.v1"
    },
    {
        "id": 26128,
        "title": "Reinforcement Learning Fuzzy Algorithm for Adaptive Cabin Management System With Application for Adaptive Interior Lighting",
        "authors": "Anas Saad",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The purpose of this thesis was to design a framework for an adaptive cabin management system that is further explored through a study of light intensity. With the goal of passenger comfort, the system developed would adjust lighting to an average setting and further adapt to an individuals preference. A fuzzy inference system was implemented that utilizes DGI, the passengers age, chronotype and the activity on board to calculate a light intensity. A reinforcement system was then developed to tune the fuzzy inference system parameters (mean and output value K) utilizing a lighting override from the passenger. A cabin mock up was then setup to observe the correctness and e▯ectiveness of the system developed. Overall, the system designed tuned accurately and e▯ectively in all case scenarios tested with varying learning rates. This thesis was concluded with discussions of future work that could further improve the implementations within a cabin.</p>",
        "link": "http://dx.doi.org/10.32920/24625128"
    },
    {
        "id": 26129,
        "title": "Reliability of Decision-Making and Reinforcement Learning Computational Parameters",
        "authors": "Anahit Mkrtchian, Vincent Valton, Jonathan P. Roiser",
        "published": "No Date",
        "citations": 4,
        "abstract": "AbstractComputational models can offer mechanistic insight into cognition and therefore have the potential to transform our understanding of psychiatric disorders and their treatment. For translational efforts to be successful, it is imperative that computational measures capture individual characteristics reliably. Here we examine the reliability of reinforcement learning and economic models derived from two commonly used tasks. Healthy individuals (N=50) completed a restless four-armed bandit and a calibrated gambling task twice, two weeks apart. Reward and punishment learning rates from the reinforcement learning model showed good reliability and reward and punishment sensitivity from the same model had fair reliability; while risk aversion and loss aversion parameters from a prospect theory model exhibited good and excellent reliability, respectively. Both models were further able to predict future behaviour above chance within individuals. This prediction was better when based on participants’ own model parameters than other participants’ parameter estimates. These results suggest that reinforcement learning, and particularly prospect theory parameters, as derived from a restless four-armed bandit and a calibrated gambling task, can be measured reliably to assess learning and decision-making mechanisms. Overall, these findings indicate the translational potential of clinically-relevant computational parameters for precision psychiatry.",
        "link": "http://dx.doi.org/10.1101/2021.06.30.450026"
    },
    {
        "id": 26130,
        "title": "Editor's evaluation: Valence biases in reinforcement learning shift across adolescence and modulate subsequent memory",
        "authors": "Margaret L Schlichting",
        "published": "2021-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.64620.sa0"
    },
    {
        "id": 26131,
        "title": "Safe Reinforcement Learning through Meta-learned Instincts",
        "authors": "Djordje Grbic, Sebastian Risi",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1162/isal_a_00318"
    },
    {
        "id": 26132,
        "title": "Deep Reinforcement Learning in Match-3 Game",
        "authors": "Ildar Kamaldinov, Ilya Makarov",
        "published": "2019-8",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8848003"
    },
    {
        "id": 26133,
        "title": "Estimation on Human Motion Posture using Improved Deep Reinforcement Learning",
        "authors": "Wenjing Ma Wenjing Ma, Jianguang Zhao Wenjing Ma, Guangquan Zhu Jianguang Zhao",
        "published": "2023-8",
        "citations": 0,
        "abstract": "\n                        <p>Estimating human motion posture can provide important data for intelligent monitoring systems, human-computer interaction, motion capture, and other fields. However, the traditional human motion posture estimation algorithm is difficult to achieve the goal of fast estimation of human motion posture. To address the problems of traditional algorithms, in the paper, we propose an estimation algorithm for human motion posture using improved deep reinforcement learning. First, the double deep Q network is constructed to improve the deep reinforcement learning algorithm. The improved deep reinforcement learning algorithm is used to locate the human motion posture coordinates and improve the effectiveness of bone point calibration. Second, the human motion posture analysis generative adversarial networks are constructed to realize the automatic recognition and analysis of human motion posture. Finally, using the preset human motion posture label, combined with the undirected graph model of the human, the human motion posture estimation is completed, and the precise estimation algorithm of the human motion posture is realized. Experiments are performed based on MPII Human Pose data set and HiEve data set. The results show that the proposed algorithm has higher positioning accuracy of joint nodes. The recognition effect of bone joint points is better, and the average is about 1.45%. The average posture accuracy is up to 98.2%, and the average joint point similarity is high. Therefore, it is proved that the proposed method has high application value in human-computer interaction, human motion capture and other fields.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/199115992023083404008"
    },
    {
        "id": 26134,
        "title": "Q-Credit Card Fraud Detector for Imbalanced Classification using Reinforcement Learning",
        "authors": "Luis Zhinin-Vera, Oscar Chang, Rafael Valencia-Ramos, Ronny Velastegui, Gissela Pilliza, Francisco Socasi",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009156102790286"
    },
    {
        "id": 26135,
        "title": "Systematic Model-based Design of a Reinforcement Learning-based Neural Adaptive Cruise Control System",
        "authors": "Or Yarom, Jannis Fritz, Florian Lange, Xiaobo Liu-Henke",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010923300003116"
    },
    {
        "id": 26136,
        "title": "Reinforcement and deep reinforcement learning for wireless Internet of Things: A survey",
        "authors": "Mohamed Said Frikha, Sonia Mettali Gammar, Abdelkader Lahmadi, Laurent Andrey",
        "published": "2021-10",
        "citations": 44,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.comcom.2021.07.014"
    },
    {
        "id": 26137,
        "title": "Markov Decision Processes",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-4"
    },
    {
        "id": 26138,
        "title": "PROXIMAL BELLMAN MAPPINGS FOR REINFORCEMENT LEARNING AND THEIR APPLICATION TO ROBUST ADAPTIVE FILTERING",
        "authors": "Yuki Akiyama, Konstantinos Slavakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper aims at the algorithmic/theoretical core of reinforcement learning (RL) by introducing the novel class of proximal Bellman mappings. These mappings are defined in reproducing kernel Hilbert spaces (RKHSs), to benefit from the rich approximation properties and inner product of RKHSs, they are shown to belong to the powerful Hilbertian family of (firmly) nonexpansive mappings, regardless of the values of their discount factors, and possess ample degrees of design freedom to even reproduce attributes of the classical Bellman mappings and to pave the way for novel RL designs. An approximate policy-iteration scheme is built on the proposed class of mappings to solve the problem of selecting online, at every time instance, the “optimal” exponent p in a p-norm loss to combat outliers in linear adaptive filtering, without training data and any knowledge on the statistical properties of the outliers. Numerical tests on synthetic data showcase the superior performance of the proposed framework over several non-RL and kernel-based RL schemes.</p>\n<p><br></p>\n<p> </p>\n<p>----</p>\n<p> © 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24123240"
    },
    {
        "id": 26139,
        "title": "Introduction",
        "authors": "Changsheng Hua",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-33034-7_1"
    },
    {
        "id": 26140,
        "title": "Personas-Based Student Grouping Using Reinforcement Learning and Linear Programming",
        "authors": "Shaojie Ma, Yawei Luo, Yi Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4526763"
    },
    {
        "id": 26141,
        "title": "Carl: A Synergistic Framework for Causal Reinforcement Learning",
        "authors": "Arquímides Méndez-Molina, Eduardo Morales, Sucar L. Enrique",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4409869"
    },
    {
        "id": 26142,
        "title": "Battery State of Health Estimation via Reinforcement Learning",
        "authors": "D. Natella, F. Vasca",
        "published": "2021-6-29",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc54610.2021.9655199"
    },
    {
        "id": 26143,
        "title": "Applying Reinforcement Learning to Option Pricing and Hedging",
        "authors": "Zoran Stoiljkovic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4546371"
    },
    {
        "id": 26144,
        "title": "Use Value to Calculate Expected Return",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_6"
    },
    {
        "id": 26145,
        "title": "Multi-Agent Reinforcement Learning Clustering Algorithm Based on Silhouette Coefficient",
        "authors": "Peng Du, Yanzhe Gao, Fenglian Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4466296"
    },
    {
        "id": 26146,
        "title": "Adaptive Ddos Response Policy by Reinforcement Learning with an Anomality Reward Function",
        "authors": "Won Sakong, Wooju Kim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4605933"
    },
    {
        "id": 26147,
        "title": "Network Structure Engineering for Dissemination Amplification in Social Networks using Reinforcement Learning",
        "authors": "Abdorasoul Ghasemi, Mohammad Ali Karami",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Network structure engineering aims to intervene in the network structure to improve a specific underlying process and has applications in many research fields like biology, transportation, and network design. Here we address the problem of online social networks (OSNs) engineering by link addition through the friend suggestion for an efficient dissemination process. We propose an end-to-end machine learning framework called <em>GCQL</em> for optimizing or generating an effective network structure to improve the dissemination process in OSNs. We first propose a dissemination model called <em>Complex-SII</em>, combining two widely used dissemination models in OSNs, SIR and IC, and complex contagion. The <em>GCQL</em> next uses a reinforcement learning algorithm called Neural Fitted <em>Q</em>-Iteration for optimizing a network structure for the dissemination process and uses a graph convolution network called <em>GCN</em> for graph embedding and learning the important patterns and characteristics of network structure for efficient spreading. The <em>GCQL</em> is trained with small-sized graphs and uses the learning outcomes for medium/large-sized graphs. Our simulation results show that <em>GCQL</em> can generate relatively more efficient network structures than the greedy approach for small-sized networks. <em>GCQL</em> also uses these results to develop relatively more efficient networks than the greedy approach for medium/large-sized network structures at a much higher speed. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21640700.v1"
    },
    {
        "id": 26148,
        "title": "Multi-Agent Reinforcement Learning Based Joint Resource Allocation for Ultra-Reliable Low-Latency Vehicular Communication",
        "authors": "Nasir Khan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Future vehicular networks must ensure ultra-reliable low-latency communication (URLLC) for the timely delivery of safety-critical information. Previously proposed resource allocation schemes for URLLC mostly rely on centralized optimization-based algorithms and cannot guarantee the reliability and latency requirements of vehicle-to-vehicle (V2V) communications. This paper investigates the joint power and blocklength allocation to minimize the worst-case decoding-error probability in the finite blocklength (FBL) regime for a URLLC-based V2V communication network. We formulate the problem as a non-convex mixed-integer nonlinear programming problem (MINLP). We first develop a centralized optimization theory-based algorithm based on the derivation of the joint convexity of the decoding error probability in the blocklength and transmit power variables within the region of interest. Next, we propose a two-layered multi-agent deep reinforcement learning based centrally trained and distributively executed framework. The first layer involves establishing multiple deep Q-networks (DQNs) at the central trainer to train the local DQNs for block length optimization. The second layer involves an actor-critic network and utilizes the deep deterministic policy-gradient (DDPG)-based algorithm to train the local actor network for each V2V link. Simulation results demonstrate that the proposed distributed scheme can achieve close-to-optimal solutions with a much lower computational complexity than the centralized optimization based solution. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21685046.v1"
    },
    {
        "id": 26149,
        "title": "Caching and Computing Resource Allocation in Cooperative Heterogeneous 5G Edge Networks using Deep Reinforcement Learning",
        "authors": "Tushar Bose",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Resource Alllocation in 5G Edge Networks</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21556605"
    },
    {
        "id": 26150,
        "title": "Applied Reinforcement Learning with Python",
        "authors": "Taweh Beysolow II",
        "published": "2019",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-5127-0"
    },
    {
        "id": 26151,
        "title": "Optimization Sliding Mode Controller Using Reinforcement Learning",
        "authors": "Somporn Tiacharoen",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incit60207.2023.10413158"
    },
    {
        "id": 26152,
        "title": "Safer Reinforcement Learning through Transferable Instinct Networks",
        "authors": "Djordje Grbic, Sebastian Risi",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1162/isal_a_00449"
    },
    {
        "id": 26153,
        "title": "Computational modelling of social cognition and behaviour – a reinforcement learning primer",
        "authors": "Patricia Lockwood, Miriam Klein-Flugge",
        "published": "No Date",
        "citations": 2,
        "abstract": "Social neuroscience aims to describe the neural systems that underpin social cognition and behaviour. Over the past decade, researchers have begun to combine computational models with neuroimaging to link social computations to the brain. Inspired by approaches from reinforcement learning theory, which describes how decisions are driven by the unexpectedness of outcomes, accounts of the neural basis of prosocial learning, observational learning, mentalising and impression formation have been developed. Here we provide an introduction for researchers who wish to use these models in their studies. We consider both theoretical and practical issues related to their implementation, with a focus on specific examples from the field.",
        "link": "http://dx.doi.org/10.31234/osf.io/r69e7"
    },
    {
        "id": 26154,
        "title": "Resource allocation of fog wireless access network based on deep reinforcement learning",
        "authors": "Jingru Tan, Wenbo Guan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Aiming at the problem of huge energy consumption in the Fog Wireless\nAccess Networks (F-RANs), the resource allocation scheme of the F-RAN\narchitecture under the cooperation of renewable energy is studied in\nthis paper. Firstly, the transmission model and Energy Harvesting (EH)\nmodel are established, the solar energy harvester is installed on each\nFog Access Point (F-AP), and each F-AP is connected to the smart grid.\nSecondly, the optimization problem is established according to the\nconstraints of Signal to Noise Ratio (SNR), available bandwidth and\nenergy harvesting, so as to maximize the average throughput of F-RAN\narchitecture with hybrid energy sources. Finally, the dynamic power\nallocation scheme in the network is studied by using Q-learning and Deep\nQ Network (DQN) respectively. Simulation results show that the proposed\ntwo algorithms can improve the average throughput of the whole network\ncompared with other traditional algorithms.",
        "link": "http://dx.doi.org/10.22541/au.163620090.07696354/v1"
    },
    {
        "id": 26155,
        "title": "Reinforcement Learning",
        "authors": "Robert H. Chen, Chelsea Chen",
        "published": "2022-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003214892-33"
    },
    {
        "id": 26156,
        "title": "An Improved Deep Reinforcement Learning for Task-oriented Dialogue System",
        "authors": "Zahra Dehghanipour, Javad Salimi Sartakhti",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNowadays, the need for dialogue systems is felt more than ever. Many services around us need continued support, while the domain of support is limited and specific. Therefore, instead of employing manpower, dialogue systems can help. In this paper, we propose a task-oriented dialogue system based on deep reinforcement learning empowered by several new ideas: first, due to the shortage of training data, we use GAN to generate more training data without human intervention. Second, we propose a new state embedding for representing environment in dialogue system. In this way, we use sentiment analysis and clustering methods. The proposed embedding makes a more accurate and deeper understanding of the environment, which ultimately leads to an increase in system performance. Additionally, we present a new reward function for the agent to better learn dialogues. In order to evaluate this system, we use both user simulator and human judges. With user simulator, we evaluate the average rewards of the proposed system and its improvements. With human judges, we investigate human evaluations about the proposed system. Based on these evaluations promising and comparable results are observed.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1792905/v1"
    },
    {
        "id": 26157,
        "title": "Reinforced-Lib: Rapid Prototyping of Reinforcement Learning Solutions",
        "authors": "Maksymilian Wojnar, Szymon Szott, Krzysztof Rusek, Wojciech Ciężobka",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4584200"
    },
    {
        "id": 26158,
        "title": "Policy Gradient Algorithms",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-14"
    },
    {
        "id": 26159,
        "title": "Autonomous Vehicle Decision-Making at Unsignalized Intersection via Deep Reinforcement Learning",
        "authors": "Junran Xie, Qingling Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper is concerned with the performance of different state representation on the vehicle decision-making problem at unsignalized intersection based on deep reinforcement learning. A hybrid state representation architecture based on attention mechanism and collision time prediction is designed, which can effectively improve the autonomous decision-making ability of vehicles. Compared with the traditional state representation method, the feature of our method is that the fusion of features can improve the vehicle’s obstacle avoidance ability, and the introduced action masking module can improve the vehicle’s traffic efficiency. Finally, test results in the simulation environment verify the effectiveness of our proposed method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1757806/v1"
    },
    {
        "id": 26160,
        "title": "Reinforcement Learning for Approximate Optimal Control",
        "authors": "Warren E. Dixon",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-44184-5_100063"
    },
    {
        "id": 26161,
        "title": "Surprise acts as a reducer of outcome value in human reinforcement learning",
        "authors": "Motofumi Sumiya, Kentaro Katahira",
        "published": "No Date",
        "citations": 0,
        "abstract": "Surprise occurs because of differences between a decision outcome and its predicted outcome (prediction error), regardless of whether the error is positive or negative. It has recently been postulated that surprise affects the reward value of the action outcome itself; studies have indicated that increasing surprise, as absolute value of prediction error, decreases the value of the outcome. However, how surprise affects the value of the outcome and subsequent decision making is unclear. We suggested that, on the assumption that surprise decreases the outcome value, agents will increase their risk averse choices when an outcome is often surprisal. Here, we propose the surprise-sensitive utility model, a reinforcement learning model that states that surprise decreases the outcome value, to explain how surprise affects subsequent decision-making. To investigate the assumption, we compared this model with previous reinforcement learning models on a risky probabilistic learning task with simulation analysis, and model selection with two experimental datasets with different tasks and population. We further simulated a simple decision-making task to investigate how parameters within the proposed model modulate the choice preference. As a result, we found the proposed model explains the risk averse choices in a manner similar to the previous models, and risk averse choices increased as the surprise-based modulation parameter of outcome value increased. The model fits these datasets better than the other models, with same free parameters, thus providing a more parsimonious and robust account for risk averse choices. These findings indicate that surprise acts as a reducer of outcome value and decreases the action value for risky choices in which prediction error often occurs.",
        "link": "http://dx.doi.org/10.31234/osf.io/5ha3y"
    },
    {
        "id": 26162,
        "title": "Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning Approach",
        "authors": "Soroush Saghafian",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3980837"
    },
    {
        "id": 26163,
        "title": "Recent Applications and Future Research",
        "authors": "Rafael Ris-Ala",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-37345-9_6"
    },
    {
        "id": 26164,
        "title": "Safety Constraints-Guided Reinforcement Learning with Linear Temporal Logic",
        "authors": "Ryeonggu Kwon, Gihwon Kwon",
        "published": "No Date",
        "citations": 0,
        "abstract": "In the context of reinforcement learning (RL), ensuring both safety and performance is crucial, especially in real-world scenarios where mistakes can lead to severe consequences. This study aims to address this challenge by integrating temporal logic constraints into RL algorithms, thereby providing a formal mechanism for safety verification. We employ a combination of theoretical and empirical methods, including the use of temporal logic for formal verification and extensive simulations to validate our approach. Our results demonstrate that the proposed method not only maintains high levels of safety but also achieves comparable performance to traditional RL algorithms. Importantly, our approach fills a critical gap in existing literature by offering a solution that is both mathematically rigorous and empirically validated. The study concludes that the integration of temporal logic into RL offers a promising avenue for developing algorithms that are both safe and efficient. This work lays the foundation for future research aimed at generalizing this approach to various complex systems and applications.",
        "link": "http://dx.doi.org/10.20944/preprints202310.1862.v1"
    },
    {
        "id": 26165,
        "title": "Dopamine and Reinforcement Learning",
        "authors": "Henry H. Yin",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429154461-12"
    },
    {
        "id": 26166,
        "title": "Buyers Collusion in Incentivized Forwarding Networks via Multi-agent Reinforcement Learning",
        "authors": "Mostafa Abdelhadi, Sabit Ekin, Ali Imran",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We present the issue of monetarily incentivized forwarding in a multi-hop mesh network architecture from an economic perspective. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23605749.v1"
    },
    {
        "id": 26167,
        "title": "Deep reinforcement learning for active hypothesis testing with heterogeneous agents and cost constraints",
        "authors": "George Stamatelis, Nicholas Kalouptsidis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We  consider active hypothesis testing with multiple heterogeneous agents. Each agent has access to its own set of experiments, has different action costs and forms its own beliefs. Additionally,  each experiment carries a global cost, and the agents must try to keep the expected cumulative cost below a certain threshold. We study a centralized and a decentralized scenario. Under the centralized scenario, the agents  are instructed how to act by a central controller. Under the decentralized scenario they communicate and exchange information over a directed graph. For each scenario we propose  separate deep reinforcement learning algorithms based on proximal policy optimization. Solutions to the decentralized problem start from fully decentralised training, and progressively introduce two levels of centralisation during training.  We assess the proposed algorithms in an example of anomaly detection over sensor networks, considering three different decentralised communication settings. We infer that all algorithms achieve the required accuracy level considerably faster than a single deep reinforcement learning agent, while satisfying the expected cost constraints when required. Under the assumption that the communication  graph is fully connected, the decentralised agents perform just as well, and sometimes better than the centralised  controller. Out of all decentralised algorithms, the one that uses a global critic is by far the best performing and can compete with the central controller even when the communication graph is not complete.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22723507.v1"
    },
    {
        "id": 26168,
        "title": "AlphaZero",
        "authors": "Hongming Zhang, Tianyang Yu",
        "published": "2020",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_15"
    },
    {
        "id": 26169,
        "title": "Reinforcement Learning Fuzzy Algorithm for Adaptive Cabin Management System With Application for Adaptive Interior Lighting",
        "authors": "Anas Saad",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The purpose of this thesis was to design a framework for an adaptive cabin management system that is further explored through a study of light intensity. With the goal of passenger comfort, the system developed would adjust lighting to an average setting and further adapt to an individuals preference. A fuzzy inference system was implemented that utilizes DGI, the passengers age, chronotype and the activity on board to calculate a light intensity. A reinforcement system was then developed to tune the fuzzy inference system parameters (mean and output value K) utilizing a lighting override from the passenger. A cabin mock up was then setup to observe the correctness and e▯ectiveness of the system developed. Overall, the system designed tuned accurately and e▯ectively in all case scenarios tested with varying learning rates. This thesis was concluded with discussions of future work that could further improve the implementations within a cabin.</p>",
        "link": "http://dx.doi.org/10.32920/24625128.v1"
    },
    {
        "id": 26170,
        "title": "Population based Reinforcement Learning",
        "authors": "Kyle W. Pretorius, Nelishia Pillay",
        "published": "2021-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9660084"
    },
    {
        "id": 26171,
        "title": "Risk-Sensitive Reinforcement Learning Part I: Constrained Optimization Framework",
        "authors": "L.A. Prashanth",
        "published": "2019-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indiancc.2019.8715578"
    },
    {
        "id": 26172,
        "title": "Testing the reinforcement learning hypothesis of social conformity",
        "authors": "Marie Levorsen, Ayahito Ito, Shinsuke Suzuki, Keise Izuma",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractOur preferences are influenced by the opinions of others. The past human neuroimaging studies on social conformity have identified a network of brain regions related to social conformity that includes the posterior medial frontal cortex (pMFC), anterior insula, and striatum. It was hypothesized that since these brain regions are also known to play important roles in reinforcement learning (i.e., processing prediction error), social conformity and reinforcement learning have a common neural mechanism. However, these two processes have previously never been directly compared; therefore, the extent to which they shared a common neural mechanism had remained unclear. This study aimed to formally test the hypothesis. The same group of participants (n = 25) performed social conformity and reinforcement learning tasks inside a functional magnetic resonance imaging (fMRI) scanner. Univariate fMRI data analyses revealed activation overlaps in the pMFC and bilateral insula between social conflict and unsigned prediction error and in the striatum between social conflict and signed prediction error. We further conducted multi-voxel pattern analysis (MVPA) for more direct evidence of a shared neural mechanism. MVPA did not reveal any evidence to support the hypothesis in any of these regions but found that activation patterns between social conflict and prediction error in these regions were largely distinct. Taken together, the present study provides no clear evidence of a common neural mechanism between social conformity and reinforcement learning.",
        "link": "http://dx.doi.org/10.1101/2020.03.11.988220"
    },
    {
        "id": 26173,
        "title": "Strain design optimization using reinforcement learning",
        "authors": "Maryam Sabzevari, Sandor Szedmak, Merja Penttilä, Paula Jouhten, Juho Rousu",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractEngineered microbial cells present a sustainable alternative to fossil-based synthesis of chemicals and fuels. Cellular synthesis routes are readily assembled and introduced into microbial strains using state-of-the-art synthetic biology tools. However, the optimization of the strains required to reach industrially feasible production levels is far less efficient. It typically relies on trial-and-error leading into high uncertainty in total duration and cost. New techniques that can cope with the complexity and limited mechanistic knowledge of the cellular regulation are called for guiding the strain optimization.In this paper, we put forward a multi-agent reinforcement learning (MARL) approach that learns from experiments to tune the metabolic enzyme levels so that the production is improved. Our method is model-free and does not assume prior knowledge of the microbe’s metabolic network or its regulation. The multi-agent approach is well-suited to make use of parallel experiments such as multi-well plates commonly used for screening microbial strains.We demonstrate the method’s capabilities using the genome-scale kinetic model of Escherichia coli, k-ecoli457, as a surrogate for an in vivo cell behaviour in cultivation experiments. We investigate the method’s performance relevant for practical applicability in strain engineering i.e. the speed of convergence towards the optimum response, noise tolerance, and the statistical stability of the solutions found. We further evaluate the proposed MARL approach in improving L-tryptophan production by yeast Saccharomyces cerevisiae, using publicly available experimental data on the performance of a combinatorial strain library.Overall, our results show that multi-agent reinforcement learning is a promising approach for guiding the strain optimization beyond mechanistic knowledge, with the goal of faster and more reliably obtaining industrially attractive production levels.Author summaryEngineered microbial cells offer a sustainable alternative solution to chemical production from fossil resources. However, to make the chemical production using microbial cells economically feasible, they need to be substantially optimized. Due to the biological complexity, this optimization to reach sufficiently high production is typically a costly trial and error process.This paper presents an Artificial Intelligence (AI) approach to guide this task. Our tool learns a model from previous experiments and uses the model to suggest improvements to the engineering design, until a satisfactory production performance is reached. This paper evaluates the behaviour of the proposed AI method from several angles, including the amount of experiments needed, the tolerance to noise as well as the stability of the proposed designs.",
        "link": "http://dx.doi.org/10.1101/2022.03.22.485285"
    },
    {
        "id": 26174,
        "title": "A Neural Network Based Automatic Generation Controller Design through Reinforcement Learning",
        "authors": "Imthias Ahamed, Nagendra Rao,  Sastry",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00236"
    },
    {
        "id": 26175,
        "title": "Optimal Monetary Policy Using Reinforcement Learning",
        "authors": "Natascha Hinterlang, Alina Tänzer",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4025682"
    },
    {
        "id": 26176,
        "title": "Maximizing Fog Computing Efficiency with Federated Multi-Agent Deep Reinforcement Learning",
        "authors": "G. Anitha, A. Jegatheesan",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003384854-8"
    },
    {
        "id": 26177,
        "title": "Learning Pessimism for Reinforcement Learning",
        "authors": "Edoardo Cetin, Oya Celiktutan",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "Off-policy deep reinforcement learning algorithms commonly compensate for overestimation bias during temporal-difference learning by utilizing pessimistic estimates of the expected target returns. In this work, we propose Generalized Pessimism Learning (GPL), a strategy employing a novel learnable penalty to enact such pessimism. In particular, we propose to learn this penalty alongside the critic with dual TD-learning, a new procedure to estimate and minimize the magnitude of the target returns bias with trivial computational cost. GPL enables us to accurately counteract overestimation bias throughout training without incurring the downsides of overly pessimistic targets. By integrating GPL with popular off-policy algorithms, we achieve state-of-the-art results in both competitive proprioceptive and pixel-based benchmarks.",
        "link": "http://dx.doi.org/10.1609/aaai.v37i6.25852"
    },
    {
        "id": 26178,
        "title": "Deep Multiagent Reinforcement Learning Methods Addressing the Scalability Challenge",
        "authors": "Theocharis Kravaris, George A. Vouros",
        "published": "2023-1-25",
        "citations": 3,
        "abstract": "Motivated to solve complex demand-capacity imbalance problems in air traffic management at the pre-tactical stage of operations, with thousands of agents (flights) daily, even in a restricted airspace, in this paper, we review deep multiagent reinforcement learning methods under the prism of their ability to scale toward solving problems with large populations of heterogeneous agents, where agents have to unavoidably decide on their joint policy, without communication constraints.",
        "link": "http://dx.doi.org/10.5772/intechopen.105627"
    },
    {
        "id": 26179,
        "title": "A Personalized E-Learning System Using Reinforcement Learning Through Satellite",
        "authors": "Pooja Kiran, B K Swathi Prasad, Jayahar Sivasubramanian",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indicon59947.2023.10440852"
    },
    {
        "id": 26180,
        "title": "Research advanced in the integration of federated learning and reinforcement learning",
        "authors": "Chengan Li",
        "published": "2024-2-21",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) and federated learning (FL) are two important machine learning paradigms. Reinforcement learning is concerned with enabling intelligence to learn optimal policies when interacting with an environment, while federated learning is concerned with collaboratively training models on distributed equipment while preserving data privacy. In recent years, the fusion and complementarity of reinforcement learning, and federated learning have attracted increasing research interest, providing new directions for the development of the machine learning community. Focusing on the integration of reinforcement learning and federated learning, this paper introduces in detail the latest technological developments in the integration of reinforcement learning and federated learning, and discusses the main challenges, existing methods and future directions of this intersection. Specifically, based on the introduction of classical reinforcement learning and federated learning. In addition, this document introduces cutting-edge results on the integration of reinforcement learning and joint learning and discusses the problems and future directions of the integration.",
        "link": "http://dx.doi.org/10.54254/2755-2721/40/20230641"
    },
    {
        "id": 26181,
        "title": "Meta-Learning for Multi-objective Reinforcement Learning",
        "authors": "Xi Chen, Ali Ghadirzadeh, Marten Bjorkman, Patric Jensfelt",
        "published": "2019-11",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros40897.2019.8968092"
    },
    {
        "id": 26182,
        "title": "Towards a Personalized Learning Experience Using Reinforcement Learning",
        "authors": "Doaa Shawky, Ashraf Badawi",
        "published": "2019",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-02357-7_8"
    },
    {
        "id": 26183,
        "title": "Network Structure Engineering for Dissemination Amplification in Social Networks using Reinforcement Learning",
        "authors": "Abdorasoul Ghasemi, Mohammad Ali Karami",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Network structure engineering aims to intervene in the network structure to improve a specific underlying process and has applications in many research fields like biology, transportation, and network design. Here we address the problem of online social networks (OSNs) engineering by link addition through the friend suggestion for an efficient dissemination process. We propose an end-to-end machine learning framework called <em>GCQL</em> for optimizing or generating an effective network structure to improve the dissemination process in OSNs. We first propose a dissemination model called <em>Complex-SII</em>, combining two widely used dissemination models in OSNs, SIR and IC, and complex contagion. The <em>GCQL</em> next uses a reinforcement learning algorithm called Neural Fitted <em>Q</em>-Iteration for optimizing a network structure for the dissemination process and uses a graph convolution network called <em>GCN</em> for graph embedding and learning the important patterns and characteristics of network structure for efficient spreading. The <em>GCQL</em> is trained with small-sized graphs and uses the learning outcomes for medium/large-sized graphs. Our simulation results show that <em>GCQL</em> can generate relatively more efficient network structures than the greedy approach for small-sized networks. <em>GCQL</em> also uses these results to develop relatively more efficient networks than the greedy approach for medium/large-sized network structures at a much higher speed. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21640700"
    },
    {
        "id": 26184,
        "title": "Front Matter",
        "authors": "",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119699057.fmatter"
    },
    {
        "id": 26185,
        "title": "Multi-Agent Reinforcement Learning Based Joint Resource Allocation for Ultra-Reliable Low-Latency Vehicular Communication",
        "authors": "Nasir Khan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Future vehicular networks must ensure ultra-reliable low-latency communication (URLLC) for the timely delivery of safety-critical information. Previously proposed resource allocation schemes for URLLC mostly rely on centralized optimization-based algorithms and cannot guarantee the reliability and latency requirements of vehicle-to-vehicle (V2V) communications. This paper investigates the joint power and blocklength allocation to minimize the worst-case decoding-error probability in the finite blocklength (FBL) regime for a URLLC-based V2V communication network. We formulate the problem as a non-convex mixed-integer nonlinear programming problem (MINLP). We first develop a centralized optimization theory-based algorithm based on the derivation of the joint convexity of the decoding error probability in the blocklength and transmit power variables within the region of interest. Next, we propose a two-layered multi-agent deep reinforcement learning based centrally trained and distributively executed framework. The first layer involves establishing multiple deep Q-networks (DQNs) at the central trainer to train the local DQNs for block length optimization. The second layer involves an actor-critic network and utilizes the deep deterministic policy-gradient (DDPG)-based algorithm to train the local actor network for each V2V link. Simulation results demonstrate that the proposed distributed scheme can achieve close-to-optimal solutions with a much lower computational complexity than the centralized optimization based solution. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21685046"
    },
    {
        "id": 26186,
        "title": "Multi-Agent Reinforcement Learning for Network Routing in Integrated Access Backhaul Networks",
        "authors": "Shahaf Yamin, Haim Permuter",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4414648"
    },
    {
        "id": 26187,
        "title": "Review for \"Prefrontal transcranial magnetic stimulation boosts response vigor during reinforcement learning in healthy adults\"",
        "authors": " Lieke Hofmans",
        "published": "2022-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/ejn.15905/v2/review1"
    },
    {
        "id": 26188,
        "title": "Decision letter for \"The value–complexity trade-off for reinforcement learning based brain–computer interfaces\"",
        "authors": "",
        "published": "2020-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1741-2552/abc8d8/v1/decision1"
    },
    {
        "id": 26189,
        "title": "Decision letter for \"The value–complexity trade-off for reinforcement learning based brain–computer interfaces\"",
        "authors": "",
        "published": "2020-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1741-2552/abc8d8/v2/decision1"
    },
    {
        "id": 26190,
        "title": "Decision letter for \"High-dimensional reinforcement learning for optimization and control of ultracold quantum gases\"",
        "authors": "",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad1437/v1/decision1"
    },
    {
        "id": 26191,
        "title": "Decision letter for \"High-dimensional reinforcement learning for optimization and control of ultracold quantum gases\"",
        "authors": "",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad1437/v2/decision1"
    },
    {
        "id": 26192,
        "title": "Neuroevolution-based Inverse Reinforcement Learning",
        "authors": "Karan K. Budhraja, Tim Oates",
        "published": "2017-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec.2017.7969297"
    },
    {
        "id": 26193,
        "title": "Laser Engraver Control System based on Reinforcement Adversarial Learning",
        "authors": "Evgeny Nikolaev",
        "published": "2019-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rusautocon.2019.8867762"
    },
    {
        "id": 26194,
        "title": "Application of Deep Reinforcement Learning in Asset Liability Management",
        "authors": "Takura Wekwete, Rodwell Kufakunesu, Gusti van Zyl",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4474207"
    },
    {
        "id": 26195,
        "title": "Integrating Deep Reinforcement Learning Networks with Health System Simulations.",
        "authors": "Michael Allen, Thomas Monks",
        "published": "No Date",
        "citations": 2,
        "abstract": "Background and motivation: Combining Deep Reinforcement Learning (Deep RL) and Health Systems Simulations has significant potential, for both research into improving Deep RL performance and safety, and in operational practice. While individual toolkits exist for Deep RL and Health Systems Simulations, no framework to integrate the two has been established. Aim: Provide a framework for integrating Deep RL Networks with Health System Simulations, and to ensure this framework is compatible with Deep RL agents that have been developed and tested using OpenAI Gym. Methods: We developed our framework based on the OpenAI Gym framework, and demonstrate its use on a simple hospital bed capacity model. We built the Deep RL agents using PyTorch, and the Hospital Simulation using SimPy. Results: We demonstrate example models using a Double Deep Q Network or a Duelling Double Deep Q Network as the Deep RL agent. Conclusion: SimPy may be used to create Health System Simulations that are compatible with agents developed and tested on OpenAI Gym environments. GitHub repository of code: https://github.com/MichaelAllen1966/learninghospital",
        "link": "http://dx.doi.org/10.20944/preprints202007.0598.v1"
    },
    {
        "id": 26196,
        "title": "A Review on Derivative Hedging using Reinforcement Learning",
        "authors": "Peng Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4217989"
    },
    {
        "id": 26197,
        "title": "Tiled Aperture Beam Combining with Reinforcement Learning",
        "authors": "Henrik Tunnermann, Akira Shirakawa",
        "published": "2020",
        "citations": 1,
        "abstract": "We show deep reinforcement learning for phase stabilization in tiled aperture coherent beam combining. The trained agent was able to stabilize the phase using only a far-field image as input.",
        "link": "http://dx.doi.org/10.1364/cleo_si.2020.sm1l.2"
    },
    {
        "id": 26198,
        "title": "Reinforcement learning influences memory specificity across development",
        "authors": "Kate Nussenbaum, Catherine Hartley",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1096-0"
    },
    {
        "id": 26199,
        "title": "A Decentralized Path Planning Model Based on Deep Reinforcement Learning",
        "authors": "dong guo, Shouwen Ji, yanke yao",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4411759"
    },
    {
        "id": 26200,
        "title": "Application of Deep Reinforcement Learning in Asset Liability Management",
        "authors": "Takura Wekwete, Rodwell Kufakunesu, Gusti van Zyl",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4417872"
    },
    {
        "id": 26201,
        "title": "Decision letter for \"High-dimensional reinforcement learning for optimization and control of ultracold quantum gases\"",
        "authors": "",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/ad1437/v2/decision1"
    },
    {
        "id": 26202,
        "title": "Neuroevolution-based Inverse Reinforcement Learning",
        "authors": "Karan K. Budhraja, Tim Oates",
        "published": "2017-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec.2017.7969297"
    },
    {
        "id": 26203,
        "title": "Laser Engraver Control System based on Reinforcement Adversarial Learning",
        "authors": "Evgeny Nikolaev",
        "published": "2019-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rusautocon.2019.8867762"
    },
    {
        "id": 26204,
        "title": "Application of Deep Reinforcement Learning in Asset Liability Management",
        "authors": "Takura Wekwete, Rodwell Kufakunesu, Gusti van Zyl",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4474207"
    },
    {
        "id": 26205,
        "title": "Integrating Deep Reinforcement Learning Networks with Health System Simulations.",
        "authors": "Michael Allen, Thomas Monks",
        "published": "No Date",
        "citations": 2,
        "abstract": "Background and motivation: Combining Deep Reinforcement Learning (Deep RL) and Health Systems Simulations has significant potential, for both research into improving Deep RL performance and safety, and in operational practice. While individual toolkits exist for Deep RL and Health Systems Simulations, no framework to integrate the two has been established. Aim: Provide a framework for integrating Deep RL Networks with Health System Simulations, and to ensure this framework is compatible with Deep RL agents that have been developed and tested using OpenAI Gym. Methods: We developed our framework based on the OpenAI Gym framework, and demonstrate its use on a simple hospital bed capacity model. We built the Deep RL agents using PyTorch, and the Hospital Simulation using SimPy. Results: We demonstrate example models using a Double Deep Q Network or a Duelling Double Deep Q Network as the Deep RL agent. Conclusion: SimPy may be used to create Health System Simulations that are compatible with agents developed and tested on OpenAI Gym environments. GitHub repository of code: https://github.com/MichaelAllen1966/learninghospital",
        "link": "http://dx.doi.org/10.20944/preprints202007.0598.v1"
    },
    {
        "id": 26206,
        "title": "A Review on Derivative Hedging using Reinforcement Learning",
        "authors": "Peng Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4217989"
    },
    {
        "id": 26207,
        "title": "Tiled Aperture Beam Combining with Reinforcement Learning",
        "authors": "Henrik Tunnermann, Akira Shirakawa",
        "published": "2020",
        "citations": 1,
        "abstract": "We show deep reinforcement learning for phase stabilization in tiled aperture coherent beam combining. The trained agent was able to stabilize the phase using only a far-field image as input.",
        "link": "http://dx.doi.org/10.1364/cleo_si.2020.sm1l.2"
    },
    {
        "id": 26208,
        "title": "Reinforcement learning influences memory specificity across development",
        "authors": "Kate Nussenbaum, Catherine Hartley",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1096-0"
    },
    {
        "id": 26209,
        "title": "A Decentralized Path Planning Model Based on Deep Reinforcement Learning",
        "authors": "dong guo, Shouwen Ji, yanke yao",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4411759"
    },
    {
        "id": 26210,
        "title": "Application of Deep Reinforcement Learning in Asset Liability Management",
        "authors": "Takura Wekwete, Rodwell Kufakunesu, Gusti van Zyl",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4417872"
    },
    {
        "id": 26211,
        "title": "Comparing Deep Reinforcement Learning Architectures for Autonomous Racing",
        "authors": "Benjamin Evans, Hendrik  Willem Jordaan, Herman  Arnold Engelbrecht",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4496980"
    },
    {
        "id": 26212,
        "title": "Explaining Voltage Control Decisions: A Scenario-Based Approach in Deep Reinforcement Learning",
        "authors": "Blaz Dobravec, Jure Žabkar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4535502"
    },
    {
        "id": 26213,
        "title": "Review for \"Mental health analysis for college students based on pattern recognition and reinforcement learning\"",
        "authors": " Jianhui Lv",
        "published": "2023-6-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/itl2.453/v2/review2"
    },
    {
        "id": 26214,
        "title": "Multi-Agent Deep Reinforcement Learning to Improve Efficiency of Autonomous Mining Transportation",
        "authors": "Shuwei Pei, Jue Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4604945"
    },
    {
        "id": 26215,
        "title": "A Micro Reinforcement Learning Architecture for Intrusion Detection Systems",
        "authors": "Boshra Darabi, Mozafar Bag-Mohammadi, Mojtaba Karami",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4701125"
    },
    {
        "id": 26216,
        "title": "Reinforcement Learning for Optimal Public Watershed and Aquifer Distribution and Management Actions",
        "authors": "Roberto Ortega, Dana Carciumaru",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4643886"
    },
    {
        "id": 26217,
        "title": "Shared Autonomy via Deep Reinforcement Learning",
        "authors": "Siddharth Reddy, Anca Dragan, Sergey Levine",
        "published": "2018-6-26",
        "citations": 73,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2018.xiv.005"
    },
    {
        "id": 26218,
        "title": "WITHDRAWN: Towards digital game-based learning content with multi-objective reinforcement learning",
        "authors": "Dineshkumar Rajendran, Prasanna Santhanam",
        "published": "2021-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.matpr.2021.03.156"
    },
    {
        "id": 26219,
        "title": "An Iterative Model-Based Reinforcement Learning Utilizing Multi-Perspective Learning with Monte-Carlo Tree Search",
        "authors": "Jiao Wang, Yijian Zhang, Yingxin Ren, Yingtong Ren, Xue Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4677186"
    },
    {
        "id": 26220,
        "title": "Learning to Take Cover with Navigation-Based Waypoints via Reinforcement Learning",
        "authors": "Timothy Aris, Volkan Ustun, Rajay Kumar",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "This paper presents a reinforcement learning model designed to learn how to take cover on geo-specific terrains, an essential behavior component for military training simulations. Training of the models is performed on the Rapid Integration and Development Environment (RIDE) leveraging the Unity ML-Agents framework. This work expands on previous work on raycast-based agents by increasing the number of enemies from one to three. We demonstrate an automated way of generating training and testing data within geo-specific terrains. We show that replacing the action space with a more abstracted, navmesh-based waypoint movement system can increase the generality and success rate of the models while providing similar results to our previous paper's results regarding retraining across terrains. We also comprehensively evaluate the differences between these and the previous models. Finally, we show that incorporating pixels into the model's input can increase performance at the cost of longer training times.",
        "link": "http://dx.doi.org/10.32473/flairs.36.133348"
    },
    {
        "id": 26221,
        "title": "Reinforcement Learning",
        "authors": "Teik Toe Teoh, Zheng Rong",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-8615-3_20"
    },
    {
        "id": 26222,
        "title": "An Improved Deep Learning Based Test Case Prioritization Using Deep Reinforcement Learning",
        "authors": "",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22266/ijies2024.0229.64"
    },
    {
        "id": 26223,
        "title": "Learning How to Self-Learn: Enhancing Self-Training Using Neural Reinforcement Learning",
        "authors": "Chenhua Chen, Yue Zhang, Yuze Gao",
        "published": "2018-11",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ialp.2018.8629107"
    },
    {
        "id": 26224,
        "title": "Effective Big Data Caching through Reinforcement Learning",
        "authors": "Mirco Tracolli, Marco Baioletti, Valentina Poggioni, Daniele Spiga",
        "published": "2020-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla51294.2020.00231"
    },
    {
        "id": 26225,
        "title": "Deep reinforcement learning applications in connected-automated transportation systems",
        "authors": "H. M. Abdul Aziz, Sanjoy Das",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003190691-9"
    },
    {
        "id": 26226,
        "title": "Active learning of causal structures with deep reinforcement learning",
        "authors": "Amir Amirinezhad, Saber Salehkaleybar, Matin Hashemi",
        "published": "2022-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.06.028"
    },
    {
        "id": 26227,
        "title": "Inverse Reinforcement Learning with Learning and Leveraging Demonstrators’ Varying Expertise Levels",
        "authors": "Somtochukwu Oguchienti, Mahsa Ghasemi",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/allerton58177.2023.10313475"
    },
    {
        "id": 26228,
        "title": "Learning Intrinsic Symbolic Rewards in Reinforcement Learning",
        "authors": "Hassam Ullah Sheikh, Shauharda Khadka, Santiago Miret, Somdeb Majumdar, Mariano Phielipp",
        "published": "2022-7-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892256"
    },
    {
        "id": 26229,
        "title": "Learning to Draw Through A Multi-Stage Environment Model Based Reinforcement Learning",
        "authors": "Ji Qiu, Peng Lu, Xujun Peng",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222280"
    },
    {
        "id": 26230,
        "title": "Learning from demonstration: Teaching a myoelectric prosthesis with an intact limb via reinforcement learning",
        "authors": "Gautham Vasan, Patrick M. Pilarski",
        "published": "2017-7",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icorr.2017.8009453"
    },
    {
        "id": 26231,
        "title": "Learning Sparse Evidence- Driven Interpretation to Understand Deep Reinforcement Learning Agents",
        "authors": "Giang Dao, Wesley Houston Huff, Minwoo Lee",
        "published": "2021-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9660192"
    },
    {
        "id": 26232,
        "title": "Leveraging Knowledge Distillation for Efficient Deep Reinforcement Learning in Resource-Constrained Environments",
        "authors": "Guanlin Meng",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424815"
    },
    {
        "id": 26233,
        "title": "Autonomous Driving using Deep Reinforcement Learning in Urban Environment",
        "authors": "Hashim Shakil Ansari, Goutam R,  ",
        "published": "2019-4-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31142/ijtsrd23442"
    },
    {
        "id": 26234,
        "title": "PARL: A Dialog System Framework with Prompts as Actions for Reinforcement Learning",
        "authors": "Tao Xiang, Yangzhe Li, Monika Wintergerst, Ana Pecini, Dominika Młynarczyk, Georg Groh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011725200003393"
    },
    {
        "id": 26235,
        "title": "Accelerated Variant of Reinforcement Learning Algorithms for Light Control with Non-stationary User Behaviour",
        "authors": "Nassim Haddam, Benjamin Boulakia, Dominique Barth",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010987900003203"
    },
    {
        "id": 26236,
        "title": "Reinforcement Learning als eine Grundlage für die Cross Domain Fusion heterogener Daten",
        "authors": "Sören Christensen, Sven Tomforde",
        "published": "2022-8",
        "citations": 0,
        "abstract": "AbstractWe propose to establish a research direction based on Reinforcement Learning in the scope of Cross Domain Fusion. More precisely, we combine the algorithmic approach of evolutionary rule-based Reinforcement Learning with the efficiency and performance of Deep Reinforcement Learning, while simultaneously developing a sound mathematical foundation. A possible scenario is traffic control in urban regions.",
        "link": "http://dx.doi.org/10.1007/s00287-022-01468-x"
    },
    {
        "id": 26237,
        "title": "A Deep Reinforcement Learning-Based Approach in Porker Game",
        "authors": "Yan Kong Yan Kong, Yefeng Rui Yan Kong, Chih-Hsien Hsia Yefeng Rui",
        "published": "2023-4",
        "citations": 1,
        "abstract": "\n                        <p>Recent years have witnessed the big success deep reinforcement learning achieved in the domain of card and board games, such as Go, chess and Texas Hold&rsquo;em poker. However, Dou Di Zhu, a traditional Chinese card game, is still a challenging task for deep reinforcement learning methods due to the enormous action space and the sparse and delayed reward of each action from the environment. Basic reinforcement learning algorithms are more effective in the simple environments which have small action spaces and valuable and concrete reward functions, and unfortunately, are shown not be able to deal with Dou Di Zhu satisfactorily. This work introduces an approach named Two-steps Q-Network based on DQN to playing Dou Di Zhu, which compresses the huge action space through dividing it into two parts according to the rules of Dou Di Zhu and fills in the sparse rewards using inverse reinforcement learning (IRL) through abstracting the reward function from experts&rsquo; demonstrations. It is illustrated by the experiments that two-steps Q-network gains great advancements compared with DQN used in Dou Di Zhu.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/199115992023043402004"
    },
    {
        "id": 26238,
        "title": "Adaptive Action Supervision in Reinforcement Learning from Real-World Multi-Agent Demonstrations",
        "authors": "Keisuke Fujii, Kazushi Tsutsui, Atom Scott, Hiroshi Nakahara, Naoya Takeishi, Yoshinobu Kawahara",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012261100003636"
    },
    {
        "id": 26239,
        "title": "Estimation on Human Motion Posture using Improved Deep Reinforcement Learning",
        "authors": "Wenjing Ma Wenjing Ma, Jianguang Zhao Wenjing Ma, Guangquan Zhu Jianguang Zhao",
        "published": "2023-8",
        "citations": 0,
        "abstract": "\n                        <p>Estimating human motion posture can provide important data for intelligent monitoring systems, human-computer interaction, motion capture, and other fields. However, the traditional human motion posture estimation algorithm is difficult to achieve the goal of fast estimation of human motion posture. To address the problems of traditional algorithms, in the paper, we propose an estimation algorithm for human motion posture using improved deep reinforcement learning. First, the double deep Q network is constructed to improve the deep reinforcement learning algorithm. The improved deep reinforcement learning algorithm is used to locate the human motion posture coordinates and improve the effectiveness of bone point calibration. Second, the human motion posture analysis generative adversarial networks are constructed to realize the automatic recognition and analysis of human motion posture. Finally, using the preset human motion posture label, combined with the undirected graph model of the human, the human motion posture estimation is completed, and the precise estimation algorithm of the human motion posture is realized. Experiments are performed based on MPII Human Pose data set and HiEve data set. The results show that the proposed algorithm has higher positioning accuracy of joint nodes. The recognition effect of bone joint points is better, and the average is about 1.45%. The average posture accuracy is up to 98.2%, and the average joint point similarity is high. Therefore, it is proved that the proposed method has high application value in human-computer interaction, human motion capture and other fields.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/199115992023083404008"
    },
    {
        "id": 26240,
        "title": "Systematic Model-based Design of a Reinforcement Learning-based Neural Adaptive Cruise Control System",
        "authors": "Or Yarom, Jannis Fritz, Florian Lange, Xiaobo Liu-Henke",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010923300003116"
    },
    {
        "id": 26241,
        "title": "Q-Credit Card Fraud Detector for Imbalanced Classification using Reinforcement Learning",
        "authors": "Luis Zhinin-Vera, Oscar Chang, Rafael Valencia-Ramos, Ronny Velastegui, Gissela Pilliza, Francisco Socasi",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009156102790286"
    },
    {
        "id": 26242,
        "title": "Robo-Advising: Enhancing Investment with Inverse Optimization and Deep Reinforcement Learning",
        "authors": "Haoran Wang, Shi Yu",
        "published": "2021-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00063"
    },
    {
        "id": 26243,
        "title": "Reinforcement Learning",
        "authors": "Xiangming Zeng, Liangqu Long",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-7915-1_14"
    },
    {
        "id": 26244,
        "title": "Cooperation Pattern Exploration for Multi-Agent Reinforcement Learning",
        "authors": "Chenran Zhao, Kecheng Peng, Xiaoqun Cao",
        "published": "2022-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlccim55934.2022.00077"
    },
    {
        "id": 26245,
        "title": "Reinforcement Learning Algorithms for Uncertain, Dynamic, Zero-Sum Games",
        "authors": "Snehasis Mukhopadhyay, Omkar Tilak, Subir Chakrabarti",
        "published": "2018-12",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00015"
    },
    {
        "id": 26246,
        "title": "Robustly Learning Composable Options in Deep Reinforcement Learning",
        "authors": "Akhil Bagaria, Jason Senthil, Matthew Slivinski, George Konidaris",
        "published": "2021-8",
        "citations": 5,
        "abstract": "Hierarchical reinforcement learning (HRL) is only effective for long-horizon problems  when high-level skills can be reliably sequentially executed. Unfortunately, learning reliably composable skills is difficult, because all the components of every skill are constantly changing during learning. We propose three methods for improving the composability of learned skills: representing skill initiation regions using a combination of pessimistic and optimistic classifiers; learning re-targetable policies that are robust to non-stationary subgoal regions; and learning robust option policies using model-based RL. We test these improvements on four sparse-reward maze navigation tasks involving a simulated quadrupedal robot. Each method successively improves the robustness of a baseline skill discovery method, substantially outperforming state-of-the-art flat and hierarchical methods.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/298"
    },
    {
        "id": 26247,
        "title": "Optimizing Hadoop parameter for speedup using Q-Learning Reinforcement Learning",
        "authors": "Nandita Yambem, A. N. Nandakumar",
        "published": "2021-9-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecct52121.2021.9616965"
    },
    {
        "id": 26248,
        "title": "An Improved Deep Reinforcement Learning for Task-oriented Dialogue System",
        "authors": "Zahra Dehghanipour, Javad Salimi Sartakhti",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNowadays, the need for dialogue systems is felt more than ever. Many services around us need continued support, while the domain of support is limited and specific. Therefore, instead of employing manpower, dialogue systems can help. In this paper, we propose a task-oriented dialogue system based on deep reinforcement learning empowered by several new ideas: first, due to the shortage of training data, we use GAN to generate more training data without human intervention. Second, we propose a new state embedding for representing environment in dialogue system. In this way, we use sentiment analysis and clustering methods. The proposed embedding makes a more accurate and deeper understanding of the environment, which ultimately leads to an increase in system performance. Additionally, we present a new reward function for the agent to better learn dialogues. In order to evaluate this system, we use both user simulator and human judges. With user simulator, we evaluate the average rewards of the proposed system and its improvements. With human judges, we investigate human evaluations about the proposed system. Based on these evaluations promising and comparable results are observed.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1792905/v1"
    },
    {
        "id": 26249,
        "title": "Resource allocation of fog wireless access network based on deep reinforcement learning",
        "authors": "Jingru Tan, Wenbo Guan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Aiming at the problem of huge energy consumption in the Fog Wireless\nAccess Networks (F-RANs), the resource allocation scheme of the F-RAN\narchitecture under the cooperation of renewable energy is studied in\nthis paper. Firstly, the transmission model and Energy Harvesting (EH)\nmodel are established, the solar energy harvester is installed on each\nFog Access Point (F-AP), and each F-AP is connected to the smart grid.\nSecondly, the optimization problem is established according to the\nconstraints of Signal to Noise Ratio (SNR), available bandwidth and\nenergy harvesting, so as to maximize the average throughput of F-RAN\narchitecture with hybrid energy sources. Finally, the dynamic power\nallocation scheme in the network is studied by using Q-learning and Deep\nQ Network (DQN) respectively. Simulation results show that the proposed\ntwo algorithms can improve the average throughput of the whole network\ncompared with other traditional algorithms.",
        "link": "http://dx.doi.org/10.22541/au.163620090.07696354/v1"
    },
    {
        "id": 26250,
        "title": "Reinforcement Learning",
        "authors": "Robert H. Chen, Chelsea Chen",
        "published": "2022-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003214892-33"
    },
    {
        "id": 26251,
        "title": "Policy Gradient Algorithms",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-14"
    },
    {
        "id": 26252,
        "title": "Surprise acts as a reducer of outcome value in human reinforcement learning",
        "authors": "Motofumi Sumiya, Kentaro Katahira",
        "published": "No Date",
        "citations": 0,
        "abstract": "Surprise occurs because of differences between a decision outcome and its predicted outcome (prediction error), regardless of whether the error is positive or negative. It has recently been postulated that surprise affects the reward value of the action outcome itself; studies have indicated that increasing surprise, as absolute value of prediction error, decreases the value of the outcome. However, how surprise affects the value of the outcome and subsequent decision making is unclear. We suggested that, on the assumption that surprise decreases the outcome value, agents will increase their risk averse choices when an outcome is often surprisal. Here, we propose the surprise-sensitive utility model, a reinforcement learning model that states that surprise decreases the outcome value, to explain how surprise affects subsequent decision-making. To investigate the assumption, we compared this model with previous reinforcement learning models on a risky probabilistic learning task with simulation analysis, and model selection with two experimental datasets with different tasks and population. We further simulated a simple decision-making task to investigate how parameters within the proposed model modulate the choice preference. As a result, we found the proposed model explains the risk averse choices in a manner similar to the previous models, and risk averse choices increased as the surprise-based modulation parameter of outcome value increased. The model fits these datasets better than the other models, with same free parameters, thus providing a more parsimonious and robust account for risk averse choices. These findings indicate that surprise acts as a reducer of outcome value and decreases the action value for risky choices in which prediction error often occurs.",
        "link": "http://dx.doi.org/10.31234/osf.io/5ha3y"
    },
    {
        "id": 26253,
        "title": "Autonomous Vehicle Decision-Making at Unsignalized Intersection via Deep Reinforcement Learning",
        "authors": "Junran Xie, Qingling Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper is concerned with the performance of different state representation on the vehicle decision-making problem at unsignalized intersection based on deep reinforcement learning. A hybrid state representation architecture based on attention mechanism and collision time prediction is designed, which can effectively improve the autonomous decision-making ability of vehicles. Compared with the traditional state representation method, the feature of our method is that the fusion of features can improve the vehicle’s obstacle avoidance ability, and the introduced action masking module can improve the vehicle’s traffic efficiency. Finally, test results in the simulation environment verify the effectiveness of our proposed method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1757806/v1"
    },
    {
        "id": 26254,
        "title": "Deep reinforcement learning for active hypothesis testing with heterogeneous agents and cost constraints",
        "authors": "George Stamatelis, Nicholas Kalouptsidis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We  consider active hypothesis testing with multiple heterogeneous agents. Each agent has access to its own set of experiments, has different action costs and forms its own beliefs. Additionally,  each experiment carries a global cost, and the agents must try to keep the expected cumulative cost below a certain threshold. We study a centralized and a decentralized scenario. Under the centralized scenario, the agents  are instructed how to act by a central controller. Under the decentralized scenario they communicate and exchange information over a directed graph. For each scenario we propose  separate deep reinforcement learning algorithms based on proximal policy optimization. Solutions to the decentralized problem start from fully decentralised training, and progressively introduce two levels of centralisation during training.  We assess the proposed algorithms in an example of anomaly detection over sensor networks, considering three different decentralised communication settings. We infer that all algorithms achieve the required accuracy level considerably faster than a single deep reinforcement learning agent, while satisfying the expected cost constraints when required. Under the assumption that the communication  graph is fully connected, the decentralised agents perform just as well, and sometimes better than the centralised  controller. Out of all decentralised algorithms, the one that uses a global critic is by far the best performing and can compete with the central controller even when the communication graph is not complete.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22723507.v1"
    },
    {
        "id": 26255,
        "title": "Buyers Collusion in Incentivized Forwarding Networks via Multi-agent Reinforcement Learning",
        "authors": "Mostafa Abdelhadi, Sabit Ekin, Ali Imran",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We present the issue of monetarily incentivized forwarding in a multi-hop mesh network architecture from an economic perspective. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23605749.v1"
    },
    {
        "id": 26256,
        "title": "Reinforcement Learning Fuzzy Algorithm for Adaptive Cabin Management System With Application for Adaptive Interior Lighting",
        "authors": "Anas Saad",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The purpose of this thesis was to design a framework for an adaptive cabin management system that is further explored through a study of light intensity. With the goal of passenger comfort, the system developed would adjust lighting to an average setting and further adapt to an individuals preference. A fuzzy inference system was implemented that utilizes DGI, the passengers age, chronotype and the activity on board to calculate a light intensity. A reinforcement system was then developed to tune the fuzzy inference system parameters (mean and output value K) utilizing a lighting override from the passenger. A cabin mock up was then setup to observe the correctness and e▯ectiveness of the system developed. Overall, the system designed tuned accurately and e▯ectively in all case scenarios tested with varying learning rates. This thesis was concluded with discussions of future work that could further improve the implementations within a cabin.</p>",
        "link": "http://dx.doi.org/10.32920/24625128.v1"
    },
    {
        "id": 26257,
        "title": "A Neural Network Based Automatic Generation Controller Design through Reinforcement Learning",
        "authors": "Imthias Ahamed, Nagendra Rao,  Sastry",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/energyo.0034.00236"
    },
    {
        "id": 26258,
        "title": "Optimal Monetary Policy Using Reinforcement Learning",
        "authors": "Natascha Hinterlang, Alina Tänzer",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4025682"
    },
    {
        "id": 26259,
        "title": "Recent Applications and Future Research",
        "authors": "Rafael Ris-Ala",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-37345-9_6"
    },
    {
        "id": 26260,
        "title": "Reinforcement Learning for Approximate Optimal Control",
        "authors": "Warren E. Dixon",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-44184-5_100063"
    },
    {
        "id": 26261,
        "title": "Dopamine and Reinforcement Learning",
        "authors": "Henry H. Yin",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429154461-12"
    },
    {
        "id": 26262,
        "title": "Safety Constraints-Guided Reinforcement Learning with Linear Temporal Logic",
        "authors": "Ryeonggu Kwon, Gihwon Kwon",
        "published": "No Date",
        "citations": 0,
        "abstract": "In the context of reinforcement learning (RL), ensuring both safety and performance is crucial, especially in real-world scenarios where mistakes can lead to severe consequences. This study aims to address this challenge by integrating temporal logic constraints into RL algorithms, thereby providing a formal mechanism for safety verification. We employ a combination of theoretical and empirical methods, including the use of temporal logic for formal verification and extensive simulations to validate our approach. Our results demonstrate that the proposed method not only maintains high levels of safety but also achieves comparable performance to traditional RL algorithms. Importantly, our approach fills a critical gap in existing literature by offering a solution that is both mathematically rigorous and empirically validated. The study concludes that the integration of temporal logic into RL offers a promising avenue for developing algorithms that are both safe and efficient. This work lays the foundation for future research aimed at generalizing this approach to various complex systems and applications.",
        "link": "http://dx.doi.org/10.20944/preprints202310.1862.v1"
    },
    {
        "id": 26263,
        "title": "Reinforced-Lib: Rapid Prototyping of Reinforcement Learning Solutions",
        "authors": "Maksymilian Wojnar, Szymon Szott, Krzysztof Rusek, Wojciech Ciężobka",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4584200"
    },
    {
        "id": 26264,
        "title": "Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning Approach",
        "authors": "Soroush Saghafian",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3980837"
    },
    {
        "id": 26265,
        "title": "Risk-Sensitive Reinforcement Learning Part I: Constrained Optimization Framework",
        "authors": "L.A. Prashanth",
        "published": "2019-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indiancc.2019.8715578"
    },
    {
        "id": 26266,
        "title": "Testing the reinforcement learning hypothesis of social conformity",
        "authors": "Marie Levorsen, Ayahito Ito, Shinsuke Suzuki, Keise Izuma",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractOur preferences are influenced by the opinions of others. The past human neuroimaging studies on social conformity have identified a network of brain regions related to social conformity that includes the posterior medial frontal cortex (pMFC), anterior insula, and striatum. It was hypothesized that since these brain regions are also known to play important roles in reinforcement learning (i.e., processing prediction error), social conformity and reinforcement learning have a common neural mechanism. However, these two processes have previously never been directly compared; therefore, the extent to which they shared a common neural mechanism had remained unclear. This study aimed to formally test the hypothesis. The same group of participants (n = 25) performed social conformity and reinforcement learning tasks inside a functional magnetic resonance imaging (fMRI) scanner. Univariate fMRI data analyses revealed activation overlaps in the pMFC and bilateral insula between social conflict and unsigned prediction error and in the striatum between social conflict and signed prediction error. We further conducted multi-voxel pattern analysis (MVPA) for more direct evidence of a shared neural mechanism. MVPA did not reveal any evidence to support the hypothesis in any of these regions but found that activation patterns between social conflict and prediction error in these regions were largely distinct. Taken together, the present study provides no clear evidence of a common neural mechanism between social conformity and reinforcement learning.",
        "link": "http://dx.doi.org/10.1101/2020.03.11.988220"
    },
    {
        "id": 26267,
        "title": "Strain design optimization using reinforcement learning",
        "authors": "Maryam Sabzevari, Sandor Szedmak, Merja Penttilä, Paula Jouhten, Juho Rousu",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractEngineered microbial cells present a sustainable alternative to fossil-based synthesis of chemicals and fuels. Cellular synthesis routes are readily assembled and introduced into microbial strains using state-of-the-art synthetic biology tools. However, the optimization of the strains required to reach industrially feasible production levels is far less efficient. It typically relies on trial-and-error leading into high uncertainty in total duration and cost. New techniques that can cope with the complexity and limited mechanistic knowledge of the cellular regulation are called for guiding the strain optimization.In this paper, we put forward a multi-agent reinforcement learning (MARL) approach that learns from experiments to tune the metabolic enzyme levels so that the production is improved. Our method is model-free and does not assume prior knowledge of the microbe’s metabolic network or its regulation. The multi-agent approach is well-suited to make use of parallel experiments such as multi-well plates commonly used for screening microbial strains.We demonstrate the method’s capabilities using the genome-scale kinetic model of Escherichia coli, k-ecoli457, as a surrogate for an in vivo cell behaviour in cultivation experiments. We investigate the method’s performance relevant for practical applicability in strain engineering i.e. the speed of convergence towards the optimum response, noise tolerance, and the statistical stability of the solutions found. We further evaluate the proposed MARL approach in improving L-tryptophan production by yeast Saccharomyces cerevisiae, using publicly available experimental data on the performance of a combinatorial strain library.Overall, our results show that multi-agent reinforcement learning is a promising approach for guiding the strain optimization beyond mechanistic knowledge, with the goal of faster and more reliably obtaining industrially attractive production levels.Author summaryEngineered microbial cells offer a sustainable alternative solution to chemical production from fossil resources. However, to make the chemical production using microbial cells economically feasible, they need to be substantially optimized. Due to the biological complexity, this optimization to reach sufficiently high production is typically a costly trial and error process.This paper presents an Artificial Intelligence (AI) approach to guide this task. Our tool learns a model from previous experiments and uses the model to suggest improvements to the engineering design, until a satisfactory production performance is reached. This paper evaluates the behaviour of the proposed AI method from several angles, including the amount of experiments needed, the tolerance to noise as well as the stability of the proposed designs.",
        "link": "http://dx.doi.org/10.1101/2022.03.22.485285"
    },
    {
        "id": 26268,
        "title": "Population based Reinforcement Learning",
        "authors": "Kyle W. Pretorius, Nelishia Pillay",
        "published": "2021-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9660084"
    },
    {
        "id": 26269,
        "title": "AlphaZero",
        "authors": "Hongming Zhang, Tianyang Yu",
        "published": "2020",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_15"
    },
    {
        "id": 26270,
        "title": "Deep Learning vs. Discrete Reinforcement Learning for Adaptive Traffic Signal Control",
        "authors": "Soheil Mohamad Alizadeh Shabestary, Baher Abdulhai",
        "published": "2018-11",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2018.8569549"
    },
    {
        "id": 26271,
        "title": "Predicting Real-time Scientific Experiments Using Transformer models and Reinforcement Learning",
        "authors": "Juan Manuel Parrilla-Gutierrez",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00084"
    },
    {
        "id": 26272,
        "title": "Metaoptimization on a Distributed System for Deep Reinforcement Learning",
        "authors": "Greg Heinrich, Iuri Frosio",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlhpc49564.2019.00008"
    },
    {
        "id": 26273,
        "title": "Deep Reinforcement Learning Applied to Active Flow Control",
        "authors": "J. Rabault, A. Kuhnle",
        "published": "2023-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108896214.025"
    },
    {
        "id": 26274,
        "title": "ENHANCING STEM EDUCATION USING MACHINE LEARNING AND REINFORCEMENT LEARNING TECHNIQUES FOR EDUCATIONAL SOFTWARE AND SERIOUS GAMES",
        "authors": "Yuexin Liu, Behbood Zoghi",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/edulearn.2023.1871"
    },
    {
        "id": 26275,
        "title": "Split Q Learning: Reinforcement Learning with Two-Stream Rewards",
        "authors": "Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi",
        "published": "2019-8",
        "citations": 9,
        "abstract": "Drawing an inspiration from behavioral studies of human decision making, we propose here a general parametric framework for a reinforcement learning problem, which extends the standard Q-learning approach to incorporate a two-stream framework of reward processing with biases biologically associated with several neurological and psychiatric conditions, including Parkinson's and Alzheimer's diseases, attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain. For AI community, the development of agents that react differently to different types of rewards can enable us to understand a wide spectrum of multi-agent interactions in complex real-world socioeconomic systems. Moreover, from the behavioral modeling perspective, our parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions and user preferences in long-term recommendation systems.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/913"
    },
    {
        "id": 26276,
        "title": "FARANE-Q: Fast Parallel and Pipeline Q-Learning Accelerator for Configurable Reinforcement Learning SoC",
        "authors": "Nana Sutisna, Andi M. R. Ilmy, Infall Syafalni, Rahmat Mulyawan, Trio Adiono",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper proposes a FAst paRAllel and pipeliNE Q-learning accelerator (FARANE-Q) for a configurable Reinforcement Learning (RL) algorithm implemented in a System on Chip (SoC). The proposed work offers flexibility, configurability, and scalability while maintaining computation speed and accuracy to overcome the challenges of a dynamic environment and increasing complexity. The proposed method includes a Hardware/Software (HW/SW) design methodology for the SoC architecture to achieve flexibility. We also propose joint optimizations on the algorithm, architecture, and implementation to obtain optimum (high efficiency) performance, specifically in energy and area efficiency. Furthermore, we implemented the proposed design in a real-time Zynq Ultra96-V2 FPGA platform to evaluate the functionality with an actual use case of smart navigation. Experimental results confirm that the proposed accelerator FARANE-Q outperforms state-of-the-art works by achieving a throughput of up to 148.55 MSps. It corresponds to the energy efficiency of 1747.64 MSps/W per agent for 32-bit and 2424.33 MSps/W per agent for 16-bit FARANE-Q. Moreover, the proposed 16-bit FARANE-Q outperforms other related works by an improvement of at least 1.23× in energy efficiency. The designed system also maintains an error accuracy of less than 0.4% with optimized bit precision for more than eight fraction bits. The proposed FARANE-Q also offers a speed up of processing time up to 1795× compared to embedded SW computation executed on ARM Zynq processor and 280× of computation of full software executed on i7 processor. Hence, the proposed work has the potential to be used for smart navigation, robotic control, and predictive maintenance.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21435606.v3"
    },
    {
        "id": 26277,
        "title": "FARANE-Q: Fast Parallel and Pipeline Q-Learning Accelerator for Configurable Reinforcement Learning SoC",
        "authors": "Nana Sutisna, Andi M. R. Ilmy, Infall Syafalni, Rahmat Mulyawan, Trio Adiono",
        "published": "No Date",
        "citations": 2,
        "abstract": "<p>This paper proposes a FAst paRAllel and pipeliNE Q-learning accelerator (FARANE-Q) for a configurable Reinforcement Learning (RL) algorithm that is implemented in a System on Chip (SoC). In order to overcome the challenges of a dynamic environment and increasing complexity, the proposed work offers flexibility, configurability, and scalability while maintaining computation speed and accuracy. The proposed method includes a HW/SW design methodology for the SoC architecture to achieve flexibility. Moreover, we also propose joint optimizations on algorithm, architecture and implementation in order to obtain optimum (high efficiency) performance, specifically in energy and area efficiency. Furthermore, we implemented the proposed design in a real-time Zynq Ultra96-V2 FPGA platform to evaluate the functionality with real use case of the smart navigation. Experimental results confirm that the proposed accelerator FARANE-Q outperforms state of the art works by achieving throughput up to 148.55 MSps. It corresponds to the energy efficiency of 1632.42 MSps/W per agent for 32-bit and 2465.42 MSps/W per agent for 16-bit FARANE-Q. Moreover, the proposed 16-bit FARANE-Q outperforms others in energy efficiency up to more than 2000×. The designed system also maintains the error accuracy less than 0.4% with optimized bit precision for more than 8 fraction bits. The proposed FARANE-Q also offers a speed up of processing time up to 1795× compared to embedded SW computation executed on ARM Zynq processor and 280× of computation of full software executed on i7 processor. Hence, the proposed work has the potential to be used for smart navigation, robotic control, and predictive maintenance.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21435606.v1"
    },
    {
        "id": 26278,
        "title": "Applying Deep Learning and Reinforcement Learning to Traveling Salesman Problem",
        "authors": "Shoma Miki, Daisuke Yamamoto, Hiroyuki Ebara",
        "published": "2018-8",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccecome.2018.8659266"
    },
    {
        "id": 26279,
        "title": "Optimizing Agent Training with Deep Q-Learning on a Self-Driving Reinforcement Learning Environment",
        "authors": "Pedro Rodrigues, Susana Vieira",
        "published": "2020-12-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci47803.2020.9308525"
    },
    {
        "id": 26280,
        "title": "Skill-based curiosity for intrinsically motivated reinforcement learning",
        "authors": "Nicolas Bougie, Ryutaro Ichise",
        "published": "2020-3",
        "citations": 15,
        "abstract": "AbstractReinforcement learning methods rely on rewards provided by the environment that are extrinsic to the agent. However, many real-world scenarios involve sparse or delayed rewards. In such cases, the agent can develop its own intrinsic reward function called curiosity to enable the agent to explore its environment in the quest of new skills. We propose a novel end-to-end curiosity mechanism for deep reinforcement learning methods, that allows an agent to gradually acquire new skills. Our method scales to high-dimensional problems, avoids the need of directly predicting the future, and, can perform in sequential decision scenarios. We formulate the curiosity as the ability of the agent to predict its own knowledge about the task. We base the prediction on the idea of skill learning to incentivize the discovery of new skills, and guide exploration towards promising solutions. To further improve data efficiency and generalization of the agent, we propose to learn a latent representation of the skills. We present a variety of sparse reward tasks in MiniGrid, MuJoCo, and Atari games. We compare the performance of an augmented agent that uses our curiosity reward to state-of-the-art learners. Experimental evaluation exhibits higher performance compared to reinforcement learning models that only learn by maximizing extrinsic rewards.",
        "link": "http://dx.doi.org/10.1007/s10994-019-05845-8"
    },
    {
        "id": 26281,
        "title": "Novel reinforcement learning paradigm based on response patterning under interval schedules of reinforcement",
        "authors": "Christin Schifani, Ilya Sukhanov, Mariia Dorofeikova, Anton Bespalov",
        "published": "2017-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.bbr.2017.04.043"
    },
    {
        "id": 26282,
        "title": "Learning key steps to attack deep reinforcement learning agents",
        "authors": "Chien-Min Yu, Ming-Hsin Chen, Hsuan-Tien Lin",
        "published": "2023-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-023-06318-9"
    },
    {
        "id": 26283,
        "title": "Learning Adaptive Graph Protection Strategy on Dynamic Networks via Reinforcement Learning",
        "authors": "Arie Wahyu Wijayanto, Tsuyoshi Murata",
        "published": "2018-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wi.2018.00-41"
    },
    {
        "id": 26284,
        "title": "Reinforcement Learning: Using Newly Gained Knowledge for Insights",
        "authors": "Andreas François Vermeulen",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-5316-8_9"
    },
    {
        "id": 26285,
        "title": "Ageing disrupts reinforcement learning whilst learning to help others is preserved",
        "authors": "Jo Cutler, Marco Wittmann, Ayat Abdurahman, Luca Hargitai, Daniel Drew, Masud Husain, Patricia Lockwood",
        "published": "No Date",
        "citations": 5,
        "abstract": "AbstractReinforcement learning is a fundamental mechanism displayed by many species. However, adaptive behaviour depends not only on learning about actions and outcomes that affect ourselves, but also those that affect others. Here, using computational reinforcement learning models, we tested whether young (age 18-36) and older (age 60-80, total n=152) adults can learn to gain rewards for themselves, another person (prosocial), or neither individual (control). Detailed model comparison showed that a model with separate learning rates for each recipient best explained behaviour. Young adults were faster to learn when their actions benefitted themselves, compared to helping others. Strikingly, compared to younger adults, older adults showed preserved prosocial learning rates but reduced self-relevant learning rates. Moreover, psychopathic traits were lower in older adults and negatively correlated with prosocial learning. These findings suggest learning how to benefit others is preserved across the lifespan with implications for reinforcement learning and theories of healthy ageing.",
        "link": "http://dx.doi.org/10.1101/2020.12.02.407718"
    },
    {
        "id": 26286,
        "title": "Tutorial on Reinforcement Learning",
        "authors": "Kishansingh Rajput",
        "published": "2022-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1963686"
    },
    {
        "id": 26287,
        "title": "Reinforcement learning-based channel sharing in wireless vehicular networks",
        "authors": "Andreas Pressas,  Zhengguo Sheng, Falah Ali",
        "published": "2019-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbte081e_ch7"
    },
    {
        "id": 26288,
        "title": "Curriculum learning for deep reinforcement learning in swarm robotic navigation task",
        "authors": "Alaa Iskandar, Béla Kovács",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "This study investigates the training of a swarm consisting of five E-puck robots using Deep reinforcement learning with curriculum learning in a 3D environment. The primary objective is to decompose the navigation task into a curriculum comprising progressively more challenging stages based on curriculum complexity metrics. These metrics encompass swarm size, collision avoidance complexity, and distances between targets and robots. The performance evaluation of the swarm includes key metrics such as success rate, collision rate, training efficiency, and generalization capabilities. To assess their effectiveness, a comparative analysis is conducted between curriculum learning and the proximal policy optimization algorithm. The results demonstrate that curriculum learning outperforms traditional one, yielding higher success rates, improved collision avoidance, and enhanced training efficiency. The trained swarm also exhibits robust generalization for novel scenarios.",
        "link": "http://dx.doi.org/10.35925/j.multi.2023.3.18"
    },
    {
        "id": 26289,
        "title": "Adaptive Natural Policy Gradient in Reinforcement Learning",
        "authors": "Dazi Li, Zengyuan Qiao, Tianheng Song, Qibing Jin",
        "published": "2018-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls.2018.8515994"
    },
    {
        "id": 26290,
        "title": "Learning Tasks in Intelligent Environments via Inverse Reinforcement Learning",
        "authors": "Syed Ihtesham Hussain Shah, Antonio Coronato",
        "published": "2021-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ie51775.2021.9486594"
    },
    {
        "id": 26291,
        "title": "Competitive-Cooperative Multi-Agent Reinforcement Learning for Auction-based Federated Learning",
        "authors": "Xiaoli Tang, Han Yu",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Auction-based Federated Learning (AFL) enables open collaboration among self-interested data consumers and data owners. Existing AFL approaches cannot manage the mutual influence among multiple data consumers competing to enlist data owners. Moreover, they cannot support a single data owner to join multiple data consumers simultaneously. To bridge these gaps, we propose the Multi-Agent Reinforcement Learning for AFL (MARL-AFL) approach to steer data consumers to bid strategically\n\ntowards an equilibrium with desirable overall system characteristics. We design a temperature-based reward reassignment scheme to make tradeoffs between cooperation and competition among AFL data consumers. In this way, it can reach an equilibrium state that ensures individual data consumers can achieve good utility, while preserving system-level social welfare. To circumvent potential collusion behaviors among data consumers, we introduce a bar agent to set a personalized bidding\n\nlower bound for each data consumer. Extensive experiments on six commonly adopted benchmark datasets show that MARL-AFL is significantly more advantageous compared to six state-of-the-art approaches, outperforming the best by 12.2%, 1.9% and 3.4% in terms of social welfare, revenue and accuracy, respectively.",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/474"
    },
    {
        "id": 26292,
        "title": "Learning Sparse Representations in Reinforcement Learning with Sparse Coding",
        "authors": "Lei Le, Raksha Kumaraswamy, Martha White",
        "published": "2017-8",
        "citations": 6,
        "abstract": "A variety of representation learning approaches have been investigated for reinforcement learning; much less attention, however, has been given to investigating the utility of sparse coding. Outside of reinforcement learning, sparse coding representations have been widely used, with non-convex objectives that result in discriminative representations. In this work, we develop a supervised sparse coding objective for policy evaluation. Despite the non-convexity of this objective, we prove that all local minima are global minima, making the approach amenable to simple optimization strategies. We empirically show that it is key to use a supervised objective, rather than the more straightforward unsupervised sparse coding approach. We then compare the learned representations to a canonical fixed sparse representation, called tile-coding, demonstrating that the sparse coding representation outperforms a wide variety of tile-coding representations.",
        "link": "http://dx.doi.org/10.24963/ijcai.2017/287"
    },
    {
        "id": 26293,
        "title": "Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning",
        "authors": "James Supancic, Deva Ramanan",
        "published": "2017-10",
        "citations": 71,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2017.43"
    },
    {
        "id": 26294,
        "title": "Q-learning based Reinforcement Learning Approach for Lane Keeping",
        "authors": "Arpad Feher, Szilard Aradi, Tamas Becsi",
        "published": "2018-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cinti.2018.8928230"
    },
    {
        "id": 26295,
        "title": "Balancing Similarity-Contrast in Unsupervised Representation Learning: Evaluation with Reinforcement Learning",
        "authors": "Menore Tekeba Mengistu, Getachew Alemu, Pierre Chevaillier, Pierre De Loor",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00273"
    },
    {
        "id": 26296,
        "title": "Attention Guided Imitation Learning and Reinforcement Learning",
        "authors": "Ruohan Zhang",
        "published": "2019-7-17",
        "citations": 1,
        "abstract": "\r\n\r\n\r\nWe propose a framework that uses learned human visual attention model to guide the learning process of an imitation learning or reinforcement learning agent. We have collected high-quality human action and eye-tracking data while playing Atari games in a carefully controlled experimental setting. We have shown that incorporating a learned human gaze model into deep imitation learning yields promising results.\r\n\r\n\r\n",
        "link": "http://dx.doi.org/10.1609/aaai.v33i01.33019906"
    },
    {
        "id": 26297,
        "title": "FARANE-Q: Fast Parallel and Pipeline Q-Learning Accelerator for Configurable Reinforcement Learning SoC",
        "authors": "Nana Sutisna, Andi M. R. Ilmy, Infall Syafalni, Rahmat Mulyawan, Trio Adiono",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper proposes a FAst paRAllel and pipeliNE Q-learning accelerator (FARANE-Q) for a configurable Reinforcement Learning (RL) algorithm implemented in a System on Chip (SoC). The proposed work offers flexibility, configurability, and scalability while maintaining computation speed and accuracy to overcome the challenges of a dynamic environment and increasing complexity. The proposed method includes a Hardware/Software (HW/SW) design methodology for the SoC architecture to achieve flexibility. We also propose joint optimizations on the algorithm, architecture, and implementation to obtain optimum (high efficiency) performance, specifically in energy and area efficiency. Furthermore, we implemented the proposed design in a real-time Zynq Ultra96-V2 FPGA platform to evaluate the functionality with an actual use case of smart navigation. Experimental results confirm that the proposed accelerator FARANE-Q outperforms state-of-the-art works by achieving a throughput of up to 148.55 MSps. It corresponds to the energy efficiency of 1747.64 MSps/W per agent for 32-bit and 2424.33 MSps/W per agent for 16-bit FARANE-Q. Moreover, the proposed 16-bit FARANE-Q outperforms other related works by an improvement of at least 1.23× in energy efficiency. The designed system also maintains an error accuracy of less than 0.4% with optimized bit precision for more than eight fraction bits. The proposed FARANE-Q also offers a speed up of processing time up to 1795× compared to embedded SW computation executed on ARM Zynq processor and 280× of computation of full software executed on i7 processor. Hence, the proposed work has the potential to be used for smart navigation, robotic control, and predictive maintenance.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21435606.v2"
    },
    {
        "id": 26298,
        "title": "FARANE-Q: Fast Parallel and Pipeline Q-Learning Accelerator for Configurable Reinforcement Learning SoC",
        "authors": "Nana Sutisna, Andi M. R. Ilmy, Infall Syafalni, Rahmat Mulyawan, Trio Adiono",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper proposes a FAst paRAllel and pipeliNE Q-learning accelerator (FARANE-Q) for a configurable Reinforcement Learning (RL) algorithm implemented in a System on Chip (SoC). The proposed work offers flexibility, configurability, and scalability while maintaining computation speed and accuracy to overcome the challenges of a dynamic environment and increasing complexity. The proposed method includes a Hardware/Software (HW/SW) design methodology for the SoC architecture to achieve flexibility. We also propose joint optimizations on the algorithm, architecture, and implementation to obtain optimum (high efficiency) performance, specifically in energy and area efficiency. Furthermore, we implemented the proposed design in a real-time Zynq Ultra96-V2 FPGA platform to evaluate the functionality with an actual use case of smart navigation. Experimental results confirm that the proposed accelerator FARANE-Q outperforms state-of-the-art works by achieving a throughput of up to 148.55 MSps. It corresponds to the energy efficiency of 1747.64 MSps/W per agent for 32-bit and 2424.33 MSps/W per agent for 16-bit FARANE-Q. Moreover, the proposed 16-bit FARANE-Q outperforms other related works by an improvement of at least 1.23× in energy efficiency. The designed system also maintains an error accuracy of less than 0.4% with optimized bit precision for more than eight fraction bits. The proposed FARANE-Q also offers a speed up of processing time up to 1795× compared to embedded SW computation executed on ARM Zynq processor and 280× of computation of full software executed on i7 processor. Hence, the proposed work has the potential to be used for smart navigation, robotic control, and predictive maintenance.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21435606"
    },
    {
        "id": 26299,
        "title": "Learning Locomotion Skills via Model-based Proximal Meta-Reinforcement Learning",
        "authors": "Qing Xiao, Zhengcai Cao, Mengchu Zhou",
        "published": "2019-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc.2019.8914406"
    },
    {
        "id": 26300,
        "title": "Pedagogic Transformation: Blending of Reinforcement and Inquiry Learning in Innovative Science as Resilience Technique",
        "authors": "Adiat Odunmbaku, Moriliat Jumoke Afolabi",
        "published": "2022-9",
        "citations": 0,
        "abstract": "In this paper, blending reinforcement and inquiry learning in innovative science (technological approaches) as resilience technique was presented. The study was designed to determine the effectiveness of these innovative approaches in improving students’ performance. The views about reinforcement and inquiry methods were used as data collection instrument. To this end two research questions were raised and two research hypotheses were formulated to answer the research questions. The study adopted the quasi-experimental design of pre-test post-test non-equivalent control groups. Sixty 100Level undergraduate students of National Open University of Nigeria (NOUN) were used. After four weeks of online facilitation of quantitative and qualitative techniques, acid-base reaction and determination of radicals, a-20 item questions which was well validated and had reliability coefficient of 0.80 were administered to the students before and after application of the teaching strategies on the students. After that the students were reinforced and tested with the same questions. Mean, Standard Deviation and t.test were the statistical tools used to analysis the data. The result of the study shows that students with the blend of guided inquiry and reinforcement learning performed significantly well than students with only guided inquiry learning. In addition, the types of degree (BSc/BSc. (Ed.)) has significant influence on students’ performance in favour of BSc. Students. Based on the findings of this study, it was recommended among others that university lecturers should be encouraged to blend guided inquiry with reinforcement as a teaching method in delivery of their lessons especially in the teaching of some concepts in chemistry.",
        "link": "http://dx.doi.org/10.56059/pcf10.1509"
    },
    {
        "id": 26301,
        "title": "Q-learning based Reinforcement Learning Approach for Lane Keeping",
        "authors": "Arpad Feher, Szilard Aradi, Tamas Becsi",
        "published": "2018-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cinti.2018.8928230"
    },
    {
        "id": 26302,
        "title": "Reinforcement Learning",
        "authors": "Xiangming Zeng, Liangqu Long",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-7915-1_14"
    },
    {
        "id": 26303,
        "title": "Cooperation Pattern Exploration for Multi-Agent Reinforcement Learning",
        "authors": "Chenran Zhao, Kecheng Peng, Xiaoqun Cao",
        "published": "2022-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlccim55934.2022.00077"
    },
    {
        "id": 26304,
        "title": "Reinforcement Learning Algorithms for Uncertain, Dynamic, Zero-Sum Games",
        "authors": "Snehasis Mukhopadhyay, Omkar Tilak, Subir Chakrabarti",
        "published": "2018-12",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00015"
    },
    {
        "id": 26305,
        "title": "Robustly Learning Composable Options in Deep Reinforcement Learning",
        "authors": "Akhil Bagaria, Jason Senthil, Matthew Slivinski, George Konidaris",
        "published": "2021-8",
        "citations": 5,
        "abstract": "Hierarchical reinforcement learning (HRL) is only effective for long-horizon problems  when high-level skills can be reliably sequentially executed. Unfortunately, learning reliably composable skills is difficult, because all the components of every skill are constantly changing during learning. We propose three methods for improving the composability of learned skills: representing skill initiation regions using a combination of pessimistic and optimistic classifiers; learning re-targetable policies that are robust to non-stationary subgoal regions; and learning robust option policies using model-based RL. We test these improvements on four sparse-reward maze navigation tasks involving a simulated quadrupedal robot. Each method successively improves the robustness of a baseline skill discovery method, substantially outperforming state-of-the-art flat and hierarchical methods.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/298"
    },
    {
        "id": 26306,
        "title": "Robo-Advising: Enhancing Investment with Inverse Optimization and Deep Reinforcement Learning",
        "authors": "Haoran Wang, Shi Yu",
        "published": "2021-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00063"
    },
    {
        "id": 26307,
        "title": "Optimizing Hadoop parameter for speedup using Q-Learning Reinforcement Learning",
        "authors": "Nandita Yambem, A. N. Nandakumar",
        "published": "2021-9-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecct52121.2021.9616965"
    },
    {
        "id": 26308,
        "title": "Reinforcement and deep reinforcement learning-based solutions for machine maintenance planning, scheduling policies, and optimization",
        "authors": "Oluwaseyi Ogunfowora, Homayoun Najjaran",
        "published": "2023-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jmsy.2023.07.014"
    },
    {
        "id": 26309,
        "title": "Robotic Manipulation with Reinforcement Learning, State Representation Learning, and Imitation Learning (Student Abstract)",
        "authors": "Hanxiao Chen",
        "published": "2021-5-18",
        "citations": 4,
        "abstract": "Humans possess the advanced ability to grab, hold, and manipulate objects with dexterous hands. What about robots? Can they interact with the surrounding world intelligently to achieve certain goals (e.g., grasping, object-relocation)? Actually, robotic manipulation is central to achieving the premise of robotics and represents immense potential to be widely applied in various scenarios like industries, hospitals, and homes. In this work, we aim to address multiple robotic manipulation tasks like grasping, button-pushing, and door-opening with reinforcement learning (RL), state representation learning (SRL), and imitation learning. For diverse missions, we self-built the PyBullet or MuJoCo simulated environments and independently explored three different learning-style methods to successfully solve such tasks: (1) Normal reinforcement learning methods; (2) Combined state representation learning (SRL) and RL approaches; (3) Imitation learning bootstrapped RL algorithms.",
        "link": "http://dx.doi.org/10.1609/aaai.v35i18.17881"
    },
    {
        "id": 26310,
        "title": "A Method of Deep Reinforcement Learning for Simulation of Autonomous Vehicle Control",
        "authors": "Anh Huynh, Ba-Tung Nguyen, Hoai-Thu Nguyen, Sang Vu, Hien Nguyen",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010478903720379"
    },
    {
        "id": 26311,
        "title": "Steuerung eines autonomen Fahrzeugs durch Deep Reinforcement Learning",
        "authors": "Andreas Folkers",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-28886-0"
    },
    {
        "id": 26312,
        "title": "Time cell encoding in deep reinforcement learning agents depends on mnemonic demands",
        "authors": "Dongyan Lin, Blake A. Richards",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractThe representation of “what happened when” is central to encoding episodic and working memories. Recently discovered hippocampal time cells are theorized to provide the neural substrate for such representations by forming distinct sequences that both encode time elapsed and sensory content. However, little work has directly addressed to what extent cognitive demands and temporal structure of experimental tasks affect the emergence and informativeness of these temporal representations. Here, we trained deep reinforcement learning (DRL) agents on a simulated trial-unique nonmatch-to-location (TUNL) task, and analyzed the activities of artificial recurrent units using neuroscience-based methods. We show that, after training, representations resembling both time cells and ramping cells (whose activity increases or decreases monotonically over time) simultaneously emerged in the same population of recurrent units. Furthermore, with simulated variations of the TUNL task that controlled for (1) memory demands during the delay period and (2) the temporal structure of the episodes, we show that memory demands are necessary for the time cells to encode information about the sensory stimuli, while the temporal structure of the task only affected the encoding of “what” and “when” by time cells minimally. Our findings help to reconcile current discrepancies regarding the involvement of time cells in memory-encoding by providing a normative framework. Our modelling results also provide concrete experimental predictions for future studies.",
        "link": "http://dx.doi.org/10.1101/2021.07.15.452557"
    },
    {
        "id": 26313,
        "title": "A deep reinforcement learning model based on deterministic policy gradient for collective neural crest cell migration",
        "authors": "George Lykotrafitis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.5f5f8e69aa777f8ba5bd6177"
    },
    {
        "id": 26314,
        "title": "Autonomous reinforcement learning agents for improving predictions and observations of extreme climate events",
        "authors": "André Goncalves, Donald Lucas",
        "published": "2021-4-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1769680"
    },
    {
        "id": 26315,
        "title": "Automatic Ultrasound Guidance Based on Deep Reinforcement Learning",
        "authors": "Piotr Jarosik, Marcin Lewandowski",
        "published": "2019-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ultsym.2019.8926041"
    },
    {
        "id": 26316,
        "title": "The computational relationship between reinforcement learning, social inference, and paranoia",
        "authors": "Joseph M Barnby, Mitul Mehta, Michael Moutoussis",
        "published": "No Date",
        "citations": 1,
        "abstract": "Theoretical accounts suggest heightened uncertainty about the state of the world underpins aberrant belief updates, which in turn increase the risk of developing a persecutory delusion. However, this raises the question as to how an agent’s uncertainty may relate to the precise phenomenology of paranoia, as opposed to other qualitatively different forms of belief. We tested whether the same population (n=693) responded similarly to non-social and social contingency changes in a probabilistic reversal learning task and a modified repeated reversal Dictator game, and the impact of paranoia on both. We fitted computational models that included closely related parameters that quantified the rigidity across contingency reversals and the uncertainty about the environment/partner. Consistent with prior work we show that paranoia was associated with uncertainty around a partner’s behavioural policy and rigidity in harmful intent attributions in the social task. In the non-social task we found that pre-existing paranoia was associated with larger decision temperatures and commitment to suboptimal cards. We show relationships between decision temperature in the non-social task and priors over harmful intent attributions and uncertainty over beliefs about partners in the social task. Our results converge across both classes of model, suggesting paranoia is associated with a general uncertainty over the state of the world (and agents within it) that takes longer to resolve, although we demonstrate that this uncertainty is expressed asymmetrically in social contexts. Our model and data allow the representation of sociocognitive mechanisms that explain persecutory delusions and provide testable, phenomenologically relevant predictions for causal experiments.",
        "link": "http://dx.doi.org/10.31234/osf.io/x4d3f"
    },
    {
        "id": 26317,
        "title": "Constraint-Based Multi-Agent Reinforcement Learning for Collaborative Tasks",
        "authors": "xiumin shang, Tengyu Xu, Ioannis Karamouzas, Marcelo Kallmann",
        "published": "No Date",
        "citations": 0,
        "abstract": "In order to be successfully executed, collaborative tasks performed by\ntwo agents often require a cooperative strategy to be learned. In this\nwork, we propose a constraint-based multi-agent reinforcement learning\napproach called Constrained Multi-agent Soft Actor Critic (C-MSAC) to\ntrain control policies for simulated agents performing collaborative\nmulti-phase tasks. Given a task with n phases, the first\nn-1 phases are treated as constraints for the final task phase\nobjective, which is addressed with a centralized training and\ndecentralized execution approach. We highlight our framework on a tray\nbalancing task including two phases: tray lifting and cooperative tray\ncontrol for target following. We evaluate our proposed approach and\ncompare it against its unconstrained variant (MSAC). The performed\ncomparisons show that C-MSAC leads to higher success rates, more robust\ncontrol policies, and better generalization performance.",
        "link": "http://dx.doi.org/10.22541/au.168301658.80453554/v1"
    },
    {
        "id": 26318,
        "title": "Reward processing and reinforcement learning: from adolescence to aging",
        "authors": "Jo Cutler, Matthew A J Apps, Patricia Lockwood",
        "published": "No Date",
        "citations": 1,
        "abstract": "The neurocognitive systems that underlie the ability to process rewards and learn from reinforcement undergo substantial changes across the adult lifespan. Adolescence is often characterized as a developmental period with a heightened sensitivity to reward and healthy aging is typically associated with a decline in learning from reinforcement. In this Chapter we review how the psychological and neural mechanisms that underpin reward processing and reinforcement learning change from adolescence to older adulthood. We consider behavioral and neuroimaging studies, as well as how different reward and learning contexts, such as gain vs. loss and social vs. non-social information, may alter reward processing and reinforcement learning abilities. We end by considering the challenges and opportunities of conducting developmental and aging studies in computational neuroscience and suggest future directions for the field.",
        "link": "http://dx.doi.org/10.31234/osf.io/pnuk8"
    },
    {
        "id": 26319,
        "title": "Hindsight Reward Shaping in Deep Reinforcement Learning",
        "authors": "Byron de Villiers, Deon Sabatta",
        "published": "2020-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/saupec/robmech/prasa48453.2020.9041058"
    },
    {
        "id": 26320,
        "title": "Order-Book Trading Algorithms",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-10"
    },
    {
        "id": 26321,
        "title": "Reinforcement learning with adaptive networks",
        "authors": "Tomoki Sasaki, Satoshi Yamada",
        "published": "2017-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icras.2017.8071905"
    },
    {
        "id": 26322,
        "title": "Reinforcement learning for the face support pressure of tunnel boring machines",
        "authors": "Enrico Soranzo, Carlotta Guardiani, Wei Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "In tunnel excavation with boring machines, the tunnel face is supported to avoid collapse and minimise settlement. This article proposes the use of reinforcement learning, specifically the Deep Q-Network algorithm, to predict the face support pressure. The approach is tested both analytically and numerically. By using the soil properties ahead of the tunnel face and the overburden depth as the input, the algorithm is capable of predicting the optimal tunnel face support pressure, adapting to changes in geological and geometrical conditions.",
        "link": "http://dx.doi.org/10.20944/preprints202302.0236.v1"
    },
    {
        "id": 26323,
        "title": "Energy Efficient Multi-dimensional Trajectory of UAV-aided IoT Networks with Reinforcement Learning",
        "authors": "SILVIRIANTI SILVIRIANTI, Soo Young Shin",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper proposed a multi-dimensional search space (or directional space) which has larger degree-of-freedom (DOF) to improve energy efficiency of limited-battery-powered UAV in Internet of Things (IoT) data collection scenario. In this paper, the UAV navigates from initial to goal point while collecting data from IoT sensors on the ground. Due to limited battery-power of UAV, optimized trajectory becomes a crucial practical issue. Based on the available directional space, direction of UAV which related to navigation trajectory is optimized using reinforcement learning. The objective of this reinforcement learning is to maximize energy efficiency of UAV as a long-term reward by selecting optimized direction. Moreover, practical energy consumption model and environment are presented in this paper. The simulation results verify the proposed scheme that has larger directional space achieved higher energy efficiency compared to benchmark models.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19028864"
    },
    {
        "id": 26324,
        "title": "Conditioned Reinforcement",
        "authors": "W. David Pierce, Carl D. Cheney",
        "published": "2017-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315200682-10"
    },
    {
        "id": 26325,
        "title": "Simulationsbasiertes Deep Reinforcement Learning für Modulare Produktionssysteme",
        "authors": "Niclas Feldkamp, Sören Bergmann, Steffen Straßburger",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.11128/arep.20.a2007"
    },
    {
        "id": 26326,
        "title": "Co-Evolutionary Traffic Signal Control Using Reinforcement Learning for Road Networks Under Stochastic Capacity",
        "authors": "Suh-Wen Chiou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4327149"
    },
    {
        "id": 26327,
        "title": "Reinforcement learning for constructing low density sign representations of Boolean functions",
        "authors": "Oytun Yapar, Erhan Oztop",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2022.es2022-25"
    },
    {
        "id": 26328,
        "title": "Name that state: How language affects human reinforcement learning",
        "authors": "Angela Radulescu, Wai Keen Vong, Todd Matthew Gureckis",
        "published": "No Date",
        "citations": 1,
        "abstract": "We describe two experiments designed to test whether the ease with which people can label features of the environment influences human reinforcement learning. The first experiment presents evidence that people are more efficient at learning to discern relevant features of a task when candidate features are easier to name. The second experiment shows that learning what action to take in a given state is easier when states have more readily nameable verbal labels, an effect that was especially pronounced in environments with more states. The interaction between CLIP, a state-of-the-art AI model trained to map images to natural language concepts, and established hu- man RL algorithms captures the key effects without the need to specify condition-specific parameters. These results suggest a possible role for language information in how humans represent the environment when learning from trial and error.",
        "link": "http://dx.doi.org/10.31234/osf.io/57wgr"
    },
    {
        "id": 26329,
        "title": "Dynamic Channel Access via Meta-Reinforcement Learning",
        "authors": "Ziyang Lu, M. Cenk Gursoy",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685347"
    },
    {
        "id": 26330,
        "title": "Coevolution of Cognition and Cooperation in Structured Populations Under Reinforcement Learning",
        "authors": "Rossana Mastrandrea, Ennio Bilancini, Leonardo Boncinelli",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4704384"
    },
    {
        "id": 26331,
        "title": "Reinforcement Learning From Scratch",
        "authors": "Uwe Lorenz",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-09030-1"
    },
    {
        "id": 26332,
        "title": "Iorl: Inductive-Offline-Reinforcement-Learning for Traffic Signal Control Warmstarting",
        "authors": "François-Xavier Devailly, Denis Larocque, Laurent Charlin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4385976"
    },
    {
        "id": 26333,
        "title": "Kinematic Synthesis Using Reinforcement Learning",
        "authors": "Kaz Vermeer, Reinier Kuppens, Justus Herder",
        "published": "2018-8-26",
        "citations": 6,
        "abstract": "The presented research demonstrates the synthesis of two-dimensional kinematic mechanisms using feature-based reinforcement learning. As a running example the classic challenge of designing a straight-line mechanism is adopted: a mechanism capable of tracing a straight line as part of its trajectory. This paper presents a basic framework, consisting of elements such as mechanism representations, kinematic simulations and learning algorithms, as well as some of the resulting mechanisms and a comparison to prior art. Series of successful mechanisms have been synthesized for path generation of a straight line and figure-eight.",
        "link": "http://dx.doi.org/10.1115/detc2018-85529"
    },
    {
        "id": 26334,
        "title": "Assimilating Human Feedback from Autonomous Vehicle Interaction in Reinforcement Learning Models",
        "authors": "Richard Fox, Elliot A. Ludvig",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nA significant challenge for real-world automated vehicles (AVs) is their interaction with human pedestrians. This paper develops a methodology to directly elicit the AV behaviour pedestrians find suitable by collecting quantitative data that can be used to measure and improve an algorithm's performance. Starting with a Deep Q Network (DQN) trained on a simple Pygame/Python-based pedestrian crossing environment, the reward structure was adapted to allow adjustment by human feedback. Feedback was collected by eliciting behavioural judgements collected from people in a controlled environment. The reward was shaped by the inter-action vector, decomposed into feature aspects for relevant behaviours, thereby facilitating both implicit preference selection and explicit task discovery in tandem. Using computational RL and behavioural-science techniques, we harness a formal iterative feedback loop where the rewards are repeatedly adapted based on human behavioural judgments. Experiments were conducted with 124 participants that showed strong initial improvement in the judgement of AV behaviours with the adaptive reward structure. The results indicate that the primary avenue for enhancing vehicle behaviour lies in the predictability of its movements when introduced. More broadly, recognising AV behaviours that receive favourable human judgments can pave the way for enhanced performance.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3405901/v1"
    },
    {
        "id": 26335,
        "title": "Deep Reinforcement Learning for NLP",
        "authors": "William Yang Wang, Jiwei Li, Xiaodong He",
        "published": "2018",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p18-5007"
    },
    {
        "id": 26336,
        "title": "Reinforcement Learning approaches to hippocampus-dependent flexible spatial navigation",
        "authors": "Charline Tessereau, Reuben O’Dea, Stephen Coombes, Tobias Bast",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractHumans and non-human animals show great flexibility in spatial navigation, including the ability to return to specific locations based on as few as one single experience. To study spatial navigation in the laboratory, watermaze tasks, in which rats have to find a hidden platform in a pool of cloudy water surrounded by spatial cues, have long been used. Analogous tasks have been developed for human participants using virtual environments. Spatial learning in the watermaze is facilitated by the hippocampus. In particular, rapid, one-trial, allocentric place learning, as measured in the Delayed-Matching-to-Place (DMP) variant of the watermaze task, which requires rodents to learn repeatedly new locations in a familiar environment, is hippocampal dependent. In this article, we review some computational principles, embedded within a Reinforcement Learning (RL) framework, that utilise hippocampal spatial representations for navigation in watermaze tasks. We consider which key elements underlie their efficacy, and discuss their limitations in accounting for hippocampus-dependent navigation, both in terms of behavioural performance (i.e., how well do they reproduce behavioural measures of rapid place learning) and neurobiological realism (i.e., how well do they map to neurobiological substrates involved in rapid place learning). We discuss how an actor-critic architecture, enabling simultaneous assessment of the value of the current location and of the optimal direction to follow, can reproduce one-trial place learning performance as shown on watermaze and virtual DMP tasks by rats and humans, respectively, if complemented with map-like place representations. The contribution of actor-critic mechanisms to DMP performance is consistent with neurobiological findings implicating the striatum and hippocampo-striatal interaction in DMP performance, given that the striatum has been associated with actor-critic mechanisms. Moreover, we illustrate that hierarchical computations embedded within an actor-critic architecture may help to account for aspects of flexible spatial navigation. The hierarchical RL approach separates trajectory control via a temporal-difference error from goal selection via a goal prediction error and may account for flexible, trial-specific, navigation to familiar goal locations, as required in some arm-maze place memory tasks, although it does not capture one-trial learning of new goal locations, as observed in open field, including watermaze and virtual, DMP tasks. Future models of one-shot learning of new goal locations, as observed on DMP tasks, should incorporate hippocampal plasticity mechanisms that integrate new goal information with allocentric place representation, as such mechanisms are supported by substantial empirical evidence.",
        "link": "http://dx.doi.org/10.1101/2020.07.30.229005"
    },
    {
        "id": 26337,
        "title": "Decision letter for \"Discovering mechanisms for materials microstructure optimization via reinforcement learning of a generative model\"",
        "authors": "",
        "published": "2022-7-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/aca004/v1/decision1"
    },
    {
        "id": 26338,
        "title": "The functional form of value normalization in human reinforcement learning",
        "authors": "Sophie Bavard, Stefano Palminteri",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractReinforcement learning research in humans and other species indicates that rewards are represented in a context-dependent manner. More specifically, reward representations seem to be normalized as a function of the value of the alternative options. The dominant view postulates that value context-dependence is achieved via a divisive normalization rule, inspired by perceptual decision-making research. However, behavioral and neural evidence points to another plausible mechanism: range normalization. Critically, previous experimental designs were ill-suited to disentangle the divisive and the range normalization accounts, which generate similar behavioral predictions in many circumstances. To address this question, we designed a new learning task where we manipulated, across learning contexts, the number of options and the value ranges. Behavioral and computational analyses falsify the divisive normalization account and rather provide support for the range normalization rule. Together, these results shed new light on the computational mechanisms underlying context-dependence in learning and decision-making.",
        "link": "http://dx.doi.org/10.1101/2022.07.14.500032"
    },
    {
        "id": 26339,
        "title": "Reinforcement learning derived chemotherapeutic schedules for robust patient-specific therapy",
        "authors": "Brydon Eastman, Michelle Przedborski, Mohammad Kohandel",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractThe in-silico development of a chemotherapeutic dosing schedule for treating cancer relies upon a parameterization of a particular tumour growth model to describe the dynamics of the cancer in response to the dose of the drug. In practice, it is often prohibitively difficult to ensure the validity of patient-specific parameterizations of these models for any particular patient. As a result, sensitivities to these particular parameters can result in therapeutic dosing schedules that are optimal in principle not performing well on particular patients. In this study, we demonstrate that chemotherapeutic dosing strategies learned via reinforcement learning methods are more robust to perturbations in patient-specific parameter values than those learned via classical optimal control methods. By training a reinforcement learning agent on mean-value parameters and allowing the agent periodic access to a more easily measurable metric, relative bone marrow density, for the purpose of optimizing dose schedule while reducing drug toxicity, we are able to develop drug dosing schedules that outperform schedules learned via classical optimal control methods, even when such methods are allowed to leverage the same bone marrow measurements.",
        "link": "http://dx.doi.org/10.1101/2021.04.23.441182"
    },
    {
        "id": 26340,
        "title": "Memory-Assisted Reinforcement Learning for Diverse Molecular De Novo Design",
        "authors": "Thomas Blaschke, Ola Engkvist, Jürgen Bajorath, Hongming Chen",
        "published": "No Date",
        "citations": 1,
        "abstract": "<div><div><div><p>In de novo molecular design, recurrent neural networks (RNN) have been shown to be effective methods for sampling and generating novel chemical structures. Using a technique called reinforcement learning (RL), an RNN can be tuned to target a particular section of chemical space with optimized desirable properties using a scoring function. However, ligands generated by current RL methods so far tend to have relatively low diversity, and sometimes even result in duplicate structures when optimizing towards particular properties. Here, we propose a new method to address the low diversity issue in RL. Memory-assisted RL is an extension of the known RL, with the introduction of a so-called memory unit.</p></div></div></div>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.12693152"
    },
    {
        "id": 26341,
        "title": "Ambulance Redeployment via Reinforcement Learning",
        "authors": "Ümitcan Şahin, Veysel YÜcesoy",
        "published": "2020-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu49456.2020.9302230"
    },
    {
        "id": 26342,
        "title": "Reviewer #3 (Public Review): Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-8-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.2.sa3"
    },
    {
        "id": 26343,
        "title": "eLife assessment: Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Claire Gillan",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.3.sa0"
    },
    {
        "id": 26344,
        "title": "eLife assessment: Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Claire Gillan",
        "published": "2023-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.1.sa0"
    },
    {
        "id": 26345,
        "title": "Reinforcement Learning for Topic Models",
        "authors": "Jeremy Costello, Marek Reformat",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.265"
    },
    {
        "id": 26346,
        "title": "Learning key steps to attack deep reinforcement learning agents",
        "authors": "Chien-Min Yu, Ming-Hsin Chen, Hsuan-Tien Lin",
        "published": "2023-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-023-06318-9"
    },
    {
        "id": 26347,
        "title": "Proposal model for e-learning based on Case Based Reasoning and Reinforcement Learning",
        "authors": "Anibal Flores, Luis Alfaro, Jose Herrera",
        "published": "2019-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/edunine.2019.8875800"
    },
    {
        "id": 26348,
        "title": "Virtual Network Embedding via Hierarchical Reinforcement Learning<sup>1</sup>",
        "authors": "",
        "published": "2022-12-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119835905.ch4"
    },
    {
        "id": 26349,
        "title": "A Deep Reinforcement Learning Approach to Learning Tree Edit Policies on Binary Trees",
        "authors": "Shirin Shirvani, Manfred Huber",
        "published": "2021-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc52423.2021.9658790"
    },
    {
        "id": 26350,
        "title": "Deep Reinforcement Learning using Cyclical Learning Rates",
        "authors": "Ralf Gulde, Marc Tuscher, Akos Csiszar, Oliver Riedel, Alexander Verl",
        "published": "2020-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ai4i49448.2020.00014"
    },
    {
        "id": 26351,
        "title": "Reinforcement Learning for Estimating Student Proficiency in Math Word Problems",
        "authors": "Jesus Perez, Eladio Dapena, Jose Aguilar, Gilberto Carrillo",
        "published": "2022-10-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/laclo56648.2022.10013399"
    },
    {
        "id": 26352,
        "title": "Learning Generalizable Locomotion Skills with Hierarchical Reinforcement Learning",
        "authors": "Tianyu Li, Nathan Lambert, Roberto Calandra, Franziska Meier, Akshara Rai",
        "published": "2020-5",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra40945.2020.9196642"
    },
    {
        "id": 26353,
        "title": "Learning to Solve Nonlinear Optimization Problem with Deep Reinforcement Learning",
        "authors": "Yue Gao, Qiyue Yang, Huajian Wu, Ming Sun",
        "published": "2022-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio55434.2022.10011977"
    },
    {
        "id": 26354,
        "title": "Sample-Efficient Goal-Conditioned Reinforcement Learning via Predictive Information Bottleneck for Goal Representation Learning",
        "authors": "Qiming Zou, Einoshin Suzuki",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161213"
    },
    {
        "id": 26355,
        "title": "An Alternative Curriculum Learning Approach with Macro Actions for Deep Reinforcement Learning",
        "authors": "Jiakai Song, Jianmin Ji",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cecit58139.2022.00016"
    },
    {
        "id": 26356,
        "title": "A Study of Imbalanced Dataset Classification on KDD99 Datasets with Reinforcement Learning Mechanism",
        "authors": "Chih-Chen Pan, Yungho Leu",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmlc58545.2023.10327999"
    },
    {
        "id": 26357,
        "title": "Accelerating Multiagent Reinforcement Learning through Transfer Learning",
        "authors": "Felipe Da Silva, Anna Costa",
        "published": "2017-2-12",
        "citations": 3,
        "abstract": "\n      \n        Reinforcement Learning (RL) is a widely used solution for sequential decision-making problems and has been used in many complex domains. However, RL algorithms suffer from scalability issues, especially when multiple agents are acting in a shared environment.  This research intends to accelerate learning in multiagent sequential decision-making tasks by reusing previous knowledge, both from past solutions and advising between agents.  We intend to contribute a Transfer Learning framework focused on Multiagent RL, requiring as few domain-specific hand-coded parameters as possible.\n      \n    ",
        "link": "http://dx.doi.org/10.1609/aaai.v31i1.10518"
    },
    {
        "id": 26358,
        "title": "A brief introduction to supervised, unsupervised, and reinforcement learning",
        "authors": "Eduardo F. Morales, Hugo Jair Escalante",
        "published": "2022",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-820125-1.00017-8"
    },
    {
        "id": 26359,
        "title": "From Reinforcement Learning to Deep Reinforcement Learning: An Overview",
        "authors": "Forest Agostinelli, Guillaume Hocquet, Sameer Singh, Pierre Baldi",
        "published": "2018",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-99492-5_13"
    },
    {
        "id": 26360,
        "title": "A Recommendation System Framework for Educational Content Reinforcement in Virtual Learning Environments",
        "authors": "Adson Damasceno, Lucas Carneiro, João T. De Sampaio, Allberson Dantas, Eudenia Magalhães, Paulo Maia, Francisco Oliveira",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011032000003182"
    },
    {
        "id": 26361,
        "title": "Background",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5_2"
    },
    {
        "id": 26362,
        "title": "Different computational strategies for different reinforcement learning problems",
        "authors": "Pieter Verbeke, Tom Verguts",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2022.1027-0"
    },
    {
        "id": 26363,
        "title": "Multi Scenario Financial Planning via Deep Reinforcement Learning AI",
        "authors": "Gordon Irlam",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3516480"
    },
    {
        "id": 26364,
        "title": "Coordinated Sensing Coverage with Distributed Deep Reinforcement Learning",
        "authors": "Tianwei Dai, Zhengtao Ding",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9188463"
    },
    {
        "id": 26365,
        "title": "COVID-19 vaccine incentive scheduling using an optimally controlled reinforcement learning model",
        "authors": "K. Stuckey, P.K. Newton",
        "published": "No Date",
        "citations": 0,
        "abstract": "We model Covid-19 vaccine uptake as a reinforcement learning dynamic between two populations: the vaccine adopters, and the vaccine hesitant. Using data available from the Center for Disease Control (CDC), we calculate a payoff matrix governing the dynamic interaction between these two groups and show they are playing a Hawk-Dove evolutionary game with an internal evolutionarily stable Nash equilibrium (the asymptotic percentage of vaccinated in the population). We then ask whether vaccine adoption can be improved by implementing dynamic incentive schedules that reward/punish the vaccine hesitant, and if so, what schedules are optimal and how effective are they likely to be? When is the optimal time to start an incentive program, and how large should the incentives be? By using a tailored replicator dynamic reinforcement learning model together with optimal control theory, we show that well designed and timed incentive programs can improve vaccine uptake by shifting the Nash equilibrium upward in large populations, but only so much, and incentive sizes above a certain threshold show diminishing returns.",
        "link": "http://dx.doi.org/10.1101/2022.02.17.22271145"
    },
    {
        "id": 26366,
        "title": "Converging evidence linking reinforcement learning deficits in schizophrenia to impairments in the representation of expected value",
        "authors": "Dennis Hernaus",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.5b681763b56e9b005965c0c5"
    },
    {
        "id": 26367,
        "title": "Optimization of Agricultural Management for Soil Carbon Sequestration UsingDeep Reinforcement Learning and Large-Scale Simulations",
        "authors": "Zahra Kalantari",
        "published": "No Date",
        "citations": 0,
        "abstract": "&lt;p&gt;Soil carbon sequestration in croplands has tremendous potential to help mitigate climate&amp;#160;change; however, it is challenging to develop the optimal management practices for maximization&amp;#160;of the sequestered carbon as well as the crop yield. We aim to develop an intelligent&amp;#160;agricultural management system using deep reinforcement learning (RL) and large-scale&amp;#160;soil and crop simulations. To achieve this, we build a simulator to model and simulate&amp;#160;the complex soil-water-plant-atmosphere interaction, which will run on high-performance computing&amp;#160;platforms. Massive simulations using such platforms allow the evaluation of the effects of&amp;#160;various management practices under different weather and soil conditions in a timely and cost effective&amp;#160;manner. By formulating the management decision as an RL problem, we can leverage&amp;#160;the state-of-the-art algorithms to train management policies through extensive interactions with&amp;#160;the simulated environment. The trained policies are expected to maximize the stored organic carbon&amp;#160;while maximizing the crop yield in the presence of uncertain weather conditions. The whole&amp;#160;system is tested using data of soil and crops in both mid-west of the United States and&amp;#160;the central region of Portugal. Our study has great potential for impact on climate&amp;#160;change and food security, two of the most significant challenges currently facing humanity.&lt;/p&gt;",
        "link": "http://dx.doi.org/10.5194/egusphere-egu22-13307"
    },
    {
        "id": 26368,
        "title": "Open Art: Methods for Curing Hypothyroidism using Reinforcement Learning",
        "authors": "Brian Haney",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3693990"
    },
    {
        "id": 26369,
        "title": "Reinforcement Learning for CVA hedging",
        "authors": "Miquel Noguer i Alonso, Ivan Zhdankin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4313177"
    },
    {
        "id": 26370,
        "title": "Conclusion",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1_10"
    },
    {
        "id": 26371,
        "title": "Reinforcement Learning",
        "authors": "Ralf Hollstein",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-39855-2_22"
    },
    {
        "id": 26372,
        "title": "Solving Biobjective Traveling Thief Problems with Multiobjective Reinforcement Learning",
        "authors": "Gemilang Santiyuda, RETANTYO WARDOYO, Prof.  Reza M.I. Pulungan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4563688"
    },
    {
        "id": 26373,
        "title": "Coevolution of Cognition and Cooperation in Structured Populations Under Reinforcement Learning",
        "authors": "Ennio Bilancini, Leonardo Boncinelli, Rossana Mastrandrea",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4569318"
    },
    {
        "id": 26374,
        "title": "Online reinforcement learning of controller parameters adaptation law",
        "authors": "Khalid Alhazmi, S. Mani Sarathy",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156644"
    },
    {
        "id": 26375,
        "title": "Approaches to Reinforcement Learning for Describing Perishable Inventory Systems Ordering Policies",
        "authors": "Osama Butt, Mustafa Mehmood",
        "published": "No Date",
        "citations": 0,
        "abstract": "Approaches to Reinforcement Learning for Describing Perishable Inventory Systems Ordering Policies",
        "link": "http://dx.doi.org/10.31219/osf.io/jv8tg"
    },
    {
        "id": 26376,
        "title": "Depth-Based Reinforcement Learning Algorithm",
        "authors": "Xiaohui Yi",
        "published": "2022-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiotcs58181.2022.00111"
    },
    {
        "id": 26377,
        "title": "Peer Review #2 of \"Quantifying the impact of non-stationarity in reinforcement learning-based traffic signal control (v0.3)\"",
        "authors": "",
        "published": "2021-5-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.575v0.3/reviews/2"
    },
    {
        "id": 26378,
        "title": "Reviewer #2 (Public Review): Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.3.sa2"
    },
    {
        "id": 26379,
        "title": "Reviewer #1 (Public Review):: Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-11-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.4.sa1"
    },
    {
        "id": 26380,
        "title": "Designing Input Shapers Using Reinforcement Learning",
        "authors": "Minh Vu, Daniel Newman, Joshua Vaughan",
        "published": "2018-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2018.8431396"
    },
    {
        "id": 26381,
        "title": "Decoding Reinforcement Learning for newcomers",
        "authors": "Francisco Neves, Matheus F. Reis, Gustavo Andrade, A. Pedro Aguiar, Andry Maykol Pinto",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>An intelligible step-by-step Reinforcement Learning (RL) problem formulation and the availability of an easy-to-use demonstrative toolbox for students at various levels (e.g., undergraduate, bachelor, master, doctorate), researchers and educators. This tool facilitates the familiarization with the key concepts of RL, its problem formulation and implementation. The results demonstrated in this paper are produced by a Python program that is released open-source, along with other lecture materials to reduce the learning barriers in such innovative research topic in robotics.</p>\n<p>The RL paradigm is showing promising results as a generic purpose framework for solving decision-making problems (e.g., robotics, games, finance). In this work, RL is used for solving a robotics 2D navigational problem where the robot needs to avoid collisions with obstacles while aiming to reach a goal point. A navigational problem is simple and convenient for educational purposes, since the outcome is unambiguous (e.g., the goal is reached or not, a collision happened or not). Thus, the intent is to accelerate the adoption of RL techniques in the field of mobile robotics.</p>\n<p>Motivate and promote the adoption of RL techniques to solve decision-making problems, specifically in robotics. </p>\n<p>Due to a lack of accessible educational and demonstrative toolboxes concerning the field of RL, this work combines theoretical exposition with an accessible open-source graphical interactive toolbox to facilitate the apprehension.</p>\n<p>This study aims to reduce the learning barriers and inspire young students, researchers and educators to use RL as an obvious tool to solve robotics problems.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21583893"
    },
    {
        "id": 26382,
        "title": "Evaluating the learning of stimulus-control associations through incidental memory of reinforcement events",
        "authors": "Christina Bejjani, Tobias Egner",
        "published": "No Date",
        "citations": 0,
        "abstract": "Cognitive control describes the ability to use internal goals to strategically guide how we process and respond to our environment. Changes in the environment lead to adaptation in control strategies. This type of control-learning can be observed in performance adjustments in response to varying proportions of easy to hard trials over blocks of trials on classic control tasks. Known as the list-wide proportion congruent (LWPC) effect, increased difficulty is met with enhanced attentional control. Recent research has shown that motivational manipulations may enhance the LWPC effect, but the underlying mechanisms are not yet understood. We manipulated Stroop proportion congruency over blocks of trials and after each trial, provided participants with either performance-contingent feedback (“correct/incorrect”) or non-contingent feedback (“response logged”) above trial-unique, task-irrelevant images (reinforcement events). The LWPC task was followed by a surprise recognition memory task, which allowed us to test whether attention to feedback (incidental memory for the images) varies as a function of proportion congruency, time, performance-contingency, and individual differences. We replicated a robust LWPC effect in a large sample (N = 402), but observed no differences in behavior between feedback groups. Importantly, the memory data revealed better encoding of feedback images from context-defining trials (e.g., congruent trials in a mostly congruent block), especially early on in a new context, and in congruent conditions. Individual differences in reward and punishment sensitivity were not strongly associated with control-learning effects. These results suggest that statistical learning of contextual demand may have a larger impact on control-learning than individual differences in motivation.",
        "link": "http://dx.doi.org/10.31234/osf.io/cdpxh"
    },
    {
        "id": 26383,
        "title": "Energy Efficient Multi-dimensional Trajectory of UAV-aided IoT Networks with Reinforcement Learning",
        "authors": "SILVIRIANTI SILVIRIANTI, Soo Young Shin",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper proposed a multi-dimensional search space (or directional space) which has larger degree-of-freedom (DOF) to improve energy efficiency of limited-battery-powered UAV in Internet of Things (IoT) data collection scenario. In this paper, the UAV navigates from initial to goal point while collecting data from IoT sensors on the ground. Due to limited battery-power of UAV, optimized trajectory becomes a crucial practical issue. Based on the available directional space, direction of UAV which related to navigation trajectory is optimized using reinforcement learning. The objective of this reinforcement learning is to maximize energy efficiency of UAV as a long-term reward by selecting optimized direction. Moreover, practical energy consumption model and environment are presented in this paper. The simulation results verify the proposed scheme that has larger directional space achieved higher energy efficiency compared to benchmark models.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19028864.v1"
    },
    {
        "id": 26384,
        "title": "Large Language Models Reasoning and Reinforcement Learning",
        "authors": "Miquel Noguer i Alonso",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4656090"
    },
    {
        "id": 26385,
        "title": "Reinforcement Learning Approach for a Cognitive Framework for Classification",
        "authors": "K. Barth, S. Brüggenwirth",
        "published": "2023-5-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2351548.2023.10149571"
    },
    {
        "id": 26386,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "Grigory Neustroev",
        "published": "2023-3-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v2/review2"
    },
    {
        "id": 26387,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "Grigory Neustroev",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v3/review2"
    },
    {
        "id": 26388,
        "title": "Solving The Lunar Lander Problem under Uncertainty using Reinforcement Learning",
        "authors": "Soham Gadgil, Yunfeng Xin, Chengzhe Xu",
        "published": "2020-3-28",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/southeastcon44009.2020.9368267"
    },
    {
        "id": 26389,
        "title": "Reinforcement Learning for Residential Heat Pump Operation",
        "authors": "Simon Schmitz, Karoline Brucke, Pranay Kasturi, Esmail Ansari, Peter Klement",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170775291.16624373/v1"
    },
    {
        "id": 26390,
        "title": "Deep Reinforcement Learning Based Resource Allocation for LoRaWAN",
        "authors": "Aohan Li",
        "published": "2022-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2022-fall57202.2022.10012698"
    },
    {
        "id": 26391,
        "title": "Optimizing Earth Moving Operations Via Reinforcement Learning",
        "authors": "Vivswan Shitole, Joseph Louis, Prasad Tadepalli",
        "published": "2019-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc40007.2019.9004935"
    },
    {
        "id": 26392,
        "title": "Reviewer #2 (Public Review): Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-8-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.2.sa2"
    },
    {
        "id": 26393,
        "title": "Reviewer #1 (Public Review): Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-8-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.2.sa1"
    },
    {
        "id": 26394,
        "title": "Reviewer #1 (Public Review): Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.3.sa1"
    },
    {
        "id": 26395,
        "title": "Reviewer #2 (Public Review):: Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-11-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.4.sa2"
    },
    {
        "id": 26396,
        "title": "EEG correlates of physical effort and reward processing during reinforcement learning",
        "authors": "Dimitrios J. Palidis, Paul L. Gribble",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractEffort-based decision making is often described by choices according to subjective value, a function of reward discounted by effort. We asked whether a neural reinforcement learning signal, the feedback related negativity (FRN), is modulated not only by reward outcomes but also physical effort. We recorded EEG from human participants while they performed a task in which they were required to accurately produce target levels of muscle activation to receive rewards. Participants performed isometric knee extensions while quadriceps muscle activation was recorded using EMG. Real-time feedback indicated muscle activation relative to a target. On a given trial, the target muscle activation required either low or high effort. The effort was determined probabilistically according to a binary choice, such that the responses were associated with 20% and 80% probability of high effort. This contingency could only be known by experience, and it reversed periodically. After each trial binary reinforcement feedback was provided to indicate whether participants were sufficiently accurate in producing the target muscle activity. Participants adaptively avoided effort by switching responses more frequently after choices that resulted in hard effort. Feedback after participants’ choices which revealed the resulting effort requirement for the subsequent knee extension did not elicit an FRN component. However, the neural response to reinforcement feedback after the knee extension was increased during and after the time period of the FRN by preceding physical effort. Thus, retrospective effort modulates reward processing which may underlie paradoxical behavioral findings whereby rewards requiring more effort to obtain can become more powerful reinforcers.Significance StatementWhen making decisions, we typically select more rewarding and less effortful options. Neural reinforcement learning signals reinforce rewarding actions and deter punishing actions. When participants received feedback that their choices would require easy or hard physical effort, we did not observe reinforcement learning signals that are typically observed in response to feedback predicting reward and punishment. Thus, the reinforcement learning system does not strictly treat effort as loss or punishment. However, when the effort was completed and participants received feedback indicating whether they successfully achieved a reward or not, reinforcement learning signals were amplified by preceding effort. Thus, retrospective effort can affect neural responses to reinforcement outcomes, which may explain how effort can enhance the motivational effect of reinforcers.",
        "link": "http://dx.doi.org/10.1101/2020.01.09.900530"
    },
    {
        "id": 26397,
        "title": "Heading Control of a Ship Based on Deep Reinforcement Learning (RL)",
        "authors": "Sivaraman Sivaraj, Suresh Rajendran",
        "published": "2022-2-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oceanschennai45887.2022.9775236"
    },
    {
        "id": 26398,
        "title": "Prefrontal solution to the bias-variance tradeoff during reinforcement learning",
        "authors": "Dongjae Kim, Jaeseung Jeong, Sang Wan Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe goal of learning is to maximize future rewards by minimizing prediction errors. Evidence have shown that the brain achieves this by combining model-based and model-free learning. However, the prediction error minimization is challenged by a bias-variance tradeoff, which imposes constraints on each strategy’s performance. We provide new theoretical insight into how this tradeoff can be resolved through the adaptive control of model-based and model-free learning. The theory predicts the baseline correction for prediction error reduces the lower bound of the bias–variance error by factoring out irreducible noise. Using a Markov decision task with context changes, we showed behavioral evidence of adaptive control. Model-based behavioral analyses show that the prediction error baseline signals context changes to improve adaptability. Critically, the neural results support this view, demonstrating multiplexed representations of prediction error baseline within the ventrolateral and ventromedial prefrontal cortex, key brain regions known to guide model-based and model-free learning.One sentence summaryA theoretical, behavioral, computational, and neural account of how the brain resolves the bias-variance tradeoff during reinforcement learning is described.",
        "link": "http://dx.doi.org/10.1101/2020.12.23.424258"
    },
    {
        "id": 26399,
        "title": "Itrpl: An Intelligent and Trusted Rpl Protocol Based on Multi-Agent Reinforcement Learning",
        "authors": "Debasmita Dey, Nirnay Ghosh",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4752236"
    },
    {
        "id": 26400,
        "title": "Reinforcement Learning and Dynamic Programming Using Function Approximators",
        "authors": "Lucian Busoniu, Robert Babuska, Bart De Schutter, Damien Ernst",
        "published": "2017-7-28",
        "citations": 321,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439821091"
    },
    {
        "id": 26401,
        "title": "Reviewer #1 (Public Review):: Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-11-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.4.sa1"
    },
    {
        "id": 26402,
        "title": "Designing Input Shapers Using Reinforcement Learning",
        "authors": "Minh Vu, Daniel Newman, Joshua Vaughan",
        "published": "2018-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2018.8431396"
    },
    {
        "id": 26403,
        "title": "Decoding Reinforcement Learning for newcomers",
        "authors": "Francisco Neves, Matheus F. Reis, Gustavo Andrade, A. Pedro Aguiar, Andry Maykol Pinto",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>An intelligible step-by-step Reinforcement Learning (RL) problem formulation and the availability of an easy-to-use demonstrative toolbox for students at various levels (e.g., undergraduate, bachelor, master, doctorate), researchers and educators. This tool facilitates the familiarization with the key concepts of RL, its problem formulation and implementation. The results demonstrated in this paper are produced by a Python program that is released open-source, along with other lecture materials to reduce the learning barriers in such innovative research topic in robotics.</p>\n<p>The RL paradigm is showing promising results as a generic purpose framework for solving decision-making problems (e.g., robotics, games, finance). In this work, RL is used for solving a robotics 2D navigational problem where the robot needs to avoid collisions with obstacles while aiming to reach a goal point. A navigational problem is simple and convenient for educational purposes, since the outcome is unambiguous (e.g., the goal is reached or not, a collision happened or not). Thus, the intent is to accelerate the adoption of RL techniques in the field of mobile robotics.</p>\n<p>Motivate and promote the adoption of RL techniques to solve decision-making problems, specifically in robotics. </p>\n<p>Due to a lack of accessible educational and demonstrative toolboxes concerning the field of RL, this work combines theoretical exposition with an accessible open-source graphical interactive toolbox to facilitate the apprehension.</p>\n<p>This study aims to reduce the learning barriers and inspire young students, researchers and educators to use RL as an obvious tool to solve robotics problems.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21583893"
    },
    {
        "id": 26404,
        "title": "Evaluating the learning of stimulus-control associations through incidental memory of reinforcement events",
        "authors": "Christina Bejjani, Tobias Egner",
        "published": "No Date",
        "citations": 0,
        "abstract": "Cognitive control describes the ability to use internal goals to strategically guide how we process and respond to our environment. Changes in the environment lead to adaptation in control strategies. This type of control-learning can be observed in performance adjustments in response to varying proportions of easy to hard trials over blocks of trials on classic control tasks. Known as the list-wide proportion congruent (LWPC) effect, increased difficulty is met with enhanced attentional control. Recent research has shown that motivational manipulations may enhance the LWPC effect, but the underlying mechanisms are not yet understood. We manipulated Stroop proportion congruency over blocks of trials and after each trial, provided participants with either performance-contingent feedback (“correct/incorrect”) or non-contingent feedback (“response logged”) above trial-unique, task-irrelevant images (reinforcement events). The LWPC task was followed by a surprise recognition memory task, which allowed us to test whether attention to feedback (incidental memory for the images) varies as a function of proportion congruency, time, performance-contingency, and individual differences. We replicated a robust LWPC effect in a large sample (N = 402), but observed no differences in behavior between feedback groups. Importantly, the memory data revealed better encoding of feedback images from context-defining trials (e.g., congruent trials in a mostly congruent block), especially early on in a new context, and in congruent conditions. Individual differences in reward and punishment sensitivity were not strongly associated with control-learning effects. These results suggest that statistical learning of contextual demand may have a larger impact on control-learning than individual differences in motivation.",
        "link": "http://dx.doi.org/10.31234/osf.io/cdpxh"
    },
    {
        "id": 26405,
        "title": "Energy Efficient Multi-dimensional Trajectory of UAV-aided IoT Networks with Reinforcement Learning",
        "authors": "SILVIRIANTI SILVIRIANTI, Soo Young Shin",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper proposed a multi-dimensional search space (or directional space) which has larger degree-of-freedom (DOF) to improve energy efficiency of limited-battery-powered UAV in Internet of Things (IoT) data collection scenario. In this paper, the UAV navigates from initial to goal point while collecting data from IoT sensors on the ground. Due to limited battery-power of UAV, optimized trajectory becomes a crucial practical issue. Based on the available directional space, direction of UAV which related to navigation trajectory is optimized using reinforcement learning. The objective of this reinforcement learning is to maximize energy efficiency of UAV as a long-term reward by selecting optimized direction. Moreover, practical energy consumption model and environment are presented in this paper. The simulation results verify the proposed scheme that has larger directional space achieved higher energy efficiency compared to benchmark models.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19028864.v1"
    },
    {
        "id": 26406,
        "title": "Large Language Models Reasoning and Reinforcement Learning",
        "authors": "Miquel Noguer i Alonso",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4656090"
    },
    {
        "id": 26407,
        "title": "Reinforcement Learning Approach for a Cognitive Framework for Classification",
        "authors": "K. Barth, S. Brüggenwirth",
        "published": "2023-5-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2351548.2023.10149571"
    },
    {
        "id": 26408,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "Grigory Neustroev",
        "published": "2023-3-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v2/review2"
    },
    {
        "id": 26409,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "Grigory Neustroev",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v3/review2"
    },
    {
        "id": 26410,
        "title": "Solving The Lunar Lander Problem under Uncertainty using Reinforcement Learning",
        "authors": "Soham Gadgil, Yunfeng Xin, Chengzhe Xu",
        "published": "2020-3-28",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/southeastcon44009.2020.9368267"
    },
    {
        "id": 26411,
        "title": "Reinforcement Learning for Residential Heat Pump Operation",
        "authors": "Simon Schmitz, Karoline Brucke, Pranay Kasturi, Esmail Ansari, Peter Klement",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170775291.16624373/v1"
    },
    {
        "id": 26412,
        "title": "Deep Reinforcement Learning Based Resource Allocation for LoRaWAN",
        "authors": "Aohan Li",
        "published": "2022-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2022-fall57202.2022.10012698"
    },
    {
        "id": 26413,
        "title": "Optimizing Earth Moving Operations Via Reinforcement Learning",
        "authors": "Vivswan Shitole, Joseph Louis, Prasad Tadepalli",
        "published": "2019-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc40007.2019.9004935"
    },
    {
        "id": 26414,
        "title": "Reviewer #2 (Public Review): Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-8-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.2.sa2"
    },
    {
        "id": 26415,
        "title": "Reviewer #1 (Public Review): Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-8-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.2.sa1"
    },
    {
        "id": 26416,
        "title": "Reviewer #1 (Public Review): Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.3.sa1"
    },
    {
        "id": 26417,
        "title": "Reviewer #2 (Public Review):: Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-11-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.4.sa2"
    },
    {
        "id": 26418,
        "title": "EEG correlates of physical effort and reward processing during reinforcement learning",
        "authors": "Dimitrios J. Palidis, Paul L. Gribble",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractEffort-based decision making is often described by choices according to subjective value, a function of reward discounted by effort. We asked whether a neural reinforcement learning signal, the feedback related negativity (FRN), is modulated not only by reward outcomes but also physical effort. We recorded EEG from human participants while they performed a task in which they were required to accurately produce target levels of muscle activation to receive rewards. Participants performed isometric knee extensions while quadriceps muscle activation was recorded using EMG. Real-time feedback indicated muscle activation relative to a target. On a given trial, the target muscle activation required either low or high effort. The effort was determined probabilistically according to a binary choice, such that the responses were associated with 20% and 80% probability of high effort. This contingency could only be known by experience, and it reversed periodically. After each trial binary reinforcement feedback was provided to indicate whether participants were sufficiently accurate in producing the target muscle activity. Participants adaptively avoided effort by switching responses more frequently after choices that resulted in hard effort. Feedback after participants’ choices which revealed the resulting effort requirement for the subsequent knee extension did not elicit an FRN component. However, the neural response to reinforcement feedback after the knee extension was increased during and after the time period of the FRN by preceding physical effort. Thus, retrospective effort modulates reward processing which may underlie paradoxical behavioral findings whereby rewards requiring more effort to obtain can become more powerful reinforcers.Significance StatementWhen making decisions, we typically select more rewarding and less effortful options. Neural reinforcement learning signals reinforce rewarding actions and deter punishing actions. When participants received feedback that their choices would require easy or hard physical effort, we did not observe reinforcement learning signals that are typically observed in response to feedback predicting reward and punishment. Thus, the reinforcement learning system does not strictly treat effort as loss or punishment. However, when the effort was completed and participants received feedback indicating whether they successfully achieved a reward or not, reinforcement learning signals were amplified by preceding effort. Thus, retrospective effort can affect neural responses to reinforcement outcomes, which may explain how effort can enhance the motivational effect of reinforcers.",
        "link": "http://dx.doi.org/10.1101/2020.01.09.900530"
    },
    {
        "id": 26419,
        "title": "Heading Control of a Ship Based on Deep Reinforcement Learning (RL)",
        "authors": "Sivaraman Sivaraj, Suresh Rajendran",
        "published": "2022-2-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oceanschennai45887.2022.9775236"
    },
    {
        "id": 26420,
        "title": "Prefrontal solution to the bias-variance tradeoff during reinforcement learning",
        "authors": "Dongjae Kim, Jaeseung Jeong, Sang Wan Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe goal of learning is to maximize future rewards by minimizing prediction errors. Evidence have shown that the brain achieves this by combining model-based and model-free learning. However, the prediction error minimization is challenged by a bias-variance tradeoff, which imposes constraints on each strategy’s performance. We provide new theoretical insight into how this tradeoff can be resolved through the adaptive control of model-based and model-free learning. The theory predicts the baseline correction for prediction error reduces the lower bound of the bias–variance error by factoring out irreducible noise. Using a Markov decision task with context changes, we showed behavioral evidence of adaptive control. Model-based behavioral analyses show that the prediction error baseline signals context changes to improve adaptability. Critically, the neural results support this view, demonstrating multiplexed representations of prediction error baseline within the ventrolateral and ventromedial prefrontal cortex, key brain regions known to guide model-based and model-free learning.One sentence summaryA theoretical, behavioral, computational, and neural account of how the brain resolves the bias-variance tradeoff during reinforcement learning is described.",
        "link": "http://dx.doi.org/10.1101/2020.12.23.424258"
    },
    {
        "id": 26421,
        "title": "Itrpl: An Intelligent and Trusted Rpl Protocol Based on Multi-Agent Reinforcement Learning",
        "authors": "Debasmita Dey, Nirnay Ghosh",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4752236"
    },
    {
        "id": 26422,
        "title": "Reinforcement Learning and Dynamic Programming Using Function Approximators",
        "authors": "Lucian Busoniu, Robert Babuska, Bart De Schutter, Damien Ernst",
        "published": "2017-7-28",
        "citations": 321,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439821091"
    },
    {
        "id": 26423,
        "title": "Enhancing the Carbon Reduction Potential in Ridesplitting Through Reinforcement Learning: A Case Study in Chengdu",
        "authors": "meiting tu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4756486"
    },
    {
        "id": 26424,
        "title": "Supplemental Material for Reinforcement Learning Modeling Reveals a Reward-History-Dependent Strategy Underlying Reversal Learning in Squirrel Monkeys",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1037/bne0000492.supp"
    },
    {
        "id": 26425,
        "title": "Information-Bottleneck-Based Behavior Representation Learning for Multi-Agent Reinforcement Learning",
        "authors": "Yue Jin, Shuangqing Wei, Jian Yuan, Xudong Zhang",
        "published": "2021-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icas49788.2021.9551171"
    },
    {
        "id": 26426,
        "title": "Reinforcement Learning",
        "authors": "Wei Qi Yan",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-61081-4_5"
    },
    {
        "id": 26427,
        "title": "Learning Options in Multiobjective Reinforcement Learning",
        "authors": "Rodrigo Bonini, Felipe Silva, Anna Costa",
        "published": "2017-2-12",
        "citations": 0,
        "abstract": "\n      \n        Reinforcement Learning (RL) is a successful technique to train autonomous agents.   However, the classical RL methods take a long time to learn how to solve tasks.  Option-based solutions can be used to accelerate learning and transfer learned behaviors across tasks by encapsulating a partial policy into an action. However, the literature report only single-agent and single-objective option-based methods, but many RL tasks, especially real-world problems, are better described through multiple objectives.  We here propose a method to learn options in Multiobjective Reinforcement Learning domains in order to accelerate learning and reuse knowledge across tasks. Our initial experiments in the Goldmine Domain show that our proposal learn useful options that accelerate learning in multiobjective domains. Our next steps are to use the learned options to transfer knowledge across tasks and evaluate this method with stochastic policies.\n      \n    ",
        "link": "http://dx.doi.org/10.1609/aaai.v31i1.11103"
    },
    {
        "id": 26428,
        "title": "Learning Path Recommendation Based on Knowledge Tracing Model and Reinforcement Learning",
        "authors": "Dejun Cai, Yuan Zhang, Bintao Dai",
        "published": "2019-12",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc47050.2019.9064104"
    },
    {
        "id": 26429,
        "title": "Sample Efficiency in Deep Reinforcement Learning based Recommender Systems with Imitation Learning",
        "authors": "Mohammad Mehdi Afsar, Trafford Crump, Behrouz Far",
        "published": "2022-5-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/594757db.6a36bb36"
    },
    {
        "id": 26430,
        "title": "Automatic Voltage Control of Differential Power Grids Based on Transfer Learning and Deep Reinforcement Learning",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2021.06320"
    },
    {
        "id": 26431,
        "title": "Optimization of Learning Cycles in Online Reinforcement Learning Systems",
        "authors": "Akira Notsu, Koji Yasuda, Seiki Ubukata, Katsuhiro Honda",
        "published": "2018-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc.2018.00597"
    },
    {
        "id": 26432,
        "title": "Temporal shift reinforcement learning",
        "authors": "Deepak George Thomas, Tichakorn Wongpiromsarn, Ali Jannesari",
        "published": "2022-4-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3517207.3526968"
    },
    {
        "id": 26433,
        "title": "RA-TSC: Learning Adaptive Traffic Signal Control Strategy via Deep Reinforcement Learning",
        "authors": "Yu Du, Wei ShangGuan, Dingchao Rong, Linguo Chai",
        "published": "2019-10",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8916967"
    },
    {
        "id": 26434,
        "title": "Learning to survive: Achieving energy neutrality in wireless sensor networks using reinforcement learning",
        "authors": "Faycal Ait Aoudia, Matthieu Gautier, Olivier Berder",
        "published": "2017-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2017.7996978"
    },
    {
        "id": 26435,
        "title": "Reinforcement of student-centered learning through social e-learning and e-assessment",
        "authors": "Abdullah M. Al-Ansi",
        "published": "2022-9-15",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s43545-022-00502-9"
    },
    {
        "id": 26436,
        "title": "A Method of Deep Reinforcement Learning for Simulation of Autonomous Vehicle Control",
        "authors": "Anh Huynh, Ba-Tung Nguyen, Hoai-Thu Nguyen, Sang Vu, Hien Nguyen",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010478903720379"
    },
    {
        "id": 26437,
        "title": "Reinforcement and deep reinforcement learning-based solutions for machine maintenance planning, scheduling policies, and optimization",
        "authors": "Oluwaseyi Ogunfowora, Homayoun Najjaran",
        "published": "2023-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jmsy.2023.07.014"
    },
    {
        "id": 26438,
        "title": "Steuerung eines autonomen Fahrzeugs durch Deep Reinforcement Learning",
        "authors": "Andreas Folkers",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-28886-0"
    },
    {
        "id": 26439,
        "title": "A deep reinforcement learning model based on deterministic policy gradient for collective neural crest cell migration",
        "authors": "George Lykotrafitis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.5f5f8e69aa777f8ba5bd6177"
    },
    {
        "id": 26440,
        "title": "Autonomous reinforcement learning agents for improving predictions and observations of extreme climate events",
        "authors": "André Goncalves, Donald Lucas",
        "published": "2021-4-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1769680"
    },
    {
        "id": 26441,
        "title": "Conditioned Reinforcement",
        "authors": "W. David Pierce, Carl D. Cheney",
        "published": "2017-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315200682-10"
    },
    {
        "id": 26442,
        "title": "Simulationsbasiertes Deep Reinforcement Learning für Modulare Produktionssysteme",
        "authors": "Niclas Feldkamp, Sören Bergmann, Steffen Straßburger",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.11128/arep.20.a2007"
    },
    {
        "id": 26443,
        "title": "The computational relationship between reinforcement learning, social inference, and paranoia",
        "authors": "Joseph M Barnby, Mitul Mehta, Michael Moutoussis",
        "published": "No Date",
        "citations": 1,
        "abstract": "Theoretical accounts suggest heightened uncertainty about the state of the world underpins aberrant belief updates, which in turn increase the risk of developing a persecutory delusion. However, this raises the question as to how an agent’s uncertainty may relate to the precise phenomenology of paranoia, as opposed to other qualitatively different forms of belief. We tested whether the same population (n=693) responded similarly to non-social and social contingency changes in a probabilistic reversal learning task and a modified repeated reversal Dictator game, and the impact of paranoia on both. We fitted computational models that included closely related parameters that quantified the rigidity across contingency reversals and the uncertainty about the environment/partner. Consistent with prior work we show that paranoia was associated with uncertainty around a partner’s behavioural policy and rigidity in harmful intent attributions in the social task. In the non-social task we found that pre-existing paranoia was associated with larger decision temperatures and commitment to suboptimal cards. We show relationships between decision temperature in the non-social task and priors over harmful intent attributions and uncertainty over beliefs about partners in the social task. Our results converge across both classes of model, suggesting paranoia is associated with a general uncertainty over the state of the world (and agents within it) that takes longer to resolve, although we demonstrate that this uncertainty is expressed asymmetrically in social contexts. Our model and data allow the representation of sociocognitive mechanisms that explain persecutory delusions and provide testable, phenomenologically relevant predictions for causal experiments.",
        "link": "http://dx.doi.org/10.31234/osf.io/x4d3f"
    },
    {
        "id": 26444,
        "title": "Reinforcement learning with adaptive networks",
        "authors": "Tomoki Sasaki, Satoshi Yamada",
        "published": "2017-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icras.2017.8071905"
    },
    {
        "id": 26445,
        "title": "Reinforcement learning for the face support pressure of tunnel boring machines",
        "authors": "Enrico Soranzo, Carlotta Guardiani, Wei Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "In tunnel excavation with boring machines, the tunnel face is supported to avoid collapse and minimise settlement. This article proposes the use of reinforcement learning, specifically the Deep Q-Network algorithm, to predict the face support pressure. The approach is tested both analytically and numerically. By using the soil properties ahead of the tunnel face and the overburden depth as the input, the algorithm is capable of predicting the optimal tunnel face support pressure, adapting to changes in geological and geometrical conditions.",
        "link": "http://dx.doi.org/10.20944/preprints202302.0236.v1"
    },
    {
        "id": 26446,
        "title": "Energy Efficient Multi-dimensional Trajectory of UAV-aided IoT Networks with Reinforcement Learning",
        "authors": "SILVIRIANTI SILVIRIANTI, Soo Young Shin",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper proposed a multi-dimensional search space (or directional space) which has larger degree-of-freedom (DOF) to improve energy efficiency of limited-battery-powered UAV in Internet of Things (IoT) data collection scenario. In this paper, the UAV navigates from initial to goal point while collecting data from IoT sensors on the ground. Due to limited battery-power of UAV, optimized trajectory becomes a crucial practical issue. Based on the available directional space, direction of UAV which related to navigation trajectory is optimized using reinforcement learning. The objective of this reinforcement learning is to maximize energy efficiency of UAV as a long-term reward by selecting optimized direction. Moreover, practical energy consumption model and environment are presented in this paper. The simulation results verify the proposed scheme that has larger directional space achieved higher energy efficiency compared to benchmark models.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19028864"
    },
    {
        "id": 26447,
        "title": "Co-Evolutionary Traffic Signal Control Using Reinforcement Learning for Road Networks Under Stochastic Capacity",
        "authors": "Suh-Wen Chiou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4327149"
    },
    {
        "id": 26448,
        "title": "Reinforcement learning for constructing low density sign representations of Boolean functions",
        "authors": "Oytun Yapar, Erhan Oztop",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2022.es2022-25"
    },
    {
        "id": 26449,
        "title": "Ambulance Redeployment via Reinforcement Learning",
        "authors": "Ümitcan Şahin, Veysel YÜcesoy",
        "published": "2020-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu49456.2020.9302230"
    },
    {
        "id": 26450,
        "title": "Name that state: How language affects human reinforcement learning",
        "authors": "Angela Radulescu, Wai Keen Vong, Todd Matthew Gureckis",
        "published": "No Date",
        "citations": 1,
        "abstract": "We describe two experiments designed to test whether the ease with which people can label features of the environment influences human reinforcement learning. The first experiment presents evidence that people are more efficient at learning to discern relevant features of a task when candidate features are easier to name. The second experiment shows that learning what action to take in a given state is easier when states have more readily nameable verbal labels, an effect that was especially pronounced in environments with more states. The interaction between CLIP, a state-of-the-art AI model trained to map images to natural language concepts, and established hu- man RL algorithms captures the key effects without the need to specify condition-specific parameters. These results suggest a possible role for language information in how humans represent the environment when learning from trial and error.",
        "link": "http://dx.doi.org/10.31234/osf.io/57wgr"
    },
    {
        "id": 26451,
        "title": "Time cell encoding in deep reinforcement learning agents depends on mnemonic demands",
        "authors": "Dongyan Lin, Blake A. Richards",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractThe representation of “what happened when” is central to encoding episodic and working memories. Recently discovered hippocampal time cells are theorized to provide the neural substrate for such representations by forming distinct sequences that both encode time elapsed and sensory content. However, little work has directly addressed to what extent cognitive demands and temporal structure of experimental tasks affect the emergence and informativeness of these temporal representations. Here, we trained deep reinforcement learning (DRL) agents on a simulated trial-unique nonmatch-to-location (TUNL) task, and analyzed the activities of artificial recurrent units using neuroscience-based methods. We show that, after training, representations resembling both time cells and ramping cells (whose activity increases or decreases monotonically over time) simultaneously emerged in the same population of recurrent units. Furthermore, with simulated variations of the TUNL task that controlled for (1) memory demands during the delay period and (2) the temporal structure of the episodes, we show that memory demands are necessary for the time cells to encode information about the sensory stimuli, while the temporal structure of the task only affected the encoding of “what” and “when” by time cells minimally. Our findings help to reconcile current discrepancies regarding the involvement of time cells in memory-encoding by providing a normative framework. Our modelling results also provide concrete experimental predictions for future studies.",
        "link": "http://dx.doi.org/10.1101/2021.07.15.452557"
    },
    {
        "id": 26452,
        "title": "Reinforcement learning derived chemotherapeutic schedules for robust patient-specific therapy",
        "authors": "Brydon Eastman, Michelle Przedborski, Mohammad Kohandel",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractThe in-silico development of a chemotherapeutic dosing schedule for treating cancer relies upon a parameterization of a particular tumour growth model to describe the dynamics of the cancer in response to the dose of the drug. In practice, it is often prohibitively difficult to ensure the validity of patient-specific parameterizations of these models for any particular patient. As a result, sensitivities to these particular parameters can result in therapeutic dosing schedules that are optimal in principle not performing well on particular patients. In this study, we demonstrate that chemotherapeutic dosing strategies learned via reinforcement learning methods are more robust to perturbations in patient-specific parameter values than those learned via classical optimal control methods. By training a reinforcement learning agent on mean-value parameters and allowing the agent periodic access to a more easily measurable metric, relative bone marrow density, for the purpose of optimizing dose schedule while reducing drug toxicity, we are able to develop drug dosing schedules that outperform schedules learned via classical optimal control methods, even when such methods are allowed to leverage the same bone marrow measurements.",
        "link": "http://dx.doi.org/10.1101/2021.04.23.441182"
    },
    {
        "id": 26453,
        "title": "Reinforcement Learning approaches to hippocampus-dependent flexible spatial navigation",
        "authors": "Charline Tessereau, Reuben O’Dea, Stephen Coombes, Tobias Bast",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractHumans and non-human animals show great flexibility in spatial navigation, including the ability to return to specific locations based on as few as one single experience. To study spatial navigation in the laboratory, watermaze tasks, in which rats have to find a hidden platform in a pool of cloudy water surrounded by spatial cues, have long been used. Analogous tasks have been developed for human participants using virtual environments. Spatial learning in the watermaze is facilitated by the hippocampus. In particular, rapid, one-trial, allocentric place learning, as measured in the Delayed-Matching-to-Place (DMP) variant of the watermaze task, which requires rodents to learn repeatedly new locations in a familiar environment, is hippocampal dependent. In this article, we review some computational principles, embedded within a Reinforcement Learning (RL) framework, that utilise hippocampal spatial representations for navigation in watermaze tasks. We consider which key elements underlie their efficacy, and discuss their limitations in accounting for hippocampus-dependent navigation, both in terms of behavioural performance (i.e., how well do they reproduce behavioural measures of rapid place learning) and neurobiological realism (i.e., how well do they map to neurobiological substrates involved in rapid place learning). We discuss how an actor-critic architecture, enabling simultaneous assessment of the value of the current location and of the optimal direction to follow, can reproduce one-trial place learning performance as shown on watermaze and virtual DMP tasks by rats and humans, respectively, if complemented with map-like place representations. The contribution of actor-critic mechanisms to DMP performance is consistent with neurobiological findings implicating the striatum and hippocampo-striatal interaction in DMP performance, given that the striatum has been associated with actor-critic mechanisms. Moreover, we illustrate that hierarchical computations embedded within an actor-critic architecture may help to account for aspects of flexible spatial navigation. The hierarchical RL approach separates trajectory control via a temporal-difference error from goal selection via a goal prediction error and may account for flexible, trial-specific, navigation to familiar goal locations, as required in some arm-maze place memory tasks, although it does not capture one-trial learning of new goal locations, as observed in open field, including watermaze and virtual, DMP tasks. Future models of one-shot learning of new goal locations, as observed on DMP tasks, should incorporate hippocampal plasticity mechanisms that integrate new goal information with allocentric place representation, as such mechanisms are supported by substantial empirical evidence.",
        "link": "http://dx.doi.org/10.1101/2020.07.30.229005"
    },
    {
        "id": 26454,
        "title": "The functional form of value normalization in human reinforcement learning",
        "authors": "Sophie Bavard, Stefano Palminteri",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractReinforcement learning research in humans and other species indicates that rewards are represented in a context-dependent manner. More specifically, reward representations seem to be normalized as a function of the value of the alternative options. The dominant view postulates that value context-dependence is achieved via a divisive normalization rule, inspired by perceptual decision-making research. However, behavioral and neural evidence points to another plausible mechanism: range normalization. Critically, previous experimental designs were ill-suited to disentangle the divisive and the range normalization accounts, which generate similar behavioral predictions in many circumstances. To address this question, we designed a new learning task where we manipulated, across learning contexts, the number of options and the value ranges. Behavioral and computational analyses falsify the divisive normalization account and rather provide support for the range normalization rule. Together, these results shed new light on the computational mechanisms underlying context-dependence in learning and decision-making.",
        "link": "http://dx.doi.org/10.1101/2022.07.14.500032"
    },
    {
        "id": 26455,
        "title": "Decision letter for \"Discovering mechanisms for materials microstructure optimization via reinforcement learning of a generative model\"",
        "authors": "",
        "published": "2022-7-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/aca004/v1/decision1"
    },
    {
        "id": 26456,
        "title": "Kinematic Synthesis Using Reinforcement Learning",
        "authors": "Kaz Vermeer, Reinier Kuppens, Justus Herder",
        "published": "2018-8-26",
        "citations": 6,
        "abstract": "The presented research demonstrates the synthesis of two-dimensional kinematic mechanisms using feature-based reinforcement learning. As a running example the classic challenge of designing a straight-line mechanism is adopted: a mechanism capable of tracing a straight line as part of its trajectory. This paper presents a basic framework, consisting of elements such as mechanism representations, kinematic simulations and learning algorithms, as well as some of the resulting mechanisms and a comparison to prior art. Series of successful mechanisms have been synthesized for path generation of a straight line and figure-eight.",
        "link": "http://dx.doi.org/10.1115/detc2018-85529"
    },
    {
        "id": 26457,
        "title": "Constraint-Based Multi-Agent Reinforcement Learning for Collaborative Tasks",
        "authors": "xiumin shang, Tengyu Xu, Ioannis Karamouzas, Marcelo Kallmann",
        "published": "No Date",
        "citations": 0,
        "abstract": "In order to be successfully executed, collaborative tasks performed by\ntwo agents often require a cooperative strategy to be learned. In this\nwork, we propose a constraint-based multi-agent reinforcement learning\napproach called Constrained Multi-agent Soft Actor Critic (C-MSAC) to\ntrain control policies for simulated agents performing collaborative\nmulti-phase tasks. Given a task with n phases, the first\nn-1 phases are treated as constraints for the final task phase\nobjective, which is addressed with a centralized training and\ndecentralized execution approach. We highlight our framework on a tray\nbalancing task including two phases: tray lifting and cooperative tray\ncontrol for target following. We evaluate our proposed approach and\ncompare it against its unconstrained variant (MSAC). The performed\ncomparisons show that C-MSAC leads to higher success rates, more robust\ncontrol policies, and better generalization performance.",
        "link": "http://dx.doi.org/10.22541/au.168301658.80453554/v1"
    },
    {
        "id": 26458,
        "title": "Reward processing and reinforcement learning: from adolescence to aging",
        "authors": "Jo Cutler, Matthew A J Apps, Patricia Lockwood",
        "published": "No Date",
        "citations": 1,
        "abstract": "The neurocognitive systems that underlie the ability to process rewards and learn from reinforcement undergo substantial changes across the adult lifespan. Adolescence is often characterized as a developmental period with a heightened sensitivity to reward and healthy aging is typically associated with a decline in learning from reinforcement. In this Chapter we review how the psychological and neural mechanisms that underpin reward processing and reinforcement learning change from adolescence to older adulthood. We consider behavioral and neuroimaging studies, as well as how different reward and learning contexts, such as gain vs. loss and social vs. non-social information, may alter reward processing and reinforcement learning abilities. We end by considering the challenges and opportunities of conducting developmental and aging studies in computational neuroscience and suggest future directions for the field.",
        "link": "http://dx.doi.org/10.31234/osf.io/pnuk8"
    },
    {
        "id": 26459,
        "title": "Hindsight Reward Shaping in Deep Reinforcement Learning",
        "authors": "Byron de Villiers, Deon Sabatta",
        "published": "2020-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/saupec/robmech/prasa48453.2020.9041058"
    },
    {
        "id": 26460,
        "title": "Order-Book Trading Algorithms",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-10"
    },
    {
        "id": 26461,
        "title": "Dynamic Channel Access via Meta-Reinforcement Learning",
        "authors": "Ziyang Lu, M. Cenk Gursoy",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685347"
    },
    {
        "id": 26462,
        "title": "Deep Reinforcement Learning for NLP",
        "authors": "William Yang Wang, Jiwei Li, Xiaodong He",
        "published": "2018",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p18-5007"
    },
    {
        "id": 26463,
        "title": "Reinforcement Learning From Scratch",
        "authors": "Uwe Lorenz",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-09030-1"
    },
    {
        "id": 26464,
        "title": "Iorl: Inductive-Offline-Reinforcement-Learning for Traffic Signal Control Warmstarting",
        "authors": "François-Xavier Devailly, Denis Larocque, Laurent Charlin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4385976"
    },
    {
        "id": 26465,
        "title": "Automatic Ultrasound Guidance Based on Deep Reinforcement Learning",
        "authors": "Piotr Jarosik, Marcin Lewandowski",
        "published": "2019-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ultsym.2019.8926041"
    },
    {
        "id": 26466,
        "title": "Assimilating Human Feedback from Autonomous Vehicle Interaction in Reinforcement Learning Models",
        "authors": "Richard Fox, Elliot A. Ludvig",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nA significant challenge for real-world automated vehicles (AVs) is their interaction with human pedestrians. This paper develops a methodology to directly elicit the AV behaviour pedestrians find suitable by collecting quantitative data that can be used to measure and improve an algorithm's performance. Starting with a Deep Q Network (DQN) trained on a simple Pygame/Python-based pedestrian crossing environment, the reward structure was adapted to allow adjustment by human feedback. Feedback was collected by eliciting behavioural judgements collected from people in a controlled environment. The reward was shaped by the inter-action vector, decomposed into feature aspects for relevant behaviours, thereby facilitating both implicit preference selection and explicit task discovery in tandem. Using computational RL and behavioural-science techniques, we harness a formal iterative feedback loop where the rewards are repeatedly adapted based on human behavioural judgments. Experiments were conducted with 124 participants that showed strong initial improvement in the judgement of AV behaviours with the adaptive reward structure. The results indicate that the primary avenue for enhancing vehicle behaviour lies in the predictability of its movements when introduced. More broadly, recognising AV behaviours that receive favourable human judgments can pave the way for enhanced performance.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3405901/v1"
    },
    {
        "id": 26467,
        "title": "Coevolution of Cognition and Cooperation in Structured Populations Under Reinforcement Learning",
        "authors": "Rossana Mastrandrea, Ennio Bilancini, Leonardo Boncinelli",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4704384"
    },
    {
        "id": 26468,
        "title": "Memory-Assisted Reinforcement Learning for Diverse Molecular De Novo Design",
        "authors": "Thomas Blaschke, Ola Engkvist, Jürgen Bajorath, Hongming Chen",
        "published": "No Date",
        "citations": 1,
        "abstract": "<div><div><div><p>In de novo molecular design, recurrent neural networks (RNN) have been shown to be effective methods for sampling and generating novel chemical structures. Using a technique called reinforcement learning (RL), an RNN can be tuned to target a particular section of chemical space with optimized desirable properties using a scoring function. However, ligands generated by current RL methods so far tend to have relatively low diversity, and sometimes even result in duplicate structures when optimizing towards particular properties. Here, we propose a new method to address the low diversity issue in RL. Memory-assisted RL is an extension of the known RL, with the introduction of a so-called memory unit.</p></div></div></div>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.12693152"
    },
    {
        "id": 26469,
        "title": "Reviewer #3 (Public Review): Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-8-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.2.sa3"
    },
    {
        "id": 26470,
        "title": "eLife assessment: Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Claire Gillan",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.3.sa0"
    },
    {
        "id": 26471,
        "title": "eLife assessment: Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Claire Gillan",
        "published": "2023-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.1.sa0"
    },
    {
        "id": 26472,
        "title": "Reinforcement Learning for Topic Models",
        "authors": "Jeremy Costello, Marek Reformat",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.265"
    },
    {
        "id": 26473,
        "title": "Federated Reinforcement Learning for Portfolio Management",
        "authors": "Pengqian Yu, Laura Wynter, Shiau Hong Lim",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-96896-0_21"
    },
    {
        "id": 26474,
        "title": "Reinforcement Learning and Genetic Algorithms",
        "authors": "Richard A. Berk",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-40189-4_9"
    },
    {
        "id": 26475,
        "title": "Learning to Survive using Reinforcement Learning with MLAgents",
        "authors": "Sarosh Dandoti",
        "published": "2022-7-31",
        "citations": 0,
        "abstract": "Abstract: Simulations have been there for a long time, in different versions and level of complexity. Training a Reinforcement Learning model in a 3D environment lets us understand a lot of new insights from the inference. There have been some examples where the AI learns to Feed Itself, Learns to Start walking, jumping etc. The reason one trains an entire model from the agent knowing nothing to being a perfect task achiever is that during the process, new behavioral patterns can be recorded. Reinforcement Learning is a feedback-based Machine Learning technique in which an agent learns how to behave in a given environment by performing actions and observing the outcomes of those actions. For each positive action, the agent receives positive feedback; for each negative action, the agent receives negative feedback or a penalty. A general simple agent would learn to perform a task and get some reward on accomplishing it. The Agent is also given punishment if it does something that it’s not supposed to do. These simple simulations can evolve, try to use their surroundings, try to fight with other agents to accomplish their goal",
        "link": "http://dx.doi.org/10.22214/ijraset.2022.45526"
    },
    {
        "id": 26476,
        "title": "Reinforcement Learning",
        "authors": "Wei Qi Yan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-4823-9_5"
    },
    {
        "id": 26477,
        "title": "A deep reinforcement learning based model supporting object familiarization",
        "authors": "Maximilian Panzner, Philipp Cimiano",
        "published": "2017-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/devlrn.2017.8329828"
    },
    {
        "id": 26478,
        "title": "Hardware Realization of Reinforcement Learning Algorithms for Edge Devices",
        "authors": "Shaik Mohammed Waseem, Subir Kumar Roy",
        "published": "2021-11-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003201038-12"
    },
    {
        "id": 26479,
        "title": "LFQ: Online Learning of Per-flow Queuing Policies using Deep Reinforcement Learning",
        "authors": "Maximilian Bachl, Joachim Fabini, Tanja Zseby",
        "published": "2020-11-16",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lcn48667.2020.9314771"
    },
    {
        "id": 26480,
        "title": "Reinforcement Learning for Mean Field Games, with Applications to Economics",
        "authors": "",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009028943.022"
    },
    {
        "id": 26481,
        "title": "State Representation Learning For Effective Deep Reinforcement Learning",
        "authors": "Jian Zhao, Wengang Zhou, Tianyu Zhao, Yun Zhou, Houqiang Li",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme46284.2020.9102924"
    },
    {
        "id": 26482,
        "title": "Cooperative and Competitive Reinforcement and Imitation Learning for a Mixture of Heterogeneous Learning Modules",
        "authors": "Eiji Uchibe",
        "published": "2018-9-27",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3389/fnbot.2018.00061"
    },
    {
        "id": 26483,
        "title": "5 Reinforcement learning",
        "authors": "Nazeer Shaik, Chandra Sekaran, Amit Mahajan, Balkeshwar Singh",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9783111323749-005"
    },
    {
        "id": 26484,
        "title": "Smooth Trajectory Collision Avoidance through Deep Reinforcement Learning",
        "authors": "Sirui Song, Kirk Saunders, Ye Yue, Jundong Liu",
        "published": "2022-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00152"
    },
    {
        "id": 26485,
        "title": "Learning at the Edge: Mobile Edge Computing and Reinforcement Learning for Enhanced Web Application Performance",
        "authors": "Komeil Moghaddasi, Shakiba Rajabi",
        "published": "2023-5-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icwr57742.2023.10138952"
    },
    {
        "id": 26486,
        "title": "Policy Learning with Constraints in Model-free Reinforcement Learning: A Survey",
        "authors": "Yongshuai Liu, Avishai Halev, Xin Liu",
        "published": "2021-8",
        "citations": 31,
        "abstract": "Reinforcement Learning (RL) algorithms have had tremendous success in simulated domains. These algorithms, however, often cannot be directly applied to physical systems, especially in cases where there are constraints to satisfy (e.g. to ensure safety or limit resource consumption). In standard RL, the agent is incentivized to explore any policy with the sole goal of maximizing reward; in the real world, however, ensuring satisfaction of certain constraints in the process is also necessary and essential. In this article, we overview existing approaches addressing constraints in model-free reinforcement learning. We model the problem of learning with constraints as a Constrained Markov Decision Process and consider two main types of constraints: cumulative and instantaneous. We summarize existing approaches and discuss their pros and cons. To evaluate policy performance under constraints, we introduce a set of standard benchmarks and metrics. We also summarize limitations of current methods and present open questions for future research.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/614"
    },
    {
        "id": 26487,
        "title": "Moral Reinforcement Learning Using Actual Causation",
        "authors": "Tue Herlau",
        "published": "2022-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccr54399.2022.9790262"
    },
    {
        "id": 26488,
        "title": "Tabular Monte Carlo Reinforcement Learning for Valve Position Control",
        "authors": "Piotr Felisiak",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4115550"
    },
    {
        "id": 26489,
        "title": "Reinforcement Learning for Traffic Signal Timing Optimization",
        "authors": "Hyunjin Joo, Yujin Lim",
        "published": "2020-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin48656.2020.9016568"
    },
    {
        "id": 26490,
        "title": "Derivatives Pricing and Hedging",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-9"
    },
    {
        "id": 26491,
        "title": "Decision letter for \"Mental health analysis for college students based on pattern recognition and reinforcement learning\"",
        "authors": "",
        "published": "2023-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/itl2.453/v2/decision1"
    },
    {
        "id": 26492,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "Grigory Neustroev",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v1/review2"
    },
    {
        "id": 26493,
        "title": "Models of Conditioning and Reinforcement Learning",
        "authors": "Daniel S. Levine",
        "published": "2018-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9780429448805-6"
    },
    {
        "id": 26494,
        "title": "Reinforcement Learning based approach for Underwater Environment to evaluate Agent Algorithm",
        "authors": "Shruthi K R, Kavitha C",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nA lot of research is undergoing in Underwater as it has huge applications. An underwater network is a delay-tolerant network [1][2] due to its intermittent characteristics. Underwater acoustic communication enables communication undersea. Wireless sensor nodes underwater are sparsely placed due to environmental characteristics [3] to gather information. Communication undersea is tedious because of noise and varying environments. Since the underwater environment is highly unpredictable due to its nature, there doesn’t exist a constant path or route between wireless sensor nodes. And the battery of sensor nodes is a major concern as they cannot be replaced frequently. Therefore, it's necessary to design an algorithm that can establish a path to the destination dynamically based on the environmental conditions and the node’s battery level. In this paper, the authors have proposed a Reinforcement Learning approach to evaluate sensor nodes’ performance. Many machine learning algorithms have used only the epsilon greedy action selection method. But here, four different types of action selection methods are used for the routing purpose. Based on the threshold level, an appropriate action selection method is chosen. The validation of the proposed approach is carried out by comparing the RL algorithm with other baseline algorithms. Experimental results showcase RL algorithm outperforms other baseline algorithms.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3291459/v1"
    },
    {
        "id": 26495,
        "title": "Reinforcement Learning-Based Workload Scheduling for Edge Computing",
        "authors": "tao zheng, Jian Wan, jilin Zhang, Congfeng Jiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nEdge computing is a new paradigm for providing cloud computing capacities at the edge of network near mobile users. It offers an effective solution to help mobile devices with computation-intensive and delay-sensitive tasks. However, the edge of network presents a dynamic environment with large number of devices, high mobility of the end user, heterogeneous applications and intermittent traffific. In such environment, edge computing always encounters workload scheduling problem of how to effificiently schedule incoming tasks from mobile devices to edge servers or cloud servers, which is a hard and online problem. In this work, we focus on the workload scheduling problem with the goal of balancing the workload, reducing the service time and minimizing the failed task rate. We proposed a reinforcement learning-based approach, which can learn from the previous actions and achieve best scheduling in the absence of a mathematical model of the environment. Simulation results show that our proposed approach achieves the best performance in aspects of service time, virtual machine utilization, and failed tasks rate compared with other approaches. Our reinforcement learning-based approach can provide an effificient solution to the workload scheduling problem in edge computing.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-349535/v1"
    },
    {
        "id": 26496,
        "title": "A Data-efficiency Training Framework for Deep Reinforcement Learning",
        "authors": "Wenhui Feng, Chongzhao Han, Feng Lian, Xia Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Sparse reward long horizon task is a major challenge for deep reinforcement learning algorithm. One of the key barriers is data-inefficiency. Even in the simulation environment, it usually takes weeks to training the agent. In this study, a data-efficiency training framework is proposed, where a curriculum learning is design for the agent in the simulation scenario. Different distributions of the initial state are set for the agent to get more informative reward during the whole training process. A fine-tuning of the parameters in the output layer of the neural network for value function is conduct to bridge the gap between sim-to-real. An experiment of UAV maneuver control   is conducted in the proposed training framework to verify the method more efficient. We demonstrate that data-efficiency is different for the same data in different training stages.",
        "link": "http://dx.doi.org/10.20944/preprints202209.0483.v1"
    },
    {
        "id": 26497,
        "title": "Automated Cryptocurrency Trading Approach Using Ensemble Deep Reinforcement Learning: Learn to Understand Candlesticks",
        "authors": "Jing Liu, Yuncheol Kang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4348791"
    },
    {
        "id": 26498,
        "title": "“TINKERING” AS LEARNING REINFORCEMENT TOWARDS MULTIDISCIPLINARITY IN RESEARCH-ORIENTED EDUCATION",
        "authors": "Alden Dochshanov",
        "published": "2017-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/edulearn.2017.0864"
    },
    {
        "id": 26499,
        "title": "Coevolutionary Deep Reinforcement Learning",
        "authors": "David Cotton, Jason Traish, Zenon Chaczko",
        "published": "2020-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci47803.2020.9308290"
    },
    {
        "id": 26500,
        "title": "Decomposing Planar Shapes into Quads Using Reinforcement Learning [Slides]",
        "authors": "Rao Garimella, Cristina Cardona, Navamita Ray, Benjamin DiPrete",
        "published": "2022-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1881769"
    },
    {
        "id": 26501,
        "title": "Background",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5_2"
    },
    {
        "id": 26502,
        "title": "Representation Learning and Reinforcement Learning for Dynamic Complex Motion Planning System",
        "authors": "Chengmin Zhou, Bingding Huang, Pasi Fränti",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3247160"
    },
    {
        "id": 26503,
        "title": "Using Reinforcement Learning for Optimization of a Workpiece Clamping Position in a Machine Tool",
        "authors": "Vladimir Samsonov, Chrismarie Enslin, Hans-Georg Köpken, Schirin Baer, Daniel Lütticke",
        "published": "2020",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009354105060514"
    },
    {
        "id": 26504,
        "title": "Accelerate Training of Reinforcement Learning Agent by Utilization of Current and Previous Experience",
        "authors": "Chenxing Li, Yinlong Liu, Zhenshan Bing, Fabian Schreier, Jan Seyler, Shahram Eivazi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011745600003393"
    },
    {
        "id": 26505,
        "title": "Moral Reinforcement Learning Using Actual Causation",
        "authors": "Tue Herlau",
        "published": "2022-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccr54399.2022.9790262"
    },
    {
        "id": 26506,
        "title": "Tabular Monte Carlo Reinforcement Learning for Valve Position Control",
        "authors": "Piotr Felisiak",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4115550"
    },
    {
        "id": 26507,
        "title": "Reinforcement Learning for Traffic Signal Timing Optimization",
        "authors": "Hyunjin Joo, Yujin Lim",
        "published": "2020-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin48656.2020.9016568"
    },
    {
        "id": 26508,
        "title": "Derivatives Pricing and Hedging",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-9"
    },
    {
        "id": 26509,
        "title": "Models of Conditioning and Reinforcement Learning",
        "authors": "Daniel S. Levine",
        "published": "2018-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9780429448805-6"
    },
    {
        "id": 26510,
        "title": "Reinforcement Learning based approach for Underwater Environment to evaluate Agent Algorithm",
        "authors": "Shruthi K R, Kavitha C",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nA lot of research is undergoing in Underwater as it has huge applications. An underwater network is a delay-tolerant network [1][2] due to its intermittent characteristics. Underwater acoustic communication enables communication undersea. Wireless sensor nodes underwater are sparsely placed due to environmental characteristics [3] to gather information. Communication undersea is tedious because of noise and varying environments. Since the underwater environment is highly unpredictable due to its nature, there doesn’t exist a constant path or route between wireless sensor nodes. And the battery of sensor nodes is a major concern as they cannot be replaced frequently. Therefore, it's necessary to design an algorithm that can establish a path to the destination dynamically based on the environmental conditions and the node’s battery level. In this paper, the authors have proposed a Reinforcement Learning approach to evaluate sensor nodes’ performance. Many machine learning algorithms have used only the epsilon greedy action selection method. But here, four different types of action selection methods are used for the routing purpose. Based on the threshold level, an appropriate action selection method is chosen. The validation of the proposed approach is carried out by comparing the RL algorithm with other baseline algorithms. Experimental results showcase RL algorithm outperforms other baseline algorithms.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3291459/v1"
    },
    {
        "id": 26511,
        "title": "Reinforcement Learning-Based Workload Scheduling for Edge Computing",
        "authors": "tao zheng, Jian Wan, jilin Zhang, Congfeng Jiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nEdge computing is a new paradigm for providing cloud computing capacities at the edge of network near mobile users. It offers an effective solution to help mobile devices with computation-intensive and delay-sensitive tasks. However, the edge of network presents a dynamic environment with large number of devices, high mobility of the end user, heterogeneous applications and intermittent traffific. In such environment, edge computing always encounters workload scheduling problem of how to effificiently schedule incoming tasks from mobile devices to edge servers or cloud servers, which is a hard and online problem. In this work, we focus on the workload scheduling problem with the goal of balancing the workload, reducing the service time and minimizing the failed task rate. We proposed a reinforcement learning-based approach, which can learn from the previous actions and achieve best scheduling in the absence of a mathematical model of the environment. Simulation results show that our proposed approach achieves the best performance in aspects of service time, virtual machine utilization, and failed tasks rate compared with other approaches. Our reinforcement learning-based approach can provide an effificient solution to the workload scheduling problem in edge computing.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-349535/v1"
    },
    {
        "id": 26512,
        "title": "A Data-efficiency Training Framework for Deep Reinforcement Learning",
        "authors": "Wenhui Feng, Chongzhao Han, Feng Lian, Xia Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Sparse reward long horizon task is a major challenge for deep reinforcement learning algorithm. One of the key barriers is data-inefficiency. Even in the simulation environment, it usually takes weeks to training the agent. In this study, a data-efficiency training framework is proposed, where a curriculum learning is design for the agent in the simulation scenario. Different distributions of the initial state are set for the agent to get more informative reward during the whole training process. A fine-tuning of the parameters in the output layer of the neural network for value function is conduct to bridge the gap between sim-to-real. An experiment of UAV maneuver control   is conducted in the proposed training framework to verify the method more efficient. We demonstrate that data-efficiency is different for the same data in different training stages.",
        "link": "http://dx.doi.org/10.20944/preprints202209.0483.v1"
    },
    {
        "id": 26513,
        "title": "Test-retest reliability of reinforcement learning parameters",
        "authors": "Jessica Schaaf, Laura Weidinger, Lucas Molleman, Wouter van den Bos",
        "published": "No Date",
        "citations": 2,
        "abstract": "Recently it has been suggested that parameters estimates of computational models can be used to understand individual differences at the process level. One area of research in which this approach, called computational phenotyping, took hold is computational psychiatry, but it is also used to understand differences in age and personality. One requirement for successful computational phenotyping is that behavior and parameters are stable over time. Surprisingly, the test-retest reliability of behavior and model parameters remains unknown for most experimental tasks and models. The present study seeks to close this gap by investigating the test-retest reliability of canonical reinforcement learning models in the context of two often-used learning paradigms: a two-armed bandit and a reversal learning task. We tested independent cohorts for the two tasks (N=142 and N=154) via an online testing platform with a between-test interval of five weeks. Whereas reliability was high for personality and cognitive measures, it was generally poor for the parameter estimates of the reinforcement learning models. Given that simulations indicated that our procedures could detect high test-retest reliability, this suggests that a significant proportion of the variability must be ascribed to the participants themselves. In support of that hypothesis, we show that mood (stress and happiness) can partly explain within-subject variability. Taken together, these results are critical for current practices in computational phenotyping and suggest that individual variability should be taken into account in the future development of the field.",
        "link": "http://dx.doi.org/10.31234/osf.io/chq5a"
    },
    {
        "id": 26514,
        "title": "Automated Cryptocurrency Trading Approach Using Ensemble Deep Reinforcement Learning: Learn to Understand Candlesticks",
        "authors": "Jing Liu, Yuncheol Kang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4348791"
    },
    {
        "id": 26515,
        "title": "5G Handover using Reinforcement Learning",
        "authors": "Vijaya Yajnanarayana, Henrik Ryden, Laszlo Hevizi",
        "published": "2020-9",
        "citations": 44,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/5gwf49715.2020.9221072"
    },
    {
        "id": 26516,
        "title": "“TINKERING” AS LEARNING REINFORCEMENT TOWARDS MULTIDISCIPLINARITY IN RESEARCH-ORIENTED EDUCATION",
        "authors": "Alden Dochshanov",
        "published": "2017-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/edulearn.2017.0864"
    },
    {
        "id": 26517,
        "title": "Coevolutionary Deep Reinforcement Learning",
        "authors": "David Cotton, Jason Traish, Zenon Chaczko",
        "published": "2020-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci47803.2020.9308290"
    },
    {
        "id": 26518,
        "title": "Decomposing Planar Shapes into Quads Using Reinforcement Learning [Slides]",
        "authors": "Rao Garimella, Cristina Cardona, Navamita Ray, Benjamin DiPrete",
        "published": "2022-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1881769"
    },
    {
        "id": 26519,
        "title": "Parallel Computing",
        "authors": "Huaqing Zhang, Tianyang Yu",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_12"
    },
    {
        "id": 26520,
        "title": "DLGNet: Adversarial Reinforcement Learning for Disease Label Generation",
        "authors": "Ce Yang, Zhenghan Chen, Ye Wang, Xunzhu Tang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nInternational Classification of Diseases (ICD) coding has been considered as a multi-label prediction problem, requiring the assignment of one or more codes to a detailed discharge summary. Existing automatic ICD coding algorithms struggle to effectively classify medical diagnosis texts representing deep sparse categories. We propose Disease Label Generation Network (DLGNet), a novel adversarial network that transforms ICD codes into a label generation challenge. This strategy faces three major challenges: (1) How to extract the relationship between clinical text and ICD codes? (2) What training methods should be used to improve the generalization and effectiveness of network? (3) How to evaluate the quality of disease labels generated by the DLGNet? For (1), we develop an information integration module (MIM) to encode the relationship between clinical text and ICD codes. For (2), we present adversarial training algorithms, such as reinforcement causal learning and adversarial perturbation regularization. For (3), we present a Label Discriminator (LD) that calculates the reward for each ICD code in the Label Generator (LG). In conclusion, DLGNet outperforms existing state-of-the-art approaches on evaluation measures such as micro-F1, leading to the formation of a new SOTA. The code is available at anonymous github link: https://anonymous.4open.science/r/DLGNet-787D.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2613478/v1"
    },
    {
        "id": 26521,
        "title": "Review for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "Grigory Neustroev",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v1/review2"
    },
    {
        "id": 26522,
        "title": "Decision letter for \"Mental health analysis for college students based on pattern recognition and reinforcement learning\"",
        "authors": "",
        "published": "2023-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/itl2.453/v2/decision1"
    },
    {
        "id": 26523,
        "title": "Multi-Agent Reinforcement Learning-based Adaptive Sampling for Conformational Sampling of Proteins",
        "authors": "Diego E. Kleiman, Diwakar Shukla",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMachine Learning is increasingly applied to improve the efficiency and accuracy of Molecular Dynamics (MD) simulations. Although the growth of distributed computer clusters has allowed researchers to obtain higher amounts of data, unbiased MD simulations have difficulty sampling rare states, even under massively parallel adaptive sampling schemes. To address this issue, several algorithms inspired by reinforcement learning (RL) have arisen to promote exploration of the slow collective variables (CVs) of complex systems. Nonetheless, most of these algorithms are not well-suited to leverage the information gained by simultaneously sampling a system from different initial states (e.g., a protein in different conformations associated with distinct functional states). To fill this gap, we propose two algorithms inspired by multi-agent RL that extend the functionality of closely-related techniques (REAP and TSLC) to situations where the sampling can be accelerated by learning from different regions of the energy landscape through coordinated agents. Essentially, the algorithms work by remembering which agent discovered each conformation and sharing this information with others at the action-space discretization step. Astakes functionis introduced to modulate how different agents sense rewards from discovered states of the system. The consequences are threefold: (i) agents learn to prioritize CVs using only relevant data, (ii) redundant exploration is reduced, and (iii) agents that obtain higher stakes are assigned more actions. We compare our algorithm with other adaptive sampling techniques (Least Counts, REAP, TSLC, and AdaptiveBandit) to show and rationalize the gain in performance.",
        "link": "http://dx.doi.org/10.1101/2022.05.31.494208"
    },
    {
        "id": 26524,
        "title": "Reinforcement Learning and Strategic Reasoning During Social Decision-Making",
        "authors": "H. Seo, D. Lee",
        "published": "2017",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-805308-9.00018-x"
    },
    {
        "id": 26525,
        "title": "A Stochastic Maximum Principle Approach for Reinforcement Learning with Parameterized Environment",
        "authors": "Feng Bao, Richard Archibald, Jiongmin Yong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4368293"
    },
    {
        "id": 26526,
        "title": "Reinforcement Learning Configuration Interaction",
        "authors": "Joshua Goings, Hang Hu, Chao Yang, Xiaosong Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "A reinforcement learning algorithm is developed for the selected configuration interaction problem. We explore how reinforcement learning can obtain compact wave functions at near full configuration interaction accuracy.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-m8x51-v3"
    },
    {
        "id": 26527,
        "title": "Reviewer #1 (Public Review): Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.1.sa1"
    },
    {
        "id": 26528,
        "title": "eLife assessment: Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Claire Gillan",
        "published": "2023-8-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.2.sa0"
    },
    {
        "id": 26529,
        "title": "Reviewer #2 (Public Review): Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "",
        "published": "2023-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.87720.1.sa2"
    },
    {
        "id": 26530,
        "title": "Grasp Control Method for Robotic Manipulator Based on Federated Reinforcement Learning",
        "authors": "Shida Zhong, Yue Wang, Tao Yuan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4687810"
    },
    {
        "id": 26531,
        "title": "Miscellaneous Topics",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_11"
    },
    {
        "id": 26532,
        "title": "Decision letter for \"Discovering mechanisms for materials microstructure optimization via reinforcement learning of a generative model\"",
        "authors": "",
        "published": "2022-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/aca004/v2/decision1"
    },
    {
        "id": 26533,
        "title": "Reinforcement Learning for Mixed Autonomy Intersections",
        "authors": "Zhongxia Yan, Cathy Wu",
        "published": "2021-9-19",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9565000"
    },
    {
        "id": 26534,
        "title": "Exchangeable Input Representations for Reinforcement Learning",
        "authors": "John Mern, Dorsa Sadigh, Mykel J. Kochenderfer",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147901"
    },
    {
        "id": 26535,
        "title": "Deep Reinforcement Learning Based Trajectory Tracking Control of Snake Robot Joints",
        "authors": "Yesim  Aysel Baysal, Ismail  Hakki Altas",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4345268"
    },
    {
        "id": 26536,
        "title": "Optimizing the microstructure in open-die forgings using reinforcement learning",
        "authors": "N. REINISCH",
        "published": "2023-5-25",
        "citations": 0,
        "abstract": "Abstract. The open-die forging process can produce large workpieces with excellent material properties that can be used for heavy-duty applications like turbine shafts. The mechanical properties result from the microstructure, which in turn directly results from the process route. Since in open-die forging processes commonly hundreds of unique forming operations are carried out, numerous process routes lead to the same final geometry but produce different microstructures. This is why the prior design of an optimal pass schedule is essential to ensure good mechanical properties of open-die forgings. In the past reinforcement learning (RL) was already used to design optimized pass schedules for open-die forging that achieve the desired geometry, utilize the available press force, and reduce the number of passes. Furthermore, the design of a single pass schedule only took a few seconds which creates opportunities for the use of RL in control systems e.g. for the microstructure. This is why in this publication an existing RL algorithm is extended so that microstructure can be included in the optimization. Within this process, a microstructure model is integrated into the RL algorithm and the reward function (defines the goal of the training process) was extended in two steps to also rate the achieved average grain size continuously dependent on the temperature of the workpiece. In addition, the RL implementation was changed to ensure the production of the desired final geometry leading to a decrease in the complexity of the optimization problem. Thus, both an improvement of the designed pass schedules and a significant reduction of the training time of the RL algorithm was achieved. ",
        "link": "http://dx.doi.org/10.21741/9781644902479-221"
    },
    {
        "id": 26537,
        "title": "Factory Simulation of Optimization Techniques Based on Deep Reinforcement Learning for Storage Devices",
        "authors": "JuBin Lim, JongPil Jeong",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this study, reinforcement learning (RL) was used in factory simulation to optimize storage devices for use in Industry 4.0 and digital twins. First, we defined an RL environment, modeled it, and validated its ability to simulate a real physical system. Subsequently, we introduced a method to calculate reward signals and apply them to the environment to ensure the alignment of the behavior of the RL agent with the task objective. The stocker simulation model was used to validate the effectiveness of RL. The model is a storage device that simulates logistics in a manufacturing production area. The results revealed that RL is a useful tool for automating and optimizing complex logistics systems and increase the applicability of RL in logistics. We proposed a novel method for creating an agent through learning using the proximal policy optimization algorithm, and the agent was optimized by configuring various learning options. The application of reinforcement learning resulted in an effectiveness of 30% to 100%, and methods can be expanded to other fields.",
        "link": "http://dx.doi.org/10.20944/preprints202308.0431.v1"
    },
    {
        "id": 26538,
        "title": "Securing Edge Computation of the Iot Device Through a Novel Reinforcement Learning Approach",
        "authors": "Anit Kumar, Dhanpratap Singh",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4542698"
    },
    {
        "id": 26539,
        "title": "Prognostic and Health Management in Ocean Energy System A Self-Healing Framework based on Reinforcement Learning",
        "authors": "Yufei Tang",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this paper, for minimizing the cost from the ocean generator power production by optimizing the operation and maintenance (O&M) policy over an infinite time horizon, while considering the uncertainty of the renewable sources and components failure behaviors, we develop a self-healing framework for ocean energy systems. It consists of three major modules: data manipulation, health assessment, and decision-making. Specifically, a graph-theoretic approach is first proposed for ocean generator health monitoring utilizing multivariate time-series data, then, reinforcement learning (RL) based technique exploits the health states of the system that provides decision support for optimal O&M management.<br>",
        "link": "http://dx.doi.org/10.36227/techrxiv.12733160"
    },
    {
        "id": 26540,
        "title": "Decoding Reinforcement Learning for newcomers",
        "authors": "Francisco Neves, Matheus F. Reis, Gustavo Andrade, A. Pedro Aguiar, Andry Maykol Pinto",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>An intelligible step-by-step Reinforcement Learning (RL) problem formulation and the availability of an easy-to-use demonstrative toolbox for students at various levels (e.g., undergraduate, bachelor, master, doctorate), researchers and educators. This tool facilitates the familiarization with the key concepts of RL, its problem formulation and implementation. The results demonstrated in this paper are produced by a Python program that is released open-source, along with other lecture materials to reduce the learning barriers in such innovative research topic in robotics.</p>\n<p>The RL paradigm is showing promising results as a generic purpose framework for solving decision-making problems (e.g., robotics, games, finance). In this work, RL is used for solving a robotics 2D navigational problem where the robot needs to avoid collisions with obstacles while aiming to reach a goal point. A navigational problem is simple and convenient for educational purposes, since the outcome is unambiguous (e.g., the goal is reached or not, a collision happened or not). Thus, the intent is to accelerate the adoption of RL techniques in the field of mobile robotics.</p>\n<p>Motivate and promote the adoption of RL techniques to solve decision-making problems, specifically in robotics. </p>\n<p>Due to a lack of accessible educational and demonstrative toolboxes concerning the field of RL, this work combines theoretical exposition with an accessible open-source graphical interactive toolbox to facilitate the apprehension.</p>\n<p>This study aims to reduce the learning barriers and inspire young students, researchers and educators to use RL as an obvious tool to solve robotics problems.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21583893.v1"
    },
    {
        "id": 26541,
        "title": "Reinforcement Learning for Multi-Scale Molecular Modeling",
        "authors": "Jun Zhang, Yaokun Lei, Yi Isaac Yang, Yi Qin Gao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Molecular simulations are widely applied in the study of chemical and bio-physical systems of interest. However, the accessible timescales of atomistic simulations are limited, and extracting equilibrium properties of systems containing rare events remains challenging. Two distinct strategies are usually adopted in this regard: either sticking to the atomistic level and performing enhanced sampling, or trading details for speed by leveraging coarse-grained models. Although both strategies are promising, either of them, if adopted individually, exhibits severe limitations. In this paper we propose a machine-learning approach to take advantage of both strategies. In this approach, simulations on different scales are executed simultaneously and benefit mutually from their cross-talks: Accurate coarse-grained (CG) models can be inferred from the fine-grained (FG) simulations; In turn, FG simulations can be boosted by the guidance of CG models. Our method grounds on unsupervised and reinforcement learning, defined by a variational and adaptive training objective, and allows end-to-end training of parametric models. Through multiple experiments, we show that our method is efficient and flexible, and performs well on challenging chemical and bio-molecular systems.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.9640814.v2"
    },
    {
        "id": 26542,
        "title": "Hierarchical Reinforcement Learning Considering Stochastic Wind Disturbance for Power Line Maintenance Robot",
        "authors": "Xiaoliang Zheng, Gongping Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nRobot intelligence includes motion intelligence and cognitive intelligence. Aiming at the motion intelligence, a hierarchical reinforcement learning architecture considering stochastic wind disturbance is proposed for the decision-making of the power line maintenance robot with autonomous operation. This architecture uses the prior information of the mechanism knowledge and empirical data to improve the safety and efficiency of the robot operation. In this architecture, the high-level policy selection and the low-level motion control at global and local levels are considered comprehensively under the condition of stochastic wind disturbance. Firstly, the operation task is decomposed into three sub-policies: global obstacle avoidance, local approach and local tightening, and each sub-policy is learned. Then, a master policy is learned to select the operation sub-policy in the current state. The dual deep Q network algorithm is used for the master policy, while the deep deterministic policy gradient algorithm is used for the operation policy. In order to improve the training efficiency, the global obstacle avoidance sub-policy takes the random forest composed of dynamic environmental decision tree as the expert algorithm for imitation learning. The architecture is applied to a power line maintenance scenario, the state function and reward function of each policy are designed, and all policies are trained in an asynchronous and parallel computing environment. It is proved that this architecture can realize stable and safe autonomous operating decision for the power line maintenance robot subjected to stochastic wind disturbance.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-783306/v1"
    },
    {
        "id": 26543,
        "title": "Conclusions",
        "authors": "",
        "published": "2021-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.ch11"
    },
    {
        "id": 26544,
        "title": "Towards Run-time Efficient Hierarchical Reinforcement Learning",
        "authors": "Sasha Abramowitz, Geoff Nitschke",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec55065.2022.9870368"
    },
    {
        "id": 26545,
        "title": "Choquet Regularization for Reinforcement Learning",
        "authors": "Xia Han, Ruodu Wang, Xunyu Zhou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4193175"
    },
    {
        "id": 26546,
        "title": "Confidence optimally modulates decision policy in reinforcement learning",
        "authors": "kobe desender, Tom Verguts",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1083-0"
    },
    {
        "id": 26547,
        "title": "Automating Aircraft Scanning For Inspection With A UAV And Reinforcement Learning Technique",
        "authors": "Yufeng Sun, Ou Ma",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nVisual inspections of aircraft exterior surface are usually required in aircraft maintenance routine. It becomes a trend to use mobile robots equipped with sensors to perform automatic inspections as a replacement of manual inspections which are time-consuming and error-prone. The sensed data such as images and point cloud can be used for further defect characterization leveraging the power of machine learning and data science. In such a robotic inspection procedure, a precise digital model of the aircraft is required for planning the inspection path, however, the original CAD model of the aircraft is often inaccessible to aircraft maintenance shops. Thus, sensors such as 3D Laser scanners and RGB-D (Red, Green, Blue, and Depth) cameras are used because of their capability of generating a 3D model of an interested object in an efficient manner. This paper presents a two-stage approach of automating aircraft scanning with a UAV (Unmanned Aerial Vehicle) equipped with an RGB-D camera for reconstructing a digital replica of the aircraft when its original CAD model is not available. In the first stage, the UAVcamera system follows a predefined path to quickly scan the aircraft and generate a coarse model of the aircraft. Then, a full-coverage scanning path is computed based on the coarse model of the aircraft. In the second stage, the UAV-Camera system follows the computed path to closely scan the aircraft for generating a dense and precise model of the aircraft. We solved the Coverage Path Planning (CPP) problem for the aircraft scanning using Monte Carlo Tree Search (MCTS) which is a reinforcement learning technique. We also implemented the Max-Min Ant System (MMAS) strategy, a population-based optimization algorithm, to solve the CPP problem and demonstrate the effectiveness of our approach.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1193469/v1"
    },
    {
        "id": 26548,
        "title": "Stocks and Options Portfolio Optimisation With Reinforcement Learning",
        "authors": "Daniel Alexandre Bloch",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4682061"
    },
    {
        "id": 26549,
        "title": "Low-Quality Image Object Detection Based on Reinforcement Learning Adaptive Enhancement",
        "authors": "Ye Jiongkai, Yong Wu, Dongliang Peng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4493593"
    },
    {
        "id": 26550,
        "title": "Trajectory Planning for Hypersonic Vehicles with Reinforcement Learning",
        "authors": "Haihong Chi, Mingxin Zhou",
        "published": "2021-7-26",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9549361"
    },
    {
        "id": 26551,
        "title": "Reinforcement Learning-Based Mixture of Vision Transformers for Video Violence Recognition",
        "authors": "Hamid Mohammdi, Ehsan Nazerfard, Tahereh Firoozi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4552237"
    },
    {
        "id": 26552,
        "title": "Assessing the optimality of reinforcement learning biases using evolutionary simulations",
        "authors": "Stefano Palminteri",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1032-0"
    },
    {
        "id": 26553,
        "title": "Task Graph Offloading Via Deep Reinforcement Learning in Mobile Edge Computing",
        "authors": "Jiagang Liu, Yun Mi, Xinyu Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4604459"
    },
    {
        "id": 26554,
        "title": "Reinforcement Learning for Residential Heat Pump Operation",
        "authors": "Simon Schmitz, Karoline Brucke, Pranay Kasturi, Esmail Ansari, Peter Klement",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4715980"
    },
    {
        "id": 26555,
        "title": "Introduction",
        "authors": "",
        "published": "2021-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.ch1"
    },
    {
        "id": 26556,
        "title": "Decision letter for \"Mental health analysis for college students based on pattern recognition and reinforcement learning\"",
        "authors": "",
        "published": "2023-5-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/itl2.453/v1/decision1"
    },
    {
        "id": 26557,
        "title": "Peer Review #2 of \"Quantifying the impact of non-stationarity in reinforcement learning-based traffic signal control (v0.1)\"",
        "authors": "",
        "published": "2021-5-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.575v0.1/reviews/2"
    },
    {
        "id": 26558,
        "title": "Reinforcement Learning in Financial Markets",
        "authors": "Terry Lingze Meng, Matloob Khushi",
        "published": "2019-7-28",
        "citations": 67,
        "abstract": "Recently there has been an exponential increase in the use of artificial intelligence for trading in financial markets such as stock and forex. Reinforcement learning has become of particular interest to financial traders ever since the program AlphaGo defeated the strongest human contemporary Go board game player Lee Sedol in 2016. We systematically reviewed all recent stock/forex prediction or trading articles that used reinforcement learning as their primary machine learning method. All reviewed articles had some unrealistic assumptions such as no transaction costs, no liquidity issues and no bid or ask spread issues. Transaction costs had significant impacts on the profitability of the reinforcement learning algorithms compared with the baseline algorithms tested. Despite showing statistically significant profitability when reinforcement learning was used in comparison with baseline models in many studies, some showed no meaningful level of profitability, in particular with large changes in the price pattern between the system training and testing data. Furthermore, few performance comparisons between reinforcement learning and other sophisticated machine/deep learning models were provided. The impact of transaction costs, including the bid/ask spread on profitability has also been assessed. In conclusion, reinforcement learning in stock/forex trading is still in its early development and further research is needed to make it a reliable method in this domain.",
        "link": "http://dx.doi.org/10.3390/data4030110"
    },
    {
        "id": 26559,
        "title": "Synchronous Reinforcement Learning-Based Control for Cognitive Autonomy",
        "authors": "Kyriakos G. Vamvoudakis, Nick-Marios T. Kokolakis",
        "published": "2020",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1561/9781680837452"
    },
    {
        "id": 26560,
        "title": "Algorithmic Fairness and Bias Mitigation for Clinical Machine Learning: A New Utility for Deep Reinforcement Learning",
        "authors": "Jenny Yang, Andrew A. S. Soltan, David A. Clifton",
        "published": "No Date",
        "citations": 4,
        "abstract": "AbstractAs machine learning-based models continue to be developed for healthcare applications, greater effort is needed in ensuring that these technologies do not reflect or exacerbate any unwanted or discriminatory biases that may be present in the data. In this study, we introduce a reinforcement learning framework capable of mitigating biases that may have been acquired during data collection. In particular, we evaluated our model for the task of rapidly predicting COVID-19 for patients presenting to hospital emergency departments, and aimed to mitigate any site-specific (hospital) and ethnicity-based biases present in the data. Using a specialized reward function and training procedure, we show that our method achieves clinically-effective screening performances, while significantly improving outcome fairness compared to current benchmarks and state-of-the-art machine learning methods. We performed external validation across three independent hospitals, and additionally tested our method on a patient ICU discharge status task, demonstrating model generalizability.",
        "link": "http://dx.doi.org/10.1101/2022.06.24.22276853"
    },
    {
        "id": 26561,
        "title": "Safe Exploration in Reinforcement Learning for Learning from Human Experts",
        "authors": "Jorge Ramirez, Wen Yu",
        "published": "2023-9-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aibthings58340.2023.10292489"
    },
    {
        "id": 26562,
        "title": "Rethinking Supervised Learning and Reinforcement Learning in Task-Oriented Dialogue Systems",
        "authors": "Ziming Li, Julia Kiseleva, Maarten de Rijke",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.316"
    },
    {
        "id": 26563,
        "title": "Learning How Pedestrians Navigate: A Deep Inverse Reinforcement Learning Approach",
        "authors": "Muhammad Fahad, Zhuo Chen, Yi Guo",
        "published": "2018-10",
        "citations": 27,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2018.8593438"
    },
    {
        "id": 26564,
        "title": "Hybrid Deep Reinforcement Learning For Online Distribution Power System Optimization and Control.",
        "authors": "Nicholas Corrado, Michael Livesay, Jay Johnson, Drew Levin",
        "published": "2021-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1883503"
    },
    {
        "id": 26565,
        "title": "Deep Reinforcement Learning for Cyber Security",
        "authors": "Thanh Thi Nguyen, Vijay Janapa Reddi",
        "published": "2023-8",
        "citations": 96,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3121870"
    },
    {
        "id": 26566,
        "title": "Resource Allocation in Dynamic Spectrum Access Using Deep Reinforcement Learning and Meta Learning",
        "authors": "Nikita Mishra, Sumit Srivastava, Shivendra Nath Sharan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4040779"
    },
    {
        "id": 26567,
        "title": "Deep Reinforcement Learning for Fairness in Distributed Robotic Multi-type Resource Allocation",
        "authors": "Qinyun Zhu, Jae Oh",
        "published": "2018-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00075"
    },
    {
        "id": 26568,
        "title": "Reinforcement learning for robotic manipulation using simulated locomotion demonstrations",
        "authors": "Ozsel Kilinc, Giovanni Montana",
        "published": "2022-2",
        "citations": 2,
        "abstract": "AbstractMastering robotic manipulation skills through reinforcement learning (RL) typically requires the design of shaped reward functions. Recent developments in this area have demonstrated that using sparse rewards, i.e. rewarding the agent only when the task has been successfully completed, can lead to better policies. However, state-action space exploration is more difficult in this case. Recent RL approaches to learning with sparse rewards have leveraged high-quality human demonstrations for the task, but these can be costly, time consuming or even impossible to obtain. In this paper, we propose a novel and effective approach that does not require human demonstrations. We observe that every robotic manipulation task could be seen as involving a locomotion task from the perspective of the object being manipulated, i.e. the object could learn how to reach a target state on its own. In order to exploit this idea, we introduce a framework whereby an object locomotion policy is initially obtained using a realistic physics simulator. This policy is then used to generate auxiliary rewards, called simulated locomotion demonstration rewards (SLDRs), which enable us to learn the robot manipulation policy. The proposed approach has been evaluated on 13 tasks of increasing complexity, and can achieve higher success rate and faster learning rates compared to alternative algorithms. SLDRs are especially beneficial for tasks like multi-object stacking and non-rigid object manipulation.",
        "link": "http://dx.doi.org/10.1007/s10994-021-06116-1"
    },
    {
        "id": 26569,
        "title": "Learning Interpretable Models of Aircraft Handling Behaviour by Reinforcement Learning from Human Feedback",
        "authors": "Tom Bewley, Jonathan Lawry, Arthur Richards",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2024-1380"
    },
    {
        "id": 26570,
        "title": "Reinforcement Learning",
        "authors": "Olivier Buffet, Olivier Pietquin, Paul Weng",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-06164-7_12"
    },
    {
        "id": 26571,
        "title": "Improved Reinforcement Learning through Imitation Learning Pretraining Towards Image-based Autonomous Driving",
        "authors": "Tianqi Wang, Dong Eui Chang",
        "published": "2019-10",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iccas47443.2019.8971737"
    },
    {
        "id": 26572,
        "title": "Experience-Driven Computational Resource Allocation of Federated Learning by Deep Reinforcement Learning",
        "authors": "Yufeng Zhan, Peng Li, Song Guo",
        "published": "2020-5",
        "citations": 72,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipdps47924.2020.00033"
    },
    {
        "id": 26573,
        "title": "Behavior Acquisition on a Mobile Robot Using Reinforcement Learning With Continuous State Space",
        "authors": "Tomoyuki Arai, Yuichiro Toda, Naoyuki Kubota",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmlc48188.2019.8949181"
    },
    {
        "id": 26574,
        "title": "On Convergence Rate of Adaptive Multiscale Value Function Approximation for Reinforcement Learning",
        "authors": "Tao Li, Quanyan Zhu",
        "published": "2019-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp.2019.8918816"
    },
    {
        "id": 26575,
        "title": "Barrier Certified Safety Learning Control: When Sum-of-Square Programming Meets Reinforcement Learning",
        "authors": "Hejun Huang, Zhenglong Li, Dongkun Han",
        "published": "2022-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccta49430.2022.9966192"
    },
    {
        "id": 26576,
        "title": "Learning to Schedule Job-Shop Problems via Hierarchical Reinforcement Learning",
        "authors": "Zijun Liao, Qiwen Li, Yuanzhi Dai, Zizhen Zhang",
        "published": "2022-10-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53654.2022.9945585"
    },
    {
        "id": 26577,
        "title": "Towards continual reinforcement learning through evolutionary meta-learning",
        "authors": "Djordje Grbic, Sebastian Risi",
        "published": "2019-7-13",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3319619.3322044"
    },
    {
        "id": 26578,
        "title": "Improving the efficiency of reinforcement learning for a spacecraft powered descent with Q-learning",
        "authors": "Callum Wilson, Annalisa Riccardi",
        "published": "2021-10-4",
        "citations": 4,
        "abstract": "AbstractReinforcement learning entails many intuitive and useful approaches to solving various problems. Its main premise is to learn how to complete tasks by interacting with the environment and observing which actions are more optimal with respect to a reward signal. Methods from reinforcement learning have long been applied in aerospace and have more recently seen renewed interest in space applications. Problems in spacecraft control can benefit from the use of intelligent techniques when faced with significant uncertainties—as is common for space environments. Solving these control problems using reinforcement learning remains a challenge partly due to long training times and sensitivity in performance to hyperparameters which require careful tuning. In this work we seek to address both issues for a sample spacecraft control problem. To reduce training times compared to other approaches, we simplify the problem by discretising the action space and use a data-efficient algorithm to train the agent. Furthermore, we employ an automated approach to hyperparameter selection which optimises for a specified performance metric. Our approach is tested on a 3-DOF powered descent problem with uncertainties in the initial conditions. We run experiments with two different problem formulations—using a ‘shaped’ state representation to guide the agent and also a ‘raw’ state representation with unprocessed values of position, velocity and mass. The results show that an agent can learn a near-optimal policy efficiently by appropriately defining the action-space and state-space. Using the raw state representation led to ‘reward-hacking’ and poor performance, which highlights the importance of the problem and state-space formulation in successfully training reinforcement learning agents. In addition, we show that the optimal hyperparameters can vary significantly based on the choice of loss function. Using two sets of hyperparameters optimised for different loss functions, we demonstrate that in both cases the agent can find near-optimal policies with comparable performance to previously applied methods.",
        "link": "http://dx.doi.org/10.1007/s11081-021-09687-z"
    },
    {
        "id": 26579,
        "title": "Multi-rotor Robot Learning to Fly in a Bio-inspired Way Using Reinforcement Learning",
        "authors": "Amir Ramezani Dooraki, Deok-Jin Lee",
        "published": "2019-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/urai.2019.8768681"
    },
    {
        "id": 26580,
        "title": "Reinforcement Learning with Function Approximation: From Linear to Nonlinear",
        "authors": "Jihao Long null, Jiequn Han",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4208/jml.230105"
    },
    {
        "id": 26581,
        "title": "All by Myself: Learning individualized competitive behavior with a contrastive reinforcement learning optimization",
        "authors": "Pablo Barros, Alessandra Sciutti",
        "published": "2022-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.03.013"
    },
    {
        "id": 26582,
        "title": "Reinforcement learning for guiding optimization processes in optical design",
        "authors": "Cailing Fu, Jochen Stollenwerk, Carlo Holly",
        "published": "2022-10-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2632425"
    },
    {
        "id": 26583,
        "title": "An FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning",
        "authors": "Hirohisa Watanabe, Mineto Tsukada, Hiroki Matsutani",
        "published": "2021-6",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipdpsw52791.2021.00022"
    },
    {
        "id": 26584,
        "title": "Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning",
        "authors": "Yufei Wang",
        "published": "2020-6",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4208/cicp.oa-2020-0194"
    },
    {
        "id": 26585,
        "title": "Reinforcement learning building control approach harnessing imitation learning",
        "authors": "Sourav Dey, Thibault Marzullo, Xiangyu Zhang, Gregor Henze",
        "published": "2023-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.egyai.2023.100255"
    },
    {
        "id": 26586,
        "title": "Resources",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5_8"
    },
    {
        "id": 26587,
        "title": "Risk-Sensitive Reinforcement Learning via Policy Gradient Search",
        "authors": "Prashanth L. A., Michael C. Fu",
        "published": "2022",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1561/2200000091"
    },
    {
        "id": 26588,
        "title": "Taxonomy",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5_3"
    },
    {
        "id": 26589,
        "title": "MLPro 1.0 - Standardized reinforcement learning and game theory in Python",
        "authors": "Detlef Arend, Steve Yuwono, Mochammad Rizky Diprasetya, Andreas Schwung",
        "published": "2022-9",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2022.100341"
    },
    {
        "id": 26590,
        "title": "Societal Biases Reinforcement Through Machine Learning – A Credit Scoring Perspective",
        "authors": "Bertrand Hassani",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3625691"
    },
    {
        "id": 26591,
        "title": "Reinforcement Learning/Bestärkendes Lernen",
        "authors": "Stefan Richter",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-662-59354-7_8"
    },
    {
        "id": 26592,
        "title": "USAK  METHOD FOR THE REINFORCEMENT LEARNING",
        "authors": "Mykhailo Novotarskyi, Valentin Kuzmich",
        "published": "2020-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.20535/2708-4930.1.2020.216042"
    },
    {
        "id": 26593,
        "title": "Deep Reinforcement Learning Based Real-Time Open-Pit Truck Dispatching System",
        "authors": "Roberto Noriega, Yashar Pourrahimian, Hooman Askari-Nasab",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4408257"
    },
    {
        "id": 26594,
        "title": "Single-point wind forecasting methods based on reinforcement learning",
        "authors": "Hui Liu",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-823706-9.00005-3"
    },
    {
        "id": 26595,
        "title": "An Efficient Reinforcement Learning Approach for Goal-Based Wealth Management",
        "authors": "Jinshan Zhang, Chengquan Wan, Ming Chen, Hengjiang Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4403929"
    },
    {
        "id": 26596,
        "title": "Reinforcement Learning Swarm of Self-Organizing Unmanned Surface Vehicles with Unavailable Dynamics",
        "authors": "Ning Wang, Yongjin Liu, Mingqian Lu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4506779"
    },
    {
        "id": 26597,
        "title": "LEARNING REINFORCEMENT IN NUMERICAL ANALYSIS BY MEANS OF FLIPPED CLASSROOM",
        "authors": "Fernando Sánchez Lasheras",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/iceri.2019.2327"
    },
    {
        "id": 26598,
        "title": "Introduction",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_1"
    },
    {
        "id": 26599,
        "title": "Deep-Reinforcement Learning for Trajectory Design of Multiple UAV-aided Access Points in Presence of Mobile Users",
        "authors": "Nipun Agarwal",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Next generation communication networks promise seamless connectivity. This mandates high user coverage. Determining optimal access point locations is essential to maximize the total user coverage. However, users are mobile in real-time. Deployment of unmanned aerial vehicle access points (UAPs) has been proposed to improve the coverage. However, the onboard battery in UAPs pose energy limitations. This paper derives the UAP trajectory with optimal energy planning using a deep reinforcement learning approach.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23989092.v2"
    },
    {
        "id": 26600,
        "title": "Fabricatio-Rl: A Reinforcement Learning Simulation Framework For Production Scheduling",
        "authors": "Alexandru Rinciog, Anne Meyer",
        "published": "2021-12-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc52266.2021.9715366"
    },
    {
        "id": 26601,
        "title": "MA2CL:Masked Attentive Contrastive Learning for Multi-Agent Reinforcement Learning",
        "authors": "Haolin Song, Mingxiao Feng, Wengang Zhou, Houqiang Li",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Recent approaches have utilized self-supervised auxiliary tasks as representation learning to improve the performance and sample efficiency of vision-based reinforcement learning algorithms in single-agent settings. However, in multi-agent reinforcement learning (MARL), these techniques face challenges because each agent only receives partial observation from an environment influenced by others, resulting in correlated observations in the agent dimension. So it is necessary to consider agent-level information in representation learning for MARL. In this paper, we propose an effective framework called Multi-Agent Masked Attentive Contrastive Learning (MA2CL), which encourages learning representation to be both temporal and agent-level predictive by reconstructing the masked agent observation in latent space. Specifically, we use an attention reconstruction model for recovering and the model is trained via contrastive learning. MA2CL allows better utilization of contextual information at the agent level, facilitating the training of MARL agents for cooperation tasks. Extensive experiments demonstrate that our method significantly improves the performance and sample efficiency of different MARL algorithms and outperforms other methods in various vision-based and state-based scenarios.",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/470"
    },
    {
        "id": 26602,
        "title": "Communication efficiency enhanced federated learning derived from quantum reinforcement learning for retrosynthesis",
        "authors": "Junjie Hu, Xiangyu Li, Dan-Dan Liu, Shiyi Wang, Yingying Fang, Peng Wu, Guang Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "The combination of parametric quantum circuits and density matrix coding can significantly reduce the number of parameters in artificial neural networks. The reduction in the number of model parameters helps to improve the commu- nication efficiency when training deep learning models under federated learning architectures. In this study, we showcase the enhanced communication efficiency achieved in federated learning by utilizing quantum neural networks in the context of the molecular inverse synthesis task within reinforcement learning. Specifically, we consider the federated learning task on the reinforcement learning-based retrosynthesis. we adopted the USPTO-50k chemical reaction dataset. the MLP and quantum neural network are used as the agent of the reinforcement learning algorithm, respectively. Enhancements in communica- tion efficiency stem from the capacity of encoded quantum states within quantum neural networks to effectively represent data. All instances are additionally situated within the framework of ligand molecules associated with Tau proteins.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2024-rqgvt"
    },
    {
        "id": 26603,
        "title": "CandyRL: A Hybrid Reinforcement Learning Model for Gameplay",
        "authors": "Sara Karimi, Sahar Asadi, Francesco Lorenzo, Amir H. Payberah",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00098"
    },
    {
        "id": 26604,
        "title": "Reinforcement Learning for Data Science",
        "authors": "Jonatan Barkan, Michal Moran, Goren Gordon",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-24628-9_24"
    },
    {
        "id": 26605,
        "title": "Inverse Reinforcement Learning Based Stochastic Driver Behavior Learning",
        "authors": "Mehmet F. Ozkan, Abishek J. Rocque, Yao Ma",
        "published": "2021",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2021.11.283"
    },
    {
        "id": 26606,
        "title": "Autonomous Planetary Landing via Deep Reinforcement Learning and Transfer Learning",
        "authors": "Giulia Ciabatti, Shreyansh Daftry, Roberto Capobianco",
        "published": "2021-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvprw53098.2021.00231"
    },
    {
        "id": 26607,
        "title": "Building Safe and Stable DNN Controllers using Deep Reinforcement Learning and Deep Imitation Learning",
        "authors": "Xudong He",
        "published": "2022-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/qrs57517.2022.00083"
    },
    {
        "id": 26608,
        "title": "Adaptive Client Model Update with Reinforcement Learning in Synchronous Federated Learning",
        "authors": "Zirou Pan, Huan Geng, Linna Wei, Wei Zhao",
        "published": "2022-11-30",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itnac55475.2022.9998360"
    },
    {
        "id": 26609,
        "title": "Learning-to-Dispatch: Reinforcement Learning Based Flight Planning under Emergency",
        "authors": "Kai Zhang, Yupeng Yang, Chengtao Xu, Dahai Liu, Houbing Song",
        "published": "2021-9-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9564684"
    },
    {
        "id": 26610,
        "title": "Deep Reinforcement Learning",
        "authors": "Paul Fergus, Carl Chalmers",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-04420-5_11"
    },
    {
        "id": 26611,
        "title": "Learning Distributed Cooperative Policies for Security Games via Deep Reinforcement Learning",
        "authors": "Hassam Ullah Sheikh, Mina Razghandi, Ladislau Boloni",
        "published": "2019-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/compsac.2019.00075"
    },
    {
        "id": 26612,
        "title": "Learning Interpretable Negation Rules via Weak Supervision at Document Level: A Reinforcement Learning Approach",
        "authors": "Nicolas Pröllochs, Stefan Feuerriegel, Dirk Neumann",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/n19-1038"
    },
    {
        "id": 26613,
        "title": "Autonomous 3-D UAV Localization Using Cellular Networks: Deep Supervised Learning Versus Reinforcement Learning Approaches",
        "authors": "Ghada Afifi, Yasser Gadallah",
        "published": "2021",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3126775"
    },
    {
        "id": 26614,
        "title": "Competitive deep reinforcement learning for robot control",
        "authors": "Lin Jiang, Weiyue Zhang, Jianyi Zhu",
        "published": "2023-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2675162"
    },
    {
        "id": 26615,
        "title": "One Shot Spatial Learning through Replay in a Hippocampus-Inspired Reinforcement Learning Model",
        "authors": "Adedapo Alabi, Ali A. Minai, Dieter Vanderelst",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207435"
    },
    {
        "id": 26616,
        "title": "Flappy Bird Game Based on Reinforcement Learning Q-Learning Algorithm",
        "authors": "Zhenni He, Yi Zhang, Dingle Zhao",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "In the field of Artificial Intelligence (AI), game AI is becoming more and more important, and the human-machine training of games is gradually driving the development of the game field. Among them, Flappy Bird is one of game which can controlled by an AI, which deserves more attention. In this work, we used Q-Learning as our main algorithm of the AI. In the flappy bird AI, the algorithm of Q-learning is used for giving the feedback through the environment which corresponding reward according to the actions of the agent. By using this method and after the training of the flappy bird AI, we can get the scores that are much more than human’s record. The highest record of the flappy bird AI is 4, 083. The average score for human is about only 100, but in the flappy bird AI, the score can easily be more than 1, 000. According to all the work we did and all the result we got, we can see that the comparison between the AI and human. In the game area, AI did much better than human in most game. That is the reason that much research is focusing on developing game AI to help us getting deeper in the game field since it is more efficient to use.",
        "link": "http://dx.doi.org/10.54097/hset.v34i.5475"
    },
    {
        "id": 26617,
        "title": "Deep Learning, Reinforcement Learning, and the Rise of Intelligent Systems",
        "authors": "",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9"
    },
    {
        "id": 26618,
        "title": "Learning the Dynamic Environment of an Original Game Using Hierarchical Reinforcement Learning Methods",
        "authors": "Vlad Batalan, Florin Leon",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icstcc59206.2023.10308465"
    },
    {
        "id": 26619,
        "title": "Predictive learning model in cognitive radio using reinforcement learning",
        "authors": "Sharada Tubachi, Mithra Venkatesan, A. V. Kulkarni",
        "published": "2017-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpcsi.2017.8391775"
    },
    {
        "id": 26620,
        "title": "Learning from Learners: Adapting Reinforcement Learning Agents to be Competitive in a Card Game",
        "authors": "Pablo Barros, Ana Tanevska, Alessandra Sciutti",
        "published": "2021-1-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr48806.2021.9412807"
    },
    {
        "id": 26621,
        "title": "QLGR: A Q-learning-based Geographic FANET Routing Algorithm Based on Multi-agent Reinforcement Learning",
        "authors": "",
        "published": "2021-11-30",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2021.11.020"
    },
    {
        "id": 26622,
        "title": "Research on Portfolio Optimization Based on Deep Reinforcement Learning",
        "authors": "Zhengyan Wang, Shurui Jin, Wen Li",
        "published": "2022-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlbdbi58171.2022.00081"
    },
    {
        "id": 26623,
        "title": "Building Adaptive Tutoring Model Using Artificial Neural Networks and Reinforcement Learning",
        "authors": "Giuseppe Fenza, Francesco Orciuoli, Demetrios G. Sampson",
        "published": "2017-7",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icalt.2017.124"
    },
    {
        "id": 26624,
        "title": "Deep Reinforcement Learning for Structural Model Updating Using Transfer Learning Mechanism",
        "authors": "Issac Kwok-Tai Pang, Yuqing Gao, Khalid M. Mosalam",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1061/9780784485231.044"
    },
    {
        "id": 26625,
        "title": "Parameter optimization design of MFAC based on Reinforcement Learning",
        "authors": "Shida Liu, Xiongbo Jia, Honghai Ji, Lingling Fan",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10167283"
    },
    {
        "id": 26626,
        "title": "Decision-Making and Learning in an Unknown Environment",
        "authors": "Uwe Lorenz",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-09030-1_4"
    },
    {
        "id": 26627,
        "title": "Deep Reinforcement Learning in Cloud Elasticity Through Offline Learning and Return Based Scaling",
        "authors": "Miltiadis Chrysopoulos, Ioannis Konstantinou, Nectarios Koziris",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cloud60044.2023.00012"
    },
    {
        "id": 26628,
        "title": "Learning of Dynamic Models",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch4"
    },
    {
        "id": 26629,
        "title": "Shielded Reinforcement Learning: A review of reactive methods for safe learning",
        "authors": "Haritz Odriozola-Olalde, Maider Zamalloa, Nestor Arana-Arexolaleiba",
        "published": "2023-1-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sii55687.2023.10039301"
    },
    {
        "id": 26630,
        "title": "Faster learning from slow features: The temporal coherence prior in human reinforcement learning",
        "authors": "Noa Hedrich, Sam Hall-McMaster, Eric Schulz, Nicolas Schuck",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1471-0"
    },
    {
        "id": 26631,
        "title": "Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning",
        "authors": "Charles Schaff, David Yunis, Ayan Chakrabarti, Matthew R. Walter",
        "published": "2019-5",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8793537"
    },
    {
        "id": 26632,
        "title": "Automatic View Generation with Deep Learning and Reinforcement Learning",
        "authors": "Haitao Yuan, Guoliang Li, Ling Feng, Ji Sun, Yue Han",
        "published": "2020-4",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icde48307.2020.00133"
    },
    {
        "id": 26633,
        "title": "Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning",
        "authors": "Laura Smith, Ilya Kostrikov, Sergey Levine",
        "published": "2023-7-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.056"
    },
    {
        "id": 26634,
        "title": "Integrating contrastive learning with dynamic models for reinforcement learning from images",
        "authors": "Bang You, Oleg Arenz, Youping Chen, Jan Peters",
        "published": "2022-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2021.12.094"
    },
    {
        "id": 26635,
        "title": "Pursuit-evasion with Decentralized Robotic Swarm in Continuous State Space and Action Space via Deep Reinforcement Learning",
        "authors": "Gurpreet Singh, Daniel Lofaro, Donald Sofge",
        "published": "2020",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008971502260233"
    },
    {
        "id": 26636,
        "title": "Research on Path Planning Strategy of Rescue Robot Based on Reinforcement Learning",
        "authors": "Ying-Ming Shi Ying-Ming Shi, Zhiyuan Zhang Ying-Ming Shi",
        "published": "2022-6",
        "citations": 1,
        "abstract": "\n                        <p>How rescue robots reach their destinations quickly and efficiently has become a hot research topic in recent years. Aiming at the complex unstructured environment faced by rescue robots, this paper proposes an artificial potential field algorithm based on reinforcement learning. Firstly, use the traditional artificial potential field method to perform basic path planning for the robot. Secondly, in order to solve the local minimum problem in planning and improve the robot’s adaptive ability, the reinforcement learning algorithm is run by fixing preset parameters on the simulation platform. After intensive training, the robot continuously improves the decision-making ability of crossing typical concave obstacles. Finally, through simulation experiments, it is concluded that the rescue robot can combine the artificial potential field method and reinforcement learning to improve the ability to adapt to the environment, and can reach the destination with the optimal route.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/199115992022063303015"
    },
    {
        "id": 26637,
        "title": "An Empirical Research on the Investment Strategy of Stock Market based on Deep Reinforcement Learning model",
        "authors": "Yuming Li, Pin Ni, Victor Chang",
        "published": "2019",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007722000520058"
    },
    {
        "id": 26638,
        "title": "Early Prediction of Human Action by Deep Reinforcement Learning",
        "authors": "Hareesh Devarakonda, Snehasis Mukherjee",
        "published": "2021-7-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ncc52529.2021.9530126"
    },
    {
        "id": 26639,
        "title": "Reinforcement Learning of Protein Conformational Ensemble",
        "authors": "Jiangyan Feng",
        "published": "2019-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.bpj.2018.11.1022"
    },
    {
        "id": 26640,
        "title": "Performance on Learning to Associate a Stimulus with Positive Reinforcement",
        "authors": "R. A. Boakes",
        "published": "2021-9-16",
        "citations": 40,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003150404-4"
    },
    {
        "id": 26641,
        "title": "Shaping Model-Free Reinforcement-Learning with Model-Based Pseudorewards",
        "authors": "Paul Krueger, Thomas Griffiths",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2018.1191-0"
    },
    {
        "id": 26642,
        "title": "Open quantum system control based on reinforcement learning",
        "authors": "Peng Wei, Na Li, Zairong Xi",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8865335"
    },
    {
        "id": 26643,
        "title": "A Reinforcement Learning Approach to Optimal Execution",
        "authors": "Ciamac C. Moallemi, Muye Wang",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3897442"
    },
    {
        "id": 26644,
        "title": "Prototype Reinforcement for Few-Shot Learning",
        "authors": "Liheng Xu, Qian Xie, Baoqing Jiang, Jiashuo Zhang",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9326820"
    },
    {
        "id": 26645,
        "title": "Deep Reinforcement Learning for Optimal Sailing Upwind",
        "authors": "Takumi Suda, Daniel Nikovski",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892369"
    },
    {
        "id": 26646,
        "title": "Mapless navigation based on continuous deep reinforcement learning",
        "authors": "Xing Chen, Lumei Su, Houde Dai",
        "published": "2021-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9727354"
    },
    {
        "id": 26647,
        "title": "Artificial Conversational Agent using Robust Adversarial Reinforcement Learning",
        "authors": "Isha Wadekar",
        "published": "2021-1-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccci50826.2021.9402336"
    },
    {
        "id": 26648,
        "title": "Dynamic Asset-Allocation and Consumption",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-8"
    },
    {
        "id": 26649,
        "title": "Multi-Agent Collision Avoidance with Provident Agents using Deep-Reinforcement Learning",
        "authors": "Mohammad Bahrami Karkevandi, Samaneh Hosseini Semnani",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAutonomous path planning is becoming more demanding, as the number of robotic agents in pedestrian-rich environments is gradually increasing. Collision-free navigation is an essential requirement for autonomous agents in a pedestrian-rich environment. In this paper, we will build upon the GA3C-CADRL algorithm to train agents that take the near future into consideration while moving around the environment. Specifically, we will introduce a new reward function and a novel rewarding mechanism to train provident agents that always prioritize finding a collision-free path. Agents that are trained with the new method will be shown to be more cautious of their surrounding environment and tend to avoid actions that may be seen as threatening by pedestrians. We will show that the new proposed algorithm outperforms the previous state-of-the-art algorithm, GA3C-CADRL, in terms of successful navigation throughout the entire test set.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2797884/v1"
    },
    {
        "id": 26650,
        "title": "Regret Analysis in Deterministic Reinforcement Learning",
        "authors": "Damianos Tranos, Alexandre Proutiere",
        "published": "2021-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9682972"
    },
    {
        "id": 26651,
        "title": "Wireless Power Control via Meta-Reinforcement Learning",
        "authors": "Ziyang Lu, M. Cenk Gursoy",
        "published": "2022-5-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45855.2022.9839179"
    },
    {
        "id": 26652,
        "title": "A novel observer-based reinforcement learning for uncertain nonlinear systems with disturbances",
        "authors": "Dexin Zhang, Xiaoping Shi, Shenmin Song",
        "published": "No Date",
        "citations": 0,
        "abstract": "This study proposes an observer-based reinforcement learning(RL) control\nscheme for uncertain nonlinear systems subject to various external\ndisturbances. The proposed approach regards the total uncertainty\nestimated by the extended state observer (ESO) as potential model\ninformation, which is incorporated into the known part of the system\ndynamics. Based on the updated known dynamics, an RL structure is\nconstructed to approximate the optimal solution of the HJB equation\nwithout the persistence of the excitation (PE) condition. The\nconvergence of the proposed policy to a neighborhood of the optimal\npolicy is proven, and the stability of the system states is guaranteed.\nThe comparative simulation results demonstrate improved performance with\na significantly reduced cost of the developed controller, and the\nsensitivity of control gain in the input channel is also relaxed.",
        "link": "http://dx.doi.org/10.22541/au.167766515.57662539/v1"
    },
    {
        "id": 26653,
        "title": "Exploring the Potential of Reinforcement Learning in Intelligent Robotics",
        "authors": "",
        "published": "2022-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.59121/kcisr22120001"
    },
    {
        "id": 26654,
        "title": "Decentralized Multi-Agent Deep Reinforcement Learning: A Competitive-Game Perspective",
        "authors": "Marc Espinós Longa, Antonios Tsourdos, Inalhan Gokhan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDeep reinforcement learning (DRL) has been widely studied in single agent learning but require further development and understanding in the multi-agent field.  As one of the most complex swarming settings, competitive learning evaluates the performance of multiple teams of agents cooperating to achieve certain goals while surpassing the rest of group candidates.  Such dynamical complexity makes the multi-agent problem hard to solve even for niche DRL methods. Within a competitive framework, we study state-of-the-art actor-critic and Q algorithms and analyze in depth their variants (e.g., prioritization, dual networks, etc.) in terms of performance and convergence. For completeness of discussion, we present and assess an asynchronous and prioritized version of proximal policy optimization actor-critic technique (P3O) against the other benchmarks.  Results prove that Q-based approaches are more robust and reliable than actor-critic configurations for the given setting. In addition, we suggest incorporating local team communication and combining DRL with direct search optimization to improve learning, especially in challenging scenarios with partial observations.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2065000/v1"
    },
    {
        "id": 26655,
        "title": "Reinforcement Learning for Few-Shot Text Generation Adaptation",
        "authors": "Pengsen Cheng, Jinqiao Dai, Jiamiao Liu, Jiayong Liu, Peng Jia",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4342101"
    },
    {
        "id": 26656,
        "title": "The pursuit of happiness: A reinforcement learning perspective on habituation and comparisons",
        "authors": "Rachit Dubey, Tom Griffiths, Peter Dayan",
        "published": "No Date",
        "citations": 0,
        "abstract": "In evaluating our choices, we often suffer from two tragic relativities. First, when our lives change for the better, we rapidly habituate to the higher standard of living. Second, we cannot escape comparingourselves to various relative standards. Habituation and comparisons can be very disruptive to decision-making and happiness, and till date, it remains a puzzle why they have come to be a part of cognition in the first place. Here, we present computational evidence that suggests that these features might play an important role in promoting adaptive behavior. Using the framework of reinforcement learning, we explore the benefit of employing a reward function that, in addition to the reward provided by the underlying task, also depends on prior expectations and relative comparisons. We find that while agents equipped with this reward function are less happy, they learn faster and significantly outperform standard reward-based agents in a wide range of environments. Specifically, we find that relative comparisons speed up learning by providing an exploration incentive to the agents, and prior expectations serve as a useful aid to comparisons, especially in sparsely-rewarded and non-stationary environments. Our simulations also reveal potential drawbacks of this reward function and show that agents perform suboptimally when comparisons are left unchecked and when there are too many similar options. Together, our results help explain why we are prone to becoming trapped in a cycle of never-ending wants and desires, and may shed light on psychopathologies such as depression, materialism, and overconsumption.",
        "link": "http://dx.doi.org/10.31234/osf.io/8jd2x"
    },
    {
        "id": 26657,
        "title": "Optimal State Estimation Using Model-Free Reinforcement Learning",
        "authors": "Haoran Ma, Ying Yang, Dingguo Liang",
        "published": "2021-10-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728004"
    },
    {
        "id": 26658,
        "title": "Optimal Market Making by Reinforcement Learning",
        "authors": "Matias Selser, Javier Kreiner, Manuel Maurette",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3829984"
    },
    {
        "id": 26659,
        "title": "A traffic light control method based on multi-agent deep reinforcement learning algorithm",
        "authors": "Dongjiang Liu, Leixiao Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAn efficient way to cope with traffic congestion is Intelligent Traffic Light Control (ITLC). Particularly, multi-agent based intelligent traffic light control algorithms are more popular and efficient than others. But there are still some problems in these algorithms. Firstly, an efficient communication mechanism is needed. As current traffic condition of an intersection can impact other intersections' future traffic condition, every agent should know the traffic condition of other intersections through communication. Thus, a new communication mechanism should be designed. By using this mechanism, traffic condition of an intersection can be passed to several relevant agents easily. And traffic condition of an intersection can be described simply and clearly. Secondly, asynchronization of message should be considered while processing received messages. As duration time of traffic lights' cycles is different from agent to agent and message sending event happens at the end of each traffic light cycle, different agents will send message at different time. Moreover, every agent tries to process the received messages at the end of each traffic light cycle as well. So it is hard for an agent to decide which message is the latest one and the most valuable. Thirdly, a new reward calculating method should be designed. In the traditional reinforcement learning based ITLC algorithms, only queue length of congested cars or waiting time of these cars is considered while calculating reward value. But, both of them are very important for measuring the level of traffic congestion. So a new reward calculation method is needed. To solve all these problems, in this paper, a new ITLC algorithm is proposed. At the same time, a new message sending and processing method is proposed and adopted by this ITLC algorithm. The first and the second problems described above are tried to be solved by using this new method. Besides, to measure traffic congestion in a more reasonable way, a new reward calculation method is proposed and used. This method takes both waiting time and queue length into consideration.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2626589/v1"
    },
    {
        "id": 26660,
        "title": "Data-Driven Output Consensus for a Class of Discrete-Time Multiagent Systems by Reinforcement Learning Techniques",
        "authors": "Yuanshan Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4697059"
    },
    {
        "id": 26661,
        "title": "Anwendung von Reinforcement Learning in industriellen cyberphysischen Systemen",
        "authors": "David Heik, Fouad Bahrpeyma, Dirk Reichelt",
        "published": "2023-3-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33968/2023.10"
    },
    {
        "id": 26662,
        "title": "Practical Reinforcement Learning of Stabilizing Economic MPC",
        "authors": "Mario Zanon, Sebastien Gros, Alberto Bemporad",
        "published": "2019-6",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc.2019.8795816"
    },
    {
        "id": 26663,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Yixuan Sun, Krishnan Raghavan, Prasanna Balaprakash",
        "published": "2023-11-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003359593-9"
    },
    {
        "id": 26664,
        "title": "Inhibitory and excitatory mechanisms in the human cingulate-cortex support reinforcement learning",
        "authors": "Vered Bezalel, Rony Paz, Assaf Tal",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe dorsal anterior cingulate cortex (dACC) is crucial for motivation, reward- and error-guided decision-making, yet its excitatory and inhibitory mechanisms remain poorly explored in humans. In particular, the balance between excitation and inhibition (E/I), demonstrated to play a role in animal studies, is difficult to measure in behaving humans. Here, we used magnetic-resonance-spectroscopy (1H-MRS) to examine these mechanisms during reinforcement learning with three different conditions: high cognitive load (uncertainty); probabilistic discrimination learning; and a control null-condition. Subjects learned to prefer the gain option in the discrimination phase and had no preference in the other conditions. We found increased GABA levels during the uncertainty condition, suggesting recruitment of inhibitory systems during high cognitive load when trying to learn. Further, higher GABA levels during the null (baseline) condition correlated with improved discrimination learning. Finally, excitatory and inhibitory levels were correlated during high cognitive load. The result suggests that availability of dACC inhibitory resources enables successful learning. Our approach establishes a novel way to examine the contribution of the balance between excitation and inhibition to learning and motivation in behaving humans.",
        "link": "http://dx.doi.org/10.1101/318659"
    },
    {
        "id": 26665,
        "title": "Reinforcement Learning and Stochastic Optimization",
        "authors": "Warren B. Powell",
        "published": "2022-4-2",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068"
    },
    {
        "id": 26666,
        "title": "Dynamic Pricing in Ride-Hailing Intelligent Transportation Systems by Using Deep Reinforcement Learning",
        "authors": "Sajad Heydari, Elham Akhondzadeh Noughabi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4746254"
    },
    {
        "id": 26667,
        "title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming",
        "authors": "",
        "published": "2017-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2017.2655663"
    },
    {
        "id": 26668,
        "title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming",
        "authors": "",
        "published": "2017-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2017.2678899"
    },
    {
        "id": 26669,
        "title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming",
        "authors": "",
        "published": "2017-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2016.2640821"
    },
    {
        "id": 26670,
        "title": "&amp;lt;p&amp;gt;Less is more: Lightweight reinforcement learning method for traffic signal control with less observation&amp;lt;/p&amp;gt;",
        "authors": "Qiang Wu",
        "published": "2023-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26599/etsd.2023.9190021"
    },
    {
        "id": 26671,
        "title": "Artificial Neural Networks and Reinforcement Learning for Model-based Design of an Automated Vehicle Guidance System",
        "authors": "Or Yarom, Soeren Scherler, Marian Goellner, Xiaobo Liu-Henke",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008995407250733"
    },
    {
        "id": 26672,
        "title": "Instrumental lever pressing for wheel running is a bitonic function of wheel revolutions per reinforcement: Effects of constraint and automatic reinforcement",
        "authors": "W. David Pierce, Terry W. Belke, Allison F. Harris",
        "published": "2018-11",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.lmot.2018.07.001"
    },
    {
        "id": 26673,
        "title": "Reinforcement Learning Based Weighting Factors’ Real-Time Updating Scheme for the FCS Model Predictive Control to Improve the Large-Signal Stability of Inverters",
        "authors": "Xin Zhang, Jinsong He, Hao Ma, Zhixun Ma, Xiaohai Ge",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7191-4_8"
    },
    {
        "id": 26674,
        "title": "Lightweight Multi Car Dynamic Simulator for Reinforcement Learning",
        "authors": "Abhijit Majumdar, Patrick Benavidez, Mo Jamshidi",
        "published": "2018-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/wac.2018.8430473"
    },
    {
        "id": 26675,
        "title": "Policy-Based Reinforcement Learning in the Generalized Rock-Paper-Scissors Game",
        "authors": "Imre Gergely Mali, Gabriela Czibula",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-92"
    },
    {
        "id": 26676,
        "title": "Hierarchical Advantage for Reinforcement Learning in Parameterized Action Space",
        "authors": "Zhejie Hu, Tomoyuki Kaneko",
        "published": "2021-8-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog52621.2021.9619068"
    },
    {
        "id": 26677,
        "title": "Reinforcement Learning for Approximate Optimal Control",
        "authors": "Warren E. Dixon",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4471-5102-9_100063-1"
    },
    {
        "id": 26678,
        "title": "Reinforcement Learning for Continuous-Time Mean-Variance Portfolio Selection in a Regime-Switching Market",
        "authors": "Bo Wu, Lingfei Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4415531"
    },
    {
        "id": 26679,
        "title": "Multiagent Reinforcement Learning",
        "authors": "Jonathan P. How, Dong-Ki Kim, Samir Wadhwania",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-44184-5_100066"
    },
    {
        "id": 26680,
        "title": "Reinforcement Learning am Beispiel Schach",
        "authors": "Thomy Phan",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42354-018-0121-3"
    },
    {
        "id": 26681,
        "title": "MOOR: Model-based Offline Reinforcement learning for sustainable fishery management",
        "authors": "",
        "published": "2021-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36334/modsim.2021.m2.ju"
    },
    {
        "id": 26682,
        "title": "Reinforcement Learning based Trajectory Planning for Autonomous Vehicles",
        "authors": "Zhaoxu Wang, Jingzheng Tu, Cailian Chen",
        "published": "2021-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9727737"
    },
    {
        "id": 26683,
        "title": "Flow-Based Reinforcement Learning",
        "authors": "Dilini Samarasinghe, Michael Barlow, Erandi Lakshika",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3209260"
    },
    {
        "id": 26684,
        "title": "Learning to Navigate Through Reinforcement Across the Sim2Real Gap",
        "authors": "Rana Azzam, Mohamad Chehadeh, Oussama Abdulhay, Igor Boiko, Yahya Zweiri",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>Amid the recent advances in robotics and machine learning, unmanned aerial vehicles (UAVs) have shown evident proliferation across various applications. Consequently, the involvement of UAVs in populated environments has progressively become inevitable, putting forward stringent safety and security measures. In this work, we develop a deep reinforcement learning-based UAV-navigation approach that blends decision making with behavioral intelligence. In particular, a reinforcement learning (RL) agent is trained to instruct the UAV on how to accomplish a goal-oriented task, while assuring the safety of the UAV and its surroundings. Upon arriving at the goal position, the RL agent slows the UAV down preparing it for landing. The safety of the UAV and the environment are attained through a robust collision avoidance capability, embedded into the RL-based navigation system and considers both static and dynamic obstacles in the environment. Training is exclusively carried out in simulation, where a high fidelity UAV controller model is used to perform the simulated maneuver. The proposed approach was tested in simulation and then shown to directly transfer to reality without explicit sim2real gap transfer techniques. Experimental results demonstrated the agent's capability to achieve the navigation task with 90% success rate.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20138960"
    },
    {
        "id": 26685,
        "title": "Imitation Reinforcement Learning with Vision and Navigation for Autonomous Driving",
        "authors": "Lei He, Mingyue Ba, Yiren Wang, Ling Han",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nAutonomous urban driving navigation remains an ongoing\nchallenge, with ample scope for improvement, particularly in navigating\nthrough unfamiliar and complex environments. The images captured by\ncameras provide a wealth of environmental information; however,\naccurately determining the positions of obstacles within these images\ncan be adversely affected by inclement weather conditions such as rain,\nsnow, or haze. In response to these challenges, this paper presents a\nhierarchical framework named CNS-DDPG. CNS, which stands for Conditional\nImitation Learning, involves the fusion of navigation state information\nwith global path and vehicle state data. DDPG, or Deterministic Policy\nGradient, is used for subsequent reinforcement learning. By carefully\nweighing the strengths and weaknesses of the image and perception\nmodule, our framework compensates for visual information captured by the\ncamera by incorporating navigation state data. This design allows our\nmodel to perform effectively even in adverse weather conditions.\nHowever, the limitations of imitation learning, particularly the\nscarcity of diverse training data, prompted us to employ the\nreinforcement learning method DDPG in the second stage of training. This\nstage benefits from the learned weights of the pre-trained and optimal\nCNS model. This approach reduces the reliance on imitation learning data\nand mitigates the challenge of low exploration efficiency associated\nwith randomly initialized weights in reinforcement learning.\nAdditionally, we implement image enhancement techniques to mitigate\noverfitting associated with simple image types. To evaluate the\neffectiveness of our approach, we conducted experiments using the CARLA\ndriving benchmark for urban driving. The car was controlled by a\nRaspberry Pi 4B, which was trained to navigate through an experimental\narea. Our experiments reveal that CNS-DDPG exhibits remarkable\ngeneralization capabilities, particularly in unfamiliar environments and\nchallenging navigation tasks.\n",
        "link": "http://dx.doi.org/10.22541/au.169957038.84697482/v1"
    },
    {
        "id": 26686,
        "title": "AIR SPRING CONTROLLED BY REINFORCEMENT LEARNING ALGORITHM",
        "authors": "J. Rágulík, M. Sivčák",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21495/5896-3-428"
    },
    {
        "id": 26687,
        "title": "On Safety and Time Efficiency Enhancement of Robot Navigation in Crowded Environment utilizing Deep Reinforcement Learning",
        "authors": "Sunil Srivatsav Samsani",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>The evolution of social robots has increased with the advent of recent artificial intelligence techniques. Alongside humans, social robots play active roles in various household and industrial applications. However, the safety of humans becomes a significant concern when robots navigate in a complex and crowded environment. In literature, the safety of humans in relation to social robots has been addressed by various methods; however, most of these methods compromise the time efficiency of the robot. For robots, safety and time-efficiency are two contrast elements where one dominates the other. To strike a balance between them, a multi-reward formulation in the reinforcement learning framework is proposed, which improves the safety together with time-efficiency of the robot. The multi-reward formulation includes both positive and negative rewards that encourage and punish the robot, respectively. The proposed reward formulation is tested on state-of-the-art methods of multi-agent navigation. In addition, an ablation study is performed to evaluate the importance of individual rewards. Experimental results signify that the proposed approach balances the safety and the time-efficiency of the robot while navigating in a crowded environment.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.17493605"
    },
    {
        "id": 26688,
        "title": "Mitigating Cowardice for Reinforcement Learning Agents in Combat Scenarios",
        "authors": "Steve Bakos, Heidar Davoudi",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog51982.2022.9893546"
    },
    {
        "id": 26689,
        "title": "Reinforcement learning of adaptive control strategies",
        "authors": "Leslie Held, Luc Vermeylen, David Dignath, Wim Notebaert, Ruth Krebs, Senne Braem",
        "published": "No Date",
        "citations": 0,
        "abstract": "Humans can up- or downregulate the degree to which they rely on task information for goal directed behaviour, a process often referred to as cognitive control. Adjustments in cognitive control are traditionally studied in response to experienced or expected task-rule conflict. However, recent theories suggest that people can also learn to adapt control settings through reinforcement. Across three preregistered task switching experiments (n=415), we selectively rewarded correct performance on trials with either more (incongruent) or less (congruent) task-rule conflict. Results confirmed the hypothesis that people rewarded more on incongruent trials showed smaller task-rule congruency effects, thus optimally adapting their control settings to the reward scheme. Using drift diffusion modelling, we further show that this reinforcement of cognitive control may occur through conflict-dependent within-trial adjustments of response thresholds after conflict detection. Together, our findings suggest that, while people remain more efficient at learning stimulus-response associations through rewards, they can similarly learn cognitive control strategies through reinforcement.",
        "link": "http://dx.doi.org/10.31234/osf.io/d8p9e"
    },
    {
        "id": 26690,
        "title": "Adaptive Strategy Templates Using Deep Reinforcement Learning for Multi-Issue Bilateral Negotiation",
        "authors": "Pallavi Bagga, Nicola Paoletti, Kostas Stathis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4531611"
    },
    {
        "id": 26691,
        "title": "Optimal detection task allocation: A reinforcement learning approach",
        "authors": "Qilong Huang, Qing Bu, Ziyi Qin",
        "published": "2017-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2017.8242794"
    },
    {
        "id": 26692,
        "title": "MoleGuLAR: Molecule Generation using Reinforcement Learning with Alternating Rewards",
        "authors": "Manan Goel, Shampa Raghunathan, Siddhartha Laghuvarapu, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Design of new inhibitors for novel targets is a very important problem especially in the current scenario with the world being plagued by COVID-19. Conventional approaches undertaken to this end, like, high-throughput virtual screening require extensive combing through existing datasets in the hope of finding possible matches. In this study we propose a computational strategy for de novo generation of molecules with high binding affinities to the specified target. A deep generative model is built using a stack augmented recurrent neural network for initially generating drug like molecules and then it is optimized using reinforcement learning to start generating molecules with desirable properties--primarily the binding affinity. The reinforcement learning section of the pipeline is further extended to multi-objective optimization showcasing the model's ability to generate molecules with a wide variety of properties desirable for drug like molecules, like, LogP, Quantitative Estimate of Drug Likeliness etc.. For multi-objective optimization, we have devised a novel strategy in which the property being used to calculate the reward is changed periodically. In comparison to the conventional approach of taking a weighted sum of all rewards, this strategy shows enhanced ability to generate a significantly higher number of molecules with desirable properties.",
        "link": "http://dx.doi.org/10.33774/chemrxiv-2021-cg9p8"
    },
    {
        "id": 26693,
        "title": "A Self-Comparison Based Reinforcement Learning Method for Dynamic Traveling Salesman Problem",
        "authors": "He Ren, Rui Zhong, Haichao Gui",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4566661"
    },
    {
        "id": 26694,
        "title": "Double Deep Reinforcement Learning",
        "authors": "Josué Kiefer, Klaus Dorer",
        "published": "2023-4-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icarsc58346.2023.10129640"
    },
    {
        "id": 26695,
        "title": "Reinforcement learning in car control: A brief survey",
        "authors": "Dawid Kalandyk",
        "published": "2021-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wzee54157.2021.9576838"
    },
    {
        "id": 26696,
        "title": "Reinforcement Learning for Solving Control Problems in Robotics",
        "authors": "Askhat Diveev, Elena Sofronova, Sergey Konstantinov, Viktoria Moiseenko",
        "published": "2023-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/engproc2023033029"
    },
    {
        "id": 26697,
        "title": "Deep Reinforcement Learning for Self-Configurable NoC",
        "authors": "Md Farhadur Reza",
        "published": "2020-9-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/socc49529.2020.9524761"
    },
    {
        "id": 26698,
        "title": "Destabilizing Attack and Robust Defense for Multi-Inverter Distribution Systems by Adversarial Deep Reinforcement Learning",
        "authors": "Yu Wang, Bikash Pal",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The droop controllers of inverter based resources (IBRs) can be adjustable by grid operators to facilitate regulation services. Considering the increasing integration of IBRs in distribution systems, cyber-security is becoming a major concern. This paper investigates the data-driven destabilizing attack and robust defense strategy based on deep reinforcement learning for inverter-integrated distribution systems. Firstly, the full-order high-fidelity model and reduced-order small-signal model of typical multi-inverter systems are derived. Then the destabilizing attack on the droop control gains is analyzed, which reveals its impact to system stability. Finally, the attack and defense problems are formulated as Markov decision process (MDP) and adversarial MDP (AMDP). The problems are solved by twin delayed deep deterministic policy gradient (TD3) algorithm to find the least effort attack path of the system and obtain the corresponding robust defense strategy. The simulation studies are conducted in a multi-inverter microgrid system with 4 IBRs and IEEE 123-bus system with 10 IBRs to evaluate the proposed method.<br>\n </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21078433.v1"
    },
    {
        "id": 26699,
        "title": "Quantum Multi-Agent Reinforcement Learning as an Emerging AI Technology: A Survey and Future Directions",
        "authors": "Jun Zhao, Wenhan Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper presents a comprehensive survey of Quantum Multi-Agent Reinforcement Learning (QMARL), a nascent field at the intersection of quantum computing and multi-agent systems. The survey begins by introducing the fundamentals of quantum computing, highlighting its potential to revolutionize computational capabilities. We then delve into the principles of multi-agent reinforcement learning (MARL), examining how quantum computing can enhance learning efficiency and decision-making processes in complex environments. The core of the survey focuses on the current state of QMARL, reviewing existing literature, methodologies, and case studies that demonstrate the integration of quantum algorithms with MARL frameworks. The paper also addresses the unique challenges and opportunities presented by quantum technologies in multi-agent systems, such as quantum entanglement and superposition, and their implications for agent coordination and learning dynamics. Additionally, the survey explores the practical applications of QMARL in various domains, including cybersecurity, finance, and robotics, underscoring its transformative potential. The paper concludes by identifying key research gaps and proposing future directions for the development of QMARL. This includes the need for scalable quantum algorithms, the exploration of quantum-resistant strategies in adversarial settings, and the integration of quantum principles in agent communication and collaboration. Overall, this survey serves as a foundational guide for researchers and practitioners interested in the emerging field of QMARL, offering insights into its current achievements and future possibilities.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24563293.v1"
    },
    {
        "id": 26700,
        "title": "Working Memory Facilitates Reinforcement Learning",
        "authors": "Kengo Shibata, Verena Klar, Masud Husain, Sanjay Manohar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1447-0"
    },
    {
        "id": 26701,
        "title": "Mitigating Cowardice for Reinforcement Learning Agents in Combat Scenarios",
        "authors": "Steve Bakos, Heidar Davoudi",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog51982.2022.9893546"
    },
    {
        "id": 26702,
        "title": "Reinforcement Learning Based Virtual Inertia Control of Multi-Area Microgrids",
        "authors": "Vjatseslav Skiparev, Komeil Nosrati, Eduard Petlenkov, Juri Belikov",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4449057"
    },
    {
        "id": 26703,
        "title": "Reinforcement learning of adaptive control strategies",
        "authors": "Leslie Held, Luc Vermeylen, David Dignath, Wim Notebaert, Ruth Krebs, Senne Braem",
        "published": "No Date",
        "citations": 0,
        "abstract": "Humans can up- or downregulate the degree to which they rely on task information for goal directed behaviour, a process often referred to as cognitive control. Adjustments in cognitive control are traditionally studied in response to experienced or expected task-rule conflict. However, recent theories suggest that people can also learn to adapt control settings through reinforcement. Across three preregistered task switching experiments (n=415), we selectively rewarded correct performance on trials with either more (incongruent) or less (congruent) task-rule conflict. Results confirmed the hypothesis that people rewarded more on incongruent trials showed smaller task-rule congruency effects, thus optimally adapting their control settings to the reward scheme. Using drift diffusion modelling, we further show that this reinforcement of cognitive control may occur through conflict-dependent within-trial adjustments of response thresholds after conflict detection. Together, our findings suggest that, while people remain more efficient at learning stimulus-response associations through rewards, they can similarly learn cognitive control strategies through reinforcement.",
        "link": "http://dx.doi.org/10.31234/osf.io/d8p9e"
    },
    {
        "id": 26704,
        "title": "Reinforcement learning approach for cooperative AUVs in underwater surveillance operations",
        "authors": "Maksim Sporyshev, Alexander Scherbatyuk",
        "published": "2019-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ut.2019.8734293"
    },
    {
        "id": 26705,
        "title": "Adaptive Strategy Templates Using Deep Reinforcement Learning for Multi-Issue Bilateral Negotiation",
        "authors": "Pallavi Bagga, Nicola Paoletti, Kostas Stathis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4531611"
    },
    {
        "id": 26706,
        "title": "Optimal detection task allocation: A reinforcement learning approach",
        "authors": "Qilong Huang, Qing Bu, Ziyi Qin",
        "published": "2017-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2017.8242794"
    },
    {
        "id": 26707,
        "title": "MoleGuLAR: Molecule Generation using Reinforcement Learning with Alternating Rewards",
        "authors": "Manan Goel, Shampa Raghunathan, Siddhartha Laghuvarapu, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Design of new inhibitors for novel targets is a very important problem especially in the current scenario with the world being plagued by COVID-19. Conventional approaches undertaken to this end, like, high-throughput virtual screening require extensive combing through existing datasets in the hope of finding possible matches. In this study we propose a computational strategy for de novo generation of molecules with high binding affinities to the specified target. A deep generative model is built using a stack augmented recurrent neural network for initially generating drug like molecules and then it is optimized using reinforcement learning to start generating molecules with desirable properties--primarily the binding affinity. The reinforcement learning section of the pipeline is further extended to multi-objective optimization showcasing the model's ability to generate molecules with a wide variety of properties desirable for drug like molecules, like, LogP, Quantitative Estimate of Drug Likeliness etc.. For multi-objective optimization, we have devised a novel strategy in which the property being used to calculate the reward is changed periodically. In comparison to the conventional approach of taking a weighted sum of all rewards, this strategy shows enhanced ability to generate a significantly higher number of molecules with desirable properties.",
        "link": "http://dx.doi.org/10.33774/chemrxiv-2021-cg9p8"
    },
    {
        "id": 26708,
        "title": "Deep Reinforcement Learning for Self-Configurable NoC",
        "authors": "Md Farhadur Reza",
        "published": "2020-9-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/socc49529.2020.9524761"
    },
    {
        "id": 26709,
        "title": "Quadrotor Aerobatic Maneuver Attitude Controller based on Reinforcement Learning",
        "authors": "Linkun He, Huifeng Li",
        "published": "2022-5-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ascc56756.2022.9828067"
    },
    {
        "id": 26710,
        "title": "Learning to Navigate Through Reinforcement Across the Sim2Real Gap",
        "authors": "Rana Azzam, Mohamad Chehadeh, Oussama Abdulhay, Igor Boiko, Yahya Zweiri",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>Amid the recent advances in robotics and machine learning, unmanned aerial vehicles (UAVs) have shown evident proliferation across various applications. Consequently, the involvement of UAVs in populated environments has progressively become inevitable, putting forward stringent safety and security measures. In this work, we develop a deep reinforcement learning-based UAV-navigation approach that blends decision making with behavioral intelligence. In particular, a reinforcement learning (RL) agent is trained to instruct the UAV on how to accomplish a goal-oriented task, while assuring the safety of the UAV and its surroundings. Upon arriving at the goal position, the RL agent slows the UAV down preparing it for landing. The safety of the UAV and the environment are attained through a robust collision avoidance capability, embedded into the RL-based navigation system and considers both static and dynamic obstacles in the environment. Training is exclusively carried out in simulation, where a high fidelity UAV controller model is used to perform the simulated maneuver. The proposed approach was tested in simulation and then shown to directly transfer to reality without explicit sim2real gap transfer techniques. Experimental results demonstrated the agent's capability to achieve the navigation task with 90% success rate.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20138960"
    },
    {
        "id": 26711,
        "title": "A Self-Comparison Based Reinforcement Learning Method for Dynamic Traveling Salesman Problem",
        "authors": "He Ren, Rui Zhong, Haichao Gui",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4566661"
    },
    {
        "id": 26712,
        "title": "Double Deep Reinforcement Learning",
        "authors": "Josué Kiefer, Klaus Dorer",
        "published": "2023-4-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icarsc58346.2023.10129640"
    },
    {
        "id": 26713,
        "title": "Reinforcement learning in car control: A brief survey",
        "authors": "Dawid Kalandyk",
        "published": "2021-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wzee54157.2021.9576838"
    },
    {
        "id": 26714,
        "title": "Reinforcement Learning for Solving Control Problems in Robotics",
        "authors": "Askhat Diveev, Elena Sofronova, Sergey Konstantinov, Viktoria Moiseenko",
        "published": "2023-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/engproc2023033029"
    },
    {
        "id": 26715,
        "title": "Destabilizing Attack and Robust Defense for Multi-Inverter Distribution Systems by Adversarial Deep Reinforcement Learning",
        "authors": "Yu Wang, Bikash Pal",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The droop controllers of inverter based resources (IBRs) can be adjustable by grid operators to facilitate regulation services. Considering the increasing integration of IBRs in distribution systems, cyber-security is becoming a major concern. This paper investigates the data-driven destabilizing attack and robust defense strategy based on deep reinforcement learning for inverter-integrated distribution systems. Firstly, the full-order high-fidelity model and reduced-order small-signal model of typical multi-inverter systems are derived. Then the destabilizing attack on the droop control gains is analyzed, which reveals its impact to system stability. Finally, the attack and defense problems are formulated as Markov decision process (MDP) and adversarial MDP (AMDP). The problems are solved by twin delayed deep deterministic policy gradient (TD3) algorithm to find the least effort attack path of the system and obtain the corresponding robust defense strategy. The simulation studies are conducted in a multi-inverter microgrid system with 4 IBRs and IEEE 123-bus system with 10 IBRs to evaluate the proposed method.<br>\n </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21078433.v1"
    },
    {
        "id": 26716,
        "title": "Quantum Multi-Agent Reinforcement Learning as an Emerging AI Technology: A Survey and Future Directions",
        "authors": "Jun Zhao, Wenhan Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper presents a comprehensive survey of Quantum Multi-Agent Reinforcement Learning (QMARL), a nascent field at the intersection of quantum computing and multi-agent systems. The survey begins by introducing the fundamentals of quantum computing, highlighting its potential to revolutionize computational capabilities. We then delve into the principles of multi-agent reinforcement learning (MARL), examining how quantum computing can enhance learning efficiency and decision-making processes in complex environments. The core of the survey focuses on the current state of QMARL, reviewing existing literature, methodologies, and case studies that demonstrate the integration of quantum algorithms with MARL frameworks. The paper also addresses the unique challenges and opportunities presented by quantum technologies in multi-agent systems, such as quantum entanglement and superposition, and their implications for agent coordination and learning dynamics. Additionally, the survey explores the practical applications of QMARL in various domains, including cybersecurity, finance, and robotics, underscoring its transformative potential. The paper concludes by identifying key research gaps and proposing future directions for the development of QMARL. This includes the need for scalable quantum algorithms, the exploration of quantum-resistant strategies in adversarial settings, and the integration of quantum principles in agent communication and collaboration. Overall, this survey serves as a foundational guide for researchers and practitioners interested in the emerging field of QMARL, offering insights into its current achievements and future possibilities.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24563293.v1"
    },
    {
        "id": 26717,
        "title": "Working Memory Facilitates Reinforcement Learning",
        "authors": "Kengo Shibata, Verena Klar, Masud Husain, Sanjay Manohar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1447-0"
    },
    {
        "id": 26718,
        "title": "Policy-Based Reinforcement Learning in the Generalized Rock-Paper-Scissors Game",
        "authors": "Imre Gergely Mali, Gabriela Czibula",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-92"
    },
    {
        "id": 26719,
        "title": "REINFORCEMENT OF SELF-REGULATED LEARNING IN THE IT LABORATORY",
        "authors": "Sławomir Przyłucki, Magdalena Czerwińska",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/iceri.2023.1249"
    },
    {
        "id": 26720,
        "title": "Flow-Based Reinforcement Learning",
        "authors": "Dilini Samarasinghe, Michael Barlow, Erandi Lakshika",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3209260"
    },
    {
        "id": 26721,
        "title": "Evolutionary Reinforcement Learning of Neural Network Controller for Acrobot Task — Part2: Genetic Algorithm",
        "authors": "Hidehiko Okada",
        "published": "No Date",
        "citations": 0,
        "abstract": "Evolutionary algorithms find applicability in reinforcement learning of neural networks due to their independence from gradient-based methods. To achieve successful training of neural networks using evolutionary algorithms, careful considerations must be made to select appropriate algorithms due to the availability of various algorithmic variations. The author previously reported experimental evaluations on Evolution Strategy for reinforcement learning of neural networks, utilizing the Acrobot control task. In this study, Genetic Algorithm is adopted as another instance of major evolutionary algorithms. Experimental results demonstrate that there was no statistically significant difference between the experimental performances of GA and ES, but the priority of generations and offsprings was different; GA performed better with a greater number of generations while ES performed better with a greater number of offsprings. Eight hidden units were the best among four variations (4, 8, 16 or 32 units), which aligns with previous study using ES.",
        "link": "http://dx.doi.org/10.20944/preprints202310.0852.v1"
    },
    {
        "id": 26722,
        "title": "Decision letter: The functional form of value normalization in human reinforcement learning",
        "authors": "Neil Garrett",
        "published": "2022-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.83891.sa1"
    },
    {
        "id": 26723,
        "title": "Review for \"Performance enhancement of the artificial neural network–based reinforcement learning for wind turbine yaw control\"",
        "authors": "Francesco Castellani",
        "published": "2019-8-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2451/v1/review1"
    },
    {
        "id": 26724,
        "title": "Review for \"Performance enhancement of the artificial neural network–based reinforcement learning for wind turbine yaw control\"",
        "authors": "Francesco Castellani",
        "published": "2019-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2451/v2/review2"
    },
    {
        "id": 26725,
        "title": "Reinforcement Learning for Multiagent-based Residential Energy Management System",
        "authors": "Aparna Kumari, Sudeep Tanwar",
        "published": "2021-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcwkshps52748.2021.9682182"
    },
    {
        "id": 26726,
        "title": "Autonomous Vehicles: Applications of Deep Reinforcement Learning",
        "authors": "Chinmay Kale -",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "This research paper offers a concise exploration into the pivotal role played by deep reinforcement learning in propelling advancements in both perception and decision-making aspects of autonomous vehicles. Through the fusion of deep learning and reinforcement learning techniques, we delve into how these methodologies synergistically contribute to augmenting the navigational proficiency of autonomous vehicles amidst intricate and dynamically changing environments. With a focus on a streamlined presentation, this paper provides a succinct yet insightful overview of various perception algorithms utilized by autonomous vehicles, emphasizing object detection, semantic segmentation, and LiDAR-based techniques.",
        "link": "http://dx.doi.org/10.36948/ijfmr.2024.v06i01.13792"
    },
    {
        "id": 26727,
        "title": "Automata Guided Semi-Decentralized Multi-Agent Reinforcement Learning",
        "authors": "Chuangchuang Sun, Xiao Li, Calin Belta",
        "published": "2020-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147704"
    },
    {
        "id": 26728,
        "title": "Oxytocin modulates neurocomputational mechanisms underlying prosocial reinforcement learning",
        "authors": "Daniel Martins, Patricia Lockwood, Jo Cutler, Rosalyn Moran, Yannis Paloyelis",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractHumans often act in the best interests of others. However, how we learn which actions result in good outcomes for other people and the neurochemical systems that support this ‘prosocial learning’ remain poorly understood. Using computational models of reinforcement learning, functional magnetic resonance imaging and dynamic causal modelling, we examined how different doses of intranasal oxytocin, a neuropeptide linked to social cognition, impact how people learn to benefit others (prosocial learning) and whether this influence could be dissociated from how we learn to benefit ourselves (self-oriented learning). We show that a low dose of oxytocin prevented decreases in prosocial performance over time, despite no impact on self-oriented learning. Critically, oxytocin produced dose-dependent changes in the encoding of prediction errors (PE) in the midbrain-subgenual anterior cingulate cortex (sgACC) pathway specifically during prosocial learning. Our findings reveal a new role of oxytocin in prosocial learning by modulating computations of PEs in the midbrain-sgACC pathway.",
        "link": "http://dx.doi.org/10.1101/2021.05.26.445739"
    },
    {
        "id": 26729,
        "title": "Memory-Assisted Reinforcement Learning for Diverse Molecular De Novo Design",
        "authors": "Thomas Blaschke, Ola Engkvist, Jürgen Bajorath, Hongming Chen",
        "published": "No Date",
        "citations": 1,
        "abstract": "In de novo molecular design, recurrent neural networks (RNN) have been shown to be effective methods for sampling and generating novel chemical structures. Using a technique called reinforcement learning (RL), an RNN can be tuned to target a particular section of chemical space with optimized desirable properties using a scoring function. However, ligands generated by current RL methods so far tend to have relatively low diversity, and sometimes even result in duplicate structures when optimizing towards particular properties. Here, we propose a new method to address the low diversity issue in RL. Memory-assisted RL is an extension of the known RL, with the introduction of a so-called memory unit.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.12693152.v1"
    },
    {
        "id": 26730,
        "title": "Devs Model Construction As A Reinforcement Learning Problem",
        "authors": "Istvan David, Eugene Syriani",
        "published": "2022-7-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/annsim55834.2022.9859369"
    },
    {
        "id": 26731,
        "title": "Investigation and Imitation of Human Captains’ Maneuver Using Inverse Reinforcement Learning",
        "authors": "Takefumi Higaki, Hirotada Hashimoto, Hitoshi Yoshioka",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAutomatic collision avoidance is of significant importance to prevent maritime collisions. Although many studies have been conducted in recent years, autonomous system has not completely replaced human captains since it is still difficult to imitate their complicated decisions. Thus, the present paper tries to investigate and imitate experienced captains’ maneuver using maximum entropy inverse reinforcement learning (MaxEnt IRL). We firstly verify that MaxEnt IRL can reproduce appropriate reward function from demonstrative trajectories. Afterwards, we conduct an experiment on a simulator where well-experienced captains maneuver in congested sea and estimate reward from the trajectories. Searching the route which maximizes the obtained reward, finally, we demonstrate the optimized route can avoid collision against multiple ships in compliance with the International Regulations for Preventing Collisions at Sea (COLREGs).",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1844861/v1"
    },
    {
        "id": 26732,
        "title": "Robotic Control in Adversarial and Sparse Reward Environments: A Robust Goal-Conditioned Reinforcement Learning Approach",
        "authors": "Xiangkun He, Chen Lv",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>With deep neural networks based function approximators, reinforcement learning holds the promise of learning complex end-to-end robotic controllers that can map high-dimensional sensory information directly to control policies.</p>\n<p>However, a common challenge, especially for robotics, is sample-efficient learning from sparse rewards, in which an agent is required to find a long sequence of “correct” actions to achieve a desired outcome.</p>\n<p>Unfortunately, inevitable perturbations on observations may make this task trickier to solve.</p>\n<p>Here, this paper advances a novel robust goal-conditioned reinforcement learning approach for end-to-end robotic control in adversarial and sparse reward environments.</p>\n<p>Specifically, a mixed adversarial attack scheme is presented to generate diverse adversarial perturbations on observations by combining white-box and black-box attacks. </p>\n<p>Meanwhile, a hindsight experience replay technique considering observation perturbations is developed to turn a failed experience into a successful one and generate the policy trajectories perturbed by the mixed adversarial attacks.</p>\n<p>Additionally, a robust goal-conditioned actor-critic method is proposed to learn goal-conditioned policies and keep the variations of the perturbed policy trajectories within bounds.</p>\n<p>Finally, the proposed method is evaluated on three tasks with adversarial attacks and sparse reward settings.</p>\n<p>The results indicate that our scheme can ensure robotic control performance and policy robustness on the adversarial and sparse reward tasks.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22324954"
    },
    {
        "id": 26733,
        "title": "Exploring the Benefits of Reinforcement Learning for Autonomous Drone Navigation and Control",
        "authors": "Ahshanul Haque, Md Naseef-Ur-Rahman Chowdhury",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This is research about how reinforcement learning benefits autonomous drone navigation and control</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22578355"
    },
    {
        "id": 26734,
        "title": "Towards Neural Charged Particle Tracking in Digital Tracking Calorimeters with Reinforcement Learning",
        "authors": "Tobias Kortus, Ralf Keidel, Nicolas R. Gauger",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We propose a novel reconstruction scheme for reconstructing charged particles in digital tracking calorimeters using model-free reinforcement learning aiming to benefit from the rapid progress and success of neural network architectures for tracking without the dependency on simulated or manually labeled data. Here we optimize by trial-and-error a behavior policy acting as a heuristic approximation to the full combinatorial optimization problem, maximizing the physical plausibility of sampled trajectories. In modern data processing pipelines used in high energy physics experiments and related high energy physics driven applications tracking plays an essential role allowing to identify and follow charged particle trajectories traversing particle detectors. Due to the usual high multiplicity of charged particles as well as the occurring physical interactions, randomly deflecting the particles from their initial path, the reconstruction is a challenging undertaking, requiring fast, accurate and robust algorithms. Our approach works on graph-structured data, capturing possible track hypotheses through edge connections between particles in the sensitive detector layers. We demonstrate in a comprehensive study on simulated data generated for a particle detector used for proton computed tomography, the overall high potential as well as the competitiveness of our approach compared to a heuristic search algorithm and a model trained on ground truth information. Finally, we point out limitations of our approach, guiding towards a robust foundation for further development of reinforcement learning based tracking algorithms in high energy physics.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21717323"
    },
    {
        "id": 26735,
        "title": "Towards Neural Charged Particle Tracking in Digital Tracking Calorimeters with Reinforcement Learning",
        "authors": "Tobias Kortus, Ralf Keidel, Nicolas R. Gauger",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>We propose a novel reconstruction scheme for reconstructing charged particles in digital tracking calorimeters using model-free reinforcement learning aiming to benefit from the rapid progress and success of neural network architectures for tracking without the dependency on simulated or manually labeled data. Here we optimize by trial-and-error a behavior policy acting as a heuristic approximation to the full combinatorial optimization problem, maximizing the physical plausibility of sampled trajectories. In modern data processing pipelines used in high energy physics experiments and related high energy physics driven applications tracking plays an essential role allowing to identify and follow charged particle trajectories traversing particle detectors. Due to the usual high multiplicity of charged particles as well as the occurring physical interactions, randomly deflecting the particles from their initial path, the reconstruction is a challenging undertaking, requiring fast, accurate and robust algorithms. Our approach works on graph-structured data, capturing possible track hypotheses through edge connections between particles in the sensitive detector layers. We demonstrate in a comprehensive study on simulated data generated for a particle detector used for proton computed tomography, the overall high potential as well as the competitiveness of our approach compared to a heuristic search algorithm and a model trained on ground truth information. Finally, we point out limitations of our approach, guiding towards a robust foundation for further development of reinforcement learning based tracking algorithms in high energy physics.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21717323.v2"
    },
    {
        "id": 26736,
        "title": "Robotic Control in Adversarial and Sparse Reward Environments: A Robust Goal-Conditioned Reinforcement Learning Approach",
        "authors": "Xiangkun He, Chen Lv",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>With deep neural networks based function approximators, reinforcement learning holds the promise of learning complex end-to-end robotic controllers that can map high-dimensional sensory information directly to control policies.</p>\n<p>However, a common challenge, especially for robotics, is sample-efficient learning from sparse rewards, in which an agent is required to find a long sequence of “correct” actions to achieve a desired outcome.</p>\n<p>Unfortunately, inevitable perturbations on observations may make this task trickier to solve.</p>\n<p>Here, this paper advances a novel robust goal-conditioned reinforcement learning approach for end-to-end robotic control in adversarial and sparse reward environments.</p>\n<p>Specifically, a mixed adversarial attack scheme is presented to generate diverse adversarial perturbations on observations by combining white-box and black-box attacks. </p>\n<p>Meanwhile, a hindsight experience replay technique considering observation perturbations is developed to turn a failed experience into a successful one and generate the policy trajectories perturbed by the mixed adversarial attacks.</p>\n<p>Additionally, a robust goal-conditioned actor-critic method is proposed to learn goal-conditioned policies and keep the variations of the perturbed policy trajectories within bounds.</p>\n<p>Finally, the proposed method is evaluated on three tasks with adversarial attacks and sparse reward settings.</p>\n<p>The results indicate that our scheme can ensure robotic control performance and policy robustness on the adversarial and sparse reward tasks.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22324954.v1"
    },
    {
        "id": 26737,
        "title": "Multi-Agent Connected Autonomous Driving using Deep Reinforcement Learning",
        "authors": "Praveen Palanisamy",
        "published": "2020-7",
        "citations": 48,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207663"
    },
    {
        "id": 26738,
        "title": "IEEE Transactions on Neural Networks and Learning Systems special section on deep reinforcement learning and adaptive dynamic programming",
        "authors": "",
        "published": "2017-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2017.2652580"
    },
    {
        "id": 26739,
        "title": "Early Prediction of Human Action by Deep Reinforcement Learning",
        "authors": "Hareesh Devarakonda, Snehasis Mukherjee",
        "published": "2021-7-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ncc52529.2021.9530126"
    },
    {
        "id": 26740,
        "title": "Reinforcement Learning of Protein Conformational Ensemble",
        "authors": "Jiangyan Feng",
        "published": "2019-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.bpj.2018.11.1022"
    },
    {
        "id": 26741,
        "title": "Performance on Learning to Associate a Stimulus with Positive Reinforcement",
        "authors": "R. A. Boakes",
        "published": "2021-9-16",
        "citations": 40,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003150404-4"
    },
    {
        "id": 26742,
        "title": "Shaping Model-Free Reinforcement-Learning with Model-Based Pseudorewards",
        "authors": "Paul Krueger, Thomas Griffiths",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2018.1191-0"
    },
    {
        "id": 26743,
        "title": "Open quantum system control based on reinforcement learning",
        "authors": "Peng Wei, Na Li, Zairong Xi",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8865335"
    },
    {
        "id": 26744,
        "title": "A Reinforcement Learning Approach to Optimal Execution",
        "authors": "Ciamac C. Moallemi, Muye Wang",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3897442"
    },
    {
        "id": 26745,
        "title": "Prototype Reinforcement for Few-Shot Learning",
        "authors": "Liheng Xu, Qian Xie, Baoqing Jiang, Jiashuo Zhang",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9326820"
    },
    {
        "id": 26746,
        "title": "Deep Reinforcement Learning for Optimal Sailing Upwind",
        "authors": "Takumi Suda, Daniel Nikovski",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892369"
    },
    {
        "id": 26747,
        "title": "Mapless navigation based on continuous deep reinforcement learning",
        "authors": "Xing Chen, Lumei Su, Houde Dai",
        "published": "2021-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9727354"
    },
    {
        "id": 26748,
        "title": "Artificial Conversational Agent using Robust Adversarial Reinforcement Learning",
        "authors": "Isha Wadekar",
        "published": "2021-1-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccci50826.2021.9402336"
    },
    {
        "id": 26749,
        "title": "Dynamic Asset-Allocation and Consumption",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-8"
    },
    {
        "id": 26750,
        "title": "Multi-Agent Collision Avoidance with Provident Agents using Deep-Reinforcement Learning",
        "authors": "Mohammad Bahrami Karkevandi, Samaneh Hosseini Semnani",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAutonomous path planning is becoming more demanding, as the number of robotic agents in pedestrian-rich environments is gradually increasing. Collision-free navigation is an essential requirement for autonomous agents in a pedestrian-rich environment. In this paper, we will build upon the GA3C-CADRL algorithm to train agents that take the near future into consideration while moving around the environment. Specifically, we will introduce a new reward function and a novel rewarding mechanism to train provident agents that always prioritize finding a collision-free path. Agents that are trained with the new method will be shown to be more cautious of their surrounding environment and tend to avoid actions that may be seen as threatening by pedestrians. We will show that the new proposed algorithm outperforms the previous state-of-the-art algorithm, GA3C-CADRL, in terms of successful navigation throughout the entire test set.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2797884/v1"
    },
    {
        "id": 26751,
        "title": "Regret Analysis in Deterministic Reinforcement Learning",
        "authors": "Damianos Tranos, Alexandre Proutiere",
        "published": "2021-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9682972"
    },
    {
        "id": 26752,
        "title": "Wireless Power Control via Meta-Reinforcement Learning",
        "authors": "Ziyang Lu, M. Cenk Gursoy",
        "published": "2022-5-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45855.2022.9839179"
    },
    {
        "id": 26753,
        "title": "A novel observer-based reinforcement learning for uncertain nonlinear systems with disturbances",
        "authors": "Dexin Zhang, Xiaoping Shi, Shenmin Song",
        "published": "No Date",
        "citations": 0,
        "abstract": "This study proposes an observer-based reinforcement learning(RL) control\nscheme for uncertain nonlinear systems subject to various external\ndisturbances. The proposed approach regards the total uncertainty\nestimated by the extended state observer (ESO) as potential model\ninformation, which is incorporated into the known part of the system\ndynamics. Based on the updated known dynamics, an RL structure is\nconstructed to approximate the optimal solution of the HJB equation\nwithout the persistence of the excitation (PE) condition. The\nconvergence of the proposed policy to a neighborhood of the optimal\npolicy is proven, and the stability of the system states is guaranteed.\nThe comparative simulation results demonstrate improved performance with\na significantly reduced cost of the developed controller, and the\nsensitivity of control gain in the input channel is also relaxed.",
        "link": "http://dx.doi.org/10.22541/au.167766515.57662539/v1"
    },
    {
        "id": 26754,
        "title": "Exploring the Potential of Reinforcement Learning in Intelligent Robotics",
        "authors": "",
        "published": "2022-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.59121/kcisr22120001"
    },
    {
        "id": 26755,
        "title": "Decentralized Multi-Agent Deep Reinforcement Learning: A Competitive-Game Perspective",
        "authors": "Marc Espinós Longa, Antonios Tsourdos, Inalhan Gokhan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDeep reinforcement learning (DRL) has been widely studied in single agent learning but require further development and understanding in the multi-agent field.  As one of the most complex swarming settings, competitive learning evaluates the performance of multiple teams of agents cooperating to achieve certain goals while surpassing the rest of group candidates.  Such dynamical complexity makes the multi-agent problem hard to solve even for niche DRL methods. Within a competitive framework, we study state-of-the-art actor-critic and Q algorithms and analyze in depth their variants (e.g., prioritization, dual networks, etc.) in terms of performance and convergence. For completeness of discussion, we present and assess an asynchronous and prioritized version of proximal policy optimization actor-critic technique (P3O) against the other benchmarks.  Results prove that Q-based approaches are more robust and reliable than actor-critic configurations for the given setting. In addition, we suggest incorporating local team communication and combining DRL with direct search optimization to improve learning, especially in challenging scenarios with partial observations.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2065000/v1"
    },
    {
        "id": 26756,
        "title": "Reinforcement Learning for Few-Shot Text Generation Adaptation",
        "authors": "Pengsen Cheng, Jinqiao Dai, Jiamiao Liu, Jiayong Liu, Peng Jia",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4342101"
    },
    {
        "id": 26757,
        "title": "The pursuit of happiness: A reinforcement learning perspective on habituation and comparisons",
        "authors": "Rachit Dubey, Tom Griffiths, Peter Dayan",
        "published": "No Date",
        "citations": 0,
        "abstract": "In evaluating our choices, we often suffer from two tragic relativities. First, when our lives change for the better, we rapidly habituate to the higher standard of living. Second, we cannot escape comparingourselves to various relative standards. Habituation and comparisons can be very disruptive to decision-making and happiness, and till date, it remains a puzzle why they have come to be a part of cognition in the first place. Here, we present computational evidence that suggests that these features might play an important role in promoting adaptive behavior. Using the framework of reinforcement learning, we explore the benefit of employing a reward function that, in addition to the reward provided by the underlying task, also depends on prior expectations and relative comparisons. We find that while agents equipped with this reward function are less happy, they learn faster and significantly outperform standard reward-based agents in a wide range of environments. Specifically, we find that relative comparisons speed up learning by providing an exploration incentive to the agents, and prior expectations serve as a useful aid to comparisons, especially in sparsely-rewarded and non-stationary environments. Our simulations also reveal potential drawbacks of this reward function and show that agents perform suboptimally when comparisons are left unchecked and when there are too many similar options. Together, our results help explain why we are prone to becoming trapped in a cycle of never-ending wants and desires, and may shed light on psychopathologies such as depression, materialism, and overconsumption.",
        "link": "http://dx.doi.org/10.31234/osf.io/8jd2x"
    },
    {
        "id": 26758,
        "title": "Optimal State Estimation Using Model-Free Reinforcement Learning",
        "authors": "Haoran Ma, Ying Yang, Dingguo Liang",
        "published": "2021-10-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728004"
    },
    {
        "id": 26759,
        "title": "Optimal Market Making by Reinforcement Learning",
        "authors": "Matias Selser, Javier Kreiner, Manuel Maurette",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3829984"
    },
    {
        "id": 26760,
        "title": "A traffic light control method based on multi-agent deep reinforcement learning algorithm",
        "authors": "Dongjiang Liu, Leixiao Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAn efficient way to cope with traffic congestion is Intelligent Traffic Light Control (ITLC). Particularly, multi-agent based intelligent traffic light control algorithms are more popular and efficient than others. But there are still some problems in these algorithms. Firstly, an efficient communication mechanism is needed. As current traffic condition of an intersection can impact other intersections' future traffic condition, every agent should know the traffic condition of other intersections through communication. Thus, a new communication mechanism should be designed. By using this mechanism, traffic condition of an intersection can be passed to several relevant agents easily. And traffic condition of an intersection can be described simply and clearly. Secondly, asynchronization of message should be considered while processing received messages. As duration time of traffic lights' cycles is different from agent to agent and message sending event happens at the end of each traffic light cycle, different agents will send message at different time. Moreover, every agent tries to process the received messages at the end of each traffic light cycle as well. So it is hard for an agent to decide which message is the latest one and the most valuable. Thirdly, a new reward calculating method should be designed. In the traditional reinforcement learning based ITLC algorithms, only queue length of congested cars or waiting time of these cars is considered while calculating reward value. But, both of them are very important for measuring the level of traffic congestion. So a new reward calculation method is needed. To solve all these problems, in this paper, a new ITLC algorithm is proposed. At the same time, a new message sending and processing method is proposed and adopted by this ITLC algorithm. The first and the second problems described above are tried to be solved by using this new method. Besides, to measure traffic congestion in a more reasonable way, a new reward calculation method is proposed and used. This method takes both waiting time and queue length into consideration.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2626589/v1"
    },
    {
        "id": 26761,
        "title": "Data-Driven Output Consensus for a Class of Discrete-Time Multiagent Systems by Reinforcement Learning Techniques",
        "authors": "Yuanshan Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4697059"
    },
    {
        "id": 26762,
        "title": "Anwendung von Reinforcement Learning in industriellen cyberphysischen Systemen",
        "authors": "David Heik, Fouad Bahrpeyma, Dirk Reichelt",
        "published": "2023-3-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33968/2023.10"
    },
    {
        "id": 26763,
        "title": "Practical Reinforcement Learning of Stabilizing Economic MPC",
        "authors": "Mario Zanon, Sebastien Gros, Alberto Bemporad",
        "published": "2019-6",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc.2019.8795816"
    },
    {
        "id": 26764,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Yixuan Sun, Krishnan Raghavan, Prasanna Balaprakash",
        "published": "2023-11-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003359593-9"
    },
    {
        "id": 26765,
        "title": "Inhibitory and excitatory mechanisms in the human cingulate-cortex support reinforcement learning",
        "authors": "Vered Bezalel, Rony Paz, Assaf Tal",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe dorsal anterior cingulate cortex (dACC) is crucial for motivation, reward- and error-guided decision-making, yet its excitatory and inhibitory mechanisms remain poorly explored in humans. In particular, the balance between excitation and inhibition (E/I), demonstrated to play a role in animal studies, is difficult to measure in behaving humans. Here, we used magnetic-resonance-spectroscopy (1H-MRS) to examine these mechanisms during reinforcement learning with three different conditions: high cognitive load (uncertainty); probabilistic discrimination learning; and a control null-condition. Subjects learned to prefer the gain option in the discrimination phase and had no preference in the other conditions. We found increased GABA levels during the uncertainty condition, suggesting recruitment of inhibitory systems during high cognitive load when trying to learn. Further, higher GABA levels during the null (baseline) condition correlated with improved discrimination learning. Finally, excitatory and inhibitory levels were correlated during high cognitive load. The result suggests that availability of dACC inhibitory resources enables successful learning. Our approach establishes a novel way to examine the contribution of the balance between excitation and inhibition to learning and motivation in behaving humans.",
        "link": "http://dx.doi.org/10.1101/318659"
    },
    {
        "id": 26766,
        "title": "Reinforcement Learning and Stochastic Optimization",
        "authors": "Warren B. Powell",
        "published": "2022-4-2",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068"
    },
    {
        "id": 26767,
        "title": "Dynamic Pricing in Ride-Hailing Intelligent Transportation Systems by Using Deep Reinforcement Learning",
        "authors": "Sajad Heydari, Elham Akhondzadeh Noughabi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4746254"
    },
    {
        "id": 26768,
        "title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming",
        "authors": "",
        "published": "2017-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2017.2655663"
    },
    {
        "id": 26769,
        "title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming",
        "authors": "",
        "published": "2017-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2017.2678899"
    },
    {
        "id": 26770,
        "title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming",
        "authors": "",
        "published": "2017-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2016.2640821"
    },
    {
        "id": 26771,
        "title": "Instrumental lever pressing for wheel running is a bitonic function of wheel revolutions per reinforcement: Effects of constraint and automatic reinforcement",
        "authors": "W. David Pierce, Terry W. Belke, Allison F. Harris",
        "published": "2018-11",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.lmot.2018.07.001"
    },
    {
        "id": 26772,
        "title": "Reinforcement Learning Based Weighting Factors’ Real-Time Updating Scheme for the FCS Model Predictive Control to Improve the Large-Signal Stability of Inverters",
        "authors": "Xin Zhang, Jinsong He, Hao Ma, Zhixun Ma, Xiaohai Ge",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7191-4_8"
    },
    {
        "id": 26773,
        "title": "Curriculum Deep Reinforcement Learning with Different Exploration Strategies: A Feasibility Study on Cardiac Landmark Detection",
        "authors": "Patricio Astudillo, Peter Mortier, Matthieu De Beule, Francis Wyffels",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008948900370045"
    },
    {
        "id": 26774,
        "title": "STRUCTURAL ADAPTATION OF DATA COLLECTION PROCESSES IN AUTONOMOUS DISTRIBUTED SYSTEMS USING REINFORCEMENT LEARNING METHODS",
        "authors": "Botchkaryov. A.,  ",
        "published": "2017-3-23",
        "citations": 2,
        "abstract": "A method of structural adaptation of data collection processes has been developed based on reinforcement learning of the decision block on the choice of actions at the structural and functional level subordinated to it, which provides a more efficient distribution of measuring and computing resources, higher reliability and survivability of information collection subsystems of an autonomous distributed system compared to methods of parametric adaptation. In particular, according to the results of experimental studies, the average amount of information collected in one step using the method of structural adaptation is 23.2% more than in the case of using the methods of parametric adaptation. At the same time, the amount of computational costs for the work of the structural adaptation method is on average 42.3% more than for the work of parametric adaptation methods. The reliability of the work of the method of structural adaptation was studied using the efficiency preservation coefficient for different values of the failure rate of data collection processes. Using the recovery rate coefficient for various values of relative simultaneous sudden failures, the survivability of a set of data collection processes organized by the method of structural adaptation has been investigated. In terms of reliability, the structural adaptation method exceeds the parametric adaptation methods by an average of 21.1%. The average survivability rate for the method of structural adaptation is greater than for methods of parametric adaptation by 18.4%. Key words: autonomous distributed system, data collection process, structural adaptation, reinforcement learning",
        "link": "http://dx.doi.org/10.23939/csn2020.01.013"
    },
    {
        "id": 26775,
        "title": "The Use of Electroencephalogram and Electrodermal Signals in Reinforcement Learning of a Brain-Computer Interface",
        "authors": "Werley de Oliveira Gonçalves, Gizelle Kupac Vianna, Luiz Maltar Castello Branco",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0006773305330539"
    },
    {
        "id": 26776,
        "title": "Decentralized Control of Adaptive Data Collection Processes Based on Equilibrium Concept and Reinforcement Learning",
        "authors": "Alexey Botchkaryov,  ",
        "published": "2020-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23939/acps2020.02.050"
    },
    {
        "id": 26777,
        "title": "A Method for Road Accident Prevention in Smart Cities based on Deep Reinforcement Learning",
        "authors": "Giuseppe Crincoli, Fabiana Fierro, Giacomo Iadarola, Piera Rocca, Fabio Martinelli, Francesco Mercaldo, Antonella Santone",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011146500003283"
    },
    {
        "id": 26778,
        "title": "Unleashing the Potential of Reinforcement Learning for Personalizing Behavioral Transformations with Digital Therapeutics: A Systematic Literature Review",
        "authors": "Thure Weimann, Carola Gißke",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012474700003657"
    },
    {
        "id": 26779,
        "title": "Quantum Multi-Agent Reinforcement Learning as an Emerging AI Technology: A Survey and Future Directions",
        "authors": "Jun Zhao, Wenhan Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper presents a comprehensive survey of Quantum Multi-Agent Reinforcement Learning (QMARL), a nascent field at the intersection of quantum computing and multi-agent systems. The survey begins by introducing the fundamentals of quantum computing, highlighting its potential to revolutionize computational capabilities. We then delve into the principles of multi-agent reinforcement learning (MARL), examining how quantum computing can enhance learning efficiency and decision-making processes in complex environments. The core of the survey focuses on the current state of QMARL, reviewing existing literature, methodologies, and case studies that demonstrate the integration of quantum algorithms with MARL frameworks. The paper also addresses the unique challenges and opportunities presented by quantum technologies in multi-agent systems, such as quantum entanglement and superposition, and their implications for agent coordination and learning dynamics. Additionally, the survey explores the practical applications of QMARL in various domains, including cybersecurity, finance, and robotics, underscoring its transformative potential. The paper concludes by identifying key research gaps and proposing future directions for the development of QMARL. This includes the need for scalable quantum algorithms, the exploration of quantum-resistant strategies in adversarial settings, and the integration of quantum principles in agent communication and collaboration. Overall, this survey serves as a foundational guide for researchers and practitioners interested in the emerging field of QMARL, offering insights into its current achievements and future possibilities.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24563293"
    },
    {
        "id": 26780,
        "title": "Two-Memory Reinforcement Learning",
        "authors": "Zhao Yang, Thomas. M. Moerland, Mike Preuss, Aske Plaat",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog57401.2023.10333174"
    },
    {
        "id": 26781,
        "title": "Online Observer-Based Inverse Reinforcement Learning",
        "authors": "Ryan Self, Kevin Coleman, He Bai, Rushikesh Kamalapurkar",
        "published": "2021-5-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9482906"
    },
    {
        "id": 26782,
        "title": "A Self-Comparison Based Reinforcement Learning Method for Dynamic Traveling Salesman Problem",
        "authors": "He Ren, Rui Zhong, Haichao Gui",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4643703"
    },
    {
        "id": 26783,
        "title": "Reinforcement Learning for Linear Exponential Quadratic Gaussian Problem",
        "authors": "Lai Jing, Junlin Xiong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4211261"
    },
    {
        "id": 26784,
        "title": "Reinforcement of learning",
        "authors": "Yana Weinstein, Megan Sumeracki, Oliver Caviglioli",
        "published": "2018-8-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9780203710463-11"
    },
    {
        "id": 26785,
        "title": "Correction to: Optimal synchronization in pulse-coupled oscillator networks using reinforcement learning",
        "authors": "",
        "published": "2023-7-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1093/pnasnexus/pgad233"
    },
    {
        "id": 26786,
        "title": "Reinforcement Learning with Foregone Payoff Information in Normal Form Games",
        "authors": "Naoki Funai",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3343373"
    },
    {
        "id": 26787,
        "title": "Rapid Reinforcement Learning by Injecting Stochasticity into Bellman",
        "authors": "Pablo Martin Rodriguez Bertorello",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3401653"
    },
    {
        "id": 26788,
        "title": "Using Inverse Reinforcement Learning to Predict Goal-directed Shifts of Attention",
        "authors": "Gregory Zelinsky",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2019.1086-0"
    },
    {
        "id": 26789,
        "title": "Adaptive Traffic Grooming Using Reinforcement Learning in Multilayer Elastic Optical Networks",
        "authors": "Takafumi Tanaka",
        "published": "2023",
        "citations": 0,
        "abstract": "We introduce a traffic grooming technique for multilayer networks that uses reinforcement learning. We confirm its superior performance over heuristic methods as regards its ability to meet several key requirements such as blocking probability and energy consumption.",
        "link": "http://dx.doi.org/10.1364/ofc.2023.tu2d.6"
    },
    {
        "id": 26790,
        "title": "Schedules of Reinforcement",
        "authors": "W. David Pierce, Carl D. Cheney",
        "published": "2017-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315200682-5"
    },
    {
        "id": 26791,
        "title": "Multi-Agent Deep Reinforcement Learning for Cooperative Connected Vehicles",
        "authors": "Dohyun Kwon, Joongheon Kim",
        "published": "2019-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9014151"
    },
    {
        "id": 26792,
        "title": "Comfortable Driving by using Deep Inverse Reinforcement Learning",
        "authors": "Daiko Kishikawa, Sachiyo Arai",
        "published": "2019-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/agents.2019.8929214"
    },
    {
        "id": 26793,
        "title": "Freight train scheduling via decentralised multi-agent deep reinforcement learning",
        "authors": "",
        "published": "2021-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36334/modsim.2021.m1.bretas"
    },
    {
        "id": 26794,
        "title": "Point Cloud Registration via Heuristic Reward Reinforcement Learning",
        "authors": "Bingren Chen",
        "published": "2023-2-6",
        "citations": 1,
        "abstract": "This paper proposes a heuristic reward reinforcement learning framework for point cloud registration. As an essential step of many 3D computer vision tasks such as object recognition and 3D reconstruction, point cloud registration has been well studied in the existing literature. This paper contributes to the literature by addressing the limitations of embedding and reward functions in existing methods. An improved state-embedding module and a stochastic reward function are proposed. While the embedding module enriches the captured characteristics of states, the newly designed reward function follows a time-dependent searching strategy, which allows aggressive attempts at the beginning and tends to be conservative in the end. We assess our method based on two public datasets (ModelNet40 and ScanObjectNN) and real-world data. The results confirm the strength of the new method in reducing errors in object rotation and translation, leading to more precise point cloud registration.",
        "link": "http://dx.doi.org/10.3390/stats6010016"
    },
    {
        "id": 26795,
        "title": "Adaptive Strategy Templates Using Deep Reinforcement Learning for Multi-Issue Bilateral Negotiation",
        "authors": "Pallavi Bagga, Nicola Paoletti, Kostas Stathis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4581290"
    },
    {
        "id": 26796,
        "title": "Auv 3d Docking Control Using Deep Reinforcement Learning",
        "authors": "Zhang Tianze, Xuhong Miao, Yibin Li, Lei Jia, Zheng Wei",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4364301"
    },
    {
        "id": 26797,
        "title": "Decision letter for \"Prefrontal transcranial magnetic stimulation boosts response vigor during reinforcement learning in healthy adults\"",
        "authors": "",
        "published": "2022-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/ejn.15905/v3/decision1"
    },
    {
        "id": 26798,
        "title": "Adaptive Modulation for Underwater Acoustic Communications Based on Reinforcement Learning",
        "authors": "Qiang Fu, Aijun Song",
        "published": "2018-10",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oceans.2018.8604746"
    },
    {
        "id": 26799,
        "title": "Time Series Anomaly Detection for Cyber-Physical Systems Via Deep Reinforcement Learning",
        "authors": "Xue Yang, Enda Howley, Michael Schukat",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4641981"
    },
    {
        "id": 26800,
        "title": "Deep Reinforcement Learning for Electric Vehicle Charging Navigation: A Price-Aware Strategy",
        "authors": "Ali Can Erüst, Fatma Yildiz Tascikaraoglu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4435704"
    },
    {
        "id": 26801,
        "title": "Knowledge Transfer in Reinforcement Learning Agent",
        "authors": "Vanya Dimitrova Markova, Ventseslav Kirilov Shopov",
        "published": "2019-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infotech.2019.8860881"
    },
    {
        "id": 26802,
        "title": "Reinforcement Learning - A Technical Introduction",
        "authors": "Elmar Diederichs",
        "published": "2019-7-30",
        "citations": 3,
        "abstract": "Reinforcement learning provides a cognitive science perspective to behavior and sequential decision making provided that RL-algorithms introduce a computational concept of agency to the learning problem. Hence it addresses an abstract class of problems that can be characterized as follows: An algorithm confronted with information from an unknown environment is supposed to find stepwise an optimal way to behave based only on some sparse, delayed or noisy feedback from some environment, that changes according to the algorithm's behavior. Hence reinforcement learning offers an abstraction to the problem of goal-directed learning from interaction. The paper offers an opintionated introduction in the algorithmic advantages and drawbacks of several algorithmic approaches such that one can understand recent developments and open problems in reinforcement learning.",
        "link": "http://dx.doi.org/10.32629/jai.v2i2.45"
    },
    {
        "id": 26803,
        "title": "Deep Reinforcement Learning-Based Approach for Video Streaming DASH",
        "authors": "Naima Souane, Malika Bourenane, Yassine Douga",
        "published": "No Date",
        "citations": 0,
        "abstract": "Dynamic adaptive video streaming over HTTP (DASH) plays a crucial role in video transmission across networks. Traditional adaptive bitrate (ABR) algorithms adjust the quality of video segments based on network conditions and buffer occupancy. However, these algorithms rely on fixed rules within a complex environment, making it challenging to achieve optimal decisions considering the overall context. In this paper, we propose a novel Deep Reinforcement Learning-based approach for streaming DASH, focusing on maintaining consistent perceived video quality throughout the streaming session to enhance user experience. Our approach optimizes the Quality of Experience (QoE) by dynamically controlling the quality distance factor between consecutive video segments. We evaluate this approach through a simulation model that encompasses diverse wireless network environments and various video sequences. Additionally, we compare our proposed approach with state-of-the-art methods. The experimental results demonstrate significant improvements in QoE, ensuring users enjoy stable, high-quality video streaming sessions.",
        "link": "http://dx.doi.org/10.20944/preprints202308.1429.v1"
    },
    {
        "id": 26804,
        "title": "Optimizing Energy Efficiency in Unrelated Parallel Machine Scheduling Problem Through Reinforcement Learning",
        "authors": "Christian Pérez, Carlos March, Miguel Salido",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4669782"
    },
    {
        "id": 26805,
        "title": "Improving Chatbot Responses with Context and Deep Seq2Seq Reinforcement Learning",
        "authors": "Quoc-Dai Luong Tran, Anh-Cuong Le",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDeveloping conversational agents that can generate appropriate responses for meaningful and natural conversations between humans and machines is a difficult task in the field of artificial intelligence. One key factor for agents to produce accurate responses is the ability to fully and effectively utilize the context of the current utterance in a conversation. However, many previous studies have neglected the relationship between utterances in a conversation, which limits their effectiveness. This paper aims to address this limitation by introducing a novel method for modeling the contextual information of the current utterance when generating a response. The goal of this research is to improve the accuracy of responses generated by conversational agents by considering the contextual information of the current utterance in a conversation. To achieve this, we propose a novel approach that combines a Deep Seq2Seq model with reinforcement learning. The Deep Seq2Seq model generates responses based on the left context of the current utterance, while reinforcement learning is used to evaluate the entire generated conversation based on the right context of the current utterance. We also utilize a pre-trained word embedding model to help build reward functions for the reinforcement learning component and to represent words in the generated responses. Experimental results show that the proposed model leads to significant improvements in BLEU scores compared to a baseline model.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2477905/v1"
    },
    {
        "id": 26806,
        "title": "Model-Driven Analysis of ECG Using Reinforcement Learning",
        "authors": "Christian O’Reilly, Sai Durga Rithvik Oruganti, Deepa Tilwani, Jessica Bradshaw",
        "published": "No Date",
        "citations": 0,
        "abstract": "Modeling is essential to understand better the generative mechanisms responsible for experimental observations gathered from complex systems. In this work, we are using such an approach to analyze the electrocardiogram (ECG). We present a systematic framework to decompose ECG signals into sums of overlapping lognormal components. We used reinforcement learning to train a deep neural network to estimate the modeling parameters from ECG recorded in babies of 1 to 24 months of age. We demonstrate this model-driven approach by showing how the extracted parameters vary with age. After correction for multiple tests, 10 of 24 modeling parameters showed statistical significance below the 0.01 threshold, with absolute Kendall rank correlation coefficients in the [0.27, 0.51] range. We presented a model-driven approach to the analysis of ECG. The impact of this framework on fundamental science and clinical applications is likely to be increased by further refining the modeling of the physiological mechanisms generating the ECG. By improving the physiological interpretability, this approach can provide a window into latent variables important for understanding the heart-beating process and its control by the autonomous nervous system.",
        "link": "http://dx.doi.org/10.20944/preprints202305.0219.v1"
    },
    {
        "id": 26807,
        "title": "Reinforcement Learning for Structural Health Monitoring based on Inspection Data",
        "authors": "",
        "published": "2021-4-20",
        "citations": 2,
        "abstract": "Abstract. Due to uncertainty associated with fatigue, mechanical structures have to be often inspected, especially in aerospace. In order to reduce inspection effort, fatigue behavior can be predicted based on measurement data and supervised learning methods, such as neural networks or particle filters. For good predictions, much data is needed. However, often only a small number of sensors to collect data are available, e.g., on airplanes due to weight limitations. This paper presents a method where data that is collected during an inspection is utilized to compute an update of the optimal inspection interval. For this purpose, we describe structural health monitoring (SHM) as a Markov decision process and use reinforcement learning for deciding when to inspect next and when to decommission the structure before failure. In order to handle the infinite state space of the SHM decision process, we use two different regression models, namely neural networks (NN) and k-nearest neighbors (KNN), and compare them to the deep Q-learning approach, which is state of the art. The models are applied to a set of crack growth data which is considered to be representative of the general damage evolution of a structure. The results show that reinforcement learning can be utilized for such a decision task, where the KNN model leads to the best performance. ",
        "link": "http://dx.doi.org/10.21741/9781644901311-24"
    },
    {
        "id": 26808,
        "title": "Reinforcement Learning Under Drift",
        "authors": "Wang Chi Cheung, David Simchi-Levi, Ruihao Zhu",
        "published": "No Date",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3397818"
    },
    {
        "id": 26809,
        "title": "REINFORCEMENT LEARNING IN INVENTORY MANAGEMENT",
        "authors": "K.A. Manhanga, H. Steenkamp",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.52202/066390-0033"
    },
    {
        "id": 26810,
        "title": "An Adaptable Fuzzy Reinforcement Learning Method for Non-Stationary Environments",
        "authors": "Rachel Haighton, Amirhossein Asgharnia, Howard Schwartz, Sidney Givigi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4648081"
    },
    {
        "id": 26811,
        "title": "Evolutionary Reinforcement Learning of Neural Network Controller for Acrobot Task – Part1: Evolution Strategy",
        "authors": "Hidehiko Okada",
        "published": "No Date",
        "citations": 0,
        "abstract": "Evolutionary algorithms find applicability in the reinforcement learning of neural networks due to their independence from gradient-based methods. To achieve successful training of neural networks using evolutionary algorithms, careful considerations must be made to select appropriate algorithms due to the availability of various algorithmic variations. The author previously reported experimental evaluations on Evolution Strategy for reinforcement learning of neural networks, utilizing the pendulum control task. In this study, the Acrobot control task is adopted as another task. Experimental results demonstrate that ES successfully trained a Multi-Layer Perceptron to achieve a remarkable height of 99.85% concerning the maximum height. However, the trained MLP failed to maintain the chain end in an upright position throughout an episode. In this study, it was observed that employing 8 hidden units in the neural network yielded better results with statistical significance compared to using 4, 16, or 32 hidden units. Furthermore, the findings indicate that a larger population size in ES led to a more extensive exploration of potential solutions over a greater number of generations, which aligns with the previous study.",
        "link": "http://dx.doi.org/10.20944/preprints202308.0081.v1"
    },
    {
        "id": 26812,
        "title": "Using HMM and Reinforcement Learning for Building Smarter Trading Strategies",
        "authors": "Samit Ahlawat",
        "published": "2022-1",
        "citations": 1,
        "abstract": "        Samit Ahlawat demonstrates the superior performance of AI-based trading strategies over static rule-based strategies. Two AI algorithms are considered for constructing the trading strategy: hidden Markov model (HMM) and asynchronous advantage actor-critic (A3C) method            ",
        "link": "http://dx.doi.org/10.54946/wilm.10988"
    },
    {
        "id": 26813,
        "title": "Deep Reinforcement Learning: Techniques for training agents to make sequential decisions in complex environments.",
        "authors": "Wasif Ali",
        "published": "No Date",
        "citations": 0,
        "abstract": "Deep reinforcement learning (DRL) is a powerful machine learning technique that allows agents to learn how to make decisions in complex environments by trial and error. DRL combines reinforcement learning (RL) with deep learning, which allows agents to learn from large amounts of data and make decisions based on complex features. In this paper, we review the techniques used in DRL, and we discuss the challenges and opportunities of this field. We also present some examples of how DRL is being used today, and we discuss the future of DRL.",
        "link": "http://dx.doi.org/10.31219/osf.io/8cuaf"
    },
    {
        "id": 26814,
        "title": "On Safety and Time Efficiency Enhancement of Robot Navigation in Crowded Environment utilizing Deep Reinforcement Learning",
        "authors": "Sunil Srivatsav Samsani",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>The evolution of social robots has increased with the advent of recent artificial intelligence techniques. Alongside humans, social robots play active roles in various household and industrial applications. However, the safety of humans becomes a significant concern when robots navigate in a complex and crowded environment. In literature, the safety of humans in relation to social robots has been addressed by various methods; however, most of these methods compromise the time efficiency of the robot. For robots, safety and time-efficiency are two contrast elements where one dominates the other. To strike a balance between them, a multi-reward formulation in the reinforcement learning framework is proposed, which improves the safety together with time-efficiency of the robot. The multi-reward formulation includes both positive and negative rewards that encourage and punish the robot, respectively. The proposed reward formulation is tested on state-of-the-art methods of multi-agent navigation. In addition, an ablation study is performed to evaluate the importance of individual rewards. Experimental results signify that the proposed approach balances the safety and the time-efficiency of the robot while navigating in a crowded environment.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.17493605.v1"
    },
    {
        "id": 26815,
        "title": "Tutoring Reinforcement Learning via Feedback Control",
        "authors": "Francesco De Lellis, Giovanni Russo, Mario Di Bernardo",
        "published": "2021-6-29",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc54610.2021.9654881"
    },
    {
        "id": 26816,
        "title": "Reinforcement learning for finance: A review",
        "authors": "Diego Ismael León Nieto",
        "published": "2023-11-30",
        "citations": 0,
        "abstract": "Este artículo ofrece una revisión exhaustiva de la aplicación del aprendizaje por refuerzo (AR) en el dominio de las finanzas, y arroja una luz sobre el innovador progreso alcanzado y los desafíos que se avecinan. Exploramos cómo el AR, un subcampo del aprendizaje automático, ha sido instrumental para resolver problemas financieros complejos al permitir procesos de toma de decisiones que optimizan las recompensas a largo plazo. El AR es una poderosa técnica de aprendizaje automático que se puede utilizar para entrenar a agentes a fin de tomar decisiones en entornos complejos. En finanzas, el AR se ha utilizado para resolver una variedad de problemas, incluyendo la ejecución óptima, la optimización de carteras, la valoración y cobertura de opciones, la creación de mercados, el enrutamiento inteligente de órdenes y el robo-asesoramiento. En este artículo revisamos los desarrollos recientes en AR para finanzas. Comenzamos proporcionando una introducción al AR y a los procesos de decisión de Markov (MDP), que es el marco matemático para el AR. Luego discutimos los diversos algoritmos de AR que se han utilizado en finanzas, con un enfoque en métodos basados en valor y políticas. También discutimos el uso de redes neuronales en AR para finanzas. Finalmente, abordamos los resultados de estudios recientes que han utilizado AR para resolver problemas financieros. Concluimos discutiendo los desafíos y las oportunidades para futuras investigaciones en AR para finanzas.",
        "link": "http://dx.doi.org/10.18601/17941113.n24.02"
    },
    {
        "id": 26817,
        "title": "A Probabilistic Perspective on Risk-sensitive Reinforcement Learning",
        "authors": "Erfaun Noorani, John S. Baras",
        "published": "2022-6-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867288"
    },
    {
        "id": 26818,
        "title": "Reinforcement Learning for Anti-Ransomware Testing",
        "authors": "Alexander Adamov, Anders Carlsson",
        "published": "2020-9",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ewdts50664.2020.9225141"
    },
    {
        "id": 26819,
        "title": "Emril:Ensemble Method Based on Reinforcement Learning for Binary Classification in Imbalanced Drifting Data Streams",
        "authors": "Muhammad Usman, Huanhuan Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4682920"
    },
    {
        "id": 26820,
        "title": "Online inverse reinforcement learning for systems with disturbances",
        "authors": "Ryan Self, Moad Abudia, Rushikesh Kamalapurkar",
        "published": "2020-7",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147344"
    },
    {
        "id": 26821,
        "title": "Peer Review #3 of \"Reactive navigation under a fuzzy rules-based scheme and reinforcement learning for mobile robots (v0.2)\"",
        "authors": "",
        "published": "2021-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.556v0.2/reviews/3"
    },
    {
        "id": 26822,
        "title": "Design of an Artificial Game Entertainer by Reinforcement Learning",
        "authors": "Takanobu Yaguchi, Hitoshi Iima",
        "published": "2020-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog47356.2020.9231551"
    },
    {
        "id": 26823,
        "title": "A Reinforcement Learning Approach to Speech Coding",
        "authors": "Jerry Gibson, Hoontaek Oh",
        "published": "2022-7-11",
        "citations": 1,
        "abstract": "Speech coding is an essential technology for digital cellular communications, voice over IP, and video conferencing systems. For more than 25 years, the main approach to speech coding for these applications has been block-based analysis-by-synthesis linear predictive coding. An alternative approach that has been less successful is sample-by-sample tree coding of speech. We reformulate this latter approach as a multistage reinforcement learning problem with L step lookahead that incorporates exploration and exploitation to adapt model parameters and to control the speech analysis/synthesis process on a sample-by-sample basis. The minimization of the spectrally shaped reconstruction error to finite depth manages complexity and serves as an effective stand in for the overall subjective evaluation of reconstructed speech quality and intelligibility. Different control policies that attempt to persistently excite the system states and that encourage exploration are studied and evaluated. The resulting methods produce reconstructed speech quality competitive with the most popular speech codec utilized today. This new reinforcement learning formulation provides new insights and opens up new directions for system design and performance improvement.",
        "link": "http://dx.doi.org/10.3390/info13070331"
    },
    {
        "id": 26824,
        "title": "Instructed Motivational States Bias Reinforcement Learning and Memory Formation",
        "authors": "Alyssa Hannah Sinclair, Yuxi Candice Wang, R. Alison Adcock",
        "published": "No Date",
        "citations": 1,
        "abstract": "Motivation influences goals, decisions, and memory formation. Imperative motivation links urgent goals to actions, narrowing the focus of attention and memory. Conversely, interrogative motivation integrates goals over time and space, supporting rich memory encoding for flexible future use. Here, we manipulated motivational states using cover stories presented before a reinforcement learning task: The Imperative group imagined executing a museum heist, whereas the Interrogative group imagined planning a future heist. Participants repeatedly chose among four doors, representing different rooms of the museum, to sample trial-unique paintings with variable rewards. The next day, participants in both groups performed a surprise next-day memory test. Crucially, only the cover stories differed between the Imperative and Interrogative groups; the reinforcement learning task and incentive contingencies were identical, and all participants only received payment after the memory test. In an initial sample and a pre-registered replication, we demonstrated that Imperative motivation increased exploitation during reinforcement learning. Conversely, Interrogative motivation increased directed (but not random) exploration, despite the cost to their earnings. At test, the Interrogative group was more accurate at recognizing paintings and recalling associated values, relative to the Imperative group. In the Interrogative group, higher-value paintings were more likely to be remembered; Imperative motivation disrupted this effect of reward modulating memory. Overall, we demonstrate that a pre-learning motivational manipulation can bias learning and memory, bearing implications for education, behavior change, clinical interventions, and communication.",
        "link": "http://dx.doi.org/10.31234/osf.io/z2rwf"
    },
    {
        "id": 26825,
        "title": "Path Design for Cellular-Connected UAV with Reinforcement Learning",
        "authors": "Yong Zeng, Xiaoli Xu",
        "published": "2019-12",
        "citations": 35,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9014041"
    },
    {
        "id": 26826,
        "title": "Pursuit-evasion with Decentralized Robotic Swarm in Continuous State Space and Action Space via Deep Reinforcement Learning",
        "authors": "Gurpreet Singh, Daniel Lofaro, Donald Sofge",
        "published": "2020",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008971502260233"
    },
    {
        "id": 26827,
        "title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming",
        "authors": "",
        "published": "2017-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2017.2655663"
    },
    {
        "id": 26828,
        "title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming",
        "authors": "",
        "published": "2017-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2017.2678899"
    },
    {
        "id": 26829,
        "title": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Special Section on Deep Reinforcement Learning and Adaptive Dynamic Programming",
        "authors": "",
        "published": "2017-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2016.2640821"
    },
    {
        "id": 26830,
        "title": "Early Prediction of Human Action by Deep Reinforcement Learning",
        "authors": "Hareesh Devarakonda, Snehasis Mukherjee",
        "published": "2021-7-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ncc52529.2021.9530126"
    },
    {
        "id": 26831,
        "title": "Performance on Learning to Associate a Stimulus with Positive Reinforcement",
        "authors": "R. A. Boakes",
        "published": "2021-9-16",
        "citations": 40,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003150404-4"
    },
    {
        "id": 26832,
        "title": "Reinforcement Learning of Protein Conformational Ensemble",
        "authors": "Jiangyan Feng",
        "published": "2019-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.bpj.2018.11.1022"
    },
    {
        "id": 26833,
        "title": "Shaping Model-Free Reinforcement-Learning with Model-Based Pseudorewards",
        "authors": "Paul Krueger, Thomas Griffiths",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2018.1191-0"
    },
    {
        "id": 26834,
        "title": "Open quantum system control based on reinforcement learning",
        "authors": "Peng Wei, Na Li, Zairong Xi",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8865335"
    },
    {
        "id": 26835,
        "title": "Reinforcement Learning and Stochastic Optimization",
        "authors": "Warren B. Powell",
        "published": "2022-4-2",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068"
    },
    {
        "id": 26836,
        "title": "Prototype Reinforcement for Few-Shot Learning",
        "authors": "Liheng Xu, Qian Xie, Baoqing Jiang, Jiashuo Zhang",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9326820"
    },
    {
        "id": 26837,
        "title": "Deep Reinforcement Learning for Optimal Sailing Upwind",
        "authors": "Takumi Suda, Daniel Nikovski",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892369"
    },
    {
        "id": 26838,
        "title": "Mapless navigation based on continuous deep reinforcement learning",
        "authors": "Xing Chen, Lumei Su, Houde Dai",
        "published": "2021-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9727354"
    },
    {
        "id": 26839,
        "title": "Artificial Conversational Agent using Robust Adversarial Reinforcement Learning",
        "authors": "Isha Wadekar",
        "published": "2021-1-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccci50826.2021.9402336"
    },
    {
        "id": 26840,
        "title": "Dynamic Asset-Allocation and Consumption",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-8"
    },
    {
        "id": 26841,
        "title": "Multi-Agent Collision Avoidance with Provident Agents using Deep-Reinforcement Learning",
        "authors": "Mohammad Bahrami Karkevandi, Samaneh Hosseini Semnani",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAutonomous path planning is becoming more demanding, as the number of robotic agents in pedestrian-rich environments is gradually increasing. Collision-free navigation is an essential requirement for autonomous agents in a pedestrian-rich environment. In this paper, we will build upon the GA3C-CADRL algorithm to train agents that take the near future into consideration while moving around the environment. Specifically, we will introduce a new reward function and a novel rewarding mechanism to train provident agents that always prioritize finding a collision-free path. Agents that are trained with the new method will be shown to be more cautious of their surrounding environment and tend to avoid actions that may be seen as threatening by pedestrians. We will show that the new proposed algorithm outperforms the previous state-of-the-art algorithm, GA3C-CADRL, in terms of successful navigation throughout the entire test set.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2797884/v1"
    },
    {
        "id": 26842,
        "title": "Introduction to Reinforcement Learning",
        "authors": "Yixuan Sun, Krishnan Raghavan, Prasanna Balaprakash",
        "published": "2023-11-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003359593-9"
    },
    {
        "id": 26843,
        "title": "Anwendung von Reinforcement Learning in industriellen cyberphysischen Systemen",
        "authors": "David Heik, Fouad Bahrpeyma, Dirk Reichelt",
        "published": "2023-3-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33968/2023.10"
    },
    {
        "id": 26844,
        "title": "Regret Analysis in Deterministic Reinforcement Learning",
        "authors": "Damianos Tranos, Alexandre Proutiere",
        "published": "2021-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9682972"
    },
    {
        "id": 26845,
        "title": "Wireless Power Control via Meta-Reinforcement Learning",
        "authors": "Ziyang Lu, M. Cenk Gursoy",
        "published": "2022-5-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45855.2022.9839179"
    },
    {
        "id": 26846,
        "title": "Reinforcement Learning for Few-Shot Text Generation Adaptation",
        "authors": "Pengsen Cheng, Jinqiao Dai, Jiamiao Liu, Jiayong Liu, Peng Jia",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4342101"
    },
    {
        "id": 26847,
        "title": "A Reinforcement Learning Approach to Optimal Execution",
        "authors": "Ciamac C. Moallemi, Muye Wang",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3897442"
    },
    {
        "id": 26848,
        "title": "Decentralized Multi-Agent Deep Reinforcement Learning: A Competitive-Game Perspective",
        "authors": "Marc Espinós Longa, Antonios Tsourdos, Inalhan Gokhan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDeep reinforcement learning (DRL) has been widely studied in single agent learning but require further development and understanding in the multi-agent field.  As one of the most complex swarming settings, competitive learning evaluates the performance of multiple teams of agents cooperating to achieve certain goals while surpassing the rest of group candidates.  Such dynamical complexity makes the multi-agent problem hard to solve even for niche DRL methods. Within a competitive framework, we study state-of-the-art actor-critic and Q algorithms and analyze in depth their variants (e.g., prioritization, dual networks, etc.) in terms of performance and convergence. For completeness of discussion, we present and assess an asynchronous and prioritized version of proximal policy optimization actor-critic technique (P3O) against the other benchmarks.  Results prove that Q-based approaches are more robust and reliable than actor-critic configurations for the given setting. In addition, we suggest incorporating local team communication and combining DRL with direct search optimization to improve learning, especially in challenging scenarios with partial observations.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2065000/v1"
    },
    {
        "id": 26849,
        "title": "A novel observer-based reinforcement learning for uncertain nonlinear systems with disturbances",
        "authors": "Dexin Zhang, Xiaoping Shi, Shenmin Song",
        "published": "No Date",
        "citations": 0,
        "abstract": "This study proposes an observer-based reinforcement learning(RL) control\nscheme for uncertain nonlinear systems subject to various external\ndisturbances. The proposed approach regards the total uncertainty\nestimated by the extended state observer (ESO) as potential model\ninformation, which is incorporated into the known part of the system\ndynamics. Based on the updated known dynamics, an RL structure is\nconstructed to approximate the optimal solution of the HJB equation\nwithout the persistence of the excitation (PE) condition. The\nconvergence of the proposed policy to a neighborhood of the optimal\npolicy is proven, and the stability of the system states is guaranteed.\nThe comparative simulation results demonstrate improved performance with\na significantly reduced cost of the developed controller, and the\nsensitivity of control gain in the input channel is also relaxed.",
        "link": "http://dx.doi.org/10.22541/au.167766515.57662539/v1"
    },
    {
        "id": 26850,
        "title": "Exploring the Potential of Reinforcement Learning in Intelligent Robotics",
        "authors": "",
        "published": "2022-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.59121/kcisr22120001"
    },
    {
        "id": 26851,
        "title": "The pursuit of happiness: A reinforcement learning perspective on habituation and comparisons",
        "authors": "Rachit Dubey, Tom Griffiths, Peter Dayan",
        "published": "No Date",
        "citations": 0,
        "abstract": "In evaluating our choices, we often suffer from two tragic relativities. First, when our lives change for the better, we rapidly habituate to the higher standard of living. Second, we cannot escape comparingourselves to various relative standards. Habituation and comparisons can be very disruptive to decision-making and happiness, and till date, it remains a puzzle why they have come to be a part of cognition in the first place. Here, we present computational evidence that suggests that these features might play an important role in promoting adaptive behavior. Using the framework of reinforcement learning, we explore the benefit of employing a reward function that, in addition to the reward provided by the underlying task, also depends on prior expectations and relative comparisons. We find that while agents equipped with this reward function are less happy, they learn faster and significantly outperform standard reward-based agents in a wide range of environments. Specifically, we find that relative comparisons speed up learning by providing an exploration incentive to the agents, and prior expectations serve as a useful aid to comparisons, especially in sparsely-rewarded and non-stationary environments. Our simulations also reveal potential drawbacks of this reward function and show that agents perform suboptimally when comparisons are left unchecked and when there are too many similar options. Together, our results help explain why we are prone to becoming trapped in a cycle of never-ending wants and desires, and may shed light on psychopathologies such as depression, materialism, and overconsumption.",
        "link": "http://dx.doi.org/10.31234/osf.io/8jd2x"
    },
    {
        "id": 26852,
        "title": "Optimal Market Making by Reinforcement Learning",
        "authors": "Matias Selser, Javier Kreiner, Manuel Maurette",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3829984"
    },
    {
        "id": 26853,
        "title": "A traffic light control method based on multi-agent deep reinforcement learning algorithm",
        "authors": "Dongjiang Liu, Leixiao Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAn efficient way to cope with traffic congestion is Intelligent Traffic Light Control (ITLC). Particularly, multi-agent based intelligent traffic light control algorithms are more popular and efficient than others. But there are still some problems in these algorithms. Firstly, an efficient communication mechanism is needed. As current traffic condition of an intersection can impact other intersections' future traffic condition, every agent should know the traffic condition of other intersections through communication. Thus, a new communication mechanism should be designed. By using this mechanism, traffic condition of an intersection can be passed to several relevant agents easily. And traffic condition of an intersection can be described simply and clearly. Secondly, asynchronization of message should be considered while processing received messages. As duration time of traffic lights' cycles is different from agent to agent and message sending event happens at the end of each traffic light cycle, different agents will send message at different time. Moreover, every agent tries to process the received messages at the end of each traffic light cycle as well. So it is hard for an agent to decide which message is the latest one and the most valuable. Thirdly, a new reward calculating method should be designed. In the traditional reinforcement learning based ITLC algorithms, only queue length of congested cars or waiting time of these cars is considered while calculating reward value. But, both of them are very important for measuring the level of traffic congestion. So a new reward calculation method is needed. To solve all these problems, in this paper, a new ITLC algorithm is proposed. At the same time, a new message sending and processing method is proposed and adopted by this ITLC algorithm. The first and the second problems described above are tried to be solved by using this new method. Besides, to measure traffic congestion in a more reasonable way, a new reward calculation method is proposed and used. This method takes both waiting time and queue length into consideration.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2626589/v1"
    },
    {
        "id": 26854,
        "title": "Data-Driven Output Consensus for a Class of Discrete-Time Multiagent Systems by Reinforcement Learning Techniques",
        "authors": "Yuanshan Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4697059"
    },
    {
        "id": 26855,
        "title": "Practical Reinforcement Learning of Stabilizing Economic MPC",
        "authors": "Mario Zanon, Sebastien Gros, Alberto Bemporad",
        "published": "2019-6",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc.2019.8795816"
    },
    {
        "id": 26856,
        "title": "Optimal State Estimation Using Model-Free Reinforcement Learning",
        "authors": "Haoran Ma, Ying Yang, Dingguo Liang",
        "published": "2021-10-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728004"
    },
    {
        "id": 26857,
        "title": "Inhibitory and excitatory mechanisms in the human cingulate-cortex support reinforcement learning",
        "authors": "Vered Bezalel, Rony Paz, Assaf Tal",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe dorsal anterior cingulate cortex (dACC) is crucial for motivation, reward- and error-guided decision-making, yet its excitatory and inhibitory mechanisms remain poorly explored in humans. In particular, the balance between excitation and inhibition (E/I), demonstrated to play a role in animal studies, is difficult to measure in behaving humans. Here, we used magnetic-resonance-spectroscopy (1H-MRS) to examine these mechanisms during reinforcement learning with three different conditions: high cognitive load (uncertainty); probabilistic discrimination learning; and a control null-condition. Subjects learned to prefer the gain option in the discrimination phase and had no preference in the other conditions. We found increased GABA levels during the uncertainty condition, suggesting recruitment of inhibitory systems during high cognitive load when trying to learn. Further, higher GABA levels during the null (baseline) condition correlated with improved discrimination learning. Finally, excitatory and inhibitory levels were correlated during high cognitive load. The result suggests that availability of dACC inhibitory resources enables successful learning. Our approach establishes a novel way to examine the contribution of the balance between excitation and inhibition to learning and motivation in behaving humans.",
        "link": "http://dx.doi.org/10.1101/318659"
    },
    {
        "id": 26858,
        "title": "Dynamic Pricing in Ride-Hailing Intelligent Transportation Systems by Using Deep Reinforcement Learning",
        "authors": "Sajad Heydari, Elham Akhondzadeh Noughabi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4746254"
    },
    {
        "id": 26859,
        "title": "Learning Path Recommendation Based on Knowledge Tracing and Reinforcement Learning",
        "authors": "Han Wan, Baoliang Che, Hongzhen Luo, Xiaoyan Luo",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icalt58122.2023.00021"
    },
    {
        "id": 26860,
        "title": "Current Challenges",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5_7"
    },
    {
        "id": 26861,
        "title": "Enabling Rewards for Reinforcement Learning in Laser Beam Welding processes through Deep Learning",
        "authors": "Markus Schmitz, Florian Pinsker, Alexander Ruhri, Beibei Jiang, Georgij Safronov",
        "published": "2020-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla51294.2020.00221"
    },
    {
        "id": 26862,
        "title": "Parameter optimization design of MFAC based on Reinforcement Learning",
        "authors": "Shida Liu, Xiongbo Jia, Honghai Ji, Lingling Fan",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10167283"
    },
    {
        "id": 26863,
        "title": "Decision-Making and Learning in an Unknown Environment",
        "authors": "Uwe Lorenz",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-09030-1_4"
    },
    {
        "id": 26864,
        "title": "Deep Reinforcement Learning in Cloud Elasticity Through Offline Learning and Return Based Scaling",
        "authors": "Miltiadis Chrysopoulos, Ioannis Konstantinou, Nectarios Koziris",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cloud60044.2023.00012"
    },
    {
        "id": 26865,
        "title": "Faster learning from slow features: The temporal coherence prior in human reinforcement learning",
        "authors": "Noa Hedrich, Sam Hall-McMaster, Eric Schulz, Nicolas Schuck",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1471-0"
    },
    {
        "id": 26866,
        "title": "Learning of Dynamic Models",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch4"
    },
    {
        "id": 26867,
        "title": "Shielded Reinforcement Learning: A review of reactive methods for safe learning",
        "authors": "Haritz Odriozola-Olalde, Maider Zamalloa, Nestor Arana-Arexolaleiba",
        "published": "2023-1-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sii55687.2023.10039301"
    },
    {
        "id": 26868,
        "title": "Deep Reinforcement Learning for Structural Model Updating Using Transfer Learning Mechanism",
        "authors": "Issac Kwok-Tai Pang, Yuqing Gao, Khalid M. Mosalam",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1061/9780784485231.044"
    },
    {
        "id": 26869,
        "title": "Integrating contrastive learning with dynamic models for reinforcement learning from images",
        "authors": "Bang You, Oleg Arenz, Youping Chen, Jan Peters",
        "published": "2022-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2021.12.094"
    },
    {
        "id": 26870,
        "title": "Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning",
        "authors": "Charles Schaff, David Yunis, Ayan Chakrabarti, Matthew R. Walter",
        "published": "2019-5",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8793537"
    },
    {
        "id": 26871,
        "title": "Automatic View Generation with Deep Learning and Reinforcement Learning",
        "authors": "Haitao Yuan, Guoliang Li, Ling Feng, Ji Sun, Yue Han",
        "published": "2020-4",
        "citations": 32,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icde48307.2020.00133"
    },
    {
        "id": 26872,
        "title": "Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning",
        "authors": "Laura Smith, Ilya Kostrikov, Sergey Levine",
        "published": "2023-7-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.056"
    },
    {
        "id": 26873,
        "title": "Tethered Multicopter Guidance in GPS-Denied Environments Through Reinforcement Learning",
        "authors": "",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2023-0507.vid"
    },
    {
        "id": 26874,
        "title": "A guided reinforcement learning approach using shared control templates for learning manipulation skills in the real world",
        "authors": "Abhishek Padalkar, Gabriel Quere, Antonin Raffin, João Silvério, Freek Stulp",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe requirement for a high number of trials has been a major limiting factor for the application of Reinforcement Learning (RL) in robotics. Learning skills directly on real robots requires time, causes wear and tear and can lead to damage to the robot and environment due to unsafe exploratory actions. The success of learning skills in simulation and transferring them to real robots has also been limited by the gap between reality and simulation. This is particularly problematic for tasks involving contact with the environment as contact dynamics are hard to model and simulate. In this paper we propose a framework which leverages a shared control framework for modeling known constraints defined by object interactions and task geometry to reduce the state and action spaces and hence the overall dimensionality of the reinforcement learning problem. The unknown task knowledge and actions are learned by a reinforcement learning agent by conducting exploration in the constrained environment. Using a pouring task and grid-clamp placement task (similar to peg-in-hole) as use cases and a 7-DoF arm, we show that our approach can be used to learn directly on the real robot. The pouring task is learned in only 65 episodes (~16 minutes) and the grid-clamp placement task is learned in 75 episodes (~17 minutes) with strong safety guarantees and simple reward functions, greatly alleviating the need for simulation.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3289569/v1"
    },
    {
        "id": 26875,
        "title": "Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning",
        "authors": "Patrick Emami, Xiangyu Zhang, David Biagioni, Ahmed S. Zamzam",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10384223"
    },
    {
        "id": 26876,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2021-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9781786349378_0008"
    },
    {
        "id": 26877,
        "title": "Reinforcement Learning Approach for Adaptive e-Learning Based on Multiple Learner Characteristics",
        "authors": "Dan Oyuga Anne, Elizaphan Maina",
        "published": "2021-12-8",
        "citations": 0,
        "abstract": "We introduce a novel three stepwise model of adaptive e-learning using multiple learner characteristics.  We design a model of a learner attributes enlisting the study domain, summary details of the student and the requirements of the student. We include the theories of learning style to categorize and identify specific individuals so as to improve their experience on the online learning platform and apply it in the model. The affective state extraction model which extracts learner emotions from text inputs during the platform interactions. We finally pass the system extracted information the adaptivity domain which uses the off-policy Q-learning model free algorithm (Jang et al., 2019) to structure the learning path into tutorials, lectures and workshops depending on predefined constraints of learning. Simulated results show better adaptivity incases of multiple characteristics as opposed to single learner characteristics. Further research to include more than three characteristics as in this research.",
        "link": "http://dx.doi.org/10.32591/coas.ojit.0402.03055o"
    },
    {
        "id": 26878,
        "title": "Cross-Domain Sentiment Classification via Deep Reinforcement Learning",
        "authors": "Lintao Dou, Jian Huang",
        "published": "2022-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3578741.3578807"
    },
    {
        "id": 26879,
        "title": "Applications of machine learning in neuroscience and inspiration of reinforcement learning for computational neuroscience",
        "authors": "Weihang Jiang",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "High-performance machine learning algorithms have always been one of the concerns of many researchers. Since its birth, machine learning has been a product of multidisciplinary integration. Especially in the field of neuroscience, models from related fields continue to inspire the development of neural networks and deepen people's understanding of neural networks. The mathematical and quantitative modeling approach to research brought about by machine learning is also feeding into the development of neuroscience. One of the emerging products of this is computational neuroscience. Computational neuroscience has been pushing the boundaries of models of brain function in recent years, and just as early studies of visual hierarchy influenced neural networks, computational neuroscience has great potential to lead to higher performance machine learning algorithms, particularly in the development of deep learning algorithms with strong links to neuroscience. In this paper, it first reviews the help and achievements of machine learning for neuroscience in recent years specially in fMRI image recognition and look at the possibilities for the future development of neural networks due to the recent development of the computational neuroscience in psychiatry of the temporal difference model for dopamine and serotonin.",
        "link": "http://dx.doi.org/10.54254/2755-2721/4/2023308"
    },
    {
        "id": 26880,
        "title": "Learning Continuous Control Actions for Robotic Grasping with Reinforcement Learning",
        "authors": "Asad Ali Shahid, Loris Roveda, Dario Piga, Francesco Braghin",
        "published": "2020-10-11",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc42975.2020.9282951"
    },
    {
        "id": 26881,
        "title": "Monocular vision guided deep reinforcement learning UAV systems with representation learning perception",
        "authors": "Zhihan Xue, Tad Gonsalves",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/09540091.2023.2183828"
    },
    {
        "id": 26882,
        "title": "Automatic RNN Cell Design for Knowledge Tracing using Reinforcement Learning",
        "authors": "Xinyi Ding, Eric C. Larson",
        "published": "2020-8-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3386527.3406729"
    },
    {
        "id": 26883,
        "title": "Resource Scheduling Based on Reinforcement Learning Based on Federated Learning",
        "authors": "Yabin Wang,  , Jing Yu",
        "published": "2021-1",
        "citations": 1,
        "abstract": "The emergence of edge computing makes up for the limited capacity of devices. By migrating intensive computing tasks from them to edge nodes (EN), we can save more energy while still maintaining the quality of service.Computing offload decision involves collaboration and complex resource management. It should be determined in real time according to dynamic workload and network environment. The simulation experiment method is used to maximize the long-term utility by deploying deep reinforcement learning agents on IOT devices and edge nodes, and the alliance learning is introduced to distribute the deep reinforcement learning agents. First, build the Internet of things system supporting edge computing, download the existing model from the edge node for training, and unload the intensive computing task to the edge node for training; upload the updated parameters to the edge node, and the edge node aggregates the parameters with the The model at the edge nodecan get a new model; the cloud can get a new model at the edge node and aggregate, and can also get updated parameters from the edge node to apply to the device.",
        "link": "http://dx.doi.org/10.17706/jsw.16.1.39-45"
    },
    {
        "id": 26884,
        "title": "Evolutionary Computation and the Reinforcement Learning Problem",
        "authors": "Stephen Kelly, Jory Schossau",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3814-8_4"
    },
    {
        "id": 26885,
        "title": "Learning to Locomote: Understanding How Environment Design Matters for Deep Reinforcement Learning",
        "authors": "Daniele Reda, Tianxin Tao, Michiel van de Panne",
        "published": "2020-10-16",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3424636.3426907"
    },
    {
        "id": 26886,
        "title": "Incremental Learning of Planning Actions in Model-Based Reinforcement Learning",
        "authors": "Jun Hao Alvin Ng, Ronald P. A. Petrick",
        "published": "2019-8",
        "citations": 2,
        "abstract": "The soundness and optimality of a plan depends on the correctness of the domain model. Specifying complete domain models can be difficult when interactions between an agent and its environment are complex. We propose a model-based reinforcement learning (MBRL) approach to solve planning problems with unknown models. The model is learned incrementally over episodes using only experiences from the current episode which suits non-stationary environments. We introduce the novel concept of reliability as an intrinsic motivation for MBRL, and a method to learn from failure to prevent repeated instances of similar failures. Our motivation is to improve the learning efficiency and goal-directedness of MBRL. We evaluate our work with experimental results for three planning domains.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/443"
    },
    {
        "id": 26887,
        "title": "Learning to Drive at Unsignalized Intersections using Attention-based Deep Reinforcement Learning",
        "authors": "Hyunki Seong, Chanyoung Jung, Seungwook Lee, David Hyunchul Shim",
        "published": "2021-9-19",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9564720"
    },
    {
        "id": 26888,
        "title": "Contour Error Modeling and Compensation of CNC Machining Based on Deep Learning and Reinforcement Learning",
        "authors": "Yakun Jiang, Jihong Chen, Huicheng Zhou, Jianzhong Yang, Pengcheng Hu, Junxiang Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nContour error compensation of the Computer Numerical Control (CNC) machine tool is a vital technology that can improve machining accuracy and quality. To achieve this goal, the tracking error of a feeding axis, which is a dominant issue incurring the contour error, should be firstly modeled and then a proper compensation strategy should be determined. However, building the precise tracking error prediction model is a challenging task because of the nonlinear issues like backlash and friction involved in the feeding axis; besides, the optimal compensation parameter is also difficult to determine because it is sensitive to the machining tool path. In this paper, a set of novel approaches for contour error prediction and compensation is presented based on the technologies of deep learning and reinforcement learning. By utilizing the internal data of the CNC system, the tracking error of the feeding axis is modeled as a Nonlinear Auto-Regressive Long-Short Term Memory (NAR-LSTM) network, considering all the nonlinear issues of the feeding axis. Given the contour error as calculated based on the predicted tracking error of each feeding axis, a compensation strategy is presented with its parameters identified efficiently by a Time-Series Deep Q-Network (TS-DQN) as designed in our work. To validate the feasibility and advantage of the proposed approaches, extensive experiments are conducted, testifying that, our approaches can predict the tracking error and contour error with very good precision (better than about 99% and 90% respectively), and the contour error compensated based on the predicted results and our compensation strategy is significantly reduced (about 70%~85% reduction) with the machining quality improved drastically (machining error reduced about 50%).",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-404176/v1"
    },
    {
        "id": 26889,
        "title": "Learning When Not to Answer: a Ternary Reward Structure for Reinforcement Learning Based Question Answering",
        "authors": "Fréderic Godin, Anjishnu Kumar, Arpit Mittal",
        "published": "2019",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/n19-2016"
    },
    {
        "id": 26890,
        "title": "Research on Automated Container Terminal Assignment Problem Based on Multi-agent Architecture Reinforcement Learning Algorithm",
        "authors": "Lu Jiang",
        "published": "2022-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml57342.2022.10009845"
    },
    {
        "id": 26891,
        "title": "It’s not Magic After All – Machine Learning in Snap! using Reinforcement Learning",
        "authors": "Sven Jatzlau, Tilman Michaeli, Stefan Seegerer, Ralf Romeike",
        "published": "2019-10",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bb48857.2019.8941208"
    },
    {
        "id": 26892,
        "title": "Frontostriatal development and probabilistic reinforcement learning during adolescence",
        "authors": "Samantha DePasque, Adriana Galván",
        "published": "2017-9",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.nlm.2017.04.009"
    },
    {
        "id": 26893,
        "title": "Stochastic Integrated Actor–Critic for Deep Reinforcement Learning",
        "authors": "Jiaohao Zheng, Mehmet Necip Kurt, Xiaodong Wang",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3212273"
    },
    {
        "id": 26894,
        "title": "Artificial Neural Networks and Reinforcement Learning for Model-based Design of an Automated Vehicle Guidance System",
        "authors": "Or Yarom, Soeren Scherler, Marian Goellner, Xiaobo Liu-Henke",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008995407250733"
    },
    {
        "id": 26895,
        "title": "&amp;lt;p&amp;gt;Less is more: Lightweight reinforcement learning method for traffic signal control with less observation&amp;lt;/p&amp;gt;",
        "authors": "Qiang Wu",
        "published": "2023-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26599/etsd.2023.9190021"
    },
    {
        "id": 26896,
        "title": "Verification of Adversarially Robust Reinforcement Learning Mechanisms in Aerospace Systems",
        "authors": "",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2023-1070.vid"
    },
    {
        "id": 26897,
        "title": "Placing approach-avoidance conflict within the framework of multi-objective reinforcement learning",
        "authors": "Enkhzaya Enkhtaivan, Joel Nishimura, Amy Cochran",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMany psychiatric disorders are marked by impaired decision-making during an approach-avoidance conflict. Current experiments elicit approachavoidance conflicts in bandit tasks by pairing an individual’s actions with consequences that are simultaneously desirable (reward) and undesirable (harm). We frame approach-avoidance conflict tasks as a multi-objective multi-armed bandit. By defining a general decision-maker as a limiting sequence of actions, we disentangle the decision process from learning. Each decision maker can then be identified as a multi-dimensional point representing its long-term average expected outcomes, while different decision making models can be associated by the geometry of their ‘feasible region’, the set of all possible long term performances on a fixed task. We introduce three example decision-makers based on popular reinforcement learning models and characterize their feasible regions, including whether they can be Pareto optimal. From this perspective, we find that existing tasks are unable to distinguish between the three examples of decision-makers. We show how to design new tasks whose geometric structure can be used to better distinguish between decision-makers. These findings are expected to guide the design of approach-avoidance conflict tasks and the modeling of resulting decision-making behavior.",
        "link": "http://dx.doi.org/10.1101/2023.01.05.522878"
    },
    {
        "id": 26898,
        "title": "Deep Reinforcement Learning for antennas positioning",
        "authors": "Yassine Hachaïchi, Houssem Mezzi",
        "published": "2022-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsis56166.2022.10118400"
    },
    {
        "id": 26899,
        "title": "Overview of Meta-Reinforcement Learning Research",
        "authors": "Peng Shengguang",
        "published": "2020-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itca52113.2020.00019"
    },
    {
        "id": 26900,
        "title": "Pytester: Deep Reinforcement Learning for Text-to-Testcase Generation",
        "authors": "Wannita Takerngsaksiri, Rujikorn Charakorn, Chakkrit Tantithamthavorn, Yuan-Fang Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4736450"
    },
    {
        "id": 26901,
        "title": "Optimal Market Making by Reinforcement Learning",
        "authors": "Matias Selser, Javier Kreiner, Manuel Maurette",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3829984"
    },
    {
        "id": 26902,
        "title": "A traffic light control method based on multi-agent deep reinforcement learning algorithm",
        "authors": "Dongjiang Liu, Leixiao Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAn efficient way to cope with traffic congestion is Intelligent Traffic Light Control (ITLC). Particularly, multi-agent based intelligent traffic light control algorithms are more popular and efficient than others. But there are still some problems in these algorithms. Firstly, an efficient communication mechanism is needed. As current traffic condition of an intersection can impact other intersections' future traffic condition, every agent should know the traffic condition of other intersections through communication. Thus, a new communication mechanism should be designed. By using this mechanism, traffic condition of an intersection can be passed to several relevant agents easily. And traffic condition of an intersection can be described simply and clearly. Secondly, asynchronization of message should be considered while processing received messages. As duration time of traffic lights' cycles is different from agent to agent and message sending event happens at the end of each traffic light cycle, different agents will send message at different time. Moreover, every agent tries to process the received messages at the end of each traffic light cycle as well. So it is hard for an agent to decide which message is the latest one and the most valuable. Thirdly, a new reward calculating method should be designed. In the traditional reinforcement learning based ITLC algorithms, only queue length of congested cars or waiting time of these cars is considered while calculating reward value. But, both of them are very important for measuring the level of traffic congestion. So a new reward calculation method is needed. To solve all these problems, in this paper, a new ITLC algorithm is proposed. At the same time, a new message sending and processing method is proposed and adopted by this ITLC algorithm. The first and the second problems described above are tried to be solved by using this new method. Besides, to measure traffic congestion in a more reasonable way, a new reward calculation method is proposed and used. This method takes both waiting time and queue length into consideration.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2626589/v1"
    },
    {
        "id": 26903,
        "title": "Data-Driven Output Consensus for a Class of Discrete-Time Multiagent Systems by Reinforcement Learning Techniques",
        "authors": "Yuanshan Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4697059"
    },
    {
        "id": 26904,
        "title": "Practical Reinforcement Learning of Stabilizing Economic MPC",
        "authors": "Mario Zanon, Sebastien Gros, Alberto Bemporad",
        "published": "2019-6",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc.2019.8795816"
    },
    {
        "id": 26905,
        "title": "Optimal State Estimation Using Model-Free Reinforcement Learning",
        "authors": "Haoran Ma, Ying Yang, Dingguo Liang",
        "published": "2021-10-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728004"
    },
    {
        "id": 26906,
        "title": "Inhibitory and excitatory mechanisms in the human cingulate-cortex support reinforcement learning",
        "authors": "Vered Bezalel, Rony Paz, Assaf Tal",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe dorsal anterior cingulate cortex (dACC) is crucial for motivation, reward- and error-guided decision-making, yet its excitatory and inhibitory mechanisms remain poorly explored in humans. In particular, the balance between excitation and inhibition (E/I), demonstrated to play a role in animal studies, is difficult to measure in behaving humans. Here, we used magnetic-resonance-spectroscopy (1H-MRS) to examine these mechanisms during reinforcement learning with three different conditions: high cognitive load (uncertainty); probabilistic discrimination learning; and a control null-condition. Subjects learned to prefer the gain option in the discrimination phase and had no preference in the other conditions. We found increased GABA levels during the uncertainty condition, suggesting recruitment of inhibitory systems during high cognitive load when trying to learn. Further, higher GABA levels during the null (baseline) condition correlated with improved discrimination learning. Finally, excitatory and inhibitory levels were correlated during high cognitive load. The result suggests that availability of dACC inhibitory resources enables successful learning. Our approach establishes a novel way to examine the contribution of the balance between excitation and inhibition to learning and motivation in behaving humans.",
        "link": "http://dx.doi.org/10.1101/318659"
    },
    {
        "id": 26907,
        "title": "Dynamic Pricing in Ride-Hailing Intelligent Transportation Systems by Using Deep Reinforcement Learning",
        "authors": "Sajad Heydari, Elham Akhondzadeh Noughabi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4746254"
    },
    {
        "id": 26908,
        "title": "Tethered Multicopter Guidance in GPS-Denied Environments Through Reinforcement Learning",
        "authors": "",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2023-0507.vid"
    },
    {
        "id": 26909,
        "title": "A guided reinforcement learning approach using shared control templates for learning manipulation skills in the real world",
        "authors": "Abhishek Padalkar, Gabriel Quere, Antonin Raffin, João Silvério, Freek Stulp",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe requirement for a high number of trials has been a major limiting factor for the application of Reinforcement Learning (RL) in robotics. Learning skills directly on real robots requires time, causes wear and tear and can lead to damage to the robot and environment due to unsafe exploratory actions. The success of learning skills in simulation and transferring them to real robots has also been limited by the gap between reality and simulation. This is particularly problematic for tasks involving contact with the environment as contact dynamics are hard to model and simulate. In this paper we propose a framework which leverages a shared control framework for modeling known constraints defined by object interactions and task geometry to reduce the state and action spaces and hence the overall dimensionality of the reinforcement learning problem. The unknown task knowledge and actions are learned by a reinforcement learning agent by conducting exploration in the constrained environment. Using a pouring task and grid-clamp placement task (similar to peg-in-hole) as use cases and a 7-DoF arm, we show that our approach can be used to learn directly on the real robot. The pouring task is learned in only 65 episodes (~16 minutes) and the grid-clamp placement task is learned in 75 episodes (~17 minutes) with strong safety guarantees and simple reward functions, greatly alleviating the need for simulation.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3289569/v1"
    },
    {
        "id": 26910,
        "title": "Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning",
        "authors": "Patrick Emami, Xiangyu Zhang, David Biagioni, Ahmed S. Zamzam",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10384223"
    },
    {
        "id": 26911,
        "title": "Reinforcement Learning",
        "authors": "",
        "published": "2021-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9781786349378_0008"
    },
    {
        "id": 26912,
        "title": "Reinforcement Learning Approach for Adaptive e-Learning Based on Multiple Learner Characteristics",
        "authors": "Dan Oyuga Anne, Elizaphan Maina",
        "published": "2021-12-8",
        "citations": 0,
        "abstract": "We introduce a novel three stepwise model of adaptive e-learning using multiple learner characteristics.  We design a model of a learner attributes enlisting the study domain, summary details of the student and the requirements of the student. We include the theories of learning style to categorize and identify specific individuals so as to improve their experience on the online learning platform and apply it in the model. The affective state extraction model which extracts learner emotions from text inputs during the platform interactions. We finally pass the system extracted information the adaptivity domain which uses the off-policy Q-learning model free algorithm (Jang et al., 2019) to structure the learning path into tutorials, lectures and workshops depending on predefined constraints of learning. Simulated results show better adaptivity incases of multiple characteristics as opposed to single learner characteristics. Further research to include more than three characteristics as in this research.",
        "link": "http://dx.doi.org/10.32591/coas.ojit.0402.03055o"
    },
    {
        "id": 26913,
        "title": "Cross-Domain Sentiment Classification via Deep Reinforcement Learning",
        "authors": "Lintao Dou, Jian Huang",
        "published": "2022-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3578741.3578807"
    },
    {
        "id": 26914,
        "title": "Applications of machine learning in neuroscience and inspiration of reinforcement learning for computational neuroscience",
        "authors": "Weihang Jiang",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "High-performance machine learning algorithms have always been one of the concerns of many researchers. Since its birth, machine learning has been a product of multidisciplinary integration. Especially in the field of neuroscience, models from related fields continue to inspire the development of neural networks and deepen people's understanding of neural networks. The mathematical and quantitative modeling approach to research brought about by machine learning is also feeding into the development of neuroscience. One of the emerging products of this is computational neuroscience. Computational neuroscience has been pushing the boundaries of models of brain function in recent years, and just as early studies of visual hierarchy influenced neural networks, computational neuroscience has great potential to lead to higher performance machine learning algorithms, particularly in the development of deep learning algorithms with strong links to neuroscience. In this paper, it first reviews the help and achievements of machine learning for neuroscience in recent years specially in fMRI image recognition and look at the possibilities for the future development of neural networks due to the recent development of the computational neuroscience in psychiatry of the temporal difference model for dopamine and serotonin.",
        "link": "http://dx.doi.org/10.54254/2755-2721/4/2023308"
    },
    {
        "id": 26915,
        "title": "Learning Continuous Control Actions for Robotic Grasping with Reinforcement Learning",
        "authors": "Asad Ali Shahid, Loris Roveda, Dario Piga, Francesco Braghin",
        "published": "2020-10-11",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc42975.2020.9282951"
    },
    {
        "id": 26916,
        "title": "Monocular vision guided deep reinforcement learning UAV systems with representation learning perception",
        "authors": "Zhihan Xue, Tad Gonsalves",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/09540091.2023.2183828"
    },
    {
        "id": 26917,
        "title": "Automatic RNN Cell Design for Knowledge Tracing using Reinforcement Learning",
        "authors": "Xinyi Ding, Eric C. Larson",
        "published": "2020-8-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3386527.3406729"
    },
    {
        "id": 26918,
        "title": "Resource Scheduling Based on Reinforcement Learning Based on Federated Learning",
        "authors": "Yabin Wang,  , Jing Yu",
        "published": "2021-1",
        "citations": 1,
        "abstract": "The emergence of edge computing makes up for the limited capacity of devices. By migrating intensive computing tasks from them to edge nodes (EN), we can save more energy while still maintaining the quality of service.Computing offload decision involves collaboration and complex resource management. It should be determined in real time according to dynamic workload and network environment. The simulation experiment method is used to maximize the long-term utility by deploying deep reinforcement learning agents on IOT devices and edge nodes, and the alliance learning is introduced to distribute the deep reinforcement learning agents. First, build the Internet of things system supporting edge computing, download the existing model from the edge node for training, and unload the intensive computing task to the edge node for training; upload the updated parameters to the edge node, and the edge node aggregates the parameters with the The model at the edge nodecan get a new model; the cloud can get a new model at the edge node and aggregate, and can also get updated parameters from the edge node to apply to the device.",
        "link": "http://dx.doi.org/10.17706/jsw.16.1.39-45"
    },
    {
        "id": 26919,
        "title": "Evolutionary Computation and the Reinforcement Learning Problem",
        "authors": "Stephen Kelly, Jory Schossau",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3814-8_4"
    },
    {
        "id": 26920,
        "title": "Learning to Locomote: Understanding How Environment Design Matters for Deep Reinforcement Learning",
        "authors": "Daniele Reda, Tianxin Tao, Michiel van de Panne",
        "published": "2020-10-16",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3424636.3426907"
    },
    {
        "id": 26921,
        "title": "Incremental Learning of Planning Actions in Model-Based Reinforcement Learning",
        "authors": "Jun Hao Alvin Ng, Ronald P. A. Petrick",
        "published": "2019-8",
        "citations": 2,
        "abstract": "The soundness and optimality of a plan depends on the correctness of the domain model. Specifying complete domain models can be difficult when interactions between an agent and its environment are complex. We propose a model-based reinforcement learning (MBRL) approach to solve planning problems with unknown models. The model is learned incrementally over episodes using only experiences from the current episode which suits non-stationary environments. We introduce the novel concept of reliability as an intrinsic motivation for MBRL, and a method to learn from failure to prevent repeated instances of similar failures. Our motivation is to improve the learning efficiency and goal-directedness of MBRL. We evaluate our work with experimental results for three planning domains.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/443"
    },
    {
        "id": 26922,
        "title": "Learning to Drive at Unsignalized Intersections using Attention-based Deep Reinforcement Learning",
        "authors": "Hyunki Seong, Chanyoung Jung, Seungwook Lee, David Hyunchul Shim",
        "published": "2021-9-19",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9564720"
    },
    {
        "id": 26923,
        "title": "Contour Error Modeling and Compensation of CNC Machining Based on Deep Learning and Reinforcement Learning",
        "authors": "Yakun Jiang, Jihong Chen, Huicheng Zhou, Jianzhong Yang, Pengcheng Hu, Junxiang Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nContour error compensation of the Computer Numerical Control (CNC) machine tool is a vital technology that can improve machining accuracy and quality. To achieve this goal, the tracking error of a feeding axis, which is a dominant issue incurring the contour error, should be firstly modeled and then a proper compensation strategy should be determined. However, building the precise tracking error prediction model is a challenging task because of the nonlinear issues like backlash and friction involved in the feeding axis; besides, the optimal compensation parameter is also difficult to determine because it is sensitive to the machining tool path. In this paper, a set of novel approaches for contour error prediction and compensation is presented based on the technologies of deep learning and reinforcement learning. By utilizing the internal data of the CNC system, the tracking error of the feeding axis is modeled as a Nonlinear Auto-Regressive Long-Short Term Memory (NAR-LSTM) network, considering all the nonlinear issues of the feeding axis. Given the contour error as calculated based on the predicted tracking error of each feeding axis, a compensation strategy is presented with its parameters identified efficiently by a Time-Series Deep Q-Network (TS-DQN) as designed in our work. To validate the feasibility and advantage of the proposed approaches, extensive experiments are conducted, testifying that, our approaches can predict the tracking error and contour error with very good precision (better than about 99% and 90% respectively), and the contour error compensated based on the predicted results and our compensation strategy is significantly reduced (about 70%~85% reduction) with the machining quality improved drastically (machining error reduced about 50%).",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-404176/v1"
    },
    {
        "id": 26924,
        "title": "Learning When Not to Answer: a Ternary Reward Structure for Reinforcement Learning Based Question Answering",
        "authors": "Fréderic Godin, Anjishnu Kumar, Arpit Mittal",
        "published": "2019",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/n19-2016"
    },
    {
        "id": 26925,
        "title": "Research on Automated Container Terminal Assignment Problem Based on Multi-agent Architecture Reinforcement Learning Algorithm",
        "authors": "Lu Jiang",
        "published": "2022-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml57342.2022.10009845"
    },
    {
        "id": 26926,
        "title": "It’s not Magic After All – Machine Learning in Snap! using Reinforcement Learning",
        "authors": "Sven Jatzlau, Tilman Michaeli, Stefan Seegerer, Ralf Romeike",
        "published": "2019-10",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bb48857.2019.8941208"
    },
    {
        "id": 26927,
        "title": "Frontostriatal development and probabilistic reinforcement learning during adolescence",
        "authors": "Samantha DePasque, Adriana Galván",
        "published": "2017-9",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.nlm.2017.04.009"
    },
    {
        "id": 26928,
        "title": "Stochastic Integrated Actor–Critic for Deep Reinforcement Learning",
        "authors": "Jiaohao Zheng, Mehmet Necip Kurt, Xiaodong Wang",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3212273"
    },
    {
        "id": 26929,
        "title": "Verification of Adversarially Robust Reinforcement Learning Mechanisms in Aerospace Systems",
        "authors": "",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2023-1070.vid"
    },
    {
        "id": 26930,
        "title": "GENERATING MUSIC THERAPY USING DEEP LEARNING AND REINFORCEMENT LEARNING",
        "authors": "Sarthak Sengupta, Dr. Anuradha Konidena",
        "published": "2020-5-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33564/ijeast.2020.v04i12.089"
    },
    {
        "id": 26931,
        "title": "Improving Deep Reinforcement Learning for Financial Trading Using Neural Network Distillation",
        "authors": "Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas",
        "published": "2020-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp49062.2020.9231849"
    },
    {
        "id": 26932,
        "title": "Towards Using Deep Reinforcement Learning for Better COVID-19 Vaccine Distribution Strategies",
        "authors": "Fouad Trad, Salah El Falou",
        "published": "2022-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdma54072.2022.00007"
    },
    {
        "id": 26933,
        "title": "Reinforcement Learning to Provide Feedback and Support",
        "authors": "Mark Hoogendoorn, Burkhardt Funk",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-66308-1_9"
    },
    {
        "id": 26934,
        "title": "Learning of binocular fixations using anomaly detection with deep reinforcement learning",
        "authors": "Francois de La Bourdonnaye, Celine Teuliere, Thierry Chateau, Jochen Triesch",
        "published": "2017-5",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7965928"
    },
    {
        "id": 26935,
        "title": "A Dynamic Financial Knowledge Graph Based on Reinforcement Learning and Transfer Learning",
        "authors": "Rui Miao, Xia Zhang, Hongfei Yan, Chong Chen",
        "published": "2019-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata47090.2019.9005691"
    },
    {
        "id": 26936,
        "title": "Graph based skill acquisition and transfer Learning for continuous reinforcement learning domains",
        "authors": "Farzaneh Shoeleh, Masoud Asadpour",
        "published": "2017-2",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patrec.2016.08.009"
    },
    {
        "id": 26937,
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations",
        "authors": "Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, Sergey Levine",
        "published": "2018-6-26",
        "citations": 197,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2018.xiv.049"
    },
    {
        "id": 26938,
        "title": "Scene-adaptive radar tracking with deep reinforcement learning",
        "authors": "Michael Stephan, Lorenzo Servadei, José Arjona-Medina, Avik Santra, Robert Wille, Georg Fischer",
        "published": "2022-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2022.100284"
    },
    {
        "id": 26939,
        "title": "Facial Emotion Recognition Using Ensemble Learning",
        "authors": "GuanQun Xu, Wei Qi Yan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Facial emotion recognition (FER) is the task of identifying human emotions from facial expressions. The purpose of this book chapter is to improve accuracy of facial emotion recognition using integrated learning of lightweight networks without increasing the complexity or depth of the network. Compared to single lightweight models, it made a significant improvement. For a solution, the authors proposed an ensemble of mini-Xception models, where each expert is trained for a specific emotion and lets confidence score for the vote. Therefore, the expert model will transform the original multiclass task into binary tasks. The authors target the model to differentiate between a specific emotion and all others, facilitating the learning process. The principal innovation lies in our confidence-based voting mechanism, in which the experts “vote” based on their confidence scores rather than binary decisions.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch007"
    },
    {
        "id": 26940,
        "title": "Transfer Learning for Computer Vision",
        "authors": "Qadeem Khan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Computer vision has benefited from deep learning, making it possible to create complex systems. Computer vision has seen radical changes using deep learning techniques, specifically transfer learning for computer vision. It enables computers to understand and interpret visual data, such as images and videos, with great precision. Transfer learning in particular, is a deep learning technique that has transformed several areas of computer vision, including face recognition, semantic segmentation, object detection, and image categorization. This powerful technology is essential in applications such as autonomous vehicles, healthcare, surveillance, and more, since it has improved our capacity to identify, locate, and classify objects in images as well as comprehend complex visual scenes. In computer vision, transfer learning is a method that allows us to use previously taught models to tackle new problems. Benefits include shortened training times and efficient feature extraction. This chapter provides a brief help for implementation of transfer learning for computer vision.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch002"
    },
    {
        "id": 26941,
        "title": "Discrete-Time Reinforcement Learning Adaptive Control for Non-Gaussian Distribution of Sampling Intervals",
        "authors": "C. Treesatayapun",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3269441"
    },
    {
        "id": 26942,
        "title": "Efficient Reinforcement Learning in Resource Allocation Problems Through Permutation Invariant Multi-task Learning",
        "authors": "Desmond Cai, Shiau Hong Lim, Laura Wynter",
        "published": "2021-12-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683491"
    },
    {
        "id": 26943,
        "title": "Learning the Quadruped Robot by Reinforcement Learning (RL)",
        "authors": "A. Issa, A. Aldair",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "In this paper, a simulation was utilized to create and test the suggested controller and to investigate the ability of a quadruped robot based on the SimScape-Multibody toolbox, with PID controllers and deep deterministic policy gradient DDPG Reinforcement learning (RL) techniques. A quadruped robot has been simulated using three different scenarios based on two methods to control its movement, namely PID and DDPG. Instead of using two links per leg, the quadruped robot was constructed with three links per leg, to maximize movement versatility. The quadruped robot-built architecture uses twelve servomotors, three per leg, and 12-PID controllers in total for each servomotor. By utilizing the SimScape-Multibody toolbox, the quadruped robot can build without needing to use the mathematical model. By varying the walking robot's carrying load, the robustness of the developed controller is investigated. Firstly, the walking robot is designed with an open loop system and the result shows that the robot falls at starting of the simulation. Secondly, auto-tuning are used to find the optimal parameter like (KP, KI, and KD) of PID controllers, and resulting shows the robot can walk in a straight line. Finally, DDPG reinforcement learning is proposed to generate and improve the walking motion of the quadruped robot, and the results show that the behaviour of the walking robot has been improved compared with the previous cases, Also, the results produced when RL is employed instead of PID controllers are better.",
        "link": "http://dx.doi.org/10.37917/ijeee.18.2.15"
    },
    {
        "id": 26944,
        "title": "A Novel Heterogeneous Swarm Reinforcement Learning Method for Sequential Decision Making Problems",
        "authors": "Zohreh Akbari, Rainer Unland",
        "published": "2019-4-16",
        "citations": 4,
        "abstract": "Sequential Decision Making Problems (SDMPs) that can be modeled as Markov Decision Processes can be solved using methods that combine Dynamic Programming (DP) and Reinforcement Learning (RL). Depending on the problem scenarios and the available Decision Makers (DMs), such RL algorithms may be designed for single-agent systems or multi-agent systems that either consist of agents with individual goals and decision making capabilities, which are influenced by other agent’s decisions, or behave as a swarm of agents that collaboratively learn a single objective. Many studies have been conducted in this area; however, when concentrating on available swarm RL algorithms, one obtains a clear view of the areas that still require attention. Most of the studies in this area focus on homogeneous swarms and so far, systems introduced as Heterogeneous Swarms (HetSs) merely include very few, i.e., two or three sub-swarms of homogeneous agents, which either, according to their capabilities, deal with a specific sub-problem of the general problem or exhibit different behaviors in order to reduce the risk of bias. This study introduces a novel approach that allows agents, which are originally designed to solve different problems and hence have higher degrees of heterogeneity, to behave as a swarm when addressing identical sub-problems. In fact, the affinity between two agents, which measures the compatibility of agents to work together towards solving a specific sub-problem, is used in designing a Heterogeneous Swarm RL (HetSRL) algorithm that allows HetSs to solve the intended SDMPs.",
        "link": "http://dx.doi.org/10.3390/make1020035"
    },
    {
        "id": 26945,
        "title": "Reinforcement Learning for Improvement Measure Selection in Learning Factories",
        "authors": "Marvin Carl May, Sara Hermeler, Eric Mauch, Julia Dvorak, Louis Schäfer, Gisela Lanza",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4470426"
    },
    {
        "id": 26946,
        "title": "Lung Cancer Classification using Reinforcement Learning-based Ensemble Learning",
        "authors": "Shengping Luo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.01408120"
    },
    {
        "id": 26947,
        "title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning",
        "authors": "Abhishek Das, Satwik Kottur, Jose M. F. Moura, Stefan Lee, Dhruv Batra",
        "published": "2017-10",
        "citations": 115,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2017.321"
    },
    {
        "id": 26948,
        "title": "CCLF: A Contrastive-Curiosity-Driven Learning Framework for Sample-Efficient Reinforcement Learning",
        "authors": "Chenyu Sun, Hangwei Qian, Chunyan Miao",
        "published": "2022-7",
        "citations": 1,
        "abstract": "In reinforcement learning (RL), it is challenging to learn directly from high-dimensional observations, where data augmentation has recently remedied it via encoding invariances from raw pixels. Nevertheless, we empirically find that not all samples are equally important and hence simply injecting more augmented inputs may instead cause instability in Q-learning. In this paper, we approach this problem systematically by developing a model-agnostic Contrastive-Curiosity-driven Learning Framework (CCLF), which can fully exploit sample importance and improve learning efficiency in a self-supervised manner. Facilitated by the proposed contrastive curiosity, CCLF is capable of prioritizing the experience replay, selecting the most informative augmented inputs, and more importantly regularizing the Q-function as well as the encoder to concentrate more on under-learned data. Moreover, it encourages the agent to explore with a curiosity-based reward. As a result, the agent can focus on more informative samples and learn representation invariances more efficiently, with significantly reduced augmented inputs. We apply CCLF to several base RL algorithms and evaluate on the DeepMind Control Suite, Atari, and MiniGrid benchmarks, where our approach demonstrates superior sample efficiency and learning performances compared with other state-of-the-art methods. Our code is available at https://github.com/csun001/CCLF.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/478"
    },
    {
        "id": 26949,
        "title": "Reinforcement Learning-based Learning from Demonstrations for Collaborative Robots",
        "authors": "W.D. Li",
        "published": "2021-8-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/case49439.2021.9551596"
    },
    {
        "id": 26950,
        "title": "Learning Reward Models for Cooperative Trajectory Planning with Inverse Reinforcement Learning and Monte Carlo Tree Search",
        "authors": "Karl Kurzer, Matthias Bitzer, J. Marius Zollner",
        "published": "2022-6-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv51971.2022.9827031"
    },
    {
        "id": 26951,
        "title": "On Developing a UAV Pursuit-Evasion Policy Using Reinforcement Learning",
        "authors": "Bogdan Vlahov, Eric Squires, Laura Strickland, Charles Pippin",
        "published": "2018-12",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00138"
    },
    {
        "id": 26952,
        "title": "A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems",
        "authors": "Felipe Leno Da Silva, Anna Helena Reali Costa",
        "published": "2019-3-11",
        "citations": 130,
        "abstract": "\r\n\r\n\r\nMultiagent Reinforcement Learning (RL) solves complex tasks that require coordination with other agents through autonomous exploration of the environment. However, learning a complex task from scratch is impractical due to the huge sample complexity of RL algorithms. For this reason, reusing knowledge that can come from previous experience or other agents is indispensable to scale up multiagent RL algorithms. This survey provides a unifying view of the literature on knowledge reuse in multiagent RL. We define a taxonomy of solutions for the general knowledge reuse problem, providing a comprehensive discussion of recent progress on knowledge reuse in Multiagent Systems (MAS) and of techniques for knowledge reuse across agents (that may be actuating in a shared environment or not). We aim at encouraging the community to work towards reusing all the knowledge sources available in a MAS. For that, we provide an in-depth discussion of current lines of research and open questions.\r\n\r\n\r\n",
        "link": "http://dx.doi.org/10.1613/jair.1.11396"
    },
    {
        "id": 26953,
        "title": "Actor vs Critic: Learning the Policy or Learning the Value",
        "authors": "Fabian Scharf, Felix Helfenstein, Jonas Jäger",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41188-6_11"
    },
    {
        "id": 26954,
        "title": "Model-based Reinforcement Learning: A Survey",
        "authors": "Thomas M. Moerland, Joost Broekens, Aske Plaat, Catholijn M. Jonker",
        "published": "2023",
        "citations": 83,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1561/2200000086"
    },
    {
        "id": 26955,
        "title": "Model-Based Reinforcement Learning for Learning Deterministic Policies",
        "authors": "Eiji Uchibe",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1299/jsmermd.2022.2p1-b09"
    },
    {
        "id": 26956,
        "title": "Improve Convergence Speed of Multi‐Agent Q‐Learning for Cooperative Task Planning",
        "authors": "",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119699057.ch2"
    },
    {
        "id": 26957,
        "title": "Team Policy Learning for Multi-agent Reinforcement Learning",
        "authors": "Lucas Cassano, Sulaiman A. Alghunaim, Ali H. Sayed",
        "published": "2019-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp.2019.8683168"
    },
    {
        "id": 26958,
        "title": "Learning Conditional Policies for Crystal Design Using Offline Reinforcement Learning",
        "authors": "Prashant Govindarajan, Santiago Miret, Jarrid Rector-Brooks, Mariano Phielipp, Janarthanan Rajendran, Sarath Chandar",
        "published": "2024",
        "citations": 0,
        "abstract": "Navigating through the exponentially large chemical space to search for desirable materials is an extremely challenging task in material discovery. Recent developments in generative and geometric deep learning have shown...",
        "link": "http://dx.doi.org/10.1039/d4dd00024b"
    },
    {
        "id": 26959,
        "title": "Learning Transferable Domain Priors for Safe Exploration in Reinforcement Learning",
        "authors": "Thommen George Karimpanal, Santu Rana, Sunil Gupta, Truyen Tran, Svetha Venkatesh",
        "published": "2020-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207344"
    },
    {
        "id": 26960,
        "title": "An efficient reinforcement learning algorithm for learning deterministic policies in continuous domains",
        "authors": "Matthieu Zimmer, Paul Weng",
        "published": "2019-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3356464.3357704"
    },
    {
        "id": 26961,
        "title": "Piecewise Learning and Control with Stability Guarantees",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch7"
    },
    {
        "id": 26962,
        "title": "Reinforcement Learning",
        "authors": "Zheng Wen",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01926-5_2"
    },
    {
        "id": 26963,
        "title": "Non-instructed Motor Skill Learning in Monkeys: Insights from Deep Reinforcement Learning Models",
        "authors": "Laurene Carminatti, Lucio Condro, Alexa Riehle, Sonja Grün, Thomas Brochier, Emmanuel Daucé",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractIn the field of motor learning, few studies have addressed the case of non-instructed movement sequences learning, as they require long periods of training and data acquisition, and are complex to interpret. In contrast, such problems are readily addressed in machine learning, using artificial agents in simulated environments. To understand the mechanisms that drive the learning behavior of two macaque monkeys in a free-moving multi-target reaching task, we created two Reinforcement Learning (RL) models with different penalty criteria: “Time” reflecting the time spent to perfom a trial, and “Power” integrating the energy cost. The initial phase of the learning process is characterized by a rapid improvement in motor performance for both the 2 monkeys and the 2 models, with hand trajectories becoming shorter and smoother while the velocity gradually increases along trials and sessions. This improvement in motor performance with training is associated with a simplification in the trajectory of the movements performed to achieve the task goal. The monkeys and models show a convergent evolution towards an optimal circular motor path, almost exclusively in counter-clockwise direction, and a persistent inter-trial variability. All these elements contribute to interpreting monkeys learning in the terms of a progressive updating of action-selection patterns, following a classic value iteration scheme as in reinforcement learning. However, in contrast with our models, the monkeys also show a specific variability in thechoiceof the motor sequences to carry out across trials. This variability reflects a form of “path selection”, that is absent in the models. Furthermore, comparing models and behavioral data also reveal sub-optimality in the way monkeys manage the trade-off between optimizing movement duration (”Time”) and minimizing its metabolic cost (”Power”), with a tendency to overemphasize one criterion at the detriment of the other one. Overall, this study reveals the subtle interplay between cognitive factors, biomechanical constraints, task achievement and motor efficacy management in motor learning, and highlights the relevance of modeling approaches in revealing the respective contribution of the different elements at play.Author summaryThe way in which animals and humans learn new motor skills through free exploratory movements sequences solely governed by success or failure outcomes is not yet fully understood. Recent advances in machine learning techniques for continuous action spaces led us to construct a motor learning model investigate how animals progressively enhance the efficiency of their behaviors through numerous trials and errors. This study conducts a comprehensive comparison between deep learning models and experimental data from monkey behavior. Notably, we show that the progressive refinement of motor sequences, as they are observed in the animals, do not require the implementation of a complete model of their environment. Rather, it merely requires the capacity to anticipate both movement costs and final reward a few steps ahead in the future following a value iteration principle. Furthermore, the systematic deviations exhibited by the monkeys with respect to the computational model inform us on the presence of individual preferences in either minimizing the duration or the energy consumption, and also on the involvement of alternative “cognitive” strategies.",
        "link": "http://dx.doi.org/10.1101/2023.12.04.569889"
    },
    {
        "id": 26964,
        "title": "Learning When to Drive in Intersections by Combining Reinforcement Learning and Model Predictive Control",
        "authors": "Tommy Tram, Ivo Batkovic, Mohammad Ali, Jonas Sjoberg",
        "published": "2019-10",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8916922"
    },
    {
        "id": 26965,
        "title": "Transfer Learning Method in Reinforcement Learning-based Traffic Signal Control",
        "authors": "Zhenyu Mao, Jialong Li, Nianzhao Zheng, Kenji Tei, Shinichi Honiden",
        "published": "2021-10-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce53005.2021.9621842"
    },
    {
        "id": 26966,
        "title": "Bayesian Sequential Optimal Experimental Design for Linear Regression with Reinforcement Learning",
        "authors": "Fadil Santosa, Loren Anderson",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00106"
    },
    {
        "id": 26967,
        "title": "Autonomous Driving in Roundabout Maneuvers Using Reinforcement Learning with Q-Learning",
        "authors": "Laura García Cuenca, Enrique Puertas, Javier Fernandez Andrés, Nourdine Aliane",
        "published": "2019-12-13",
        "citations": 39,
        "abstract": "Navigating roundabouts is a complex driving scenario for both manual and autonomous vehicles. This paper proposes an approach based on the use of the Q-learning algorithm to train an autonomous vehicle agent to learn how to appropriately navigate roundabouts. The proposed learning algorithm is implemented using the CARLA simulation environment. Several simulations are performed to train the algorithm in two scenarios: navigating a roundabout with and without surrounding traffic. The results illustrate that the Q-learning-algorithm-based vehicle agent is able to learn smooth and efficient driving to perform maneuvers within roundabouts.",
        "link": "http://dx.doi.org/10.3390/electronics8121536"
    },
    {
        "id": 26968,
        "title": "The application of actor-critic reinforcement learning for fab dispatching scheduling",
        "authors": "Namyong Kim, Hayong Shin",
        "published": "2017-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc.2017.8248209"
    },
    {
        "id": 26969,
        "title": "MolOpt: Autonomous Molecular Geometry Optimization using Multi-Agent Reinforcement Learning",
        "authors": "Rohit Modee, Sarvesh Mehta, Siddhartha Laghuvarapu, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this paper, we propose MolOpt, the first attempt of its kind to use Multi-Agent Reinforcement Learning (MARL) for autonomous molecular geometry optimization (MGO). Typically MGO algorithms are hand-designed, but MolOpt uses MARL to learn a learned optimizer (policy) that can perform MGO without depending on other hand-designed optimizers. We cast MGO as a MARL problem, where each agent corresponds to a single atom in the molecule. MolOpt performs MGO by minimizing the forces on each atom in the molecule. Our experiments demonstrate the generalizing ability of MolOpt for MGO of Propane, Pentane, Heptane, Hexane, and Octane when trained on Ethane, Butane, and Isobutane. In terms of performance, MolOpt outperforms the MDMin optimizer and demonstrates similar performance to the FIRE optimizer. However, it does not surpass the BFGS optimizer. The results demonstrate that MolOpt has the potential to introduce innovative advancements in MGO by providing a novel approach using reinforcement learning (RL), which may open up new research directions for MGO. Overall, this work serves as a proof-of-concept for the potential of MARL in MGO.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2023-1hv4w"
    },
    {
        "id": 26970,
        "title": "On Passivity and Reinforcement Learning in Finite Games",
        "authors": "Bolin Gao, Lacra Pavel",
        "published": "2018-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc.2018.8619157"
    },
    {
        "id": 26971,
        "title": "DRLCOA: Deep Reinforcement Learning Computation Offloading Algorithm in Mobile Cloud Computing",
        "authors": "Pravneet Kaur",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3446601"
    },
    {
        "id": 26972,
        "title": "Exit Decisions Inspired by Reinforcement Learning",
        "authors": "Huaidian Hou",
        "published": "2022-2-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eebda53927.2022.9744817"
    },
    {
        "id": 26973,
        "title": "Power optimization with Reinforcement Learning in Logic Synthesis",
        "authors": "Chenghao Yang, Yinshui Xia",
        "published": "2021-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asicon52560.2021.9620356"
    },
    {
        "id": 26974,
        "title": "A Reinforcement Learning Algorithm for Trading Commodities",
        "authors": "Federico Giorgi, Stefano Herzel, Paolo Pigato",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4363174"
    },
    {
        "id": 26975,
        "title": "Deep Reinforcement Learning on Wind Power Optimization",
        "authors": "Yuanxi Arielsie Li",
        "published": "2022-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cncit56797.2022.00016"
    },
    {
        "id": 26976,
        "title": "Metagrating Design based on Reinforcement Learning",
        "authors": "Zi Wang, Wenqi Zhu, Junyeob Song, Lu Chen, Okan Koksal, Amit Agrawal",
        "published": "2023",
        "citations": 0,
        "abstract": "We demonstrate an algorithm based on reinforcement learning to realize high efficiency multifunctional metagrating devices without the requirement of a training dataset.",
        "link": "http://dx.doi.org/10.1364/cleo_fs.2023.fw4c.1"
    },
    {
        "id": 26977,
        "title": "SPF ICE: A Novel Approach to Predict the Optimal Amount of Silica to Preserve Glaciers Using Reinforcement Learning",
        "authors": "Aadhav Prabu",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Glaciers cover nearly 10 percent of the earth’s surface but are melting at an inexorable rate. According to the Pacific Standard magazine, the Arctic Sea ice has lost 80 percent of its volume since 1979. Antarctica’s ’Doomsday Glacier’ is melting faster and could raise global sea levels by two feet. As three-quarters of the earth’s fresh water is stored in glaciers, its melting depletes freshwater resources for millions of people. Glaciers also play a huge role in the climate crisis. Silica microspheres are promising materials to prevent glacier melting as it reflects most of the sun’s radiation. When spread in layers over the glacier, it can slow the rate of melt and aid in new ice formation. However, it is necessary to determine the ideal amount of silica to achieve the desired result with minimum environmental impact. This paper introduces a novel method SPF ICE to determine the optimal amount of silica based on glacier’s properties using reinforcement learning agents and a custom OpenAI Gym environment. The environment simulates a real-world model of a glacial setting using specific data, such as the glacier’s mass balance, temperature, and average accumulation and ablation. After testing the agents, the proposed solution reduced glacial melting by an average of 60.40% using the optimal amount of silica. The results indicate SPF ICE is a promising and cost-effective solution to curb glacier melting.<br></p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14774967.v2"
    },
    {
        "id": 26978,
        "title": "Nonlinear Relationships in Soybean Commodities Pairs Trading：Test by Deep Reinforcement Learning",
        "authors": "Jianhe Liu, Luze Lu, Xiangyu Zong, Baao Xie",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4504053"
    },
    {
        "id": 26979,
        "title": "Towards Safe Continuing Task Reinforcement Learning",
        "authors": "Miguel Calvo-Fullana, Luiz F. O. Chamon, Santiago Paternain",
        "published": "2021-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9482748"
    },
    {
        "id": 26980,
        "title": "Autonomous UAV Navigation via Deep Reinforcement Learning Using PPO",
        "authors": "Bilal Kabas",
        "published": "2022-5-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu55565.2022.9864769"
    },
    {
        "id": 26981,
        "title": "MetaSignal: Meta Reinforcement Learning for Traffic Signal Control via Fourier Basis Approximation",
        "authors": "Shuning Huang, Kaoru Ota, Mianxiong Dong, Huan Zhou",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Traffic signal control plans significantly impact transportation system  efficiency at intersections. Adaptive plans that adjust to real-time  conditions are more effective. Reinforcement learning (RL) adapts  strategies based on environmental feedback, making it proficient in  handling dynamic traffic scenarios. However, current RL methods have  long computational periods, hindering their adoption for new scenarios.  Another approach is to optimize the RL model itself for fast learning or  make it transferable with learned experience. The underlying control  algorithm should ensure convergence and minimize parameter sensitivity  in diverse migration scenarios. We propose MetaSignal, an efficient  meta-RL method for traffic signal control. Our approach uses Fourier  basis as the value function approximation in RL, offering advantages  like convergence facilitation, error bound achievement, and reduced  parameter dependence. The model-agnostic meta-learning framework allows  for effective adaptation to target scenarios with limited training cost.  Empirical evaluation shows promising and stable performance in  comprehensive experiments in synthetic and real-world traffic networks. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24188556"
    },
    {
        "id": 26982,
        "title": "Multiagent Manuvering with the Use of Reinforcement Learning",
        "authors": "Mateusz Orłowski, Paweł Skruch",
        "published": "2023-4-17",
        "citations": 0,
        "abstract": "This paper presents an approach for defining, solving, and implementing dynamic cooperative maneuver problems in autonomous driving applications. The formulation of these problems considers a set of cooperating cars as part of a multiagent system. A reinforcement learning technique is applied to find a suboptimal policy. The key role in the presented approach is a multiagent maneuvering environment that allows for the simulation of car-like agents within an obstacle-constrained space. Each of the agents is tasked with reaching an individual goal, defined as a specific location in space. The policy is determined during the reinforcement learning process to reach a predetermined goal position for each of the simulated cars. In the experiments, three road scenarios—zipper, bottleneck, and crossroads—were used. The trained policy has been successful in solving the cooperation problem in all scenarios and the positive effects of applying shared rewards between agents have been presented and studied. The results obtained in this work provide a window of opportunity for various automotive applications.",
        "link": "http://dx.doi.org/10.3390/electronics12081894"
    },
    {
        "id": 26983,
        "title": "Scaling up Deep Reinforcement Learning for Intelligent Video Game Agents",
        "authors": "Anton Debner",
        "published": "2022-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smartcomp55677.2022.00050"
    },
    {
        "id": 26984,
        "title": "Automatic Composite Action Discovery for Hierarchical Reinforcement Learning",
        "authors": "Josiah Laivins, Minwoo Lee",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci44817.2019.9003053"
    },
    {
        "id": 26985,
        "title": "Reinforcement Learning-Based Wireless Communications Against Jamming and Interference",
        "authors": "Liang Xiao",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-32903-1_71-1"
    },
    {
        "id": 26986,
        "title": "Rogue-Gym: A New Challenge for Generalization in Reinforcement Learning",
        "authors": "Yuji Kanagawa, Tomoyuki Kaneko",
        "published": "2019-8",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8848075"
    },
    {
        "id": 26987,
        "title": "Magicmol - Fast and variable synthetic accessibility molecule generative model with reinforcement learning",
        "authors": "Lin Chen, Qing Shen, Jungang Lou",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe flourishment of machine learning and deep learning methods have boosted the development of cheminformatics, especially when it comes to the application of drug discovery and new materials exploration. Lower time and space expenses make it possible for scientists to search the enormous chemical space for novel molecules. During this process, the generative model plays a significant role in quickly generating novel molecules, including predefined properties. Recently, some work combines reinforcement learning strategies with a generative model to optimize the property of generated drug-like molecules, which notably improved a batch of critical factors when it comes to drug discovery. However, a common problem of these RNN (recurrent neural network) based methods is that the novelty and validity of generated molecules depend on the “teacher-forcing strategy.” which is learned from an extensive database. Thus, there is inevitable to meet this situation that several generated molecules have difficulty in synthesizing even if owning higher desired properties such as binding score. In this paper, we proposed a new pipeline called Magicmol to directly change the synthetic accessibility of RNN-based generative models using reinforcement learning without introducing extra parameters. With proper strategies, the model can generate either hard-to-synthesis or easy-to-synthesis molecules with high validity. And our result shows a significant shift in the distribution of molecules with different synthetic accessibility when compared with the original model ,and we concluded some possible applications of Magicmol.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1670181/v1"
    },
    {
        "id": 26988,
        "title": "Routing and Security Based Ad-hoc Networks Configuration for Identification of Attack Using Reinforcement Learning Approach",
        "authors": "J. Avinash",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn many fields of Research, wireless sensor networks have grown in popularity. Depending on the dangerous situation the networks fields, there are more chances to attack wireless sensor network. This research explains how to locate rogue nodes in the networks based on investigation and responses from each node in the networks. In generally a malicious node blocks the transmission of signal to other nodes. So, in order to increase network performance, avoid much traffic for the server to buffer, causing them to slow down and eventually stop. Hence to identify the fake node in wireless sensor network is main aim of this research through Network Simulator with the help of Reinforcement Learning based routing algorithms. Further, attack vectors allow hackers to exploit system’s vulnerabilities including manual elements. It is suggested that a workable security framework for the WSN (Wireless Sensor Network) used to estimate traffic or packet loss and throughput ratio in order to detect fault identity. So enhancing the networks performance is based on request-response mechanism of the nodes, which engages with features at various levels of the protocol's system, monitors and analyzes typical patterns and alerts node to ensure their dangerous activities cannot transmit across the network. Some of monitoring basic functions through routing algorithm and NS2 include overseeing server CPUs, paying attention to network traffic, identifying patterns in error rates, alerting you about slow pages and combing through your access logs to find out how long requests usually take.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-4045649/v1"
    },
    {
        "id": 26989,
        "title": "Reinforcement Learning assisted Routing for Time Sensitive Networks",
        "authors": "Nurefsan Sertbas Bulbul, Mathias Fischer",
        "published": "2022-12-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001630"
    },
    {
        "id": 26990,
        "title": "Traffic signal control by distributed Reinforcement Learning with min-sum communication",
        "authors": "Tianshu Chu, Jie Wang",
        "published": "2017-5",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2017.7963745"
    },
    {
        "id": 26991,
        "title": "Macro and Micro Reinforcement Learning for Playing Nine-ball Pool",
        "authors": "Yu Chen, Yujun Li",
        "published": "2019-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8848113"
    },
    {
        "id": 26992,
        "title": "Solving Hard Bi-Objective Knapsack Problems Using Deep Reinforcement Learning",
        "authors": "Hadi Charkhgard, Hanieh Rastegar Moghaddam, Ali Eshragh, Sasan Mahmoudinazlou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4585010"
    },
    {
        "id": 26993,
        "title": "Reasoning Graph-Based Reinforcement Learning to Cooperate Mixed Connected and Autonomous Traffic at Unsignalized Intersections",
        "authors": "Donghao Zhou, Jian Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4598458"
    },
    {
        "id": 26994,
        "title": "Reinforcement Learning for Weighted p-median Problem",
        "authors": "Dávid Matis, Peter Tarábek",
        "published": "2023-6-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/idt59031.2023.10194404"
    },
    {
        "id": 26995,
        "title": "Dynamic Sparse Coding-Based Value Estimation Network for Deep Reinforcement Learning",
        "authors": "Haoli Zhao, Zhenni Li, Wensheng Su, Shengli Xie",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4390629"
    },
    {
        "id": 26996,
        "title": "Self-Attention for Visual Reinforcement Learning",
        "authors": "Zachary Fernandes, Ethan Joseph, Dean Vogel, Mei Si",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog57401.2023.10333243"
    },
    {
        "id": 26997,
        "title": "MetaSignal: Meta Reinforcement Learning for Traffic Signal Control via Fourier Basis Approximation",
        "authors": "Shuning Huang, Kaoru Ota, Mianxiong Dong, Huan Zhou",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Traffic signal control plans significantly impact transportation system  efficiency at intersections. Adaptive plans that adjust to real-time  conditions are more effective. Reinforcement learning (RL) adapts  strategies based on environmental feedback, making it proficient in  handling dynamic traffic scenarios. However, current RL methods have  long computational periods, hindering their adoption for new scenarios.  Another approach is to optimize the RL model itself for fast learning or  make it transferable with learned experience. The underlying control  algorithm should ensure convergence and minimize parameter sensitivity  in diverse migration scenarios. We propose MetaSignal, an efficient  meta-RL method for traffic signal control. Our approach uses Fourier  basis as the value function approximation in RL, offering advantages  like convergence facilitation, error bound achievement, and reduced  parameter dependence. The model-agnostic meta-learning framework allows  for effective adaptation to target scenarios with limited training cost.  Empirical evaluation shows promising and stable performance in  comprehensive experiments in synthetic and real-world traffic networks. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24188556.v1"
    },
    {
        "id": 26998,
        "title": "Image-Goal Navigation via Keypoint-Based Reinforcement Learning",
        "authors": "Yunho Choi, Songhwai Oh",
        "published": "2021-7-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ur52253.2021.9494664"
    },
    {
        "id": 26999,
        "title": "Exploration Methods in Reinforcement Learning",
        "authors": "Bingjie Shen",
        "published": "2022-8-20",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aeeca55500.2022.9918998"
    },
    {
        "id": 27000,
        "title": "Collaborative Computation Offloading Scheme Based on Deep Reinforcement Learning",
        "authors": "Jinho Park, Kwangsue Chung",
        "published": "2023-1-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin56518.2023.10048957"
    },
    {
        "id": 27001,
        "title": "Reinforcement Learning with Auxiliary Localization Task for Mapless Navigation",
        "authors": "Cong He, Wengang Zhang, Teng Wang",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327835"
    },
    {
        "id": 27002,
        "title": "Applying Reinforcement Learning for Shortest Path Problem",
        "authors": "Zhixuan Sun",
        "published": "2022-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bdicn55575.2022.00100"
    },
    {
        "id": 27003,
        "title": "LEARNING A FOREIGN LANGUAGE AS THINKING ACTIVITY REINFORCEMENT",
        "authors": "Alla Belousova, Larisa Abrosimova, Marina Bogdanova",
        "published": "2017-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/inted.2017.0576"
    },
    {
        "id": 27004,
        "title": "The construction and deconstruction of sub-optimal preferences through range-adapting reinforcement learning",
        "authors": "Sophie Bavard, Aldo Rustichini, Stefano Palminteri",
        "published": "No Date",
        "citations": 0,
        "abstract": "Converging evidence suggests that economic values are rescaled as a function of the range of the available options. Critically, although locally adaptive, range adaptation has been shown to lead to suboptimal choices. This is particularly striking in reinforcement learning (RL) situations when options are extrapolated from their original context. Range adaptation can be seen as the result of an adaptive coding process aiming at increasing the signal-to-noise ratio. However, this hypothesis leads to a counterintuitive prediction: decreasing outcome uncertainty should increase range adaptation and, consequently, extrapolation errors. Here, we tested the paradoxical relation between range adaptation and performance in a large sample of subjects performing variants of a RL task, where we manipulated task difficulty. Results confirmed that range adaptation induces systematic extrapolation errors and is stronger when decreasing outcome uncertainty. Finally, we propose a range-adapting model and show that it is able to parsimoniously capture all the observed results.",
        "link": "http://dx.doi.org/10.31234/osf.io/3xt69"
    },
    {
        "id": 27005,
        "title": "New reinforcement learning algorithm for robot soccer",
        "authors": "M Yoon, J Bekker, S Kroon",
        "published": "2017-6-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5784/33-1-542"
    },
    {
        "id": 27006,
        "title": "Distributional Reinforcement Learning with Ensembles",
        "authors": "Björn Lindenberg, Jonas Nordqvist, Karl-Olof Lindahl",
        "published": "2020-5-7",
        "citations": 0,
        "abstract": "It is well known that ensemble methods often provide enhanced performance in reinforcement learning. In this paper, we explore this concept further by using group-aided training within the distributional reinforcement learning paradigm. Specifically, we propose an extension to categorical reinforcement learning, where distributional learning targets are implicitly based on the total information gathered by an ensemble. We empirically show that this may lead to much more robust initial learning, a stronger individual performance level, and good efficiency on a per-sample basis.",
        "link": "http://dx.doi.org/10.3390/a13050118"
    },
    {
        "id": 27007,
        "title": "Robust Reinforcement Learning via Genetic Curriculum",
        "authors": "Yeeho Song, Jeff Schneider",
        "published": "2022-5-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9812420"
    },
    {
        "id": 27008,
        "title": "Spectra to Structure: Deep Reinforcement Learning for Molecular Inverse Problem",
        "authors": "Bhuvanesh Sridharan, Sarvesh Mehta, Yashaswi Pathak, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 1,
        "abstract": "Spectroscopy is the study of how matter interacts with electromagnetic radiations of specific frequencies that has led to several monumental discoveries in science. The spectra of any particular molecule is highly information-rich, yet the inverse relation from the spectra to the molecular structure is still an unsolved problem. Nuclear Magnetic Resonance (NMR) spectroscopy is one such critical tool in the tool-set for scientists to characterise any chemical sample. In this work, a novel framework is proposed that attempts to solve this inverse problem by navigating the chemical space to find the correct structure that resulted in the target spectra. The proposed framework uses a combination of online Monte- Carlo-Tree-Search (MCTS) and a set of offline trained Graph Convolution Networks to build a molecule iteratively from scratch. Our method is able to predict the correct structure of the molecule ∼80% of the time in its top 3 guesses. We believe that the proposed framework is a significant step in solving the inverse design problem of NMR spectra to molecule.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-4hc7k"
    },
    {
        "id": 27009,
        "title": "Vision-driven Deep Reinforcement Learning for Electronic Components Robotic Insertion Tasks",
        "authors": "Grzegorz Marcin Bartyzel, Wojciech Półchłopek, Dominik Rzepka",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nOne of the challenges of robotics in the modern manufacturing industry is assembly task. The manufacturing industry requires various insertion tasks, from peg-in-hole tasks to electronic parts assembly. Nowadays, robotic solutions for this problem often use the conventional methods. In those methods, the industrial robot is controlled by the hybrid force-position control and performs preprogrammed trajectories, such as a spiral path. However, electronic parts require more sophisticated techniques due to their complex geometry and susceptibility to damage. We propose a vision-driven method based on reinforcement learning (RL) for assembling electronic parts. In our approach, the input image for the RL agent is acquired from two cameras mounted to the robot's end-effector. In this work, we also analyze the influence of the observation modalities on the RL's agent performance metrics, such as insertion success rate and average assembly time. Results show that visual information acquired from a double-camera vision system significantly improves the RL method's robustness on the position disturbance in insertion tasks. The proposed method in this work outperforms conventional methods, such as random search, spiral search, and straight-down insertion in terms of success rate and robustness for the robot's initial position disturbances. Moreover, our approach is more robust for disturbance than a method that uses an external camera or a single camera mounted to the robot's end-effector for image acquisition.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2233330/v1"
    },
    {
        "id": 27010,
        "title": "5G Scheduling using Reinforcement Learning",
        "authors": "D. Zavyalova, V. Drozdova",
        "published": "2020-10-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fareastcon50210.2020.9271421"
    },
    {
        "id": 27011,
        "title": "Principles of RL Problems",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_2"
    },
    {
        "id": 27012,
        "title": "Robotic Obstacle Avoidance: A Virtual Modeling And Reinforcement Learning",
        "authors": "Ming-Fei Chen, Han-Hsien Tsai, Wen-Tse Hsiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis study developed a robotic arm self-learning system based on virtual modeling and reinforcement learning. Using the model of a robotic arm, information concerning obstacles in the environment, initial coordinates of the robotic arm, and the target position, this system automatically generated a set of rotational angles to enable a robotic arm to be positioned such that it can avoid all obstacles and reach a target. The developed program was divided into three parts. The first part involves robotic arm simulation and collision detection; specifically, images of a six-axis robotic arm and obstacles were input to the Visualization ToolKit library to visualize the movements and surrounding environment of the robotic arm. Subsequently, an oriented bounding box algorithm was used to determine whether collisions had occurred. The second part concerned machine-learning–based route planning. The TensorFlow was used to establish a deep deterministic policy gradient model, and reinforcement learning was employed for the response to environmental variables. Different reward functions were designed for tests and discussions, and the program’s practicality was verified through actual machine operations. Finally, the application of reinforcement learning in route planning for a robotic arm was proved feasible by the experiment. Such an application facilitated automatic route planning and achieved an error of less than 10 mm from the target position.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-601053/v1"
    },
    {
        "id": 27013,
        "title": "Research on Agent Control Algorithm Based on Reinforcement Learning",
        "authors": "Zhongqiu Zhang",
        "published": "2020-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccea50009.2020.00127"
    },
    {
        "id": 27014,
        "title": "Deep Reinforcement Learning for Contagion Control",
        "authors": "Diego R. Benalcazar, Chinwendu Enyioha",
        "published": "2021-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccta48906.2021.9659238"
    },
    {
        "id": 27015,
        "title": "Behavior Constraining in Weight Space for Offline Reinforcement Learning",
        "authors": "Phillip Swazinna, Steffen Udluft, Daniel Hein, Thomas Runkler",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2021.es2021-83"
    },
    {
        "id": 27016,
        "title": "Can Deep Reinforcement Learning Solve the Portfolio Allocation Problem? (PhD Manuscript)",
        "authors": "Eric Benhamou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4599800"
    },
    {
        "id": 27017,
        "title": "Risk-Based Adaptive Stock Trading System Using Reinforcement Learning",
        "authors": "Chai Quek, Qi CAO",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4237367"
    },
    {
        "id": 27018,
        "title": "Deep Residual Attention Reinforcement Learning",
        "authors": "Hanhua Zhu, Tomoyuki Kaneko",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/taai48200.2019.8959896"
    },
    {
        "id": 27019,
        "title": "How working memory and reinforcement learning are intertwined: a cognitive, neural, and computational perspective",
        "authors": "Aspen H. Yoo, Anne Collins",
        "published": "No Date",
        "citations": 0,
        "abstract": "Reinforcement learning and working memory are two core processes of human cognition, and are often considered cognitively, neuroscientifically, and algorithmically distinct. Here, we show that the brain networks that support them actually overlap significantly, and that they are less distinct cognitive processes than often assumed. We review literature demonstrating the benefits of considering each process to explain properties of the other, and highlight recent work investigating their more complex interactions. We discuss how future research in both computational and cognitive sciences can benefit from one another, suggesting that a key missing piece for artificial agents to learn to behave with more human-like efficiency is taking working memory’s role in learning seriously. This review highlights the risks of neglecting the interplay between different processes when studying human behavior (in particular when considering individual differences). We emphasize the importance of investigating these dynamics in order to build a comprehensive understanding of human cognition.",
        "link": "http://dx.doi.org/10.31234/osf.io/ebtn6"
    },
    {
        "id": 27020,
        "title": "Federated Multi Agent Deep Reinforcement Learning for Optimized Design of Future Wireless Networks",
        "authors": "Hugo De Oliveira, Megumi Kaneko, Lila Boukhatem",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Federated Multi-Agent Deep Reinforcement Learning (F-MADRL) has been recently attracting increasing research interests, as it offers efficient solutions towards meeting the extreme requirements of Beyond 5G (B5G) and 6G applications. By contrast to centralized Deep Reinforcement Learning (DRL) and Multi-Agent DRL (MADRL), F-MADRL enables edge devices to cooperate without sharing their private data, while reducing the delays and signaling costs inherent to centralized approaches. In this article, we explore the new opportunities brought by F-MADRL by conducting a holistic survey on its related recent works. Firstly, we categorize state-of-the-art F-MADRL approaches, based on some distinctive features such as incurred signaling overhead, privacy level, and aggregation frequency. To better illustrate the behavior and advantages of F-MADRL, it is numerically compared to its centralized and distributed DRL counterparts, through a Sub-6GHz/mmWave band association optimization problem for IoT short packet communications. Finally, we identify and discuss the open research directions and challenges, in order to spur further interests in this promising area.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24007503.v1"
    },
    {
        "id": 27021,
        "title": "A Population-Based Approach for Multi-Agent Interpretable Reinforcement Learning",
        "authors": "Marco Crespi, Andrea Ferigo, Leonardo  Lucio Custode, Giovanni Iacca",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4467882"
    },
    {
        "id": 27022,
        "title": "The role of subgoals in hierarchical reinforcement learning",
        "authors": "Milena Rmus, Maria Eckstein, Anne Collins",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1587-0"
    },
    {
        "id": 27023,
        "title": "Recovering From Cyber-Manufacturing Attacks by Reinforcement Learning",
        "authors": "Romesh Prasad, Matthew K. Swanson, Young Moon",
        "published": "2022-10-30",
        "citations": 0,
        "abstract": "Abstract\nA Cyber-Manufacturing systems (CMS) is an integration of informational and operational entities that are synchronized with manufacturing processes to increase productivity. However, this integration enlarges the scope for cyber attackers to intrude manufacturing processes, which are called cyber-manufacturing attacks. They can have significant impacts on physical operations within a CMS, such as shutting down plants, production interruption, premature failure of products, and fatal accidents. Although research activities in this emerging problem have been increased recently, existing research has been limited to detection and prevention solutions. However, these strategies cannot ensure a continuous function of an attacked CMS. To ensure continuous functioning of a CMS, a robust recovery strategy must be developed and employed. Current research in recovery has been limited to feedback controllers with an assumption of a complete knowledge of a system model. To overcome this limitation, a recovery agent augmented by reinforcement learning was developed. This is to utilize the ability of reinforcement learning to handle sequential decisions and to proceed even without a complete knowledge of a system model. A virtual environment for recovery agents has been developed to assist efforts needed to obtain sample data, experiment various scenarios, and explore with reinforcement learning. Two cyber-manufacturing attack scenarios have been developed: (i) spoofing a stepper motor controlling additive manufacturing processes, (ii) disrupting the sequence of the pick and place robot. The recovery agent takes random actions by exploring its environment and receives rewards from the actions. After many iterations, it learns proper actions to take.",
        "link": "http://dx.doi.org/10.1115/imece2022-93982"
    },
    {
        "id": 27024,
        "title": "Reinforcement Learning based Intelligent Semiconductor Manufacturing Applied to Laser Annealing",
        "authors": "Tejender Rawat, Chang-Yuan Chung, Shih-Wei Chen, Albert Lin",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2449"
    },
    {
        "id": 27025,
        "title": "Reinforcement Learning for Financial Index Tracking",
        "authors": "Xianhua Peng, Chenyin Gong, Xue Dong He",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4532072"
    },
    {
        "id": 27026,
        "title": "Network Intrusion Detection System using Reinforcement learning",
        "authors": "Malika Malik, Kamaljit Singh Saini",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incet57972.2023.10170630"
    },
    {
        "id": 27027,
        "title": "Testosterone and estradiol affect adolescent reinforcement learning",
        "authors": "Sina Kohne, Esther K. Diekhof",
        "published": "2022-2-3",
        "citations": 1,
        "abstract": "During adolescence, gonadal hormones influence brain maturation and behavior. The impact of 17β-estradiol and testosterone on reinforcement learning was previously investigated in adults, but studies with adolescents are rare. We tested 89 German male and female adolescents (mean age ± sd = 14.7 ± 1.9 years) to determine the extent 17β-estradiol and testosterone influenced reinforcement learning capacity in a response time adjustment task. Our data showed, that 17β-estradiol correlated with an enhanced ability to speed up responses for reward in both sexes, while the ability to wait for higher reward correlated with testosterone primary in males. This suggests that individual differences in reinforcement learning may be associated with variations in these hormones during adolescence, which may shift the balance between a more reward- and an avoidance-oriented learning style.",
        "link": "http://dx.doi.org/10.7717/peerj.12653"
    },
    {
        "id": 27028,
        "title": "Reinforcement Learning Based Pricing for Demand Response",
        "authors": "Amir Ghasemkhani, Lei Yang",
        "published": "2018-5",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccw.2018.8403783"
    },
    {
        "id": 27029,
        "title": "Deep Reinforcement Learning for Energy Management in a Microgrid with Flexible Demand",
        "authors": "Taha abd el halim Nakabi, Pekka Toivanen",
        "published": "No Date",
        "citations": 2,
        "abstract": "In this paper, we study the performance of various deep reinforcement learning algorithms to enhance the energy management system of a microgrid. We propose a novel microgrid model that consists of a wind turbine generator, an energy storage system, a set of thermostatically controlled loads, a set of price-responsive loads, and a connection to the main grid. The proposed energy management system is designed to coordinate among the different flexible sources by defining the priority resources, direct demand control signals, and electricity prices. Seven deep reinforcement learning algorithms were implemented and are empirically compared in this paper. The numerical results show that the deep reinforcement learning algorithms differ widely in their ability to converge to optimal policies. By adding an experience replay and a semi-deterministic training phase to the well-known asynchronous advantage actor-critic algorithm, we achieved the highest model performance as well as convergence to near-optimal policies.",
        "link": "http://dx.doi.org/10.20944/preprints202010.0156.v1"
    },
    {
        "id": 27030,
        "title": "Experimental Evaluation of Deep Reinforcement Learning Algorithms",
        "authors": "Nikola Mrzljak, Tomislav Hrkać",
        "published": "2019-2-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.20532/ccwv.2018.0003"
    },
    {
        "id": 27031,
        "title": "Inertia-Constrained Reinforcement Learning to Enhance Human Motor Control Modeling",
        "authors": "Soroush Korivand, Nader Jalili, Jiaqi Gong",
        "published": "No Date",
        "citations": 0,
        "abstract": "Locomotor impairment is a high-prevalent and significant source of disability and significantly impacts a large population’s quality of life. Despite decades of research in human locomotion, the challenges of simulating human movement to study the features of musculoskeletal drivers and clinical conditions remain. Most recent efforts in utilizing reinforcement learning (RL) techniques are promising to simulate human locomotion and reveal musculoskeletal drives. However, these simulations often failed to mimic natural human locomotion because most reinforcement strategies have yet to consider any reference data regarding human movement. To address these challenges, in this study, we designed a reward function based on the trajectory optimization rewards (TOR), and bio-inspired rewards, which includes the rewards obtained from reference motion data captured by a single Intertial Moment Unit (IMU) sensor. The sensor was equipped on the participants’ pelvis to capture reference motion data. Also, we adapted the reward function by leveraging previous research in walking simulation for TOR. The experimental results showed that the simulated agents with the modified reward function performed better in mimicking the collected IMU data from participants, which means the simulated human locomotion was more realistic. Also, as this bio-inspired defined cost, IMU data enhanced the agent’s capacity to converge during the training process. As a result, the models’ convergence is faster than those developed without reference motion data. Consequently, human locomotion can be simulated more quicker and in a broader range of environments with a better simulation performance.",
        "link": "http://dx.doi.org/10.20944/preprints202212.0167.v1"
    },
    {
        "id": 27032,
        "title": "Diversity Oriented Deep Reinforcement Learning for Targeted Molecule Generation",
        "authors": "Tiago Pereira, Maryam Abbasi, Bernardete Ribeiro, Joel P. Arrais",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this work, we explore the potential of deep learning to streamline the process of identifying new potential drugs through the computational generation of molecules with interesting biological properties. Two deep neural networks compose our targeted generation framework: the Generator, which is trained to learn the building rules of valid molecules employing SMILES strings notation, and the Predictor which evaluates the newly generated compounds by predicting their aﬃnity for the desired target. Then, the Generator is optimized through Reinforcement Learning to produce molecules with bespoken properties. The innovation of this approach is the exploratory strategy applied during the reinforcement training process that seeks to add novelty to the generated compounds. This training strategy employs two Generators interchangeably to sample new SMILES: the initially trained model that will remain ﬁxed and a copy of the previous one that will be updated during the training to uncover the most promising molecules. The evolution of the reward assigned by the Predictor determines how often each one is employed to select the next token of the molecule. This strategy establishes a compromise between the need to acquire more information about the chemical space and the need to sample new molecules, with the experience gained so far. To demonstrate the eﬀectiveness of the method, the Generator is trained to design molecules with high inhibitory power for the adenosine A2A and κ opioid receptors. The results reveal that the model can eﬀectively modify the biological aﬃnity of the newly generated molecules towards the craved direction. More importantly, it was possible to ﬁnd auspicious sets of unique and diverse molecules, which was the main purpose of the newly implemented strategy.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-110570/v1"
    },
    {
        "id": 27033,
        "title": "Leveraging Optimal Transport for Enhanced Offline Reinforcement Learning in Surgical Robotic Environments",
        "authors": "Maryam Zare, Parham Mohsenzadeh Kebria, Abbas Khosravi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4627353"
    },
    {
        "id": 27034,
        "title": "Quality-Diversity Based Semi-Autonomous Teleoperation Using Reinforcement Learning",
        "authors": "Sangbeom Park, Taerim Yoon, Joonhyung Lee, Sunghyun Park, Sungjoon Choi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4706203"
    },
    {
        "id": 27035,
        "title": "Reinforcement Learning and Deep Stochastic Optimal Control for Final Quadratic Hedging",
        "authors": "Bernhard Hientzsch",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4645455"
    },
    {
        "id": 27036,
        "title": "Track 5: Inspection Planning for Transmission Line Systems\n  Exposed to Hurricanes using Reinforcement Learning",
        "authors": "Ashkan B., Nariman L., Abdollah Shafieezadeh",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/m.6321e2e2f30377bc3baf6628"
    },
    {
        "id": 27037,
        "title": "Abstraction for Deep Reinforcement Learning",
        "authors": "Murray Shanahan, Melanie Mitchell",
        "published": "2022-7",
        "citations": 5,
        "abstract": "We characterise the problem of abstraction in the context of deep reinforcement learning. Various well established approaches to analogical reasoning and associative memory might be brought to bear on this issue, but they present difficulties because of the need for end-to-end differentiability. We review developments in AI and machine learning that could facilitate their adoption.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/780"
    },
    {
        "id": 27038,
        "title": "Model-based reinforcement learning for service mesh fault resiliency in a web application-level",
        "authors": "Fanfei Meng, Lalita Jagadeesa, Marina Thottan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Microservice-based architectures enable different aspects of web applications to be created and updated independently, even after deployment. Associated technologies such as service mesh provide application-level fault resilience through attribute configurations that govern the behavior of request - response service -- and the interactions among them -- in the presence of failures. While this provides tremendous flexibility, the configured values of these attributes -- and the relationships among them -- can significantly affect the performance and fault resilience of the overall application. Furthermore, it is impossible to determine the best and worst combinations of attribute values with respect to fault resiliency via testing, due to the complexities of the underlying distributed system and the many possible attribute value combinations. In this paper, we present a model-based reinforcement learning workflow towards service mesh fault resiliency. Our approach enables the prediction of the most significant fault resilience behaviors at a web application-level, scratching from single service to aggregated multi-service management with efficient agent collaborations.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24582396.v1"
    },
    {
        "id": 27039,
        "title": "Scalable Multi-Agent Reinforcement Learning-Based Distributed Channel Access",
        "authors": "Zhenyu Chen, Xinghua Sun",
        "published": "2023-5-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278960"
    },
    {
        "id": 27040,
        "title": "Decision letter for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v3/decision1"
    },
    {
        "id": 27041,
        "title": "Decision letter for \"Performance enhancement of the artificial neural network–based reinforcement learning for wind turbine yaw control\"",
        "authors": "",
        "published": "2019-8-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2451/v1/decision1"
    },
    {
        "id": 27042,
        "title": "Intelligent demand response resource trading using deep reinforcement learning",
        "authors": "",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2020.05540"
    },
    {
        "id": 27043,
        "title": "Quality of service based radar resource management using deep reinforcement learning",
        "authors": "Sebastian Durst, Stefan Bruggenwirth",
        "published": "2021-5-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2147009.2021.9455234"
    },
    {
        "id": 27044,
        "title": "Decision letter: Humans perseverate on punishment avoidance goals in multigoal reinforcement learning",
        "authors": "Claire M Gillan",
        "published": "2021-11-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.74402.sa1"
    },
    {
        "id": 27045,
        "title": "Author response for \"The value–complexity trade-off for reinforcement learning based brain–computer interfaces\"",
        "authors": " Hadar Levi-Aharoni,  Naftali Tishby",
        "published": "2020-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1741-2552/abc8d8/v2/response1"
    },
    {
        "id": 27046,
        "title": "Reinforcement Learning with Deep Deterministic Policy Gradient",
        "authors": "Haining Tan",
        "published": "2021-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/caibda53561.2021.00025"
    },
    {
        "id": 27047,
        "title": "Quantum Reinforcement Learning for Solving a Stochastic Frozen Lake Environment and the Impact of Quantum Architecture Choices",
        "authors": "Theodora-Augustina Drăgan, Maureen Monnet, Christian Mendl, Jeanette Lorenz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011673400003393"
    },
    {
        "id": 27048,
        "title": "Intra-Agent Transfer Methods",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5_4"
    },
    {
        "id": 27049,
        "title": "Experiment Domains and Applications",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5_6"
    },
    {
        "id": 27050,
        "title": "Optimal recovery of unsecured debt via interpretable reinforcement learning",
        "authors": "Michael Mark, Naveed Chehrazi, Huanxi Liu, Thomas A. Weber",
        "published": "2022-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2022.100280"
    },
    {
        "id": 27051,
        "title": "Bridging Reinforcement Learning and Iterative Learning Control: Autonomous Motion Learning for Unknown, Nonlinear Dynamics",
        "authors": "Michael Meindl, Dustin Lehmann, Thomas Seel",
        "published": "2022-7-12",
        "citations": 4,
        "abstract": "This work addresses the problem of reference tracking in autonomously learning robots with unknown, nonlinear dynamics. Existing solutions require model information or extensive parameter tuning, and have rarely been validated in real-world experiments. We propose a learning control scheme that learns to approximate the unknown dynamics by a Gaussian Process (GP), which is used to optimize and apply a feedforward control input on each trial. Unlike existing approaches, the proposed method neither requires knowledge of the system states and their dynamics nor knowledge of an effective feedback control structure. All algorithm parameters are chosen automatically, i.e. the learning method works plug and play. The proposed method is validated in extensive simulations and real-world experiments. In contrast to most existing work, we study learning dynamics for more than one motion task as well as the robustness of performance across a large range of learning parameters. The method’s plug and play applicability is demonstrated by experiments with a balancing robot, in which the proposed method rapidly learns to track the desired output. Due to its model-agnostic and plug and play properties, the proposed method is expected to have high potential for application to a large class of reference tracking problems in systems with unknown, nonlinear dynamics.",
        "link": "http://dx.doi.org/10.3389/frobt.2022.793512"
    },
    {
        "id": 27052,
        "title": "Proposal for Selecting a Cooperation Partner in Distributed Control of Traffic Signals using Deep Reinforcement Learning",
        "authors": "Shinya Matsuta, Naoki Kodama, Taku Harada",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12792/icisip2021.027"
    },
    {
        "id": 27053,
        "title": "A New Entity-relation Joint Extraction Model using Reinforcement Learning and Its Application Test",
        "authors": "Heping Peng, Zhong Xu, Wenxiong Mo, Yong Wang, Qingdan Huang, Chengzhu Sun, Ting He",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011361800003440"
    },
    {
        "id": 27054,
        "title": "Development and Validation of Active Roll Control based on Actor-critic Neural Network Reinforcement Learning",
        "authors": "Matthias Bahr, Sebastian Reicherts, Philipp Sieberg, Luca Morss, Dieter Schramm",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007787400360046"
    },
    {
        "id": 27055,
        "title": "The elusive effects of incidental anxiety on reinforcement-learning",
        "authors": "Chih-Chung Ting, Stefano Palminteri, Maël Lebreton, Jan Benjamin Engelmann",
        "published": "No Date",
        "citations": 1,
        "abstract": "Anxiety is a common affective state, characterized by the subjectively unpleasant feelings of dread over an anticipated event. Anxiety is suspected to have important negative consequences on cognition, decision-making and learning. Yet, despite a recent surge in studies investigating the specific effects of anxiety on reinforcement-learning, no coherent picture has emerged. Here, we investigated the effects of incidental anxiety on instrumental reinforcement learning, while addressing several issues and defaults identified in a focused literature review. We used a rich experimental design, featuring both a learning and a transfer phase, and a manipulation of outcomes valence (gains vs losses). In two variants (N = 2x50) of this experimental paradigm, incidental anxiety was induced with an established threat-of-shock paradigm. Model-free results show that incidental anxiety effects seem limited to a small, but specific increase in post-learning performance measured by a transfer task. A comprehensive modelling effort revealed that, irrespective of the effects of anxiety, individuals give more weight to positive than negative outcomes, and tend to experience the omission of a loss as a gain (and vice versa). However, in line with results from our targeted literature survey, isolating specific computational effects of anxiety on learning per se proved to be challenging. Overall, our results suggest that learning mechanisms are more complex than traditionally presumed, and raise important concerns about the robustness of the effects of anxiety previously identified in simple reinforcement-learning studies.",
        "link": "http://dx.doi.org/10.31234/osf.io/7d4tc"
    },
    {
        "id": 27056,
        "title": "Function Approximation and Approximate Dynamic Programming",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-6"
    },
    {
        "id": 27057,
        "title": "Graph Neural Networks with Reinforcement Learning for Advanced VNF Scaling in Network Management",
        "authors": "Namjin Seo, DongNyeong Heo, Heeyoul Choi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4353607"
    },
    {
        "id": 27058,
        "title": "Reinforcement Machine Learning Robots with the Reset Policy and Leptokurtic Noise",
        "authors": "Jiayuan Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3637255"
    },
    {
        "id": 27059,
        "title": "Reinforcement Machine Learning Robots With the Reset Policy and Leptokurtic Noise",
        "authors": "Jiayuan Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3637260"
    },
    {
        "id": 27060,
        "title": "Experimental Evaluation of Deep Reinforcement Learning Algorithms",
        "authors": "Nikola Mrzljak, Tomislav Hrkać",
        "published": "2019-2-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.20532/ccvw.2018.0003"
    },
    {
        "id": 27061,
        "title": "Deep Reinforcement Learning for Analog Circuit Sizing",
        "authors": "Zhenxin Zhao, Lihong Zhang",
        "published": "2020-10",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscas45731.2020.9181149"
    },
    {
        "id": 27062,
        "title": "Access control of MTC devices using reinforcement learning approach",
        "authors": " Jihun Moon,  Yujin Lim",
        "published": "2017",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin.2017.7899576"
    },
    {
        "id": 27063,
        "title": "Reinforcement Learning based on MPC and the Stochastic Policy Gradient Method",
        "authors": "Sebastien Gros, Mario Zanon",
        "published": "2021-5-25",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9482765"
    },
    {
        "id": 27064,
        "title": "Basic things about reinforcement learning",
        "authors": "Ziyu Zhang",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "Artificial Intelligence has been a very popular topic at present, machine learning is also one of the main algorithms in AI, which consisted of Supervised learning, Unsupervised learning and Reinforcement learning, and Supervised learning and Unsupervised learning have been relatively mature. Reinforcement learning technology has a long history, it wasn't until the late '80s and early' 90s that reinforcement learning became widely used in artificial intelligence, machine learning. Generally, Reinforcement learning is a process of trial and error, agent will choose to make an action according to the feedback from the environment, this step will repeat a lot of times until it find the best policy, mapping it to reality, it can help human to fulfill some missions which are nearly impossible before. However, there is still some potential problems in Reinforcement learning. This essay compares some basic algorithms related to RL to help reader to have a basic understanding of RL and propose some exsiting defects about it.",
        "link": "http://dx.doi.org/10.54254/2755-2721/6/20230788"
    },
    {
        "id": 27065,
        "title": "Renewal Monte Carlo: Renewal Theory Based Reinforcement Learning",
        "authors": "Jayakumar Subramanian, Aditya Mahajan",
        "published": "2018-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc.2018.8619180"
    },
    {
        "id": 27066,
        "title": "A Reinforcement Learning for Criminal’s Escape Path Prediction",
        "authors": "Pakamaj Wongsai, Wichai Pawgasame",
        "published": "2018-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acdt.2018.8593191"
    },
    {
        "id": 27067,
        "title": "Evaluation of Techniques for Sim2Real Reinforcement Learning",
        "authors": "Mahesh Ranaweera, Qusay Mahmoud",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) has demonstrated promising results in transferring learned policies from simulation to real-world environments. However, inconsistencies and discrepancies between the two environments cause a negative transfer. The phenomenon is commonly known as the “reality gap.” The reality gap prevents learned policies from generalizing to the physical environment. This paper aims to evaluate techniques to improve sim2real learning and bridge the reality gap using RL. For this research, a 3-DOF Stewart Platform was built virtually and physically. The goal of the platform was to guide and balance the marble towards the center of the Stewart platform. Custom API was created to induce noise, manipulate in-game physics, dynamics, and lighting conditions, and perform domain randomization to improve generalization. Two RL algorithms; Q-Learning and Actor-Critic were implemented to train the agent and to evaluate the performance in bridging the reality gap. This paper outlines the techniques utilized to create noise, domain randomization, perform training, results, and observations. Overall, the obtained results show the effectiveness of domain randomization and inducing noise during the agents' learning process. Additionally, the findings provide valuable insights into implementing sim2real RL algorithms to bridge the reality gap.",
        "link": "http://dx.doi.org/10.32473/flairs.36.133317"
    },
    {
        "id": 27068,
        "title": "General Deep Reinforcement Learning in NES Games",
        "authors": "David Gregory LeBlanc, Greg Lee",
        "published": "2021-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/594757db.8472938b"
    },
    {
        "id": 27069,
        "title": "SPF ICE: A Novel Approach to Predict the Optimal Amount of Silica to Preserve Glaciers Using Reinforcement Learning",
        "authors": "Aadhav Prabu",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Glaciers cover nearly 10 percent of the earth’s surface but are melting at an inexorable rate. According to the Pacific Standard magazine, the Arctic Sea ice has lost 80 percent of its volume since 1979. Antarctica’s ’Doomsday Glacier’ is melting faster and could raise global sea levels by two feet. As three-quarters of the earth’s fresh water is stored in glaciers, its melting depletes freshwater resources for millions of people. Glaciers also play a huge role in the climate crisis. Silica microspheres are promising materials to prevent glacier melting as it reflects most of the sun’s radiation. When spread in layers over the glacier, it can slow the rate of melt and aid in new ice formation. However, it is necessary to determine the ideal amount of silica to achieve the desired result with minimum environmental impact. This paper introduces a novel method SPF ICE to determine the optimal amount of silica based on glacier’s properties using reinforcement learning agents and a custom OpenAI Gym environment. The environment simulates a real-world model of a glacial setting using specific data, such as the glacier’s mass balance, temperature, and average accumulation and ablation. After testing the agents, the proposed solution reduced glacial melting by an average of 60.40% using the optimal amount of silica. The results indicate SPF ICE is a promising and cost-effective solution to curb glacier melting.<br></p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14774967.v1"
    },
    {
        "id": 27070,
        "title": "Value-free reinforcement learning: Policy optimization as a minimal model of operant behavior",
        "authors": "Daniel Bennett, Yael Niv, Angela Langdon",
        "published": "No Date",
        "citations": 4,
        "abstract": "Reinforcement learning is a powerful framework for modelling the cognitive and neural substrates of learning and decision making. Contemporary research in cognitive neuroscience and neuroeconomics typically uses value-based reinforcement-learning models, which assume that decision-makers choose by comparing learned values for different actions. However, another possibility is suggested by a simpler family of models, called policy-gradient reinforcement learning. Policy-gradient models learn by optimizing a behavioral policy directly, without the intermediate step of value-learning. Here we review recent behavioral and neural findings that are more parsimoniously explained by policy-gradient models than by value-based models. We conclude that, despite the ubiquity of `value' in reinforcement-learning models of decision making, policy-gradient models provide a lightweight and compelling alternative model of operant behavior.",
        "link": "http://dx.doi.org/10.31234/osf.io/ew58m"
    },
    {
        "id": 27071,
        "title": "Group Search Optimization-Assisted Deep Reinforcement Learning Intelligencedecisionfor Virtual Network Mapping",
        "authors": "Xiancui Xiao, Feng Yuan, Xiaoming Wu, Jie Tian",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4710410"
    },
    {
        "id": 27072,
        "title": "Deep Reinforcement Learning based Haptic Enhancement for Tele-Diagnosis",
        "authors": "Wenjie Lin",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fuzz-ieee55066.2022.9882866"
    },
    {
        "id": 27073,
        "title": "Safety Margins for Reinforcement Learning",
        "authors": "Alexander Grushin, Walt Woods, Alvaro Velasquez, Simon Khan",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00026"
    },
    {
        "id": 27074,
        "title": "Reinforcement Learning-Based Underwater Acoustic Channel Tracking with Forgetting Factors",
        "authors": "Yuhang Wang, Wei Li, Zhonghan Hao",
        "published": "2022-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oceans47191.2022.9977266"
    },
    {
        "id": 27075,
        "title": "Reinforcement Learning - Scalability Review",
        "authors": "Ezekiel Uzor Okike, Boikobo Ernest Seboko",
        "published": "2021-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.20533/icitst.2021.0016"
    },
    {
        "id": 27076,
        "title": "Solve Large-Scale Strip Packing Problem Via Reinforcement Learning",
        "authors": "Yang Xu, Zhouwang Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4267698"
    },
    {
        "id": 27077,
        "title": "Reinforcement Learning and Its Applications in Finance",
        "authors": "Amin Ilyas, Kholifatul Husna Asri",
        "published": "2023-8-14",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) models, a practical application drawn upon deep neural networks, are among the models examined in to identify its applicability to solve various problems related to financial areas including stock markets, portfolio management, forex markets, bankruptcy and insolvency, financial crisis, and cryptocurrency. A comprehensive introductory text focusing on financial applications of RL is rare if not difficult to find. This essay is aimed at presenting a short yet concise one-stop-resource that covers: (a) few important basics of RL, (b) types of problems it can address, (c) how it works, (d) its strength and limitations especially when compared to other approaches, (e) scopes within which the use of RL is recommended, and (f) examples of its applications in finance. Getting this writing to be comprehensive and effective in practice is a much more ambitious attempt, but it does highlight what it makes to work in practice.  sample/object of research, research instruments, and research results",
        "link": "http://dx.doi.org/10.37010/alif.v2i1.1238"
    },
    {
        "id": 27078,
        "title": "Biped Robot Walking based on Deep Reinforcement Learning",
        "authors": "Tomislav Tadić, Petar Ćurković",
        "published": "2023-5-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/mipro57284.2023.10159946"
    },
    {
        "id": 27079,
        "title": "NOVEL TRANSFORMER-BASED APPROACH ENHANCED BY REINFORCEMENT LEARNING AND ATTENTION MECHANISMS",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.35741/issn.0258-2724.58.6.5"
    },
    {
        "id": 27080,
        "title": "Examinations of Biases by Model Misspecification and Parameter Reliability of Reinforcement Learning Models",
        "authors": "Asako Toyama, Kentaro Katahira, Yoshihiko Kunisato",
        "published": "No Date",
        "citations": 0,
        "abstract": "Reinforcement learning models have the potential to clarify meaningful individual differences in the decision-making process. This study focused on two aspects regarding the nature of a reinforcement learning model and its parameters: the problems of model misspecification and reliability. Online participants, N=453, completed self-report measures and a probabilistic learning task twice 1.5 months apart, and data from the task were fitted using several reinforcement learning models. To address the problem of model misspecification, we compared the models with and without the influence of choice history, or perseveration. Results showed that the lack of a perseveration term in the model led to a decrease in learning rates for win and loss outcomes, with slightly different influences depending on outcome volatility, and increases in inverse temperature. We also conducted simulations to examine the mechanism of the observed biases and revealed that failure to incorporate perseveration directly affected the estimation bias in the learning rate and indirectly affected that in inverse temperature. Furthermore, in both model fittings and model simulations, the lack of perseveration caused win-stay probability underestimation and loss-shift probability overestimation. We also assessed the parameter reliability. Test-retest reliabilities were poor (learning rates) to moderate (inverse temperature and perseveration magnitude). A learning effect was noted in the inverse temperature and perseveration magnitude parameters, showing an increment of the estimates in the second session. We discuss possible misinterpretations of results and limitations considering the estimation biases and parameter reliability.",
        "link": "http://dx.doi.org/10.31234/osf.io/jz5rv"
    },
    {
        "id": 27081,
        "title": "Scheduling of Twin Automated Stacking Cranes Based on Deep Reinforcement Learning",
        "authors": "Xin Jin, Nan Mi, Wen Song, Qiqiang Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4474871"
    },
    {
        "id": 27082,
        "title": "Enhanced Resilience in Battery Charging through Co-Simulation with Reinforcement Learning",
        "authors": "Mohammad Seyedi, Kouhyar Sheida, Savion Siner, Farzad Ferdowsi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Controlling the charging and discharging procedures of Lithium-Ion\nBatteries is of paramount importance as violating safety constraints,\nsuch as current deviations, can lead to significant damage to the\nbattery or circuit or interruption in service. Thus, it is crucial to\nemploy a robust controller capable of handling uncertainties and\nunexpected scenarios. PI controllers have become prevalent in recent\nyears for managing battery dynamics, but they exhibit limited robustness\nin unpredictable situations. In this paper, we propose a Reinforcement\nLearning (RL) driven control method as a substitute for the PI\ncontroller. The agent is trained using a co-simulation approach with\nsimultaneous employment of Python and Matlab, ensuring an accurate\nestimation of the environment and, consequently, enhanced performance. A\nprototype of the proposed controller is developed using dSPACE rapid\ncontrol prototyper. The performance is compared with the benchmark\ncontroller (PI) across different fault scenarios, considering three\ncriteria: overshoot, undershoot, and stabilization time. The comparative\nanalysis reveals that, in most scenarios, the RL agent outperforms the\nPI controller, exhibiting a remarkable 50% reduction in both overshoot\nand undershoot compared to the benchmark controller. This research\ncontributes to advancing battery control systems by introducing an\nRL-based controller that proves to be a more robust alternative,\ndelivering improved performance in the face of uncertainties and fault\nscenarios.",
        "link": "http://dx.doi.org/10.36227/techrxiv.170846720.02245839/v1"
    },
    {
        "id": 27083,
        "title": "Value Decomposition with Maximum Correntropy for Multi-Agent Deep Reinforcement Learning",
        "authors": "Kai Liu, Tianxian Zhang, Lingjiang Kong, Xiangliang Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4580788"
    },
    {
        "id": 27084,
        "title": "Evaluation of CNN Models Using Deep Reinforcement Learning for Band Selection on Hyperspectral Image Classification",
        "authors": "Saziye Ozge Atik",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAlong with the high spectral rich information it provides, one of the difficulties in processing a hyperspectral image is the need for expert knowledge and high-spec hardware to process very high-dimensional data. The use of the most relevant bands in the hyperspectral image is quite decisive in deep CNN networks without loss of information and loss of accuracy. It is crucial to classify hyperspectral images with faster and less hardware-requiring models by creating subset groups by choosing a limited number of optimal bands. In this study, a comparative analysis about the effect of deep reinforcement learning (DRL)-based hyperspectral band selection on the classification performance of deep learning networks is presented. 3D CNN, 3D + 1D CNN and Multiscale 3D deep convolutional neural network (M3D-DCNN) algorithms were used for hyperspectral image classification. By choosing the most effective bands determined by DRL, it is aimed to perform classification with high accuracy with fewer bands instead of all bands. All tests were performed on popular hyperspectral datasets, Indian Pines, Salinas, and Pavia Center. The 3D + 1D approach reached 92.28% OA in the IP dataset. In Salinas, 94.87% OA with 3D CNN and 94.62% OA with M3D-DCNN was obtained. 3D + 1D CNN has 98.64% OA in PaviaC.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3378269/v1"
    },
    {
        "id": 27085,
        "title": "Dynamic Graph Dismantling with Reinforcement Supervised Learning",
        "authors": "Yumei Wang, Chuancong Tang, Hai-Tao Zhang",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240999"
    },
    {
        "id": 27086,
        "title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
        "authors": "Chayan Banerjee, Kien Nguyen, Clinton Fookes, Maziar Raissi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4597487"
    },
    {
        "id": 27087,
        "title": "Generalized attention-weighted reinforcement learning",
        "authors": "Lennart Bramlage, Aurelio Cortese",
        "published": "2022-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2021.09.023"
    },
    {
        "id": 27088,
        "title": "Timar: Transition-Informed Representation for Sample-Efficient Multi-Agent Reinforcement Learning",
        "authors": "Mingxiao Feng, Yaodong Yang, Wengang Zhou, Houqiang Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4706110"
    },
    {
        "id": 27089,
        "title": "A Boosting-based Deep Neural Networks Algorithm for Reinforcement Learning",
        "authors": "Yu Wang, Hongxia Jin",
        "published": "2018-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2018.8431647"
    },
    {
        "id": 27090,
        "title": "Deep Reinforcement Learning for Algorithmic Trading",
        "authors": "Álvaro Cartea, Sebastian Jaimungal, Leandro Sánchez-Betancourt",
        "published": "No Date",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3812473"
    },
    {
        "id": 27091,
        "title": "A Deep Graph Reinforcement Learning-Based Investment Strategy: Evidence from China's Stock Market",
        "authors": "Hongduo Cao, Ziran Zhao, Ying Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4725385"
    },
    {
        "id": 27092,
        "title": "Reinforcement Learning Method for AUV Hover Control in Uncertain Environment",
        "authors": "Yueteng Zhao, Yuping Tian",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240614"
    },
    {
        "id": 27093,
        "title": "Approximation of Convex Envelope Using Reinforcement Learning (Extended Abstract)",
        "authors": "Vivek S. Borkar, Adit Akarsh",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10442393"
    },
    {
        "id": 27094,
        "title": "Improved Estimation of the Covariance Matrix using Reinforcement Learning",
        "authors": "Cheng Lu, Majeed Simaan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4081502"
    },
    {
        "id": 27095,
        "title": "Information-Directed Exploration via Distributional Deep Reinforcement Learning",
        "authors": "Zijie He",
        "published": "2021-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isctis51085.2021.00052"
    },
    {
        "id": 27096,
        "title": "Memory-assisted Reinforcement Learning for Diverse Molecular De Novo Design",
        "authors": "Thomas Blaschke, Ola Engkvist, Jürgen Bajorath, Hongming Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn de novo molecular design, recurrent neural networks (RNN) have been shown to be effective methods for sampling and generating novel chemical structures. Using a technique called reinforcement learning (RL), an RNN can be tuned to target a particular section of chemical space with optimized desirable properties using a scoring function. However, ligands generated by current RL methods so far tend to have relatively low diversity, and sometimes even result in duplicate structures when optimizing towards particular properties. Here, we propose a new method to address the low diversity issue in RL. Memory-assisted RL is an extension of the known RL, with the introduction of a so-called memory unit. As proof of concept, we applied our method to generate structures with an optimized logP. In a second case study, we applied our method to design ligands for the dopamine 2 receptor and the 5-hydroxytryptamine 1A receptor. For both receptors, a machine learning model was developed to predict whether generated molecules were active or not for the receptor. In both case studies, it was found that memory-assisted RL led to the generation of more active compounds and with higher chemical diversity, thus achieving better coverage of chemical space of known ligands compared to established RL method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-52871/v1"
    },
    {
        "id": 27097,
        "title": "Memory-assisted reinforcement learning for diverse molecular de novo design",
        "authors": "Thomas Blaschke, Ola Engkvist, Jürgen Bajorath, Hongming Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn de novo molecular design, recurrent neural networks (RNN) have been shown to be effective methods for sampling and generating novel chemical structures. Using a technique called reinforcement learning (RL), an RNN can be tuned to target a particular section of chemical space with optimized desirable properties using a scoring function. However, ligands generated by current RL methods so far tend to have relatively low diversity, and sometimes even result in duplicate structures when optimizing towards desired properties. Here, we propose a new method to address the low diversity issue in RL for molecular design. Memory-assisted RL is an extension of the known RL, with the introduction of a so-called memory unit. As proof of concept, we applied our method to generate structures with a desired AlogP value. In a second case study, we applied our method to design ligands for the dopamine type 2 receptor and the 5-hydroxytryptamine type 1A receptor. For both receptors, a machine learning model was developed to predict whether generated molecules were active or not for the receptor. In both case studies, it was found that memory-assisted RL led to the generation of more compounds predicted to be active having higher chemical diversity, thus achieving better coverage of chemical space of known ligands compared to established RL methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-52871/v2"
    },
    {
        "id": 27098,
        "title": "Distilling deep neural networks with reinforcement learning",
        "authors": "You Huang, Yuanlong Yu",
        "published": "2018-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icinfa.2018.8812321"
    },
    {
        "id": 27099,
        "title": "Valence biases in reinforcement learning shift across adolescence and modulate subsequent memory",
        "authors": "Gail Rosenbaum, Hannah Grassie, Catherine A. Hartley",
        "published": "No Date",
        "citations": 6,
        "abstract": "Individuals learn differently through trial and error, with some more influenced by good outcomes, and others weighting bad outcomes more heavily. Such valence biases may also influence memory for past experiences. Here, we examined whether valence asymmetries in reinforcement learning change across adolescence, and whether individual learning asymmetries bias the content of subsequent memory. Participants ages 8-27 learned the values of “point machines”, after which their memory for trial-unique images presented with choice outcomes was assessed. Relative to children and adults, adolescents overweighted worse-than-expected outcomes during learning. Individuals’ valence biases modulated incidental memory, such that those who prioritized worse- (or better-) than-expected outcomes during learning were also more likely to remember images paired with these outcomes, an effect reproduced in an independent dataset. Collectively, these results highlight age-related changes in the computation of subjective value, and demonstrate that a valence-asymmetric valuation process influences how information is prioritized in episodic memory.",
        "link": "http://dx.doi.org/10.31234/osf.io/n3vsr"
    },
    {
        "id": 27100,
        "title": "Deep Reinforcement Learning with Godot Game Engine",
        "authors": "Mahesh Ranaweera, Qusay H. Mahmoud",
        "published": "2024-3-5",
        "citations": 0,
        "abstract": "This paper introduces a Python framework for developing Deep Reinforcement Learning (DRL) in an open-source Godot game engine to tackle sim-to-real research. A framework was designed to communicate and interface with the Godot game engine to perform the DRL. With the Godot game engine, users will be able to set up their environment while defining the constraints, motion, interactive objects, and actions to be performed. The framework interfaces with the Godot game engine to perform defined actions. It can be further extended to perform domain randomization and enhance overall learning by increasing the complexity of the environment. Unlike other proprietary physics or game engines, Godot provides extensive developmental freedom under an open-source licence. By incorporating Godot’s built-in powerful node-based environment system, flexible user interface, and the proposed Python framework, developers can extend its features to develop deep learning applications. Research performed on Sim2Real using this framework has provided great insight into the factors that affect the gap in reality. It also demonstrated the effectiveness of this framework in Sim2Real applications and research.",
        "link": "http://dx.doi.org/10.3390/electronics13050985"
    },
    {
        "id": 27101,
        "title": "Basic protocols in quantum reinforcement learning with superconducting circuits",
        "authors": "Lucas Lamata",
        "published": "2017-5-9",
        "citations": 50,
        "abstract": "AbstractSuperconducting circuit technologies have recently achieved quantum protocols involving closed feedback loops. Quantum artificial intelligence and quantum machine learning are emerging fields inside quantum technologies which may enable quantum devices to acquire information from the outer world and improve themselves via a learning process. Here we propose the implementation of basic protocols in quantum reinforcement learning, with superconducting circuits employing feedback- loop control. We introduce diverse scenarios for proof-of-principle experiments with state-of-the-art superconducting circuit technologies and analyze their feasibility in presence of imperfections. The field of quantum artificial intelligence implemented with superconducting circuits paves the way for enhanced quantum control and quantum computation protocols.",
        "link": "http://dx.doi.org/10.1038/s41598-017-01711-6"
    },
    {
        "id": 27102,
        "title": "Reinforcement Learning with Auxiliary Localization Task for Mapless Navigation",
        "authors": "Cong He, Wengang Zhang, Teng Wang",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327835"
    },
    {
        "id": 27103,
        "title": "Applying Reinforcement Learning for Shortest Path Problem",
        "authors": "Zhixuan Sun",
        "published": "2022-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bdicn55575.2022.00100"
    },
    {
        "id": 27104,
        "title": "LEARNING A FOREIGN LANGUAGE AS THINKING ACTIVITY REINFORCEMENT",
        "authors": "Alla Belousova, Larisa Abrosimova, Marina Bogdanova",
        "published": "2017-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/inted.2017.0576"
    },
    {
        "id": 27105,
        "title": "The construction and deconstruction of sub-optimal preferences through range-adapting reinforcement learning",
        "authors": "Sophie Bavard, Aldo Rustichini, Stefano Palminteri",
        "published": "No Date",
        "citations": 0,
        "abstract": "Converging evidence suggests that economic values are rescaled as a function of the range of the available options. Critically, although locally adaptive, range adaptation has been shown to lead to suboptimal choices. This is particularly striking in reinforcement learning (RL) situations when options are extrapolated from their original context. Range adaptation can be seen as the result of an adaptive coding process aiming at increasing the signal-to-noise ratio. However, this hypothesis leads to a counterintuitive prediction: decreasing outcome uncertainty should increase range adaptation and, consequently, extrapolation errors. Here, we tested the paradoxical relation between range adaptation and performance in a large sample of subjects performing variants of a RL task, where we manipulated task difficulty. Results confirmed that range adaptation induces systematic extrapolation errors and is stronger when decreasing outcome uncertainty. Finally, we propose a range-adapting model and show that it is able to parsimoniously capture all the observed results.",
        "link": "http://dx.doi.org/10.31234/osf.io/3xt69"
    },
    {
        "id": 27106,
        "title": "New reinforcement learning algorithm for robot soccer",
        "authors": "M Yoon, J Bekker, S Kroon",
        "published": "2017-6-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5784/33-1-542"
    },
    {
        "id": 27107,
        "title": "Distributional Reinforcement Learning with Ensembles",
        "authors": "Björn Lindenberg, Jonas Nordqvist, Karl-Olof Lindahl",
        "published": "2020-5-7",
        "citations": 0,
        "abstract": "It is well known that ensemble methods often provide enhanced performance in reinforcement learning. In this paper, we explore this concept further by using group-aided training within the distributional reinforcement learning paradigm. Specifically, we propose an extension to categorical reinforcement learning, where distributional learning targets are implicitly based on the total information gathered by an ensemble. We empirically show that this may lead to much more robust initial learning, a stronger individual performance level, and good efficiency on a per-sample basis.",
        "link": "http://dx.doi.org/10.3390/a13050118"
    },
    {
        "id": 27108,
        "title": "Robust Reinforcement Learning via Genetic Curriculum",
        "authors": "Yeeho Song, Jeff Schneider",
        "published": "2022-5-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9812420"
    },
    {
        "id": 27109,
        "title": "Spectra to Structure: Deep Reinforcement Learning for Molecular Inverse Problem",
        "authors": "Bhuvanesh Sridharan, Sarvesh Mehta, Yashaswi Pathak, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 1,
        "abstract": "Spectroscopy is the study of how matter interacts with electromagnetic radiations of specific frequencies that has led to several monumental discoveries in science. The spectra of any particular molecule is highly information-rich, yet the inverse relation from the spectra to the molecular structure is still an unsolved problem. Nuclear Magnetic Resonance (NMR) spectroscopy is one such critical tool in the tool-set for scientists to characterise any chemical sample. In this work, a novel framework is proposed that attempts to solve this inverse problem by navigating the chemical space to find the correct structure that resulted in the target spectra. The proposed framework uses a combination of online Monte- Carlo-Tree-Search (MCTS) and a set of offline trained Graph Convolution Networks to build a molecule iteratively from scratch. Our method is able to predict the correct structure of the molecule ∼80% of the time in its top 3 guesses. We believe that the proposed framework is a significant step in solving the inverse design problem of NMR spectra to molecule.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-4hc7k"
    },
    {
        "id": 27110,
        "title": "Vision-driven Deep Reinforcement Learning for Electronic Components Robotic Insertion Tasks",
        "authors": "Grzegorz Marcin Bartyzel, Wojciech Półchłopek, Dominik Rzepka",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nOne of the challenges of robotics in the modern manufacturing industry is assembly task. The manufacturing industry requires various insertion tasks, from peg-in-hole tasks to electronic parts assembly. Nowadays, robotic solutions for this problem often use the conventional methods. In those methods, the industrial robot is controlled by the hybrid force-position control and performs preprogrammed trajectories, such as a spiral path. However, electronic parts require more sophisticated techniques due to their complex geometry and susceptibility to damage. We propose a vision-driven method based on reinforcement learning (RL) for assembling electronic parts. In our approach, the input image for the RL agent is acquired from two cameras mounted to the robot's end-effector. In this work, we also analyze the influence of the observation modalities on the RL's agent performance metrics, such as insertion success rate and average assembly time. Results show that visual information acquired from a double-camera vision system significantly improves the RL method's robustness on the position disturbance in insertion tasks. The proposed method in this work outperforms conventional methods, such as random search, spiral search, and straight-down insertion in terms of success rate and robustness for the robot's initial position disturbances. Moreover, our approach is more robust for disturbance than a method that uses an external camera or a single camera mounted to the robot's end-effector for image acquisition.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2233330/v1"
    },
    {
        "id": 27111,
        "title": "5G Scheduling using Reinforcement Learning",
        "authors": "D. Zavyalova, V. Drozdova",
        "published": "2020-10-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fareastcon50210.2020.9271421"
    },
    {
        "id": 27112,
        "title": "Principles of RL Problems",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_2"
    },
    {
        "id": 27113,
        "title": "Robotic Obstacle Avoidance: A Virtual Modeling And Reinforcement Learning",
        "authors": "Ming-Fei Chen, Han-Hsien Tsai, Wen-Tse Hsiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis study developed a robotic arm self-learning system based on virtual modeling and reinforcement learning. Using the model of a robotic arm, information concerning obstacles in the environment, initial coordinates of the robotic arm, and the target position, this system automatically generated a set of rotational angles to enable a robotic arm to be positioned such that it can avoid all obstacles and reach a target. The developed program was divided into three parts. The first part involves robotic arm simulation and collision detection; specifically, images of a six-axis robotic arm and obstacles were input to the Visualization ToolKit library to visualize the movements and surrounding environment of the robotic arm. Subsequently, an oriented bounding box algorithm was used to determine whether collisions had occurred. The second part concerned machine-learning–based route planning. The TensorFlow was used to establish a deep deterministic policy gradient model, and reinforcement learning was employed for the response to environmental variables. Different reward functions were designed for tests and discussions, and the program’s practicality was verified through actual machine operations. Finally, the application of reinforcement learning in route planning for a robotic arm was proved feasible by the experiment. Such an application facilitated automatic route planning and achieved an error of less than 10 mm from the target position.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-601053/v1"
    },
    {
        "id": 27114,
        "title": "Research on Agent Control Algorithm Based on Reinforcement Learning",
        "authors": "Zhongqiu Zhang",
        "published": "2020-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccea50009.2020.00127"
    },
    {
        "id": 27115,
        "title": "Deep Reinforcement Learning for Contagion Control",
        "authors": "Diego R. Benalcazar, Chinwendu Enyioha",
        "published": "2021-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccta48906.2021.9659238"
    },
    {
        "id": 27116,
        "title": "Behavior Constraining in Weight Space for Offline Reinforcement Learning",
        "authors": "Phillip Swazinna, Steffen Udluft, Daniel Hein, Thomas Runkler",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2021.es2021-83"
    },
    {
        "id": 27117,
        "title": "Can Deep Reinforcement Learning Solve the Portfolio Allocation Problem? (PhD Manuscript)",
        "authors": "Eric Benhamou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4599800"
    },
    {
        "id": 27118,
        "title": "Risk-Based Adaptive Stock Trading System Using Reinforcement Learning",
        "authors": "Chai Quek, Qi CAO",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4237367"
    },
    {
        "id": 27119,
        "title": "Deep Residual Attention Reinforcement Learning",
        "authors": "Hanhua Zhu, Tomoyuki Kaneko",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/taai48200.2019.8959896"
    },
    {
        "id": 27120,
        "title": "How working memory and reinforcement learning are intertwined: a cognitive, neural, and computational perspective",
        "authors": "Aspen H. Yoo, Anne Collins",
        "published": "No Date",
        "citations": 0,
        "abstract": "Reinforcement learning and working memory are two core processes of human cognition, and are often considered cognitively, neuroscientifically, and algorithmically distinct. Here, we show that the brain networks that support them actually overlap significantly, and that they are less distinct cognitive processes than often assumed. We review literature demonstrating the benefits of considering each process to explain properties of the other, and highlight recent work investigating their more complex interactions. We discuss how future research in both computational and cognitive sciences can benefit from one another, suggesting that a key missing piece for artificial agents to learn to behave with more human-like efficiency is taking working memory’s role in learning seriously. This review highlights the risks of neglecting the interplay between different processes when studying human behavior (in particular when considering individual differences). We emphasize the importance of investigating these dynamics in order to build a comprehensive understanding of human cognition.",
        "link": "http://dx.doi.org/10.31234/osf.io/ebtn6"
    },
    {
        "id": 27121,
        "title": "Federated Multi Agent Deep Reinforcement Learning for Optimized Design of Future Wireless Networks",
        "authors": "Hugo De Oliveira, Megumi Kaneko, Lila Boukhatem",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Federated Multi-Agent Deep Reinforcement Learning (F-MADRL) has been recently attracting increasing research interests, as it offers efficient solutions towards meeting the extreme requirements of Beyond 5G (B5G) and 6G applications. By contrast to centralized Deep Reinforcement Learning (DRL) and Multi-Agent DRL (MADRL), F-MADRL enables edge devices to cooperate without sharing their private data, while reducing the delays and signaling costs inherent to centralized approaches. In this article, we explore the new opportunities brought by F-MADRL by conducting a holistic survey on its related recent works. Firstly, we categorize state-of-the-art F-MADRL approaches, based on some distinctive features such as incurred signaling overhead, privacy level, and aggregation frequency. To better illustrate the behavior and advantages of F-MADRL, it is numerically compared to its centralized and distributed DRL counterparts, through a Sub-6GHz/mmWave band association optimization problem for IoT short packet communications. Finally, we identify and discuss the open research directions and challenges, in order to spur further interests in this promising area.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24007503.v1"
    },
    {
        "id": 27122,
        "title": "A Population-Based Approach for Multi-Agent Interpretable Reinforcement Learning",
        "authors": "Marco Crespi, Andrea Ferigo, Leonardo  Lucio Custode, Giovanni Iacca",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4467882"
    },
    {
        "id": 27123,
        "title": "The role of subgoals in hierarchical reinforcement learning",
        "authors": "Milena Rmus, Maria Eckstein, Anne Collins",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1587-0"
    },
    {
        "id": 27124,
        "title": "Recovering From Cyber-Manufacturing Attacks by Reinforcement Learning",
        "authors": "Romesh Prasad, Matthew K. Swanson, Young Moon",
        "published": "2022-10-30",
        "citations": 0,
        "abstract": "Abstract\nA Cyber-Manufacturing systems (CMS) is an integration of informational and operational entities that are synchronized with manufacturing processes to increase productivity. However, this integration enlarges the scope for cyber attackers to intrude manufacturing processes, which are called cyber-manufacturing attacks. They can have significant impacts on physical operations within a CMS, such as shutting down plants, production interruption, premature failure of products, and fatal accidents. Although research activities in this emerging problem have been increased recently, existing research has been limited to detection and prevention solutions. However, these strategies cannot ensure a continuous function of an attacked CMS. To ensure continuous functioning of a CMS, a robust recovery strategy must be developed and employed. Current research in recovery has been limited to feedback controllers with an assumption of a complete knowledge of a system model. To overcome this limitation, a recovery agent augmented by reinforcement learning was developed. This is to utilize the ability of reinforcement learning to handle sequential decisions and to proceed even without a complete knowledge of a system model. A virtual environment for recovery agents has been developed to assist efforts needed to obtain sample data, experiment various scenarios, and explore with reinforcement learning. Two cyber-manufacturing attack scenarios have been developed: (i) spoofing a stepper motor controlling additive manufacturing processes, (ii) disrupting the sequence of the pick and place robot. The recovery agent takes random actions by exploring its environment and receives rewards from the actions. After many iterations, it learns proper actions to take.",
        "link": "http://dx.doi.org/10.1115/imece2022-93982"
    },
    {
        "id": 27125,
        "title": "Reinforcement Learning based Intelligent Semiconductor Manufacturing Applied to Laser Annealing",
        "authors": "Tejender Rawat, Chang-Yuan Chung, Shih-Wei Chen, Albert Lin",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2449"
    },
    {
        "id": 27126,
        "title": "Reinforcement Learning for Financial Index Tracking",
        "authors": "Xianhua Peng, Chenyin Gong, Xue Dong He",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4532072"
    },
    {
        "id": 27127,
        "title": "Network Intrusion Detection System using Reinforcement learning",
        "authors": "Malika Malik, Kamaljit Singh Saini",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incet57972.2023.10170630"
    },
    {
        "id": 27128,
        "title": "Testosterone and estradiol affect adolescent reinforcement learning",
        "authors": "Sina Kohne, Esther K. Diekhof",
        "published": "2022-2-3",
        "citations": 1,
        "abstract": "During adolescence, gonadal hormones influence brain maturation and behavior. The impact of 17β-estradiol and testosterone on reinforcement learning was previously investigated in adults, but studies with adolescents are rare. We tested 89 German male and female adolescents (mean age ± sd = 14.7 ± 1.9 years) to determine the extent 17β-estradiol and testosterone influenced reinforcement learning capacity in a response time adjustment task. Our data showed, that 17β-estradiol correlated with an enhanced ability to speed up responses for reward in both sexes, while the ability to wait for higher reward correlated with testosterone primary in males. This suggests that individual differences in reinforcement learning may be associated with variations in these hormones during adolescence, which may shift the balance between a more reward- and an avoidance-oriented learning style.",
        "link": "http://dx.doi.org/10.7717/peerj.12653"
    },
    {
        "id": 27129,
        "title": "Reinforcement Learning Based Pricing for Demand Response",
        "authors": "Amir Ghasemkhani, Lei Yang",
        "published": "2018-5",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccw.2018.8403783"
    },
    {
        "id": 27130,
        "title": "Deep Reinforcement Learning for Energy Management in a Microgrid with Flexible Demand",
        "authors": "Taha abd el halim Nakabi, Pekka Toivanen",
        "published": "No Date",
        "citations": 2,
        "abstract": "In this paper, we study the performance of various deep reinforcement learning algorithms to enhance the energy management system of a microgrid. We propose a novel microgrid model that consists of a wind turbine generator, an energy storage system, a set of thermostatically controlled loads, a set of price-responsive loads, and a connection to the main grid. The proposed energy management system is designed to coordinate among the different flexible sources by defining the priority resources, direct demand control signals, and electricity prices. Seven deep reinforcement learning algorithms were implemented and are empirically compared in this paper. The numerical results show that the deep reinforcement learning algorithms differ widely in their ability to converge to optimal policies. By adding an experience replay and a semi-deterministic training phase to the well-known asynchronous advantage actor-critic algorithm, we achieved the highest model performance as well as convergence to near-optimal policies.",
        "link": "http://dx.doi.org/10.20944/preprints202010.0156.v1"
    },
    {
        "id": 27131,
        "title": "Experimental Evaluation of Deep Reinforcement Learning Algorithms",
        "authors": "Nikola Mrzljak, Tomislav Hrkać",
        "published": "2019-2-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.20532/ccwv.2018.0003"
    },
    {
        "id": 27132,
        "title": "Inertia-Constrained Reinforcement Learning to Enhance Human Motor Control Modeling",
        "authors": "Soroush Korivand, Nader Jalili, Jiaqi Gong",
        "published": "No Date",
        "citations": 0,
        "abstract": "Locomotor impairment is a high-prevalent and significant source of disability and significantly impacts a large population’s quality of life. Despite decades of research in human locomotion, the challenges of simulating human movement to study the features of musculoskeletal drivers and clinical conditions remain. Most recent efforts in utilizing reinforcement learning (RL) techniques are promising to simulate human locomotion and reveal musculoskeletal drives. However, these simulations often failed to mimic natural human locomotion because most reinforcement strategies have yet to consider any reference data regarding human movement. To address these challenges, in this study, we designed a reward function based on the trajectory optimization rewards (TOR), and bio-inspired rewards, which includes the rewards obtained from reference motion data captured by a single Intertial Moment Unit (IMU) sensor. The sensor was equipped on the participants’ pelvis to capture reference motion data. Also, we adapted the reward function by leveraging previous research in walking simulation for TOR. The experimental results showed that the simulated agents with the modified reward function performed better in mimicking the collected IMU data from participants, which means the simulated human locomotion was more realistic. Also, as this bio-inspired defined cost, IMU data enhanced the agent’s capacity to converge during the training process. As a result, the models’ convergence is faster than those developed without reference motion data. Consequently, human locomotion can be simulated more quicker and in a broader range of environments with a better simulation performance.",
        "link": "http://dx.doi.org/10.20944/preprints202212.0167.v1"
    },
    {
        "id": 27133,
        "title": "Diversity Oriented Deep Reinforcement Learning for Targeted Molecule Generation",
        "authors": "Tiago Pereira, Maryam Abbasi, Bernardete Ribeiro, Joel P. Arrais",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this work, we explore the potential of deep learning to streamline the process of identifying new potential drugs through the computational generation of molecules with interesting biological properties. Two deep neural networks compose our targeted generation framework: the Generator, which is trained to learn the building rules of valid molecules employing SMILES strings notation, and the Predictor which evaluates the newly generated compounds by predicting their aﬃnity for the desired target. Then, the Generator is optimized through Reinforcement Learning to produce molecules with bespoken properties. The innovation of this approach is the exploratory strategy applied during the reinforcement training process that seeks to add novelty to the generated compounds. This training strategy employs two Generators interchangeably to sample new SMILES: the initially trained model that will remain ﬁxed and a copy of the previous one that will be updated during the training to uncover the most promising molecules. The evolution of the reward assigned by the Predictor determines how often each one is employed to select the next token of the molecule. This strategy establishes a compromise between the need to acquire more information about the chemical space and the need to sample new molecules, with the experience gained so far. To demonstrate the eﬀectiveness of the method, the Generator is trained to design molecules with high inhibitory power for the adenosine A2A and κ opioid receptors. The results reveal that the model can eﬀectively modify the biological aﬃnity of the newly generated molecules towards the craved direction. More importantly, it was possible to ﬁnd auspicious sets of unique and diverse molecules, which was the main purpose of the newly implemented strategy.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-110570/v1"
    },
    {
        "id": 27134,
        "title": "Leveraging Optimal Transport for Enhanced Offline Reinforcement Learning in Surgical Robotic Environments",
        "authors": "Maryam Zare, Parham Mohsenzadeh Kebria, Abbas Khosravi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4627353"
    },
    {
        "id": 27135,
        "title": "Quality-Diversity Based Semi-Autonomous Teleoperation Using Reinforcement Learning",
        "authors": "Sangbeom Park, Taerim Yoon, Joonhyung Lee, Sunghyun Park, Sungjoon Choi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4706203"
    },
    {
        "id": 27136,
        "title": "Reinforcement Learning and Deep Stochastic Optimal Control for Final Quadratic Hedging",
        "authors": "Bernhard Hientzsch",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4645455"
    },
    {
        "id": 27137,
        "title": "Track 5: Inspection Planning for Transmission Line Systems\n  Exposed to Hurricanes using Reinforcement Learning",
        "authors": "Ashkan B., Nariman L., Abdollah Shafieezadeh",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/m.6321e2e2f30377bc3baf6628"
    },
    {
        "id": 27138,
        "title": "Abstraction for Deep Reinforcement Learning",
        "authors": "Murray Shanahan, Melanie Mitchell",
        "published": "2022-7",
        "citations": 5,
        "abstract": "We characterise the problem of abstraction in the context of deep reinforcement learning. Various well established approaches to analogical reasoning and associative memory might be brought to bear on this issue, but they present difficulties because of the need for end-to-end differentiability. We review developments in AI and machine learning that could facilitate their adoption.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/780"
    },
    {
        "id": 27139,
        "title": "Model-based reinforcement learning for service mesh fault resiliency in a web application-level",
        "authors": "Fanfei Meng, Lalita Jagadeesa, Marina Thottan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Microservice-based architectures enable different aspects of web applications to be created and updated independently, even after deployment. Associated technologies such as service mesh provide application-level fault resilience through attribute configurations that govern the behavior of request - response service -- and the interactions among them -- in the presence of failures. While this provides tremendous flexibility, the configured values of these attributes -- and the relationships among them -- can significantly affect the performance and fault resilience of the overall application. Furthermore, it is impossible to determine the best and worst combinations of attribute values with respect to fault resiliency via testing, due to the complexities of the underlying distributed system and the many possible attribute value combinations. In this paper, we present a model-based reinforcement learning workflow towards service mesh fault resiliency. Our approach enables the prediction of the most significant fault resilience behaviors at a web application-level, scratching from single service to aggregated multi-service management with efficient agent collaborations.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24582396.v1"
    },
    {
        "id": 27140,
        "title": "Scalable Multi-Agent Reinforcement Learning-Based Distributed Channel Access",
        "authors": "Zhenyu Chen, Xinghua Sun",
        "published": "2023-5-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278960"
    },
    {
        "id": 27141,
        "title": "Decision letter for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v3/decision1"
    },
    {
        "id": 27142,
        "title": "Decision letter for \"Performance enhancement of the artificial neural network–based reinforcement learning for wind turbine yaw control\"",
        "authors": "",
        "published": "2019-8-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2451/v1/decision1"
    },
    {
        "id": 27143,
        "title": "Intelligent demand response resource trading using deep reinforcement learning",
        "authors": "",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2020.05540"
    },
    {
        "id": 27144,
        "title": "Quality of service based radar resource management using deep reinforcement learning",
        "authors": "Sebastian Durst, Stefan Bruggenwirth",
        "published": "2021-5-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2147009.2021.9455234"
    },
    {
        "id": 27145,
        "title": "Decision letter: Humans perseverate on punishment avoidance goals in multigoal reinforcement learning",
        "authors": "Claire M Gillan",
        "published": "2021-11-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.74402.sa1"
    },
    {
        "id": 27146,
        "title": "Author response for \"The value–complexity trade-off for reinforcement learning based brain–computer interfaces\"",
        "authors": " Hadar Levi-Aharoni,  Naftali Tishby",
        "published": "2020-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1741-2552/abc8d8/v2/response1"
    },
    {
        "id": 27147,
        "title": "Reinforcement Learning with Deep Deterministic Policy Gradient",
        "authors": "Haining Tan",
        "published": "2021-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/caibda53561.2021.00025"
    },
    {
        "id": 27148,
        "title": "A Reinforcement Learning and Recurrent Neural Network Based Dynamic User Modeling System",
        "authors": "Abhishek Tripathi, Ashwin T.S., Ram Mohana Reddy Guddeti",
        "published": "2018-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icalt.2018.00103"
    },
    {
        "id": 27149,
        "title": "Visuotactile-RL: Learning Multimodal Manipulation Policies with Deep Reinforcement Learning",
        "authors": "Johanna Hansen, Francois Hogan, Dmitriy Rivkin, David Meger, Michael Jenkin, Gregory Dudek",
        "published": "2022-5-23",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9812019"
    },
    {
        "id": 27150,
        "title": "Semisupervised Learning for Noise Suppression Using Deep Reinforcement Learning of Contrastive Features",
        "authors": "Ehsan Kazemi, Fariborz Taherkhani, Liqiang Wang",
        "published": "2023-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lsens.2023.3264998"
    },
    {
        "id": 27151,
        "title": "Context-dependent extinction learning emerging from raw sensory inputs: A reinforcement learning approach",
        "authors": "Thomas Walther, Nicolas Diekmann, Sandhiya Vijayabaskaran, José R. Donoso, Denise Manahan-Vaughan, Laurenz Wiskott, Sen Cheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe context-dependence of extinction learning has been well studied and requires the hippocampus. However, the underlying neural mechanisms are still poorly understood. Using memory-driven reinforcement learning and deep neural networks, we developed a model that learns to navigate autonomously in biologically realistic VR environments based on raw camera inputs alone. Neither is context represented explicitly in our model, nor is context change signaled. We find that memory-intact agents learn distinct context representations, and develop ABA renewal, whereas memory-impaired agents do not. These findings reproduce the behavior of control and hippocampal animals, respectively. We therefore propose that the role of the hippocampus in the context-dependence of extinction learning might stem from its function in episodic-like memory and not in context-representation per se. We conclude that context-dependence can emerge from raw visual inputs.",
        "link": "http://dx.doi.org/10.1101/2020.04.27.059121"
    },
    {
        "id": 27152,
        "title": "Simulating Early Childhood Drawing Behaviors under Physical Constraints Using Reinforcement Learning",
        "authors": "Yoshia Abe, Yoshiyuki Ohmura, Shogo Yonekura, Hoshinori Kanazawa, Yasuo Kuniyoshi",
        "published": "2023-11-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl55364.2023.10364388"
    },
    {
        "id": 27153,
        "title": "An AGV Task Scheduling Method Based on Multi-Agent Reinforcement Learning",
        "authors": "Yuxin Zhao, Ke Zhu, Xueming Song, Jianming Zhang",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10166593"
    },
    {
        "id": 27154,
        "title": "Cooperative Spectrum Sensing Meets Machine Learning: Deep Reinforcement Learning Approach",
        "authors": "Rahil Sarikhani, Farshid Keynia",
        "published": "2020-7",
        "citations": 52,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lcomm.2020.2984430"
    },
    {
        "id": 27155,
        "title": "Multi-strategy self-learning particle swarm optimization algorithm based on reinforcement learning",
        "authors": "Xiaoding Meng, Hecheng Li, Anshan Chen",
        "published": "2023",
        "citations": 2,
        "abstract": "<abstract><p>The trade-off between exploitation and exploration is a dilemma inherent to particle swarm optimization (PSO) algorithms. Therefore, a growing body of PSO variants is devoted to solving the balance between the two. Among them, the method of self-adaptive multi-strategy selection plays a crucial role in improving the performance of PSO algorithms but has yet to be well exploited. In this research, with the aid of the reinforcement learning technique to guide the generation of offspring, a novel self-adaptive multi-strategy selection mechanism is designed, and then a multi-strategy self-learning PSO algorithm based on reinforcement learning (MPSORL) is proposed. First, the fitness value of particles is regarded as a set of states that are divided into several state subsets non-uniformly. Second, the $ \\varepsilon $-greedy strategy is employed to select the optimal strategy for each particle. The personal best particle and the global best particle are then updated after executing the strategy. Subsequently, the next state is determined. Thus, the value of the Q-table, as a scheme adopted in self-learning, is reshaped by the reward value, the action and the state in a non-stationary environment. Finally, the proposed algorithm is compared with other state-of-the-art algorithms on two well-known benchmark suites and a real-world problem. Extensive experiments indicate that MPSORL has better performance in terms of accuracy, convergence speed and non-parametric tests in most cases. The multi-strategy selection mechanism presented in the manuscript is effective.</p></abstract>",
        "link": "http://dx.doi.org/10.3934/mbe.2023373"
    },
    {
        "id": 27156,
        "title": "Self-Learning Exploration and Mapping for Mobile Robots via Deep Reinforcement Learning",
        "authors": "Fanfei Chen, Shi Bai, Tixiao Shan, Brendan Englot",
        "published": "2019-1-7",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2019-0396"
    },
    {
        "id": 27157,
        "title": "Learning Representations in Model-Free Hierarchical Reinforcement Learning",
        "authors": "Jacob Rafati, David C. Noelle",
        "published": "2019-7-17",
        "citations": 22,
        "abstract": "Common approaches to Reinforcement Learning (RL) are seriously challenged by large-scale applications involving huge state spaces and sparse delayed reward feedback. Hierarchical Reinforcement Learning (HRL) methods attempt to address this scalability issue by learning action selection policies at multiple levels of temporal abstraction. Abstraction can be had by identifying a relatively small set of states that are likely to be useful as subgoals, in concert with the learning of corresponding skill policies to achieve those subgoals. Many approaches to subgoal discovery in HRL depend on the analysis of a model of the environment, but the need to learn such a model introduces its own problems of scale. Once subgoals are identified, skills may be learned through intrinsic motivation, introducing an internal reward signal marking subgoal attainment. We present a novel model-free method for subgoal discovery using incremental unsupervised learning over a small memory of the most recent experiences of the agent. When combined with an intrinsic motivation learning mechanism, this method learns subgoals and skills together, based on experiences in the environment. Thus, we offer an original approach to HRL that does not require the acquisition of a model of the environment, suitable for large-scale applications. We demonstrate the efficiency of our method on a variant of the rooms environment.",
        "link": "http://dx.doi.org/10.1609/aaai.v33i01.330110009"
    },
    {
        "id": 27158,
        "title": "Human-like Autonomous Vehicle Speed Control by Deep Reinforcement Learning with Double Q-Learning",
        "authors": "Yi Zhang, Ping Sun, Yuhan Yin, Lin Lin, Xuesong Wang",
        "published": "2018-6",
        "citations": 69,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2018.8500630"
    },
    {
        "id": 27159,
        "title": "Intra-Agent Transfer Methods",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5_4"
    },
    {
        "id": 27160,
        "title": "Experiment Domains and Applications",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5_6"
    },
    {
        "id": 27161,
        "title": "Optimal recovery of unsecured debt via interpretable reinforcement learning",
        "authors": "Michael Mark, Naveed Chehrazi, Huanxi Liu, Thomas A. Weber",
        "published": "2022-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2022.100280"
    },
    {
        "id": 27162,
        "title": "Bridging Reinforcement Learning and Iterative Learning Control: Autonomous Motion Learning for Unknown, Nonlinear Dynamics",
        "authors": "Michael Meindl, Dustin Lehmann, Thomas Seel",
        "published": "2022-7-12",
        "citations": 4,
        "abstract": "This work addresses the problem of reference tracking in autonomously learning robots with unknown, nonlinear dynamics. Existing solutions require model information or extensive parameter tuning, and have rarely been validated in real-world experiments. We propose a learning control scheme that learns to approximate the unknown dynamics by a Gaussian Process (GP), which is used to optimize and apply a feedforward control input on each trial. Unlike existing approaches, the proposed method neither requires knowledge of the system states and their dynamics nor knowledge of an effective feedback control structure. All algorithm parameters are chosen automatically, i.e. the learning method works plug and play. The proposed method is validated in extensive simulations and real-world experiments. In contrast to most existing work, we study learning dynamics for more than one motion task as well as the robustness of performance across a large range of learning parameters. The method’s plug and play applicability is demonstrated by experiments with a balancing robot, in which the proposed method rapidly learns to track the desired output. Due to its model-agnostic and plug and play properties, the proposed method is expected to have high potential for application to a large class of reference tracking problems in systems with unknown, nonlinear dynamics.",
        "link": "http://dx.doi.org/10.3389/frobt.2022.793512"
    },
    {
        "id": 27163,
        "title": "Development and Validation of Active Roll Control based on Actor-critic Neural Network Reinforcement Learning",
        "authors": "Matthias Bahr, Sebastian Reicherts, Philipp Sieberg, Luca Morss, Dieter Schramm",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007787400360046"
    },
    {
        "id": 27164,
        "title": "Robust Waypoint Guidance of a Hexacopter on Mars using Meta-Reinforcement Learning",
        "authors": "",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2023-2663.vid"
    },
    {
        "id": 27165,
        "title": "Feedback Decision Transformer: Offline Reinforcement Learning With Feedback",
        "authors": "Liad Giladi, Gilad Katz",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdm58522.2023.00120"
    },
    {
        "id": 27166,
        "title": "Comparing Physics Effects through Reinforcement Learning in the ARORA Simulator",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46354/i3m.2021.emss.015"
    },
    {
        "id": 27167,
        "title": "Playing Geister by Estimating Hidden Information with Deep Reinforcement Learning",
        "authors": "Keisuke Tomoda, Koji Hasebe",
        "published": "2021-8-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog52621.2021.9618992"
    },
    {
        "id": 27168,
        "title": "Multimedia Meets Deep Reinforcement Learning",
        "authors": "Shu-Ching Chen",
        "published": "2022-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mmul.2022.3196479"
    },
    {
        "id": 27169,
        "title": "Reinforcement-learning-based Identification of the System for the Purpose of Structural Change Detection",
        "authors": "ZIEMOWIT DWORAKOWSKI",
        "published": "2019-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12783/shm2019/32479"
    },
    {
        "id": 27170,
        "title": "What do Reinforcement Learning Models Measure? Interpreting Model Parameters in Cognition and Neuroscience",
        "authors": "Maria Eckstein, Linda Wilbrecht, Anne Collins",
        "published": "No Date",
        "citations": 1,
        "abstract": "Reinforcement learning (RL) is a concept that has been invaluable to research fields including machine learning, neuroscience, and cognitive science. However, what RL entails partly differs between fields, leading to difficulties when interpreting and translating findings.This paper lays out these differences and zooms in on cognitive (neuro)science, revealing that we often overinterpret RL modeling results, with severe consequences for future research. Specifically, researchers often assume---implicitly---that model parameters \\textit{generalize} between tasks, models, and participant populations, despite overwhelming negative empirical evidence for this assumption. We also often assume that parameters measure specific, unique, and meaningful (neuro)cognitive processes, a concept we call \\textit{interpretability}, for which empirical evidence is also lacking. We conclude that future computational research needs to pay increased attention to these implicit assumptions when using RL models, and suggest an alternative framework that resolves these issues and allows us to unleash the potential of RL in cognitive (neuro)science.",
        "link": "http://dx.doi.org/10.31234/osf.io/e7kwx"
    },
    {
        "id": 27171,
        "title": "Supplemental Material for Cocaine Addiction as a Homeostatic Reinforcement Learning Disorder",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1037/rev0000046.supp"
    },
    {
        "id": 27172,
        "title": "Enhanced Particle Swarm Optimization via Reinforcement Learning",
        "authors": "Di Wu, G. Gary Wang",
        "published": "2020-8-17",
        "citations": 0,
        "abstract": "Abstract\nParticle swarm optimization (PSO) method is a well-known optimization algorithm, which shows good performance in solving different optimization problems. However, PSO usually suffers from slow convergence. In this paper, a reinforcement learning method is used to enhance PSO in convergence by replacing the uniformly distributed random number in the updating function by a random number generated from a well-selected normal distribution. The mean and variance of the normal distribution are estimated from the current state of each individual through a policy net. The historic behavior of the swarm group is learned to update the policy net and guide the selection of parameters of the normal distribution. The proposed algorithm is tested with numerical test functions and the results show that the convergence rate of PSO can be improved with the proposed Reinforcement Learning method (RL-PSO).",
        "link": "http://dx.doi.org/10.1115/detc2020-22519"
    },
    {
        "id": 27173,
        "title": "Adaptive Coding is Optimal in Reinforcement Learning",
        "authors": "Aldo Rustichini, Magdalena Soukupova, Stefano Palminteri",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4320894"
    },
    {
        "id": 27174,
        "title": "Deep Reinforcement Learning Based Automatic Control in Semi-Closed Greenhouse Systems",
        "authors": "Akshay Ajagekar, Fengqi You",
        "published": "2022-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867162"
    },
    {
        "id": 27175,
        "title": "Reinforcement Learning in Non-Markovian Environments",
        "authors": "Siddharth Chandak, Vivek Shripad Borkar, Parth Dodhia",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4293001"
    },
    {
        "id": 27176,
        "title": "Factor Selection with Deep Reinforcement Learning for Financial Forecasting",
        "authors": "Ziwei Wang, Nelson Leung",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3128678"
    },
    {
        "id": 27177,
        "title": "Topological Phase Space Reconstruction For Augmented Real-World Reinforcement Learning",
        "authors": "Amine Mohamed Aboussalah",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3990516"
    },
    {
        "id": 27178,
        "title": "Conclusion and Future Work",
        "authors": "Changsheng Hua",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-33034-7_6"
    },
    {
        "id": 27179,
        "title": "Linear reinforcement learning: Flexible reuse of computation in planning, grid fields, and cognitive control",
        "authors": "Payam Piray, Nathaniel D. Daw",
        "published": "No Date",
        "citations": 12,
        "abstract": "AbstractIt is thought that the brain’s judicious reuse of previous computation underlies our ability to plan flexibly, but also that inappropriate reuse gives rise to inflexibilities like habits and compulsion. Yet we lack a complete, realistic account of either. Building on control engineering, we introduce a new model for decision making in the brain that reuses a temporally abstracted map of future events to enable biologically-realistic, flexible choice at the expense of specific, quantifiable biases. It replaces the classic nonlinear, model-based optimization with a linear approximation that softly maximizes around (and is weakly biased toward) a default policy. This solution exposes connections between seemingly disparate phenomena across behavioral neuroscience, notably flexible replanning with biases and cognitive control. It also gives new insight into how the brain can represent maps of long-distance contingencies stably and componentially, as in entorhinal response fields, and exploit them to guide choice even under changing goals.",
        "link": "http://dx.doi.org/10.1101/856849"
    },
    {
        "id": 27180,
        "title": "Reinforcement Learning for Multi-Robot System: A Review",
        "authors": "Xudong Yang",
        "published": "2021-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cds52072.2021.00043"
    },
    {
        "id": 27181,
        "title": "Target Tracking Method based on Reinforcement Learning",
        "authors": "Zhixing Yang",
        "published": "2022-1-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccece54139.2022.9712808"
    },
    {
        "id": 27182,
        "title": "Attitude Control for Quadcopters using Reinforcement Learning",
        "authors": "Shun Nakasone, Renato Galluzzi, Rogelio Bustamante-Bello",
        "published": "2022-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isem55847.2022.9976737"
    },
    {
        "id": 27183,
        "title": "Continuous Control of Complex Chemical Reaction Network with Reinforcement Learning",
        "authors": "Khalid Alhazmi, S. Mani Sarathy",
        "published": "2020-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc51009.2020.9143688"
    },
    {
        "id": 27184,
        "title": "Fast Reinforcement Learning with Incremental Gaussian Mixture Models",
        "authors": "Rafael Pinto",
        "published": "2021-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533632"
    },
    {
        "id": 27185,
        "title": "Heterogeneous Retirement Savings Strategy Selection with Reinforcement Learning",
        "authors": "Fatih Ozhamaratli, Paolo Barucca",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "Saving and investment behaviour is crucial for all individuals to guarantee their welfare during work-life and retirement. We introduce a deep reinforcement learning model in which agents learn optimal portfolio allocation and saving strategies suitable for their heterogeneous profiles. The environment is calibrated with occupation- and age-dependent income dynamics. The research focuses on heterogeneous income trajectories dependent on agents’ profiles and incorporates the parameterisation of agents’ behaviours. The model provides a new flexible methodology to estimate lifetime consumption and investment choices for individuals with heterogeneous profiles.",
        "link": "http://dx.doi.org/10.3390/e25070977"
    },
    {
        "id": 27186,
        "title": "Reinforcement Learning-based Control System of a Hybrid Power Supply",
        "authors": "Francisca Daniel, Arnold Rix",
        "published": "2020-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/saupec/robmech/prasa48453.2020.9041138"
    },
    {
        "id": 27187,
        "title": "Optimal Order Routing with Reinforcement Learning",
        "authors": "Lars ter Braak, Martin van der Schans",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4611420"
    },
    {
        "id": 27188,
        "title": "Assessing the Security of Inter-App Communications in Android through Reinforcement Learning",
        "authors": "Andrea Romdhana, Alessio Merlo, Mariano Ceccato, Paolo Tonella",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>A central aspect of the Android platform is Inter-Component Communication (ICC), which enables the reuse of functionality across apps and components via message passing. While a powerful feature, ICC still constitutes a serious attack surface. This paper addresses the issue of generating exploits for a subset of Android ICC vulnerabilities (i.e., IDOS, XAS, and FI) through static analysis, Deep Reinforcement Learning-based dynamic analysis and software instrumentation. Our approach, called RONIN, achieves better results than state-of-the-art and baseline tools, in the number of exploited vulnerabilities.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21310770"
    },
    {
        "id": 27189,
        "title": "Baselines for Reinforcement Learning in Text Games",
        "authors": "Mikulas Zelinka",
        "published": "2018-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai.2018.00058"
    },
    {
        "id": 27190,
        "title": "Dynamic Programming",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_3"
    },
    {
        "id": 27191,
        "title": "Adaptive Disassembly Sequence Planning for VR Maintenance Training via Deep Reinforcement Learning",
        "authors": "Haoyang Mao, Zhenyu Liu, Chan Qiu",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nGiven the great inconvenience caused by the randomness of the fault to the maintenance work, it is necessary to perform on-site and efficient disassembly planning for the faulty parts and present them in combination with virtual reality (VR) technology to achieve rapid repair. As a promising method in solving dynamistic and stochastic problems, deep reinforcement learning (DRL) is adopted in this paper for the solution of adaptive disassembly sequence planning (DSP) in the VR maintenance training system, in which sequences can be generated dynamically based on user inputs. Disassembly Petri net is established to describe and model the disassembly process, and then the DSP problem is defined as a Markov decision process (MDP) that can be solved by the deep Q-network (DQN). For handling the temporal credit assignment with sparse rewards, the long-term return in DQN is replaced with the fitness function of the genetic algorithm (GA). Meanwhile, the update method of gradient descent in DQN is adopted to speed up the iteration of the population in GA. A case study has been conducted to prove that the proposed method can provide better solutions for DSP problems in terms of VR maintenance training.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-497853/v1"
    },
    {
        "id": 27192,
        "title": "Consistent Epistemic Planning for Multiagent Deep Reinforcement Learning",
        "authors": "Peiliang Wu, ShiCheng Luo, LiQiang Tian, BingYi Mao, enbai Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nMulti-agent cooperation needs to reason about beliefs in the partially observable environment without communication, but the traditional Multi-agent Deep Reinforcement Learning (MADRL) algorithm struggles to handle the uncertainty of agents. Multi-agent Epistemic planning (MEP) tries to let the agent find a best plan to complete the cooperation task, so as to more effectively solve the uncertainty. However, inconsistent planning arises if the MADRL only adds MEP. We propose a MADRL-based policy network architecture called SMM-MEPP: Shared Mental Model - Multi-agent Epistemic Planning Policy. Firstly, Multi-agent Epistemic Planning and MADRL are investigated to build the \"Perception-Planning-Action\" multi-agent epistemic planning framework. Then, mental model in psychology is introduced and descript as a neural network. Thirdly, parameter sharing mechanism is utilized to achieve the shared mental model and maintain the consistency of epistemic planning. Finally, we apply the SMM-MEPP architecture to three advanced MADRL algorithms (i.e., MAAC, MADDPG and MAPPO) and conduct comparative experiments in multi-agent cooperation tasks. Experiments show that the proposed method can bring consistent planning for multiple agents, and improves convergence speed or training effect in partially observable environment without communication.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2576428/v2"
    },
    {
        "id": 27193,
        "title": "MAIDRL: Semi-centralized Multi-Agent Reinforcement Learning using Agent Influence",
        "authors": "Anthony Harris, Siming Liu",
        "published": "2021-8-17",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog52621.2021.9619002"
    },
    {
        "id": 27194,
        "title": "Reinforcement Learning Based Time-Adaptive Power Transformer Differential Protection",
        "authors": "xiaopeng wang, Anyang He, Zongbo Li, Zaibin Jiao, Na Lu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4685791"
    },
    {
        "id": 27195,
        "title": "Molecular complex detection in protein interaction networks through reinforcement learning",
        "authors": "Meghana V. Palukuri, Ridhi S. Patil, Edward M. Marcotte",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractMany, if not most, proteins assemble into higher-order complexes to perform their biological functions. Such protein-protein interactions (PPI) are often experimentally measured for pairs of proteins and summarized in a weighted PPI network, to which community detection algorithms can be applied to define the various higher-order protein complexes. Current methods, which include both unsupervised and supervised approaches, often assume that protein complexes manifest only as dense subgraphs, and in the case of supervised approaches, focus only on learning which subgraphs correspond to complexes, not how to find them in a network, a task that is currently solved using heuristics. However, learning to walk trajectories on a network with the goal of finding protein complexes lends itself naturally to a reinforcement learning (RL) approach, a strategy that has not been extensively explored for community detection. Here, we evaluated the use of a reinforcement learning pipeline for community detection in weighted protein-protein interaction networks to detect new protein complexes. Using known complexes, the algorithm is trained to calculate the value of different possible subgraph densities in the process of walking on the network to find a protein complex. Then, a distributed prediction algorithm scales the RL pipeline to search for protein complexes on large PPI networks. The reinforcement learning pipeline applied to a human PPI network consisting of 8k proteins and 60k PPI results in 1,157 protein complexes and shows competitive accuracy with improved speed when compared to previous algorithms. We highlight protein complexes harboring minimally characterized proteins including C4orf19, C18orf21, and KIAA1522, suggest TMC04 to be a putative additional subunit of the KICSTOR complex, and confirm the participation of C15orf41 in a higher-order complex with CDAN1, ASF1A, and HIRA by 3D structural modeling. Reinforcement learning offers several distinct advantages for community detection, including scalability and knowledge of the walk trajectories defining those communities.",
        "link": "http://dx.doi.org/10.1101/2022.06.20.496772"
    },
    {
        "id": 27196,
        "title": "Simulation-Based Deep Reinforcement Learning For Modular Production Systems",
        "authors": "Niclas Feldkamp, Soeren Bergmann, Steffen Strassburger",
        "published": "2020-12-14",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc48552.2020.9384089"
    },
    {
        "id": 27197,
        "title": "Integrated Clinical Environment Security Analysis Using Reinforcement Learning",
        "authors": "Mariam Ibrahim, Ruba Elhafiz",
        "published": "2022-6-13",
        "citations": 6,
        "abstract": "Many communication standards have been proposed recently and more are being developed as a vision for dynamically composable and interoperable medical equipment. However, few have security systems that are sufficiently extensive or flexible to meet current and future safety requirements. This paper aims to analyze the cybersecurity of the Integrated Clinical Environment (ICE) through the investigation of its attack graph and the application of artificial intelligence techniques that can efficiently demonstrate the subsystems’ vulnerabilities. Attack graphs are widely used for assessing network security. On the other hand, they are typically too huge and sophisticated for security administrators to comprehend and evaluate. Therefore, this paper presents a Q-learning-based attack graph analysis approach in which an attack graph that is generated for the Integrated Clinical Environment system resembles the environment, and the agent is assumed to be the attacker. Q-learning can aid in determining the best route that the attacker can take in order to damage the system as much as possible with the least number of actions. Numeric values will be assigned to the attack graph to better determine the most vulnerable part of the system and suggest this analysis to be further utilized for bigger graphs.",
        "link": "http://dx.doi.org/10.3390/bioengineering9060253"
    },
    {
        "id": 27198,
        "title": "Deep Reinforcement Learning Approach for Service Function Chain Embedding Oriented to Sdn/Nfv-Enabled Networks",
        "authors": "Yicen Liu, junning Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4542888"
    },
    {
        "id": 27199,
        "title": "Autonomous Command and Control for Earth-Observing Satellites using Deep Reinforcement Learning",
        "authors": "Andrew Harris, Kedar Naik",
        "published": "2023-3-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aero55745.2023.10115916"
    },
    {
        "id": 27200,
        "title": "Optimizing the agent decisions for a Cloud actuator using Deep reinforcement learning",
        "authors": "Lakshmi Sankaran, Saleema JS, Basem Suleiman",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWith the increasing use of deep reinforcement learning (DRL) techniques to build intelligent systems, the application of it to real-world problems is rampant. Resource allocation in a cloud environment that need dynamic and auto-scaling features is evolving. The agent-based decisions that are offered by DRL are in use by software robotics. Auto-scaling of resources in cloud applications introduces intelligence to agents thus built by these DRL techniques. Markov decision process as a tool minimizes the target rewards to agents such that auto-scaling of applications is performed by agent decisions. Analysis of optimizing the convergence errors that are measured while the agent performs in an online environment is the challenge. Speedy Q-learning (SQL), Generalized SQL(GSQL) algorithm variants relax the parameter values of convergence with a model-free space. The authors applied heuristic values for one such relaxation parameter in our experiments. The study is an extension of works that introduced GSQL-w, where w is the convergence parameter. The authors designed a new GSQL-wh algorithm that heuristically fixes a value for w optimally in cases with over-utilization of resources. This is presented as a novel solution in this study for cloud resource workloads.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-4000624/v1"
    },
    {
        "id": 27201,
        "title": "Distributional Reinforcement Learning with Ensembles",
        "authors": "Björn Lindenberg, Jonas Nordqvist, Karl-Olof Lindahl",
        "published": "2020-5-7",
        "citations": 0,
        "abstract": "It is well known that ensemble methods often provide enhanced performance in reinforcement learning. In this paper, we explore this concept further by using group-aided training within the distributional reinforcement learning paradigm. Specifically, we propose an extension to categorical reinforcement learning, where distributional learning targets are implicitly based on the total information gathered by an ensemble. We empirically show that this may lead to much more robust initial learning, a stronger individual performance level, and good efficiency on a per-sample basis.",
        "link": "http://dx.doi.org/10.3390/a13050118"
    },
    {
        "id": 27202,
        "title": "Robust Reinforcement Learning via Genetic Curriculum",
        "authors": "Yeeho Song, Jeff Schneider",
        "published": "2022-5-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9812420"
    },
    {
        "id": 27203,
        "title": "Spectra to Structure: Deep Reinforcement Learning for Molecular Inverse Problem",
        "authors": "Bhuvanesh Sridharan, Sarvesh Mehta, Yashaswi Pathak, U. Deva Priyakumar",
        "published": "No Date",
        "citations": 1,
        "abstract": "Spectroscopy is the study of how matter interacts with electromagnetic radiations of specific frequencies that has led to several monumental discoveries in science. The spectra of any particular molecule is highly information-rich, yet the inverse relation from the spectra to the molecular structure is still an unsolved problem. Nuclear Magnetic Resonance (NMR) spectroscopy is one such critical tool in the tool-set for scientists to characterise any chemical sample. In this work, a novel framework is proposed that attempts to solve this inverse problem by navigating the chemical space to find the correct structure that resulted in the target spectra. The proposed framework uses a combination of online Monte- Carlo-Tree-Search (MCTS) and a set of offline trained Graph Convolution Networks to build a molecule iteratively from scratch. Our method is able to predict the correct structure of the molecule ∼80% of the time in its top 3 guesses. We believe that the proposed framework is a significant step in solving the inverse design problem of NMR spectra to molecule.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2021-4hc7k"
    },
    {
        "id": 27204,
        "title": "Vision-driven Deep Reinforcement Learning for Electronic Components Robotic Insertion Tasks",
        "authors": "Grzegorz Marcin Bartyzel, Wojciech Półchłopek, Dominik Rzepka",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nOne of the challenges of robotics in the modern manufacturing industry is assembly task. The manufacturing industry requires various insertion tasks, from peg-in-hole tasks to electronic parts assembly. Nowadays, robotic solutions for this problem often use the conventional methods. In those methods, the industrial robot is controlled by the hybrid force-position control and performs preprogrammed trajectories, such as a spiral path. However, electronic parts require more sophisticated techniques due to their complex geometry and susceptibility to damage. We propose a vision-driven method based on reinforcement learning (RL) for assembling electronic parts. In our approach, the input image for the RL agent is acquired from two cameras mounted to the robot's end-effector. In this work, we also analyze the influence of the observation modalities on the RL's agent performance metrics, such as insertion success rate and average assembly time. Results show that visual information acquired from a double-camera vision system significantly improves the RL method's robustness on the position disturbance in insertion tasks. The proposed method in this work outperforms conventional methods, such as random search, spiral search, and straight-down insertion in terms of success rate and robustness for the robot's initial position disturbances. Moreover, our approach is more robust for disturbance than a method that uses an external camera or a single camera mounted to the robot's end-effector for image acquisition.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2233330/v1"
    },
    {
        "id": 27205,
        "title": "Principles of RL Problems",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_2"
    },
    {
        "id": 27206,
        "title": "5G Scheduling using Reinforcement Learning",
        "authors": "D. Zavyalova, V. Drozdova",
        "published": "2020-10-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fareastcon50210.2020.9271421"
    },
    {
        "id": 27207,
        "title": "Robotic Obstacle Avoidance: A Virtual Modeling And Reinforcement Learning",
        "authors": "Ming-Fei Chen, Han-Hsien Tsai, Wen-Tse Hsiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis study developed a robotic arm self-learning system based on virtual modeling and reinforcement learning. Using the model of a robotic arm, information concerning obstacles in the environment, initial coordinates of the robotic arm, and the target position, this system automatically generated a set of rotational angles to enable a robotic arm to be positioned such that it can avoid all obstacles and reach a target. The developed program was divided into three parts. The first part involves robotic arm simulation and collision detection; specifically, images of a six-axis robotic arm and obstacles were input to the Visualization ToolKit library to visualize the movements and surrounding environment of the robotic arm. Subsequently, an oriented bounding box algorithm was used to determine whether collisions had occurred. The second part concerned machine-learning–based route planning. The TensorFlow was used to establish a deep deterministic policy gradient model, and reinforcement learning was employed for the response to environmental variables. Different reward functions were designed for tests and discussions, and the program’s practicality was verified through actual machine operations. Finally, the application of reinforcement learning in route planning for a robotic arm was proved feasible by the experiment. Such an application facilitated automatic route planning and achieved an error of less than 10 mm from the target position.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-601053/v1"
    },
    {
        "id": 27208,
        "title": "Research on Agent Control Algorithm Based on Reinforcement Learning",
        "authors": "Zhongqiu Zhang",
        "published": "2020-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccea50009.2020.00127"
    },
    {
        "id": 27209,
        "title": "Deep Reinforcement Learning for Contagion Control",
        "authors": "Diego R. Benalcazar, Chinwendu Enyioha",
        "published": "2021-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccta48906.2021.9659238"
    },
    {
        "id": 27210,
        "title": "Behavior Constraining in Weight Space for Offline Reinforcement Learning",
        "authors": "Phillip Swazinna, Steffen Udluft, Daniel Hein, Thomas Runkler",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2021.es2021-83"
    },
    {
        "id": 27211,
        "title": "Reinforcement Learning and Deep Stochastic Optimal Control for Final Quadratic Hedging",
        "authors": "Bernhard Hientzsch",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4645455"
    },
    {
        "id": 27212,
        "title": "Quality of service based radar resource management using deep reinforcement learning",
        "authors": "Sebastian Durst, Stefan Bruggenwirth",
        "published": "2021-5-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2147009.2021.9455234"
    },
    {
        "id": 27213,
        "title": "Risk-Based Adaptive Stock Trading System Using Reinforcement Learning",
        "authors": "Chai Quek, Qi CAO",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4237367"
    },
    {
        "id": 27214,
        "title": "Deep Residual Attention Reinforcement Learning",
        "authors": "Hanhua Zhu, Tomoyuki Kaneko",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/taai48200.2019.8959896"
    },
    {
        "id": 27215,
        "title": "How working memory and reinforcement learning are intertwined: a cognitive, neural, and computational perspective",
        "authors": "Aspen H. Yoo, Anne Collins",
        "published": "No Date",
        "citations": 0,
        "abstract": "Reinforcement learning and working memory are two core processes of human cognition, and are often considered cognitively, neuroscientifically, and algorithmically distinct. Here, we show that the brain networks that support them actually overlap significantly, and that they are less distinct cognitive processes than often assumed. We review literature demonstrating the benefits of considering each process to explain properties of the other, and highlight recent work investigating their more complex interactions. We discuss how future research in both computational and cognitive sciences can benefit from one another, suggesting that a key missing piece for artificial agents to learn to behave with more human-like efficiency is taking working memory’s role in learning seriously. This review highlights the risks of neglecting the interplay between different processes when studying human behavior (in particular when considering individual differences). We emphasize the importance of investigating these dynamics in order to build a comprehensive understanding of human cognition.",
        "link": "http://dx.doi.org/10.31234/osf.io/ebtn6"
    },
    {
        "id": 27216,
        "title": "Federated Multi Agent Deep Reinforcement Learning for Optimized Design of Future Wireless Networks",
        "authors": "Hugo De Oliveira, Megumi Kaneko, Lila Boukhatem",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Federated Multi-Agent Deep Reinforcement Learning (F-MADRL) has been recently attracting increasing research interests, as it offers efficient solutions towards meeting the extreme requirements of Beyond 5G (B5G) and 6G applications. By contrast to centralized Deep Reinforcement Learning (DRL) and Multi-Agent DRL (MADRL), F-MADRL enables edge devices to cooperate without sharing their private data, while reducing the delays and signaling costs inherent to centralized approaches. In this article, we explore the new opportunities brought by F-MADRL by conducting a holistic survey on its related recent works. Firstly, we categorize state-of-the-art F-MADRL approaches, based on some distinctive features such as incurred signaling overhead, privacy level, and aggregation frequency. To better illustrate the behavior and advantages of F-MADRL, it is numerically compared to its centralized and distributed DRL counterparts, through a Sub-6GHz/mmWave band association optimization problem for IoT short packet communications. Finally, we identify and discuss the open research directions and challenges, in order to spur further interests in this promising area.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24007503.v1"
    },
    {
        "id": 27217,
        "title": "Recovering From Cyber-Manufacturing Attacks by Reinforcement Learning",
        "authors": "Romesh Prasad, Matthew K. Swanson, Young Moon",
        "published": "2022-10-30",
        "citations": 0,
        "abstract": "Abstract\nA Cyber-Manufacturing systems (CMS) is an integration of informational and operational entities that are synchronized with manufacturing processes to increase productivity. However, this integration enlarges the scope for cyber attackers to intrude manufacturing processes, which are called cyber-manufacturing attacks. They can have significant impacts on physical operations within a CMS, such as shutting down plants, production interruption, premature failure of products, and fatal accidents. Although research activities in this emerging problem have been increased recently, existing research has been limited to detection and prevention solutions. However, these strategies cannot ensure a continuous function of an attacked CMS. To ensure continuous functioning of a CMS, a robust recovery strategy must be developed and employed. Current research in recovery has been limited to feedback controllers with an assumption of a complete knowledge of a system model. To overcome this limitation, a recovery agent augmented by reinforcement learning was developed. This is to utilize the ability of reinforcement learning to handle sequential decisions and to proceed even without a complete knowledge of a system model. A virtual environment for recovery agents has been developed to assist efforts needed to obtain sample data, experiment various scenarios, and explore with reinforcement learning. Two cyber-manufacturing attack scenarios have been developed: (i) spoofing a stepper motor controlling additive manufacturing processes, (ii) disrupting the sequence of the pick and place robot. The recovery agent takes random actions by exploring its environment and receives rewards from the actions. After many iterations, it learns proper actions to take.",
        "link": "http://dx.doi.org/10.1115/imece2022-93982"
    },
    {
        "id": 27218,
        "title": "A Population-Based Approach for Multi-Agent Interpretable Reinforcement Learning",
        "authors": "Marco Crespi, Andrea Ferigo, Leonardo  Lucio Custode, Giovanni Iacca",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4467882"
    },
    {
        "id": 27219,
        "title": "The role of subgoals in hierarchical reinforcement learning",
        "authors": "Milena Rmus, Maria Eckstein, Anne Collins",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1587-0"
    },
    {
        "id": 27220,
        "title": "Reinforcement Learning based Intelligent Semiconductor Manufacturing Applied to Laser Annealing",
        "authors": "Tejender Rawat, Chang-Yuan Chung, Shih-Wei Chen, Albert Lin",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2449"
    },
    {
        "id": 27221,
        "title": "Reinforcement Learning for Financial Index Tracking",
        "authors": "Xianhua Peng, Chenyin Gong, Xue Dong He",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4532072"
    },
    {
        "id": 27222,
        "title": "Network Intrusion Detection System using Reinforcement learning",
        "authors": "Malika Malik, Kamaljit Singh Saini",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incet57972.2023.10170630"
    },
    {
        "id": 27223,
        "title": "Quality-Diversity Based Semi-Autonomous Teleoperation Using Reinforcement Learning",
        "authors": "Sangbeom Park, Taerim Yoon, Joonhyung Lee, Sunghyun Park, Sungjoon Choi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4706203"
    },
    {
        "id": 27224,
        "title": "Diversity Oriented Deep Reinforcement Learning for Targeted Molecule Generation",
        "authors": "Tiago Pereira, Maryam Abbasi, Bernardete Ribeiro, Joel P. Arrais",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this work, we explore the potential of deep learning to streamline the process of identifying new potential drugs through the computational generation of molecules with interesting biological properties. Two deep neural networks compose our targeted generation framework: the Generator, which is trained to learn the building rules of valid molecules employing SMILES strings notation, and the Predictor which evaluates the newly generated compounds by predicting their aﬃnity for the desired target. Then, the Generator is optimized through Reinforcement Learning to produce molecules with bespoken properties. The innovation of this approach is the exploratory strategy applied during the reinforcement training process that seeks to add novelty to the generated compounds. This training strategy employs two Generators interchangeably to sample new SMILES: the initially trained model that will remain ﬁxed and a copy of the previous one that will be updated during the training to uncover the most promising molecules. The evolution of the reward assigned by the Predictor determines how often each one is employed to select the next token of the molecule. This strategy establishes a compromise between the need to acquire more information about the chemical space and the need to sample new molecules, with the experience gained so far. To demonstrate the eﬀectiveness of the method, the Generator is trained to design molecules with high inhibitory power for the adenosine A2A and κ opioid receptors. The results reveal that the model can eﬀectively modify the biological aﬃnity of the newly generated molecules towards the craved direction. More importantly, it was possible to ﬁnd auspicious sets of unique and diverse molecules, which was the main purpose of the newly implemented strategy.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-110570/v1"
    },
    {
        "id": 27225,
        "title": "Inertia-Constrained Reinforcement Learning to Enhance Human Motor Control Modeling",
        "authors": "Soroush Korivand, Nader Jalili, Jiaqi Gong",
        "published": "No Date",
        "citations": 0,
        "abstract": "Locomotor impairment is a high-prevalent and significant source of disability and significantly impacts a large population’s quality of life. Despite decades of research in human locomotion, the challenges of simulating human movement to study the features of musculoskeletal drivers and clinical conditions remain. Most recent efforts in utilizing reinforcement learning (RL) techniques are promising to simulate human locomotion and reveal musculoskeletal drives. However, these simulations often failed to mimic natural human locomotion because most reinforcement strategies have yet to consider any reference data regarding human movement. To address these challenges, in this study, we designed a reward function based on the trajectory optimization rewards (TOR), and bio-inspired rewards, which includes the rewards obtained from reference motion data captured by a single Intertial Moment Unit (IMU) sensor. The sensor was equipped on the participants’ pelvis to capture reference motion data. Also, we adapted the reward function by leveraging previous research in walking simulation for TOR. The experimental results showed that the simulated agents with the modified reward function performed better in mimicking the collected IMU data from participants, which means the simulated human locomotion was more realistic. Also, as this bio-inspired defined cost, IMU data enhanced the agent’s capacity to converge during the training process. As a result, the models’ convergence is faster than those developed without reference motion data. Consequently, human locomotion can be simulated more quicker and in a broader range of environments with a better simulation performance.",
        "link": "http://dx.doi.org/10.20944/preprints202212.0167.v1"
    },
    {
        "id": 27226,
        "title": "Experimental Evaluation of Deep Reinforcement Learning Algorithms",
        "authors": "Nikola Mrzljak, Tomislav Hrkać",
        "published": "2019-2-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.20532/ccwv.2018.0003"
    },
    {
        "id": 27227,
        "title": "Deep Reinforcement Learning for Energy Management in a Microgrid with Flexible Demand",
        "authors": "Taha abd el halim Nakabi, Pekka Toivanen",
        "published": "No Date",
        "citations": 2,
        "abstract": "In this paper, we study the performance of various deep reinforcement learning algorithms to enhance the energy management system of a microgrid. We propose a novel microgrid model that consists of a wind turbine generator, an energy storage system, a set of thermostatically controlled loads, a set of price-responsive loads, and a connection to the main grid. The proposed energy management system is designed to coordinate among the different flexible sources by defining the priority resources, direct demand control signals, and electricity prices. Seven deep reinforcement learning algorithms were implemented and are empirically compared in this paper. The numerical results show that the deep reinforcement learning algorithms differ widely in their ability to converge to optimal policies. By adding an experience replay and a semi-deterministic training phase to the well-known asynchronous advantage actor-critic algorithm, we achieved the highest model performance as well as convergence to near-optimal policies.",
        "link": "http://dx.doi.org/10.20944/preprints202010.0156.v1"
    },
    {
        "id": 27228,
        "title": "Reinforcement Learning Based Pricing for Demand Response",
        "authors": "Amir Ghasemkhani, Lei Yang",
        "published": "2018-5",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccw.2018.8403783"
    },
    {
        "id": 27229,
        "title": "Can Deep Reinforcement Learning Solve the Portfolio Allocation Problem? (PhD Manuscript)",
        "authors": "Eric Benhamou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4599800"
    },
    {
        "id": 27230,
        "title": "Leveraging Optimal Transport for Enhanced Offline Reinforcement Learning in Surgical Robotic Environments",
        "authors": "Maryam Zare, Parham Mohsenzadeh Kebria, Abbas Khosravi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4627353"
    },
    {
        "id": 27231,
        "title": "Reinforcement Learning with Deep Deterministic Policy Gradient",
        "authors": "Haining Tan",
        "published": "2021-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/caibda53561.2021.00025"
    },
    {
        "id": 27232,
        "title": "Model-based reinforcement learning for service mesh fault resiliency in a web application-level",
        "authors": "Fanfei Meng, Lalita Jagadeesa, Marina Thottan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Microservice-based architectures enable different aspects of web applications to be created and updated independently, even after deployment. Associated technologies such as service mesh provide application-level fault resilience through attribute configurations that govern the behavior of request - response service -- and the interactions among them -- in the presence of failures. While this provides tremendous flexibility, the configured values of these attributes -- and the relationships among them -- can significantly affect the performance and fault resilience of the overall application. Furthermore, it is impossible to determine the best and worst combinations of attribute values with respect to fault resiliency via testing, due to the complexities of the underlying distributed system and the many possible attribute value combinations. In this paper, we present a model-based reinforcement learning workflow towards service mesh fault resiliency. Our approach enables the prediction of the most significant fault resilience behaviors at a web application-level, scratching from single service to aggregated multi-service management with efficient agent collaborations.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24582396.v1"
    },
    {
        "id": 27233,
        "title": "Scalable Multi-Agent Reinforcement Learning-Based Distributed Channel Access",
        "authors": "Zhenyu Chen, Xinghua Sun",
        "published": "2023-5-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278960"
    },
    {
        "id": 27234,
        "title": "Track 5: Inspection Planning for Transmission Line Systems\n  Exposed to Hurricanes using Reinforcement Learning",
        "authors": "Ashkan B., Nariman L., Abdollah Shafieezadeh",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/m.6321e2e2f30377bc3baf6628"
    },
    {
        "id": 27235,
        "title": "Abstraction for Deep Reinforcement Learning",
        "authors": "Murray Shanahan, Melanie Mitchell",
        "published": "2022-7",
        "citations": 5,
        "abstract": "We characterise the problem of abstraction in the context of deep reinforcement learning. Various well established approaches to analogical reasoning and associative memory might be brought to bear on this issue, but they present difficulties because of the need for end-to-end differentiability. We review developments in AI and machine learning that could facilitate their adoption.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/780"
    },
    {
        "id": 27236,
        "title": "Author response for \"The value–complexity trade-off for reinforcement learning based brain–computer interfaces\"",
        "authors": " Hadar Levi-Aharoni,  Naftali Tishby",
        "published": "2020-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1741-2552/abc8d8/v2/response1"
    },
    {
        "id": 27237,
        "title": "Testosterone and estradiol affect adolescent reinforcement learning",
        "authors": "Sina Kohne, Esther K. Diekhof",
        "published": "2022-2-3",
        "citations": 1,
        "abstract": "During adolescence, gonadal hormones influence brain maturation and behavior. The impact of 17β-estradiol and testosterone on reinforcement learning was previously investigated in adults, but studies with adolescents are rare. We tested 89 German male and female adolescents (mean age ± sd = 14.7 ± 1.9 years) to determine the extent 17β-estradiol and testosterone influenced reinforcement learning capacity in a response time adjustment task. Our data showed, that 17β-estradiol correlated with an enhanced ability to speed up responses for reward in both sexes, while the ability to wait for higher reward correlated with testosterone primary in males. This suggests that individual differences in reinforcement learning may be associated with variations in these hormones during adolescence, which may shift the balance between a more reward- and an avoidance-oriented learning style.",
        "link": "http://dx.doi.org/10.7717/peerj.12653"
    },
    {
        "id": 27238,
        "title": "Decision letter: Humans perseverate on punishment avoidance goals in multigoal reinforcement learning",
        "authors": "Claire M Gillan",
        "published": "2021-11-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.74402.sa1"
    },
    {
        "id": 27239,
        "title": "Decision letter for \"Performance enhancement of the artificial neural network–based reinforcement learning for wind turbine yaw control\"",
        "authors": "",
        "published": "2019-8-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2451/v1/decision1"
    },
    {
        "id": 27240,
        "title": "Decision letter for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v3/decision1"
    },
    {
        "id": 27241,
        "title": "Intelligent demand response resource trading using deep reinforcement learning",
        "authors": "",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2020.05540"
    },
    {
        "id": 27242,
        "title": "Deep Reinforcement Learning of the Model Fusion with Double Q-learning",
        "authors": "KANG WANG, WEI ZHANG, XU HE, SHENG GAO",
        "published": "2017-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12783/dtcse/aiea2017/14930"
    },
    {
        "id": 27243,
        "title": "Learning to play Football using Distributional Reinforcement Learning and Depthwise separable convolution feature extraction",
        "authors": "Aniruddha Datta, Swapnamoy Bhowmick, Kunal Kulkarni",
        "published": "2021-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icacc-202152719.2021.9708400"
    },
    {
        "id": 27244,
        "title": "Learning separate objects in clutter with deep reinforcement learning",
        "authors": "Jiangming Li, Xinyan Li, Hao Li",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2680487"
    },
    {
        "id": 27245,
        "title": "Vision Based Leader-Follower Control of Wheeled Mobile Robots using Reinforcement Learning and Deep Learning",
        "authors": "Kayleb Garmon, Ying Wang",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isssr58837.2023.00071"
    },
    {
        "id": 27246,
        "title": "Multiagent Reinforcement Learning With Learning Automata for Microgrid Energy Management and Decision Optimization",
        "authors": "Xiaohan Fang, Jinkuan Wang, Chunhui Yin, Yinghua Han, Qiang Zhao",
        "published": "2020-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc49329.2020.9164742"
    },
    {
        "id": 27247,
        "title": "Resource allocation of English intelligent learning system based on reinforcement learning",
        "authors": "Jin Jingbo",
        "published": "2021-4-12",
        "citations": 1,
        "abstract": "The communication energy efficiency of the English intelligent learning system is affected by many factors. In order to improve the system operation efficiency, it is necessary to carry out analysis from the perspective of the mobile edge server and the system structure. However, there are still many research gaps and deficiencies. Based on a dynamic and heterogeneous English intelligent learning system, this paper designs an adaptive offloading decision algorithm ADCO and an online spring slide task scheduling algorithm SSLS. Moreover, this paper has proposed corresponding solutions to the computational offloading problem and online scheduling problem in a dynamic environment and compared and analyzed the performance of this research algorithm through simulation experiments. The research results show that the method proposed in this paper has certain effects.",
        "link": "http://dx.doi.org/10.3233/jifs-189516"
    },
    {
        "id": 27248,
        "title": "Infrared Image Captioning Based on Unsupervised Learning and Reinforcement Learning",
        "authors": "Chenjun Gao, Ganghui Bian, Yanzhi Dong, Xiaohu Yuan, Huaping Liu",
        "published": "2022-12-16",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icarce55724.2022.10046598"
    },
    {
        "id": 27249,
        "title": "Application of Reinforcement Learning and Deep Learning in Multiple-Input and Multiple-Output (MIMO) Systems",
        "authors": "Muddasar Naeem, Giuseppe De Pietro, Antonio Coronato",
        "published": "2021-12-31",
        "citations": 22,
        "abstract": "The current wireless communication infrastructure has to face exponential development in mobile traffic size, which demands high data rate, reliability, and low latency. MIMO systems and their variants (i.e., Multi-User MIMO and Massive MIMO) are the most promising 5G wireless communication systems technology due to their high system throughput and data rate. However, the most significant challenges in MIMO communication are substantial problems in exploiting the multiple-antenna and computational complexity. The recent success of RL and DL introduces novel and powerful tools that mitigate issues in MIMO communication systems. This article focuses on RL and DL techniques for MIMO systems by presenting a comprehensive review on the integration between the two areas. We first briefly provide the necessary background to RL, DL, and MIMO. Second, potential RL and DL applications for different MIMO issues, such as detection, classification, and compression; channel estimation; positioning, sensing, and localization; CSI acquisition and feedback, security, and robustness; mmWave communication and resource allocation, are presented.",
        "link": "http://dx.doi.org/10.3390/s22010309"
    },
    {
        "id": 27250,
        "title": "Differential reinforcement of low rate responding in social skills training",
        "authors": "Dana M. Gadaire, Genevieve Marshall, Elanor Brissett",
        "published": "2017-11",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.lmot.2017.08.005"
    },
    {
        "id": 27251,
        "title": "Learning From Big Data: A Survey and Evaluation of Approximation Technologies for Large-Scale Reinforcement Learning",
        "authors": "Cheng Wu, Yiming Wang",
        "published": "2017-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cit.2017.11"
    },
    {
        "id": 27252,
        "title": "Active learning-based hyperspectral image classification: a reinforcement learning approach",
        "authors": "Usha Patel, Vibha Patel",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11227-023-05568-7"
    },
    {
        "id": 27253,
        "title": "Deep Reinforcement Learning-Based Computation Offloading and Resource Allocation in IoT",
        "authors": "Yongli Yang, Qi Liu, Qinghua Zhu, Yong Liu, Wei Han",
        "published": "2022-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaml57167.2022.00058"
    },
    {
        "id": 27254,
        "title": "Improving reinforcement learning algorithms: Towards optimal learning rate policies",
        "authors": "Othmane Mounjid, Charles‐Albert Lehalle",
        "published": "2024-4",
        "citations": 0,
        "abstract": "AbstractThis paper shows how to use results of statistical learning theory and stochastic algorithms to have a better understanding of the convergence of Reinforcement Learning (RL) once it is formulated as a fixed point problem. This can be used to propose improvement of RL learning rates. First, our analysis shows that the classical asymptotic convergence rate  is pessimistic and can be replaced by  with , and  the number of iterations. Second, we propose a dynamic optimal policy for the choice of the learning rate used in RL. We decompose our policy into two interacting levels: the inner and outer levels. In the inner level, we present the PASS algorithm (for “PAst Sign Search”) which, based on a predefined sequence of learning rates, constructs a new sequence for which the error decreases faster. The convergence of PASS is proved and error bounds are established. In the outer level, we propose an optimal methodology for the selection of the predefined sequence. Third, we show empirically that our selection methodology of the learning rate outperforms significantly standard algorithms used in RL for the three following applications: the estimation of a drift, the optimal placement of limit orders, and the optimal execution of a large number of shares.",
        "link": "http://dx.doi.org/10.1111/mafi.12378"
    },
    {
        "id": 27255,
        "title": "Learning financial asset-specific trading rules via deep reinforcement learning",
        "authors": "Mehran Taghian, Ahmad Asadi, Reza Safabakhsh",
        "published": "2022-6",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2022.116523"
    },
    {
        "id": 27256,
        "title": "Learning Structured Representation for Text Classification via Reinforcement Learning",
        "authors": "Tianyang Zhang, Minlie Huang, Li Zhao",
        "published": "2018-4-26",
        "citations": 51,
        "abstract": "\n      \n        Representation learning is a fundamental problem in natural language processing. This paper studies how to learn a structured representation for text classification. Unlike most existing representation models that either use no structure or rely on pre-specified structures, we propose a reinforcement learning (RL) method to learn sentence representation by discovering optimized structures automatically. We demonstrate two attempts to build structured representation: Information Distilled LSTM (ID-LSTM) and Hierarchically Structured LSTM (HS-LSTM). ID-LSTM selects only important, task-relevant words, and HS-LSTM discovers phrase structures in a sentence. Structure discovery in the two representation models is formulated as a sequential decision problem: current decision of structure discovery affects following decisions, which can be addressed by policy gradient RL. Results show that our method can learn task-friendly representations by identifying important words or task-relevant structures without explicit structure annotations, and thus yields competitive performance.\n      \n    ",
        "link": "http://dx.doi.org/10.1609/aaai.v32i1.12047"
    },
    {
        "id": 27257,
        "title": "Learning Effective Communication for Cooperative Pursuit with Multi-Agent Reinforcement Learning",
        "authors": "Yubang Deng, Xianghui Cao, Qinmin Yang",
        "published": "2022-11-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055979"
    },
    {
        "id": 27258,
        "title": "Intra-Agent Transfer Methods",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5_4"
    },
    {
        "id": 27259,
        "title": "Experiment Domains and Applications",
        "authors": "Felipe Leno da Silva, Anna Helena Reali Costa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-01591-5_6"
    },
    {
        "id": 27260,
        "title": "Optimal recovery of unsecured debt via interpretable reinforcement learning",
        "authors": "Michael Mark, Naveed Chehrazi, Huanxi Liu, Thomas A. Weber",
        "published": "2022-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.mlwa.2022.100280"
    },
    {
        "id": 27261,
        "title": "Bridging Reinforcement Learning and Iterative Learning Control: Autonomous Motion Learning for Unknown, Nonlinear Dynamics",
        "authors": "Michael Meindl, Dustin Lehmann, Thomas Seel",
        "published": "2022-7-12",
        "citations": 4,
        "abstract": "This work addresses the problem of reference tracking in autonomously learning robots with unknown, nonlinear dynamics. Existing solutions require model information or extensive parameter tuning, and have rarely been validated in real-world experiments. We propose a learning control scheme that learns to approximate the unknown dynamics by a Gaussian Process (GP), which is used to optimize and apply a feedforward control input on each trial. Unlike existing approaches, the proposed method neither requires knowledge of the system states and their dynamics nor knowledge of an effective feedback control structure. All algorithm parameters are chosen automatically, i.e. the learning method works plug and play. The proposed method is validated in extensive simulations and real-world experiments. In contrast to most existing work, we study learning dynamics for more than one motion task as well as the robustness of performance across a large range of learning parameters. The method’s plug and play applicability is demonstrated by experiments with a balancing robot, in which the proposed method rapidly learns to track the desired output. Due to its model-agnostic and plug and play properties, the proposed method is expected to have high potential for application to a large class of reference tracking problems in systems with unknown, nonlinear dynamics.",
        "link": "http://dx.doi.org/10.3389/frobt.2022.793512"
    },
    {
        "id": 27262,
        "title": "Learning to Fly with Deep Reinforcement Learning",
        "authors": "Faisal A. A. Mohamed, Sondos W. A. Mohamed, Ahmed M. O. Mohamed",
        "published": "2021-2-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccceee49695.2021.9429576"
    },
    {
        "id": 27263,
        "title": "Online Hybrid Learning to Speed Up Deep Reinforcement Learning Method for Commercial Aircraft Control",
        "authors": "Minjian Xin, Yue Gao, Tianhao Mou, Jianlong Ye",
        "published": "2019-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isass.2019.8757756"
    },
    {
        "id": 27264,
        "title": "Predictive reward-prediction errors of climbing fiber inputs integrate modular reinforcement learning with supervised learning",
        "authors": "Huu Hoang, Shinichiro Tsutsumi, Masanori Matsuzaki, Masanobu Kano, Keisuke Toyama, Kazuo Kitamura, Mitsuo Kawato",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractAlthough the cerebellum is typically linked to supervised learning algorithms, it also exhibits extensive connections to reward processing. In this study, we investigated the cerebellum’s role in executing reinforcement learning algorithms, with a particular emphasis on essential reward-prediction errors. We employed the Q-learning model to accurately reproduce the licking responses of mice in a Go/No-go auditory-discrimination task. This method enabled the calculation of reinforcement learning variables, such as reward, predicted reward, and reward-prediction errors in each learning trial. By tensor component analysis of two-photon Ca2+imaging data, we found that climbing fiber inputs of the two distinct components, which were specifically activated during Go and No-go cues in the learning process, showed an inverse relationship with predictive reward-prediction errors. Given the hypothesis of bidirectional parallel-fiber Purkinje-cell synaptic plasticity, Purkinje cells in these components could develop specific motor commands for their respective auditory cues, guided by the predictive reward-prediction errors from their climbing fiber inputs. These results indicate a possible role of context-specific actors in modular reinforcement learning, integrating with cerebellar supervised learning capabilities.",
        "link": "http://dx.doi.org/10.1101/2023.03.13.532374"
    },
    {
        "id": 27265,
        "title": "Sampling Efficiency in Learning Robot Motion",
        "authors": "Adrià Colomé, Carme Torras",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-26326-3_6"
    },
    {
        "id": 27266,
        "title": "Image Classification by Reinforcement Learning With Two‐State Q‐Learning",
        "authors": "Abdul Mueed Hafiz",
        "published": "2022-2-25",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119792642.ch9"
    },
    {
        "id": 27267,
        "title": "Ordinal Inverse Reinforcement Learning Applied to Robot Learning with Small Data",
        "authors": "Adrià Colomé, Carme Torras",
        "published": "2022-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981731"
    },
    {
        "id": 27268,
        "title": "Inverse reinforcement learning through logic constraint inference",
        "authors": "Mattijs Baert, Sam Leroux, Pieter Simoens",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-023-06311-2"
    },
    {
        "id": 27269,
        "title": "Learning Free Energy Pathways through Reinforcement Learning of Adaptive Steered Molecular Dynamics",
        "authors": "Nicholas Ho, John Kevin Cava, John Vant, Ankita Shukla, Jake Miratsky, Pavan Turaga, Ross Maciejewski, Abhishek Singharoy",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractIn this paper, we develop a formulation to utilize reinforcement learning and sampling-based robotics planning to derive low free energy transition pathways between two known states. Our formulation uses Jarzynski’s equality and the stiffspring approximation to obtain point estimates of energy, and construct an informed path search with atomistic resolution. At the core of this framework, is our first ever attempt we use a policy driven adaptive steered molecular dynamics (SMD) to control our molecular dynamics simulations. We show that both the reinforcement learning and robotics planning realization of the RL-guided framework can solve for pathways on toy analytical surfaces and alanine dipeptide.",
        "link": "http://dx.doi.org/10.1101/2022.10.04.510845"
    },
    {
        "id": 27270,
        "title": "Coordination of PV Smart Inverters Using Deep Reinforcement Learning for Grid Voltage Regulation",
        "authors": "Changfu Li, Chenrui Jin, Ratnesh Sharma",
        "published": "2019-12",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2019.00310"
    },
    {
        "id": 27271,
        "title": "Rl-Ncs: Reinforcement Learning Based Data-Driven Approach For Nonuniform Compressed Sensing",
        "authors": "Nazmul Karim, Alireza Zaeemzadeh, Nazanin Rahnavard",
        "published": "2019-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp.2019.8918768"
    },
    {
        "id": 27272,
        "title": "Safe Robot Navigation Using Constrained Hierarchical Reinforcement Learning",
        "authors": "Felippe Schmoeller Roza, Hassan Rasheed, Karsten Roscher, Xiangyu Ning, Stephan Günnemann",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00123"
    },
    {
        "id": 27273,
        "title": "Applications of Deep Learning and Reinforcement Learning to Biological Data",
        "authors": "Mufti Mahmud, Mohammed Shamim Kaiser, Amir Hussain, Stefano Vassanelli",
        "published": "2018-6",
        "citations": 566,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2018.2790388"
    },
    {
        "id": 27274,
        "title": "Automatically Learning Fallback Strategies with Model-Free Reinforcement Learning in Safety-Critical Driving Scenarios",
        "authors": "Ugo Lecerf, Christelle Yemdji-Tchassi, Sebastien Aubert, Pietro Michiardi",
        "published": "2022-3-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3529399.3529432"
    },
    {
        "id": 27275,
        "title": "Proposal for Selecting a Cooperation Partner in Distributed Control of Traffic Signals using Deep Reinforcement Learning",
        "authors": "Shinya Matsuta, Naoki Kodama, Taku Harada",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12792/icisip2021.027"
    },
    {
        "id": 27276,
        "title": "A New Entity-relation Joint Extraction Model using Reinforcement Learning and Its Application Test",
        "authors": "Heping Peng, Zhong Xu, Wenxiong Mo, Yong Wang, Qingdan Huang, Chengzhu Sun, Ting He",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011361800003440"
    },
    {
        "id": 27277,
        "title": "Reinforcement, Rationality, and Intentions: How Robust Is Automatic Reinforcement Learning in Economic Decision Making?",
        "authors": "Sabine Hügelschäfer, Anja Achtziger",
        "published": "2017-10",
        "citations": 8,
        "abstract": "AbstractReinforcement learning is often observed in economic decision making and may lead to detrimental decisions. Because of its automaticity, it is difficult to avoid. In three experimental studies, we investigated whether this process could be controlled by goal intentions and implementation intentions. Participants' decisions were investigated in a probability‐updating task in which the normative rule to maximize expected payoff (Bayes' rule) conflicted with the reinforcement heuristic as a simple decision rule. Some participants were asked to set goal intentions designated to foster the optimization of rational decision making, while other participants were asked to furnish these goal intentions with implementation intentions. Results showed that controlling automatic processes of reinforcement learning is possible by means of goal intentions or implementation intentions that focus decision makers on the analysis of decision feedback. Importantly, such beneficial effects were not achieved by simply instructing participants to analyze the feedback, without defining a goal as the desired end state from a first‐person perspective. Regarding intentions supposed to shut down reinforcement processes by controlling negative affect, effects were more complex and depended on the specified goal‐directed behavior. The goal intention to suppress the disappointment elicited by negative feedback was not effective in controlling reinforcement processes. Furnishing this goal with an implementation intention even backfired and strengthened unwanted reinforcement processes. In contrast, asking participants to keep cool in response to negative decision outcomes through the use of goal intentions or implementation intentions increased decisions in line with Bayes' rule. Copyright © 2017 John Wiley & Sons, Ltd.",
        "link": "http://dx.doi.org/10.1002/bdm.2008"
    },
    {
        "id": 27278,
        "title": "Find Optimal Values and Optimal Policies for Finite MDP",
        "authors": "Zhiqing Xiao",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-99-3740-0_14"
    },
    {
        "id": 27279,
        "title": "Assessing the Security of Inter-App Communications in Android through Reinforcement Learning",
        "authors": "Andrea Romdhana, Alessio Merlo, Mariano Ceccato, Paolo Tonella",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>A central aspect of the Android platform is Inter-Component Communication (ICC), which enables the reuse of functionality across apps and components via message passing. While a powerful feature, ICC still constitutes a serious attack surface. This paper addresses the issue of generating exploits for a subset of Android ICC vulnerabilities (i.e., IDOS, XAS, and FI) through static analysis, Deep Reinforcement Learning-based dynamic analysis and software instrumentation. Our approach, called RONIN, achieves better results than state-of-the-art and baseline tools, in the number of exploited vulnerabilities.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21310770.v1"
    },
    {
        "id": 27280,
        "title": "Neural circuits for reinforcement learning and mental simulation",
        "authors": "Kenji Doya",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ibror.2019.07.160"
    },
    {
        "id": 27281,
        "title": "Function Approximation",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_5"
    },
    {
        "id": 27282,
        "title": "Reinforcement Learning Based Attitude Fault-Tolerant Control of Spacecraft with Unknown System Model",
        "authors": "Shaolong Yang, Lei Jin, Jiaxuan Rao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4692007"
    },
    {
        "id": 27283,
        "title": "Dynamic Replication and Hedging: A Reinforcement Learning Approach",
        "authors": "Gordon Ritter, Petter N. Kolm",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3281235"
    },
    {
        "id": 27284,
        "title": "Instance-Based Ensemble Selection Using Deep Reinforcement Learning",
        "authors": "Zhengshang Liu, Kotagiri Ramamohanarao",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207215"
    },
    {
        "id": 27285,
        "title": "Zero-shot compositional reinforcement learning in humans",
        "authors": "Akshay Kumar Jagadish, Marcel Binz, Tankred Saanum, Jan X. Wang, Eric Schulz",
        "published": "No Date",
        "citations": 0,
        "abstract": "People can easily evoke previously learned concepts, compose them, and apply the result to solve novel tasks on the first attempt. The aim of this paper is to improve our understanding of how people make such zero-shot compositional inferences in a reinforcement learning setting. To achieve this, we introduce an experimental paradigm where people learn two latent reward functions and need to compose them correctly to solve a novel task. We find that people have the capability to engage in zero-shot compositional reinforcement learning but deviate systematically from optimality. However, their mistakes are structured and can be explained by their performance in the sub-tasks leading up to the composition. Through extensive model-based analyses, we found that a meta-learned neural network model that accounts for limited computational resources best captures participants’ behaviour. Moreover, the amount of computational resources this model identified reliably quantifies how good individual participants are at zero-shot compositional reinforcement learning. Taken together, our work takes a considerable step towards studying compositional reasoning in agents – both natural and artificial – with limited computational resources.",
        "link": "http://dx.doi.org/10.31234/osf.io/ymve5"
    },
    {
        "id": 27286,
        "title": "Intermittent Reinforcement Learning with Sparse Rewards",
        "authors": "Prachi Pratyusha Sahoo, Kyriakos G. Vamvoudakis",
        "published": "2022-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867602"
    },
    {
        "id": 27287,
        "title": "The Evolution of Reinforcement Learning",
        "authors": "Dean Frederick Hougen, Syed Naveed Hussain Shah",
        "published": "2019-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci44817.2019.9003146"
    },
    {
        "id": 27288,
        "title": "Approximate policy search with cross-entropy optimization of basis functions",
        "authors": "",
        "published": "2017-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781439821091-11"
    },
    {
        "id": 27289,
        "title": "Coaching: Human-assisted approach for reinforcement learning",
        "authors": "Nakarin Suppakun, Thavida Maneewarn",
        "published": "2017-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icras.2017.8071940"
    },
    {
        "id": 27290,
        "title": "MERMAID: An Open Source Automated Hit-to-Lead Method Based on Deep Reinforcement Learning",
        "authors": "Daiki Erikawa, Nobuaki Yasuo, Masakazu Sekijima",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>The hit-to-lead process makes the physicochemical properties of the hit compounds that show the desired type of activity obtained in the screening assay more drug-like. Deep learning-based molecular generative models are expected to contribute to the hit-to-lead process.</div><div>The simplified molecular input line entry system (SMILES), which is a string of alphanumeric characters representing the chemical structure of a molecule, is one of the most commonly used representations of molecules, and molecular generative models based on SMILES have achieved significant success. However, in contrast to molecular graphs, during the process of generation, SMILES are not considered as valid SMILES. Further, it is quite difficult to generate molecules starting from a certain molecule, thus making it difficult to apply SMILES to the hit-to-lead process.In this study, we have developed a SMILES-based generative model that can be generated starting from a certain compound. This method generates partial SMILES and inserts it into the original SMILES using Monte Carlo Tree Search and a Recurrent Neural Network.We validated our method using a molecule dataset obtained from the ZINC database and successfully generated molecules that were both well optimized for the objectives of the quantitative estimate of drug-likeness (QED) and penalized octanol-water partition coefficient (PLogP) optimization.</div><div>The source code is available at https: //github.com/sekijima-lab/mermaid.</div>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14450313"
    },
    {
        "id": 27291,
        "title": "Proceedings of the 1st International Workshop on Reinforcement Learning for Energy Management in Buildings &amp; Cities",
        "authors": "",
        "published": "2020-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3427773"
    },
    {
        "id": 27292,
        "title": "Evolving Intertask Mappings for Transfer in Reinforcement Learning",
        "authors": "Minh Hua, John W. Sheppard",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec53210.2023.10254100"
    },
    {
        "id": 27293,
        "title": "4d Printing-Enabled Circular Economy: Disassembly Sequence Planning Using Reinforcement Learning",
        "authors": "Di Wang, Jing Zhao, Muyue Han, Lin Li",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4429186"
    },
    {
        "id": 27294,
        "title": "Symmetry Augmentation Using Direct Sum for Time Series Reinforcement Learning",
        "authors": "Amine Mohamed Aboussalah",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3748152"
    },
    {
        "id": 27295,
        "title": "An Overview of Robust Reinforcement Learning",
        "authors": "Shiyu Chen, Yanjie Li",
        "published": "2020-10-30",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnsc48988.2020.9238129"
    },
    {
        "id": 27296,
        "title": "Reinforcement Learning for Predictive Analytics in Smart Cities",
        "authors": "Kostas Kolomvatsos, Christos Anagnostopoulos",
        "published": "2017-6-24",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/informatics4030016"
    },
    {
        "id": 27297,
        "title": "DDT: A Reinforcement Learning Approach to Dynamic Flow Timeout Assignment in Software Defined Networks",
        "authors": "Nathan Harris, Sajad Khorsandroo",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nOpenFlow-compliant commodity switches face challenges in efficiently managing flow rules due to the limited capacity of expensive high-speed memories used to store them. The accumulation of inactive flows can disrupt ongoing communication, necessitating an optimized approach to flow rule timeouts. This paper proposes Delayed Dynamic Timeout (DDT), a Reinforcement Learning-based approach to dynamically adjust flow rule timeouts and enhance the utilization of a switch's flow table(s) for improved efficiency. Despite the dynamic nature of network traffic, our DDT algorithm leverages advancements in Reinforcement Learning algorithms to adapt and achieve flow-specific optimization objectives.\nThe evaluation results demonstrate that DDT outperforms static timeout values in terms of both flow rule match rate and flow rule activity. By continuously adapting to changing network conditions, DDT showcases the potential of Reinforcement Learning algorithms to effectively optimize flow rule management. This research contributes to the advancement of flow rule optimization techniques and highlights the feasibility of applying Reinforcement Learning in the context of SDN.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3200987/v1"
    },
    {
        "id": 27298,
        "title": "Deep Reinforcement Learning in Medicine",
        "authors": "Anders Jonsson",
        "published": "2019",
        "citations": 50,
        "abstract": "Reinforcement learning has achieved tremendous success in recent years, notably in complex games such as Atari, Go, and chess. In large part, this success has been made possible by powerful function approximation methods in the form of deep neural networks. The objective of this paper is to introduce the basic concepts of reinforcement learning, explain how reinforcement learning can be effectively combined with deep learning, and explore how deep reinforcement learning could be useful in a medical context.",
        "link": "http://dx.doi.org/10.1159/000492670"
    },
    {
        "id": 27299,
        "title": "Satellite Attitude Control with Deep Reinforcement Learning",
        "authors": "Duozhi Gao, Haibo Zhang, Chuanjiang Li, Xinzhou Gao",
        "published": "2020-11-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9326605"
    },
    {
        "id": 27300,
        "title": "Model-free Reinforcement Learning for Demand Response in PV-rich Distribution Systems",
        "authors": "Ibrahim Alsaleh",
        "published": "2022-12-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sasg57022.2022.10200928"
    },
    {
        "id": 27301,
        "title": "Model-based reinforcement learning for service mesh fault resiliency in a web application-level",
        "authors": "Fanfei Meng, Lalita Jagadeesa, Marina Thottan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Microservice-based architectures enable different aspects of web applications to be created and updated independently, even after deployment. Associated technologies such as service mesh provide application-level fault resilience through attribute configurations that govern the behavior of request - response service -- and the interactions among them -- in the presence of failures. While this provides tremendous flexibility, the configured values of these attributes -- and the relationships among them -- can significantly affect the performance and fault resilience of the overall application. Furthermore, it is impossible to determine the best and worst combinations of attribute values with respect to fault resiliency via testing, due to the complexities of the underlying distributed system and the many possible attribute value combinations. In this paper, we present a model-based reinforcement learning workflow towards service mesh fault resiliency. Our approach enables the prediction of the most significant fault resilience behaviors at a web application-level, scratching from single service to aggregated multi-service management with efficient agent collaborations.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24582396"
    },
    {
        "id": 27302,
        "title": "Deadlock-Detection via Reinforcement Learning",
        "authors": "Mengmeng Chen, Luis Rabelo",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4172/2169-0316.1000215"
    },
    {
        "id": 27303,
        "title": "Value Function Optimistic Initialization with Uncertainty and Confidence Awareness in Lifelong Reinforcement Learning",
        "authors": "Soumia Mehimeh, xianglong tang, wei zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4420270"
    },
    {
        "id": 27304,
        "title": "Causal Reinforcement Learning: An Instrumental Variable Approach",
        "authors": "Jin Li, Ye Luo, Xiaowei Zhang",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3792824"
    },
    {
        "id": 27305,
        "title": "A Reinforcement Meta-Learning Framework of Executive Function and Information Demand",
        "authors": "Massimo Silvetti, Stefano Lasaponara, Mattias Horan, Jacqueline Gottlieb",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractGathering information is crucial for maximizing fitness, but requires diverting resources from searching directly for primary rewards to actively exploring the environment. Optimal decision-making thus maximizes information while reducing effort costs, but little is known about the neural implementation of these tradeoffs. We present a Reinforcement Meta-Learning (RML) computational mechanism that solves the trade-offs between the value and costs of gathering information. We implement the RML in a biologically plausible architecture that links catecholaminergic neuromodulators, the medial prefrontal cortex and topographically organized visual maps and show that it accounts for neural and behavioral findings on information demand motivated by instrumental incentives and intrinsic utility. Moreover, the utility function used by the RML, encoded by dopamine, is an approximation of free-energy. Thus, the RML presents a biologically plausible mechanism through which coordinated motivational, executive and sensory systems generate visual information gathering policies that minimize free energy.",
        "link": "http://dx.doi.org/10.1101/2021.07.18.452793"
    },
    {
        "id": 27306,
        "title": "PWR Loading Pattern Optimization with Reinforcement Learning",
        "authors": "Paul Seurin, Koroush Shirvan",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.13182/physor22-37773"
    },
    {
        "id": 27307,
        "title": "Reinforcement Learning to Maximise Wind Turbine Energy Generation",
        "authors": "Daniel Soler, Oscar Marino, David Huergo, martín de Frutos, Esteban Ferrer",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4626683"
    },
    {
        "id": 27308,
        "title": "Leveraging Sequentiality in Reinforcement Learning from a Single Demonstration",
        "authors": "Alexandre  Vincent Chenu, Olivier Serris, Olivier Sigaud, Nicolas Perrin-Gilbert",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4627349"
    },
    {
        "id": 27309,
        "title": "A Dynamic Hierarchical Hyperheuristic Based Automatic Algorithm Design with Reinforcement Learning",
        "authors": "Ningning Zhu, Fuqing Zhao, Jie Cao, Jonrinaldi Jonrinaldi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4632040"
    },
    {
        "id": 27310,
        "title": "Cherry-Picking with Reinforcement Learning",
        "authors": "Yunchu Zhang, Liyiming Ke, Abhay Deshpande, Abhishek Gupta, Siddhartha Srinivasa",
        "published": "2023-7-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.021"
    },
    {
        "id": 27311,
        "title": "First-Principle-Like Reinforcement Learning of Nonlinear Numerical Schemes for Conservation Laws",
        "authors": "Hao-Chen Wang, Meilin Yu, Heng Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4697759"
    },
    {
        "id": 27312,
        "title": "Reinforcement learning in EEG-based human-robot interaction",
        "authors": "Jiali Huang, Chang S. Nam",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-323-85648-5.00020-7"
    },
    {
        "id": 27313,
        "title": "Computationally Efficient Safe Reinforcement Learning for Power Systems",
        "authors": "Daniel Tabas, Baosen Zhang",
        "published": "2022-6-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867652"
    },
    {
        "id": 27314,
        "title": "Reinforcement Learning for the Interception of Hypersonic Vehicles",
        "authors": "Daniel Porter, John Gilbert",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2024-1301"
    },
    {
        "id": 27315,
        "title": "Evolutionary Reinforcement Learning of Binary Neural Network Controllers for Pendulum Task — Part1: Evolution Strategy",
        "authors": "Hidehiko Okada",
        "published": "No Date",
        "citations": 0,
        "abstract": "The author previously reported an experimental result of evolutionary reinforcement learning of neural network controllers for the pendulum task. In the previous work, a conventional multilayer perceptron was employed in which connection weights were real numbers. In this study, the author experimentally applies an evolutionary algorithm to the reinforcement training of binary neural networks. In both studies, the same task and the same evolutionary algorithm are utilized, i.e. the pendulum control problem and Evolution Strategy respectively. The only differences lie in the memory size per connection weight and the model size of the neural network. The findings from this study are (1) the performance of the binary MLP with 32 hidden units was inferior to that of the real-valued MLP with 16 hidden units; however, this difference was not statistically significant (p &gt;.05); (2) the trained binary MLP successfully swung the pendulum swiftly into an inverted position and maintained its stability after inversion, as the real-valued MLP had done; and (3) the memory size required to record the binary MLP with 32 hidden units is 3.1% or 6.2% of the memory size required to record the real-valued MLP with 16 hidden units. ",
        "link": "http://dx.doi.org/10.20944/preprints202312.1537.v1"
    },
    {
        "id": 27316,
        "title": "Reinforcement Learning in Latent Heterogeneous Environments",
        "authors": "Elynn Chen, Rui Song, Michael I. Jordan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4694618"
    },
    {
        "id": 27317,
        "title": "Author response for \"Deep reinforcement learning for conservation decisions\"",
        "authors": " Marcus Lapeyrolerie,  Melissa S. Chapman,  Kari E. A. Norman,  Carl Boettiger",
        "published": "2022-2-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/2041-210x.13954/v2/response1"
    },
    {
        "id": 27318,
        "title": "Variational Skill Embeddings for Meta Reinforcement Learning",
        "authors": "Jen-Tzung Chien, Weiwei Lai",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191425"
    },
    {
        "id": 27319,
        "title": "Deep Reinforcement Learning for an Empirical Approach to Value-at-Risk",
        "authors": "POKOU FREDY MANUEL, Jules SADEFO KAMDEM, François Benhmad",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4701537"
    },
    {
        "id": 27320,
        "title": "Developing Combat Behavior through Reinforcement Learning in Wargames and Simulations",
        "authors": "Jonathan Boron, Chris Darken",
        "published": "2020-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog47356.2020.9231609"
    },
    {
        "id": 27321,
        "title": "A Law of Iterated Logarithm for Multi-Agent Reinforcement Learning",
        "authors": "Gugan Thoppe, Bhumesh Kumar",
        "published": "2021-12-20",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc54714.2021.9702912"
    },
    {
        "id": 27322,
        "title": "Decision letter for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v1/decision1"
    },
    {
        "id": 27323,
        "title": "Decision letter for \"Model‐free closed‐loop wind farm control using reinforcement learning with recursive least squares\"",
        "authors": "",
        "published": "2023-3-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/we.2852/v2/decision1"
    },
    {
        "id": 27324,
        "title": "Optimized Deep Reinforcement Learning Approach for Dynamic System",
        "authors": "Ziya Tan, Mehmet Karakose",
        "published": "2020-10-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isse49799.2020.9272245"
    },
    {
        "id": 27325,
        "title": "A Reinforcement Learning Approach for Disassembly Line Balancing Problem",
        "authors": "Suleyman Mete, Faruk Serin",
        "published": "2021-7-14",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit52682.2021.9491689"
    },
    {
        "id": 27326,
        "title": "Exploration by Distributional Reinforcement Learning",
        "authors": "Yunhao Tang, Shipra Agrawal",
        "published": "2018-7",
        "citations": 3,
        "abstract": "We propose a framework based on distributional reinforcement learning and recent attempts to combine Bayesian parameter updates with deep reinforcement learning. We show that our proposed framework conceptually unifies multiple previous methods in exploration. We also derive a practical algorithm that achieves efficient exploration on challenging control tasks.",
        "link": "http://dx.doi.org/10.24963/ijcai.2018/376"
    },
    {
        "id": 27327,
        "title": "Joint Edge Association and Aggregation Frequency for Energy-Efficient Hierarchical Federated Learning by Deep Reinforcement Learning",
        "authors": "Yijing Ren, Changxiang Wu, Daniel K.C. So",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279332"
    },
    {
        "id": 27328,
        "title": "Reinforcement Learning based Evolutionary Metric Filtering for High Dimensional Problems",
        "authors": "Bassel Ali, Koichi Moriyama, Masayuki Numao, Ken-ichi Fukui",
        "published": "2020-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla51294.2020.00045"
    },
    {
        "id": 27329,
        "title": "English information teaching resource sharing based on deep reinforcement learning",
        "authors": "Chao Han",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijceell.2024.10057351"
    },
    {
        "id": 27330,
        "title": "Video Representation Learning for Decoupled Deep Reinforcement Learning Applied to Autonomous Driving",
        "authors": "Shawan Taha Mohammed, Mohamed Kastouri, Artur Niederfahrenhorst, Gerd Ascheid",
        "published": "2023-1-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sii55687.2023.10039291"
    },
    {
        "id": 27331,
        "title": "Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation",
        "authors": "Yang Gao, Christian M. Meyer, Mohsen Mesgar, Iryna Gurevych",
        "published": "2019-8",
        "citations": 2,
        "abstract": "Document summarisation can be formulated as a sequential decision-making problem, which can be solved by Reinforcement Learning (RL) algorithms. The predominant RL paradigm for summarisation learns a cross-input policy, which requires considerable time, data and parameter tuning due to the huge search spaces and the delayed rewards. Learning input-specific RL policies is a more efficient alternative, but so far depends on handcrafted rewards, which are difficult to design and yield poor performance. We propose RELIS, a novel RL paradigm that learns a reward function with Learning-to-Rank (L2R) algorithms at training time and uses this reward function to train an input-specific RL policy at test time. We prove that RELIS guarantees to generate near-optimal summaries with appropriate L2R and RL algorithms. Empirically, we evaluate our approach on extractive multi-document summarisation. We show that RELIS reduces the training time by two orders of magnitude compared to the state-of-the-art models while performing on par with them.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/326"
    },
    {
        "id": 27332,
        "title": "Introduction to Machine Learning",
        "authors": "F. Richard Yu, Ying He",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-10546-4_1"
    },
    {
        "id": 27333,
        "title": "Video Game Recommender System Using Deep Reinforcement Learning",
        "authors": "Mohammad Akbar Fauzy Ali, Z. K. A. Baizal",
        "published": "2023-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icadeis58666.2023.10270905"
    },
    {
        "id": 27334,
        "title": "Reinforcement Learning and Stochastic Optimization with Deep&#x0D;\nLearning based Forecasting on Power Grid Scheduling",
        "authors": "Cheng Yang, Jihai Zhang, Wei Jiang, Li Wang, Hanwei Zhang, Zhongkai Yi, Fangquan Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "Continuous greenhouse gas emissions are causing global warming and impacting the habitats of many animals. Researchers in the field of electric power are making efforts to mitigate this situation. Operating and maintaining the power grid in an economic, low-carbon, and stable is challenging. To address the issue, we propose a grid dispatching technique that combines prediction technology, reinforcement learning, and optimization technology. Prediction technology can forecast future power demand and solar power generation, while reinforcement learning and optimization technology can make charging and discharging decisions for energy storage devices based on current and future grid conditions. In the power system, the aggregation of distributed energy resources increases uncertainty, particularly due to the fluctuating generation of renewable energy. This requires the use of advanced predictive control techniques to ensure long-term economic and decarbonization goals. In this paper, we present a real-time dispatching framework that integrates deep learning-based prediction, reinforcement learning-based decision-making, and stochastic optimization techniques. The framework can rapidly adapt to target uncertainty caused by various factors in real-time data distribution and control processes. The proposed framework achieved global Champion in the NeurIPS Challenge 2022 competition and demonstrated its effectiveness in practical scenarios of intelligent building energy management.",
        "link": "http://dx.doi.org/10.20944/preprints202309.1975.v1"
    },
    {
        "id": 27335,
        "title": "Self-Communicating Deep Reinforcement Learning Agents Develop External Number Representations",
        "authors": "Silvester Sabathiel, Trygve Solstad, Alberto Testolin, Flavio Petruzzellis",
        "published": "2022-6-16",
        "citations": 2,
        "abstract": "Symbolic numbers are a remarkable product of human cultural development. The developmental process involved the creation and progressive refinement of material representational tools, such as notched tallies, knotted strings, and counting boards. In this paper, we introduce a computational framework that allows the investigation of how material representations might support number processing in a deep reinforcement learning scenario. In this framework, agents can use an external, discrete state to communicate information to solve a simple numerical estimation task. We find that different perceptual and processing constraints result in different emergent representations, whose specific characteristics can facilitate the learning and communication of numbers.",
        "link": "http://dx.doi.org/10.7557/18.6291"
    },
    {
        "id": 27336,
        "title": "Enhancing the performance of adaptive iterative learning control with reinforcement learning",
        "authors": "Bojan Nemec, Mihael Simonic, Nejc Likar, Ales Ude",
        "published": "2017-9",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2017.8206038"
    },
    {
        "id": 27337,
        "title": "Adaptive Reinforcement Learning Tracking Control for Second-Order Multi-Agent Systems",
        "authors": "Weiwei Bai, Liang Cao, Guowei Dong, Hongyi Li",
        "published": "2019-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls.2019.8908978"
    },
    {
        "id": 27338,
        "title": "Supplemental Material for Reinforcement Learning In and Out of Context: The Effects of Attentional Focus",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1037/xlm0001145.supp"
    },
    {
        "id": 27339,
        "title": "Uprising E-sports Industry: machine learning/AI improve in-game performance using deep reinforcement learning",
        "authors": "Xianzuo Du, Xiwei Fuqian, Jiaxi Hu, Zechen Wang, Dongju Yang",
        "published": "2021-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlise54096.2021.00112"
    },
    {
        "id": 27340,
        "title": "Learning to Play Precision Ball Sports from scratch: a Deep Reinforcement Learning Approach",
        "authors": "Liliana Antao, Armando Sousa, Luis Paulo Reis, Gil Goncalves",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207518"
    },
    {
        "id": 27341,
        "title": "Timing and Parameter Optimization for One-time Motion Problem Based on Reinforcement Learning",
        "authors": "Boxuan Fan, Guiming Chen, Hongtao Lin",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.11648/j.mlr.20200501.12"
    },
    {
        "id": 27342,
        "title": "DreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning",
        "authors": "I Made Aswin Nahrendra, Byeongho Yu, Hyun Myung",
        "published": "2023-5-29",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161144"
    },
    {
        "id": 27343,
        "title": "Skill based transfer learning with domain adaptation for continuous reinforcement learning domains",
        "authors": "Farzaneh Shoeleh, Masoud Asadpour",
        "published": "2020-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-019-01527-z"
    },
    {
        "id": 27344,
        "title": "Deep Reinforcement Learning Visual-Text Attention for Multimodal Video Classification",
        "authors": "Mengyi Liu, Zhu Liu",
        "published": "2019-10-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3347450.3357654"
    },
    {
        "id": 27345,
        "title": "POEM: A Personalized Online Education Scheme Based on Reinforcement Learning",
        "authors": "Yufeng Wang, Wenjie Cai, Meijuan Chen, Jianhua Shen",
        "published": "2020-12-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tale48869.2020.9368369"
    },
    {
        "id": 27346,
        "title": "Reinforcement Learning based Data-driven Optimal Control Strategy for Systems with Disturbance",
        "authors": "Zhong-Xin Fan, Shihua Li, Rongjie Liu",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10167230"
    },
    {
        "id": 27347,
        "title": "Deep reinforcement learning and imitation learning based on VizDoom",
        "authors": "Yingyu Xu",
        "published": "2022-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3573428.3573729"
    },
    {
        "id": 27348,
        "title": "Biological View of Reinforcement",
        "authors": "Franco J. Vaccarino, Bernard B. Schiff, Stephen E. Glickman",
        "published": "2019-2-21",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315788982-7"
    },
    {
        "id": 27349,
        "title": "Joint Action Learning for Multi-Agent Cooperation using Recurrent Reinforcement Learning",
        "authors": "Jorrit E. Posor, Lenz Belzner, Alexander Knapp",
        "published": "2020-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42354-019-0239-y"
    },
    {
        "id": 27350,
        "title": "Joint Path Planning and Power Allocation of a Cellular-Connected Uav Using Apprenticeship Learning via Deep Inverse Reinforcement Learning",
        "authors": "Alireza Shamsoshoaraa, Fatemeh Lotfi, Sajad Mousavi, Fatemeh Afghah, Ismail Guvenc",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4511065"
    },
    {
        "id": 27351,
        "title": "Inverse reinforcement learning in contextual MDPs",
        "authors": "Stav Belogolovsky, Philip Korsunsky, Shie Mannor, Chen Tessler, Tom Zahavy",
        "published": "2021-9",
        "citations": 4,
        "abstract": "AbstractWe consider the task of Inverse Reinforcement Learning in Contextual Markov Decision Processes (MDPs). In this setting, contexts, which define the reward and transition kernel, are sampled from a distribution. In addition, although the reward is a function of the context, it is not provided to the agent. Instead, the agent observes demonstrations from an optimal policy. The goal is to learn the reward mapping, such that the agent will act optimally even when encountering previously unseen contexts, also known as zero-shot transfer. We formulate this problem as a non-differential convex optimization problem and propose a novel algorithm to compute its subgradients. Based on this scheme, we analyze several methods both theoretically, where we compare the sample complexity and scalability, and empirically. Most importantly, we show both theoretically and empirically that our algorithms perform zero-shot transfer (generalize to new and unseen contexts). Specifically, we present empirical experiments in a dynamic treatment regime, where the goal is to learn a reward function which explains the behavior of expert physicians based on recorded data of them treating patients diagnosed with sepsis.",
        "link": "http://dx.doi.org/10.1007/s10994-021-05984-x"
    },
    {
        "id": 27352,
        "title": "Offloading Mechanisms Based on Reinforcement Learning and Deep Learning Algorithms in the Fog Computing Environment",
        "authors": "Dezheen H. Abdulazeez, Shavan K. Askar",
        "published": "2023",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3241881"
    },
    {
        "id": 27353,
        "title": "Manipulating Reinforcement Learning: Stealthy Attacks on Cost Signals",
        "authors": "Yunhan Huang, Quanyan Zhu",
        "published": "2021-9-22",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119723950.ch19"
    },
    {
        "id": 27354,
        "title": "A Hybrid Grey Wolf Optimizer Using Opposition-Based Learning, Sine Cosine Algorithm and Reinforcement Learning for Reliable Scheduling and Resource Allocation",
        "authors": "Man ZHAO, Rui Hou, Hui Li, Min Ren",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4374576"
    },
    {
        "id": 27355,
        "title": "Deep Reinforcement Factorization Machines: A Deep Reinforcement Learning Model with Random Exploration Strategy and High Deployment Efficiency",
        "authors": "Huaidong Yu, Jian Yin",
        "published": "2022-5-24",
        "citations": 1,
        "abstract": "In recent years, the recommendation system and robot learning are undoubtedly the two most popular application fields, and the core algorithms supporting these two fields are deep learning based on perception and reinforcement learning based on exploration learning, respectively. How to combine these two fields to better improve the development of the whole machine learning field is the dream of numerous researchers. The Deep Reinforcement Network (DRN) model successfully embedded reinforcement learning into the recommendation system, which provided a good idea for subsequent researchers. However, the disadvantage is also obvious, that is, the DRN model is built for news recommendations, meaning that the DRN model is not transferable, which is also the defect of many current recommendation system models. Meanwhile, the agent learning method adopted by the DRN model is primitive and inefficient. Among many models and algorithms that have emerged in recent years, we use the newly proposed deployment efficiency to measure their comprehensive quality and found that few models focus on both efficiency and performance improvement. To fill the gap of model deployment efficiency neglected by many researchers and to create a model of reinforcement learning agents with stronger performance, we have been exploring and trying to complete research on the Gate Attentional Factorization Machines (GAFM) model. Finally, we successfully integrated the GAFM model and reinforcement learning. The Deep Reinforcement Factorization Machines (DRFM) model proposed in this paper is based on the combination of deep learning with high perception ability and reinforcement learning with high exploration ability, centered on improving the deployment efficiency and learning performance of the model. The GAFM model is modified and upgraded using multidisciplinary techniques, and a new model-based random exploration strategy is proposed to update and optimize the recommendation list efficiently. Through parallel contrast experiments on various datasets, it is proved that the DRFM model surpasses the traditional recommendation system model in all aspects. The DRFM model is far superior to other models in terms of performance and robustness, and also significantly improved in terms of deployment efficiency. At the same time, we conduct a comparative analysis with the latest deep reinforcement learning algorithm and prove the unique advantages of the DRFM model.",
        "link": "http://dx.doi.org/10.3390/app12115314"
    },
    {
        "id": 27356,
        "title": "Reinforcement Learning, Bit by Bit",
        "authors": "Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, Zheng Wen",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1561/2200000097"
    },
    {
        "id": 27357,
        "title": "Multi-UCAV Air Combat in Short-Range Maneuver Strategy Generation using Reinforcement Learning and Curriculum Learning",
        "authors": "Weiren Kong, Deyun Zhou, Kai Zhang, Zhen Yang, Wansha Yang",
        "published": "2020-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla51294.2020.00238"
    },
    {
        "id": 27358,
        "title": "Reinforcement learning based adaptive metaheuristics",
        "authors": "Michele Tessari, Giovanni Iacca",
        "published": "2022-7-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3520304.3533983"
    },
    {
        "id": 27359,
        "title": "Reinforcement learning predicts frustration-related motor invigoration",
        "authors": "Bowen Fung, Xin Sui, Colin Camerer, Dean Mobbs",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2019.1020-0"
    },
    {
        "id": 27360,
        "title": "Multiagent Reinforcement Learning",
        "authors": "Jonathan P. How, Dong-Ki Kim, Samir Wadhwania",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4471-5102-9_100066-1"
    },
    {
        "id": 27361,
        "title": "Setting Up ML Agents Toolkit",
        "authors": "Abhilash Majumder",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6503-1_3"
    },
    {
        "id": 27362,
        "title": "Vocal Learning: Shaping by Social Reinforcement",
        "authors": "Daniel Y. Takahashi",
        "published": "2019-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cub.2019.01.001"
    },
    {
        "id": 27363,
        "title": "Generalized Deep Reinforcement Learning for Trading",
        "authors": "Junyoung Sim, Benjamin Kirk",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "This paper proposes generalized deep reinforcement learning with multivariate state space, discrete rewards, and adaptive synchronization for trading any stock held in the S&P 500. Specifically, the proposed trading model observes the daily historical data of a stock held in the S&P 500 and multiple market-indicating securities (SPY, IEF, EUR=X, GSG), selects a trading action, and observes a discrete reward that is based on the correctness of the selected action and independent of the volatility of stocks. The proposed trading model’s reward-maximizing behavior is optimized by using a standard deep q-network (DQN) with adaptive synchronization that stabilizes and enables to track learning performance on generalizing new experiences from each stock. The proposed trading model was trained on the top 50 holdings of the S&P 500 and tested on the top 100 holdings of the S&P 500 starting from 2006 to 2022. Experimental results suggest that the proposed trading model significantly outperforms the 100% long-strategy benchmark in terms of annualized return, Sharpe ratio, and maximum drawdown.",
        "link": "http://dx.doi.org/10.47611/jsrhs.v12i1.4316"
    },
    {
        "id": 27364,
        "title": "Conclusions and Future Directions",
        "authors": "",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119699057.ch6"
    },
    {
        "id": 27365,
        "title": "Reinforcement learning in queues",
        "authors": "U. Ayesta",
        "published": "2022-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11134-022-09844-w"
    },
    {
        "id": 27366,
        "title": "Accelerated Reinforcement Learning for Sentence Generation by Vocabulary Prediction",
        "authors": "Kazuma Hashimoto, Yoshimasa Tsuruoka",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/n19-1315"
    },
    {
        "id": 27367,
        "title": "Ramping and phasic dopamine activity accounts for efficient cognitive resource allocation during reinforcement learning",
        "authors": "Minryung R. Song, Sang Wan Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractDopamine activity may transition between two patterns: phasic responses to reward-predicting cues and ramping activity arising when an agent approaches the reward. However, when and why dopamine activity transitions between these modes is not understood. We hypothesize that the transition between ramping and phasic patterns reflects resource allocation which addresses the task dimensionality problem during reinforcement learning (RL). By parsimoniously modifying a standard temporal difference (TD) learning model to accommodate a mixed presentation of both experimental and environmental stimuli, we simulated dopamine transitions and compared it with experimental data from four different studies. The results suggested that dopamine transitions from ramping to phasic patterns as the agent narrows down candidate stimuli for the task; the opposite occurs when the agent needs to re-learn candidate stimuli due to a value change. These results lend insight into how dopamine deals with the tradeoff between cognitive resource and task dimensionality during RL.",
        "link": "http://dx.doi.org/10.1101/381103"
    },
    {
        "id": 27368,
        "title": "Einführung",
        "authors": "Andreas Folkers",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-28886-0_1"
    },
    {
        "id": 27369,
        "title": "Deep Reinforcement Learning based Energy Scheduling for Edge Computing",
        "authors": "Qinglin Yang, Peng Li",
        "published": "2020-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smartcloud49737.2020.00041"
    },
    {
        "id": 27370,
        "title": "AMF Optimal Placement based on Deep Reinforcement Learning in Heterogeneous Radio Access Network",
        "authors": "hao jin, Wenzhe Pang, Chenglin Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTo support various service requirements such as massive Machine Type Communications, Ultra-Reliable and Low-Latency Communications in 5G scenario, Network Function Virtualization (NFV) plays an important role in the 5G network architecture to manage and orchestrate network services. As the key network function responsible for mobility management, Access and Mobility Management Function (AMF) can be deployed flexibly at the edge of the radio access network to improve the performance of mobility management based on NFV. In this paper, the optimal placement of AMF is addressed based on Deep Reinforcement Learning (DRL) in a heterogeneous radio access network, which aims to minimize the network utility including the average delay of mobility management requests at AMF, the average wired hops to relay the requests and the cost of AMF instances. By considering time-varying features including user mobility and the arrival rate of user mobility management requests, an AMF optimal placement approach is proposed for the long term optimization. Simulation results show that the performance of the proposed DRL based AMF optimal placement approach outperforms that of the baselines.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-14323/v1"
    },
    {
        "id": 27371,
        "title": "Approximation in Quantum Computing",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1_5"
    },
    {
        "id": 27372,
        "title": "Case Studies in ML Agents",
        "authors": "Abhilash Majumder",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6503-1_7"
    },
    {
        "id": 27373,
        "title": "A Reinforcement Learning Algorithm based on Neural Network for Economic Dispatch",
        "authors": "Liying Yu, Ning Li",
        "published": "2020-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9188641"
    },
    {
        "id": 27374,
        "title": "Policy Gradient Methods",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_9"
    },
    {
        "id": 27375,
        "title": "Curiosity-Driven Exploration",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_13"
    },
    {
        "id": 27376,
        "title": "Adaptive Data Replication Optimization Based on Reinforcement Learning",
        "authors": "Chee Keong Wee, Richi Nayak",
        "published": "2020-12-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci47803.2020.9308306"
    },
    {
        "id": 27377,
        "title": "Model-Free Reinforcement Learning-Based Control for Continuous-Time Systems",
        "authors": "Kyriakos G. Vamvoudakis",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-44184-5_100065"
    },
    {
        "id": 27378,
        "title": "Portfolio Optimization using Reinforcement Learning",
        "authors": "Dhruv Joshi",
        "published": "2022-10-1",
        "citations": 0,
        "abstract": "Reinforcement Learning has contributed to the automation of various tasks, including electronic algorithmic trading. The main benefit of Reinforcement Learning in financial applications is the relaxation of assumptions on market models and hand-picked features, allowing a data-driven, automated process. The performance of Reinforcement Learning agents on the portfolio management problem is measured. A finite universe of financial instruments such as stocks is selected and the trained agent is constructing an internal representation (model) of the market, allowing it to determine how to optimally allocate funds of a finite budget to those assets. The agent is trained on the actual market prices. The performance metrics are then compared with those of standard portfolio management algorithms on a dataset that has not been used before. A successful agent can be used as a consulting software for portfolio managers or it can be used for low-frequency algorithmic trading. Moreover, it can allow us to identify the missing pieces of the existing models and suggest directions to improve them.",
        "link": "http://dx.doi.org/10.55041/ijsrem16672"
    },
    {
        "id": 27379,
        "title": "Challenging Risk-Neutrality, Reinforcement Learning for Options Pricing in Indian Options market",
        "authors": "Dhruv Mahajan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3917860"
    },
    {
        "id": 27380,
        "title": "Refinement Of Reinforcement Learning Algorithms Guided By Counterexamples",
        "authors": "Briti Gangopadhyay, Somi Vishnoi, Pallab Dasgupta",
        "published": "2022-6-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wintechcon55229.2022.9832063"
    },
    {
        "id": 27381,
        "title": "Deep Reinforcement Learning for Stabilization of Large-scale Probabilistic Boolean Networks",
        "authors": "Sotiris Moschoyiannis, Evangelos Chatzaroulas, Vytenis Sliogeris, Yuhu Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe ability to direct a Probabilistic Boolean Network (PBN) to a desired state is important to applications such as targeted therapeutics in cancer biology. Reinforcement Learning (RL) has been proposed as a framework that solves a discrete-time optimal control problem cast as a Markov Decision Process. We focus on an integrative framework powered by a model-free deep RL method that can address different flavours of the control problem (e.g., withorwithout control inputs; attractor stateora subset of the state space as the target domain). The method is agnostic to the distribution of probabilities for the next state, hence it does not use the probability transition matrix. The time complexity is onlylinearon the time steps, or interactions between the agent (deep RL) and the environment (PBN), during training. Indeed, we explore thescalabilityof the deep RL approach to (set) stabilization of large-scale PBNs and demonstrate successful control on large networks, including a metastatic melanoma PBN with200 nodes.",
        "link": "http://dx.doi.org/10.1101/2022.10.21.513276"
    },
    {
        "id": 27382,
        "title": "Mutual Reinforcement Learning with Heterogenous Agents",
        "authors": "Cameron Reid, Snehasis Mukhopadhyay",
        "published": "2021-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smartcomp52413.2021.00081"
    },
    {
        "id": 27383,
        "title": "Evolvable Motion-planning Method using Deep Reinforcement Learning",
        "authors": "Kaichiro Nishi, Nobuaki Nakasu",
        "published": "2021-5-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9561602"
    },
    {
        "id": 27384,
        "title": "Monte-Carlo and Temporal-Difference for Prediction",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-11"
    },
    {
        "id": 27385,
        "title": "Tag-Aware Recommender System Based on Deep Reinforcement Learning",
        "authors": "Zhiruo Zhao, Lei Cao, Xiliang Chen, Zhixiong Xu",
        "published": "No Date",
        "citations": 2,
        "abstract": "Recently, the application of deep reinforcement learning in recommender system is flourishing and stands out by overcoming drawbacks of traditional methods and achieving high recommendation quality. The dynamics, long-term returns and sparse data issues in recommender system have been effectively solved. But the application of deep reinforcement learning brings problems of interpretability, overfitting, complex reward function design, and user cold start. This paper proposed a tag-aware recommender system based on deep reinforcement learning without complex function design, taking advantage of tags to make up for the interpretability problems existing in recommender system. Our experiment is carried out on MovieLens dataset. The result shows that, DRL based recommender system is superior than traditional algorithms in minimum error and the application of tags has little effect on accuracy when making up for interpretability. In addition, DRL based recommender system has excellent performance on user cold start problems.",
        "link": "http://dx.doi.org/10.20944/preprints202101.0176.v1"
    },
    {
        "id": 27386,
        "title": "ML-based Reinforcement Learning Approach for Power Management in SoCs",
        "authors": "David Akselrod",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/socc46988.2019.1570548498"
    },
    {
        "id": 27387,
        "title": "Model Free DEAP Controller Learned by Reinforcement Learning DDPG Algorithm",
        "authors": "Jakub Bernat, Dawid Apanasiewicz",
        "published": "2020-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/argencon49523.2020.9505344"
    },
    {
        "id": 27388,
        "title": "Deep Reinforcement Learning solution for Scheduling critical notifications in a Digital Twin cluster",
        "authors": "Mira Vrbaski, Miodrag Bolic, Shikharesh Majumdar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nA Scheduling approach for a Critical Monitoring System in a Digital Twin (DT) cluster based on Deep Reinforcement Learning (DRL) is presented. Recent advances in the DRL field inspired us to research how to build a solution that learns to manage the cloud container’s resources directly from experience. The paper presents a multi-objective Scheduling approach for containerized microservice Critical Notification system applications based on DRL (SCN-DRL), where Neural Networks (NN) are used RL Agents. This paper implements, compares and evaluates three Neural Networks (NN) that: a) provide scheduling of the DT cluster’s notification jobs, b) outperform state-of-art heuristics, and c) keep steady performance when notification workload increases from 10 to 90%. Furthermore, resilience to resource container failures is a critical component of the distributed system; our proposed research shows that SCN-DRL is resilient to sudden resource drops by 10%.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3667938/v1"
    },
    {
        "id": 27389,
        "title": "Deep Reinforcement Learning for Alleviation of Unbalanced Active Powers Using Distributed Batteries in LV Residential Distribution System",
        "authors": "Watcharakorn Pinthurat, Branislav Hredzak",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The high penetration and uneven distribution of single-phase rooftop PVs and load demands in power systems may lead to unbalanced active powers, adversely impacting power quality and system reliability. This paper introduces a strategy based on multi-agent deep reinforcement learning to address these unbalanced active powers. The approach involves deploying single-phase battery systems throughout the LV residential distribution system, subsidized by the utility. Initially, the unbalanced active powers are framed as a Markov game, which is then addressed using a multi-agent deep deterministic policy gradient algorithm. The strategy relies on local measurements, with agents' experiences centrally shared during training for cooperative tasks. Notably, information about the phase connections of the battery systems becomes unnecessary. The strategy learns from historical data, gradually mastering the process. Real data from rooftop PVs and demands in a four-wire LV residential distribution system validate the effectiveness of the proposed approach. Acting as adaptive agents, the battery systems collaboratively operate by adjusting active powers to minimize neutral current at the point of common connection.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24499090"
    },
    {
        "id": 27390,
        "title": "Deep Reinforcement Learning for Mobile Robot Navigation",
        "authors": "Martin Gromniak, Jonas Stenzel",
        "published": "2019-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acirs.2019.8935944"
    },
    {
        "id": 27391,
        "title": "Deep Reinforcement Learning Based Grid-Forming Inverter",
        "authors": "Ebrahim Balouji, Karl Bäckstrüm, Tomas McKelvey",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ias54024.2023.10406664"
    },
    {
        "id": 27392,
        "title": "Stabilized Platform Attitude Control Based on Deep Reinforcement Learning Using Disturbance Observer-Based",
        "authors": "Aiqing Huo, Xue Jiang, Shuhan Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn order to address the difficulties of attitude control for stabilized platform in rotary steerable drilling, including instability, difficult to control, and severe friction, we proposed a Disturbance Observer-Based Deep Deterministic Policy Gradient (DDPG_DOB) control algorithm. The stabilized platform in rotary steering drilling was taken as a research object. On the basis of building a stabilized platform controlled object model and a LuGre friction model, DDPG algorithm is used to design a deep reinforcement learning controller. After the overall framework of the stabilized platform control system was given, appropriate state vectors were selected, a reward function satisfying the system requirement was designed, an Actor-Critic network structure was constructed and the network parameters was updated. Moreover considering the non-linear friction disturbance that causes steady-state errors, oscillations, and hysteresis phenomena in the stabilized platform control system, a DDPG algorithm based on the disturbance observer was proposed to eliminate the effects of friction disturbance so that to enhance robustness and anti-interference ability of the stabilized platform control system. Experimental results show that the DDPG_DOB control method had good set-point control performance and tracking effect. The tracking error of the tool face angle can be maintained within ± 8.7% and the DDPG_DOB control method can effectively suppress friction interference and improve the nonlinear hysteresis phenomenon when the system is affected by friction interference,enhancing the robustness of the system.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2905841/v1"
    },
    {
        "id": 27393,
        "title": "Enhancing Hybrid System Based on Reinforcement Learning",
        "authors": "",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22266/ijies2024.0229.50"
    },
    {
        "id": 27394,
        "title": "Pairs Trading Using Clustering and Deep Reinforcement Learning",
        "authors": "Raktim Roychoudhury, Rahul Bhagtani, Aditya Daftari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4504599"
    },
    {
        "id": 27395,
        "title": "Recurrent neural networks that learn multi-step visual routines with reinforcement learning",
        "authors": "Sami Mollard, Catherine Wacongne, Sander Bohte, Pieter Roelfsema",
        "published": "No Date",
        "citations": 1,
        "abstract": "Many problems can be decomposed into series of subproblems that are solved sequentially. When the subproblems are solved, relevant intermediate results have to be stored and propagated from one subproblem to the next, until the overarching goal has been completed. The same applies for visual tasks that can be decomposed into sequences of elemental visual operations: experimental evidence suggests that for visual tasks, intermediate results are stored in working memory as an enhancement of neural activity in the visual cortex. The focus of enhanced activity is then available for subsequent subroutines to act upon. However, it remains unknown how those dynamics can emerge in neural networks that are trained with only rewards, as animals are. Here, we propose a new recurrent architecture capable of solving composite visual tasks in a reinforcement learning context. We trained neural networks on three visual tasks for which electrophysiological recordings of monkeys' visual cortex are available and updated weights following RELEARNN, a biologically plausible four-factor Hebbian learning rule, which is local both in time and space. The networks learned an abstract, general rule and selected the appropriate sequence of elemental operations, solely based on the characteristics of the visual stimuli and the reward structure. The activity of the units of the neural network resembled the activity of neurons in the visual cortex of monkeys solving the same tasks. Relevant information that needed to be exchanged between subroutines was maintained as a focus of enhanced activity and passed on to the subsequent subroutines. Our results demonstrate how a biologically plausible learning rule can train a recurrent neural network on multistep visual tasks.",
        "link": "http://dx.doi.org/10.1101/2023.07.03.547198"
    },
    {
        "id": 27396,
        "title": "FWA-RL: Fireworks Algorithm with Policy Gradient for Reinforcement Learning",
        "authors": "Maiyue Chen, Ying Tan",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec53210.2023.10254081"
    },
    {
        "id": 27397,
        "title": "Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning Approach",
        "authors": "Soroush Saghafian",
        "published": "2023-10-4",
        "citations": 0,
        "abstract": "A main research goal in various studies is to use an observational data set and provide a new set of counterfactual guidelines that can yield causal improvements. Dynamic Treatment Regimes (DTRs) are widely studied to formalize this process and enable researchers to find guidelines that are both personalized and dynamic. However, available methods in finding optimal DTRs often rely on assumptions that are violated in real-world applications (e.g., medical decision making or public policy), especially when (a) the existence of unobserved confounders cannot be ignored, and (b) the unobserved confounders are time varying (e.g., affected by previous actions). When such assumptions are violated, one often faces ambiguity regarding the underlying causal model that is needed to be assumed to obtain an optimal DTR. This ambiguity is inevitable because the dynamics of unobserved confounders and their causal impact on the observed part of the data cannot be understood from the observed data. Motivated by a case study of finding superior treatment regimes for patients who underwent transplantation in our partner hospital (Mayo Clinic) and faced a medical condition known as new-onset diabetes after transplantation, we extend DTRs to a new class termed Ambiguous Dynamic Treatment Regimes (ADTRs), in which the causal impact of treatment regimes is evaluated based on a “cloud” of potential causal models. We then connect ADTRs to Ambiguous Partially Observable Markov Decision Processes (APOMDPs) proposed by Saghafian (2018) , and consider unobserved confounders as latent variables but with ambiguous dynamics and causal effects on observed variables. Using this connection, we develop two reinforcement learning methods termed Direct Augmented V-Learning (DAV-Learning) and Safe Augmented V-Learning (SAV-Learning), which enable using the observed data to effectively learn an optimal treatment regime. We establish theoretical results for these learning methods, including (weak) consistency and asymptotic normality. We further evaluate the performance of these learning methods both in our case study (using clinical data) and in simulation experiments (using synthetic data). We find promising results for our proposed approaches, showing that they perform well even compared with an imaginary oracle who knows both the true causal model (of the data-generating process) and the optimal regime under that model. Finally, we highlight that our approach enables a two-way personalization; obtained treatment regimes can be personalized based on both patients’ characteristics and physicians’ preferences.This paper was accepted by David Simchi-Levi, data science.Supplemental Material: The data files and online appendix are available at https://doi.org/10.1287/mnsc.2022.00883 .",
        "link": "http://dx.doi.org/10.1287/mnsc.2022.00883"
    },
    {
        "id": 27398,
        "title": "Deep reinforcement learning for partial differential equation control",
        "authors": "Amir-massoud Farahmand, Saleh Nabi, Daniel N. Nikovski",
        "published": "2017-5",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2017.7963427"
    },
    {
        "id": 27399,
        "title": "A Reinforcement Learning based Eye-Gaze Behavior Tracking",
        "authors": "R Deepalakshmi, J Amudha",
        "published": "2021-10-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcat52182.2021.9587480"
    },
    {
        "id": 27400,
        "title": "A Novel Deep Reinforcement Learning Algorithm for Online Antenna Tuning",
        "authors": "Eren Balevi, Jeffrey G. Andrews",
        "published": "2019-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9013308"
    },
    {
        "id": 27401,
        "title": "Group value learned through interactions with members: A reinforcement learning account",
        "authors": "Leor M Hackel, Drew Kogon, David Amodio, Wendy Wood",
        "published": "No Date",
        "citations": 1,
        "abstract": "How do group-based interaction tendencies form through encounters with individual group members? In three experiments, in which participants interacted with group members in a reinforcement learning task presented as a money sharing game, participants formed instrumental reward associations with individual group members through direct interaction and feedback. Results revealed that individual-level reward learning generalized to a group-based representation, as indicated in self-reported group attitudes, trait impressions, and the tendency to choose subsequent interactions with novel members of the group. Moreover, group-based reward values continued to predict interactions with novel members after controlling for explicit attitudes and impressions, suggesting that instrumental learning contributes to an implicit form of group-based choice. Experiment 3 further demonstrated that group-based reward effects on interaction choices persisted even when group reward value was no longer predicted of positive outcomes, consistent with a habit-like expression of group bias. These results demonstrate a novel process of prejudice formation based on instrumental reward learning from direct interactions with individual group members. We discuss implications for existing theories of prejudice, the role of habit in intergroup bias, and intervention strategies to reduce prejudice.",
        "link": "http://dx.doi.org/10.31234/osf.io/25bc6"
    },
    {
        "id": 27402,
        "title": "Modeling and optimization of epidemiological control policies through reinforcement learning",
        "authors": "Ishir Rao, Achuth Rao",
        "published": "2023",
        "citations": 0,
        "abstract": "Pandemics involve the high transmission of a disease that impacts global and local health and economic patterns. The impact of a pandemic can be minimized by enforcing certain restrictions on a community. However, while minimizing infection and death rates, these restrictions can also lead to economic crises. Epidemiological models help propose pandemic control strategies based on non-pharmaceutical interventions such as social distancing, curfews, and lockdowns, reducing the economic impact of these restrictions. However, designing manual control strategies while considering disease spread and economic status is non-trivial. Optimal strategies can be designed through multi-objective reinforcement learning (MORL) models, which demonstrate how restrictions can be used to optimize the outcome of a pandemic. In this research, we utilized an epidemiological Susceptible, Exposed, Infected, Recovered, Deceased (SEIRD) model – a compartmental model for virtually simulating a pandemic day by day. We combined the SEIRD model with a deep double recurrent Q-network to train a reinforcement learning agent to enforce the optimal restriction on the SEIRD simulation based on a reward function. We tested two agents with unique reward functions and pandemic goals to obtain two strategies. The first agent placed long lockdowns to reduce the initial spread of the disease, followed by cyclical and shorter lockdowns to mitigate the resurgence of the disease. The second agent provided similar infection rates but an improved economy by implementing a 10-day lockdown and 20-day no-restriction cycle. This use of reinforcement learning and epidemiological modeling allowed for both economic and infection mitigation in multiple pandemic scenarios.",
        "link": "http://dx.doi.org/10.59720/22-157"
    },
    {
        "id": 27403,
        "title": "Inverse Reinforcement Learning for Text Summarization",
        "authors": "Yu Fu, Deyi Xiong, Yue Dong",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.436"
    },
    {
        "id": 27404,
        "title": "Application of Phase and Time-Optimized Parallel Deep Reinforcement Learning Model to Sequential Junctions",
        "authors": "ERHAN TURAN, Beşir DANDIL, Engin AVCI",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4494609"
    },
    {
        "id": 27405,
        "title": "Reinforcement learning for robust radar tracking",
        "authors": "Q. Cheng, L. Chen, K. Zhang, Z. Wang",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2021.0507"
    },
    {
        "id": 27406,
        "title": "Direct RL with Policy Gradient",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_7"
    },
    {
        "id": 27407,
        "title": "Reinforcement Learning for Decentralized Stochastic Control",
        "authors": "Bora Yongacoglu, Gurdal Arslan, Serdar Yuksel",
        "published": "2019-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc40024.2019.9030158"
    },
    {
        "id": 27408,
        "title": "Reinforcement Learning for Shared Driving",
        "authors": "Sangjin Ko, Reza Langari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.12.032"
    },
    {
        "id": 27409,
        "title": "Decentralized Deterministic Multi-Agent Reinforcement Learning",
        "authors": "Antoine Grosnit, Desmond Cai, Laura Wynter",
        "published": "2021-12-14",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683356"
    },
    {
        "id": 27410,
        "title": "Expert Initialized Hybrid Model-Based and Model-Free Reinforcement Learning",
        "authors": "Jeppe Langaa, Christoffer Sloth",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178306"
    },
    {
        "id": 27411,
        "title": "Reinforcement learning for Admission Control in 5G Wireless Networks",
        "authors": "Youri Raaijmakers, Silvio Mandelli, Mark Doll",
        "published": "2021-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685108"
    },
    {
        "id": 27412,
        "title": "Efficient Indoor Localization via Reinforcement Learning",
        "authors": "Dimitris Milioris",
        "published": "2019-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp.2019.8683598"
    },
    {
        "id": 27413,
        "title": "Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones",
        "authors": "Ugurkan Ates",
        "published": "2020-10-15",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asyu50717.2020.9259811"
    },
    {
        "id": 27414,
        "title": "Enviroment Representations with Bisimulation Metrics for Hierarchical Reinforcement Learning",
        "authors": "Chao Zhang",
        "published": "2023-1-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccrd56364.2023.10080162"
    },
    {
        "id": 27415,
        "title": "Risk-Sensitive Reinforcement Learning Via Entropic- VaR Optimization",
        "authors": "Xinyi Ni, Lifeng Lai",
        "published": "2022-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf56349.2022.10052026"
    },
    {
        "id": 27416,
        "title": "A Universal Offline Reinforcement Learning Model for Adaptive Traffic Signal Control at Heterogeneous Intersections",
        "authors": "Jiaming Lu, Ying Zeng, Feng Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4760719"
    },
    {
        "id": 27417,
        "title": "Performance of Reinforcement Learning on Traditional Video Games",
        "authors": "Yuanxi Sun",
        "published": "2021-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiam54119.2021.00063"
    },
    {
        "id": 27418,
        "title": "Author response: Neural computations underlying inverse reinforcement learning in the human brain",
        "authors": "Sven Collette, Wolfgang M Pauli, Peter Bossaerts, John O'Doherty",
        "published": "2017-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.29718.017"
    },
    {
        "id": 27419,
        "title": "Deep Reinforcement Learning for the Computation Offloading in MIMO-based Edge Computing",
        "authors": "Abdeladim Sadiki, Jamal Bentahar, Rachida Dssouli, Abdeslam En-Nouaary",
        "published": "No Date",
        "citations": 1,
        "abstract": "Multi-access Edge Computing (MEC) has recently emerged as a potential technology to serve the needs of mobile devices (MDs) in 5G and 6G cellular networks. By offloading tasks to high-performance servers installed at the edge of the wireless networks, resource-limited MDs can cope with the proliferation of the recent computationally-intensive applications. In this paper, we study the computation offloading problem in a massive multiple-input multiple-output (MIMO)-based MEC system where the base stations are equipped with a large number of antennas. Our objective is to minimize the power consumption and offloading delay at the MDs under the stochastic system environment. To this end, we formulate the problem as a Markov Decision Process (MDP) and propose two Deep Reinforcement Learning (DRL) strategies to learn the optimal offloading policy without any prior knowledge of the environment dynamics. First, a Deep Q-Network (DQN) strategy to solve the curse of the state space explosion is analyzed. Then, a more general Proximal Policy Optimization (PPO) strategy to solve the problem of discrete action space is introduced. Simulation results show that the proposed DRL-based strategies outperform the baseline and state-of-the-art algorithms. Moreover, our PPO algorithm exhibits stable performance and efficient offloading results compared to the benchmark DQN strategy.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16869119.v1"
    },
    {
        "id": 27420,
        "title": "Peer Review #1 of \"Heterogeneous mission planning for a single unmanned aerial vehicle (UAV) with attention-based deep reinforcement learning (v0.1)\"",
        "authors": "",
        "published": "2022-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1119v0.1/reviews/1"
    },
    {
        "id": 27421,
        "title": "Deep reinforcement learning for quantum gate control",
        "authors": "Zheng An, D. L. Zhou",
        "published": "2019-7-22",
        "citations": 71,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1209/0295-5075/126/60002"
    },
    {
        "id": 27422,
        "title": "Deep Reinforcement Learning for Greenhouse Climate Control",
        "authors": "Lu Wang, Xiaofeng He, Dijun Luo",
        "published": "2020-8",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icbk50248.2020.00073"
    },
    {
        "id": 27423,
        "title": "Minimize Pressure Difference Traffic Signal Control Based on Deep Reinforcement Learning",
        "authors": "Pengcheng Yu, Jie Luo",
        "published": "2022-7-25",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9901790"
    },
    {
        "id": 27424,
        "title": "Reliability-Driven Distribution Power Network Dynamic Reconfiguration in Presence of Distributed Generation by the Deep Reinforcement Learning method",
        "authors": "soheil malekshah",
        "published": "No Date",
        "citations": 0,
        "abstract": "with high penetration of distributed generation integrated with\ndistribution networks, the reliability of the network becoming\nincreasingly common. To address the reliability, this paper propose a\nnew approach based on the dynamic reconfiguration to improve the\nreliability of the distribution network in presence of distributed\ngeneration by Reinforcement Learning algorithm. In this research, the\ndeep reinforcement learning approach find the optimal switches to change\nthe power flow when the network reliability is reduced. The objective\nfunction as a reward in reinforcement learning algorithm included active\npower loss, voltage deviation, and reliability. Furthermore, the\nfailure-rate reduction strategy was computed as a reliability indices of\nthe load points in the distribution network. To calculate the\nprobabilistic load flow, the 2m Point Estimate Method (PEM) has been\nproposed. The proposed approach is evaluated using the IEEE 33-bus and\nthe IEEE118-bus. The suggested structure has been implemented by deep\nQ-learning method. The Guaranteed Convergence Particle Swarm\nOptimization (GCPSO) and Honeybee-Graph (HG) algorithms has been\nimplemented to comparison with presented approach. The results\nillustrate the usefulness of the methodology. In the IEEE – 33 bus test\nnetwork, by considering DQL method there was near to 7% improvement in\nthe voltage profile and 50 % reducing in active power loss for the bus\nnumber 2 and 5. Also, computing time in this method lower than the other\nalgorithms.",
        "link": "http://dx.doi.org/10.22541/au.170666974.47075939/v1"
    },
    {
        "id": 27425,
        "title": "Study of Uplink Resource Allocation for 5G IoT Services by Using Reinforcement Learning",
        "authors": "Yen-Wen Chen Yen-Wen Chen, ChengYu Tsai Yen-Wen Chen",
        "published": "2023-5",
        "citations": 0,
        "abstract": "\n                        <p>In order to support real time IoT services, the ultra Reliable and Low Latency Communications (uRLLC) was proposed in 5G wireless communication network. Different from the grant based access in 4G, the grant free technique is proposed in 5G to reduce the random access delay of uRLLC-required applications. This paper proposes the dedicated resource for exclusive access of individual UE and the shared resource pool for the contention of multiple UEs by adopting the reinforcement learning approach. The objective of this paper is to accomplish the uplink successful rate above 99.9% under certain transmission error probability. The proposed Prediction based Hybrid Resource Allocation (PHRA) scheme allocates the access resource in a heuristic manner by referring to the activity of UEs. The dedicated resource is mainly allocated to the high activity UEs and the initial transmission of UEs with medium activity while the shared resource pool is allocated for the re-transmission of medium activity UEs and low activity UEs by using the reinforcement learning model. The burst traffic model was applied during the exhaustive experiments. And the simulation results show that the proposed scheme achieves higher uplink packet delivery ratio and more effective resource utilization than the other schemes.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/160792642023052403013"
    },
    {
        "id": 27426,
        "title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-based Robot Navigation",
        "authors": "Thomas Chaffre, Julien Moras, Adrien Chan-Hon-Tong, Julien Marzat",
        "published": "2020",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009821603140323"
    },
    {
        "id": 27427,
        "title": "Offline Feature-Based Reinforcement Learning with Preprocessed Image Inputs for Liquid Pouring Control",
        "authors": "Stephan Pareigis, Jesus Hermosilla-Diaz, Jeeangh Reyes-Montiel, Fynn Maaß, Helen Haase, Maximilian Mang, Antonio Marin-Hernandez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012235700003543"
    },
    {
        "id": 27428,
        "title": "Learning to Rock-and-Walk: Dynamic, Non-Prehensile, and Underactuated Object Locomotion Through Reinforcement Learning",
        "authors": "Abdullah Nazir, Xu Pu, Juan Rojas, Jungwon Seo",
        "published": "2022-5-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9811554"
    },
    {
        "id": 27429,
        "title": "Step 6: Deep and Reinforcement Learning",
        "authors": "Manohar Swamynathan",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-4947-5_6"
    },
    {
        "id": 27430,
        "title": "An animal model of differential reinforcement of alternative behavior",
        "authors": "Clare J. Liddon, Michael E. Kelley, Christopher A. Podlesnik",
        "published": "2017-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.lmot.2017.04.001"
    },
    {
        "id": 27431,
        "title": "Learning-Based Privacy-Aware Maritime IoT Communications",
        "authors": "Liang Xiao, Helin Yang, Weihua Zhuang, Minghui Min",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-32138-2_3"
    },
    {
        "id": 27432,
        "title": "Learning wavefront shaping through reinforcement learning in a simulation environment",
        "authors": "Rahmetullah Varol, Tülay Aydın",
        "published": "2023-3-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2650698"
    },
    {
        "id": 27433,
        "title": "Future Trajectory Prediction via RNN and Maximum Margin Inverse Reinforcement Learning",
        "authors": "Dooseop Choi, Taeg-Hyun An, Kyounghwan Ahn, Jeongdan Choi",
        "published": "2018-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00026"
    },
    {
        "id": 27434,
        "title": "The Combination of Flappy Bird and Reinforcement Learning Using Q-learning",
        "authors": "Xinlu Dong, Xiaole Fan, Chenhao Sun, Zhengyang Xu",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "The development of artificial intelligence has expanded application fields gradually. In recent years, the combination of artificial intelligence and games attracted much attention. In this case, reinforcement learning is often chosen as an effective method to let the computer play the game by itself. In this study, the Q-learning algorithm from reinforcement learning was applied to Flappy Bird. There are the important factors of Q-learning, including state (S), action (A), reward (R), policy (π), time (t), Epsilon Greedy policy and Q-table. After that, a python class called “bot” was used, and it is used as an intelligent agent in the project. In order to implement the Q-learning algorithm, the state of each element of the game was adjusted continuously through the “mainGame” function of the Flappy Bird game. Finally, the survival reward was set to 1 and the death reward to -1000 to increase the survival rate. In addition, coins with different reward values were added to increase the difficulty of training. After training, the survival rate of the bird is improved, and it is clear that the reward value of gold coins will affect the agent's choice tendency. To combine artificial intelligence and games means that computers can be trained to deal with the complex and changing situations in games, and the progress will affect the application of artificial intelligence in real life more deeply.",
        "link": "http://dx.doi.org/10.54097/hset.v34i.5480"
    },
    {
        "id": 27435,
        "title": "LJIR: Learning Joint-Action Intrinsic Reward in cooperative multi-agent reinforcement learning",
        "authors": "Zihan Chen, Biao Luo, Tianmeng Hu, Xiaodong Xu",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.08.016"
    },
    {
        "id": 27436,
        "title": "Multi-modal Active Learning From Human Data: A Deep Reinforcement Learning Approach",
        "authors": "Ognjen Rudovic, Meiru Zhang, Bjorn Schuller, Rosalind Picard",
        "published": "2019-10-14",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3340555.3353742"
    },
    {
        "id": 27437,
        "title": "The Dynamic Mode Construction of Mixed English Learning Based on Reinforcement Learning",
        "authors": "Yongfen Liu, Weihua Tang, Piyush Kumar Pareek",
        "published": "2022-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmnwc56175.2022.10031831"
    },
    {
        "id": 27438,
        "title": "A Deep Reinforcement Learning Approach to the Flexible Flowshop Scheduling Problem with Makespan Minimization",
        "authors": "Jialin Zhu, Huangang Wang, Tao Zhang",
        "published": "2020-11-20",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls49620.2020.9275080"
    },
    {
        "id": 27439,
        "title": "Learning to Maximize Network Bandwidth Utilization with Deep Reinforcement Learning",
        "authors": "Hasibul Jamil, Elvis Rodrigues, Jacob Goldverg, Tevfik Kosar",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437507"
    },
    {
        "id": 27440,
        "title": "Monte-Carlo and Temporal-Difference for Control",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-12"
    },
    {
        "id": 27441,
        "title": "A Deep Reinforcement Learning Approach to Collision Avoidance",
        "authors": "Barton J. Bacon",
        "published": "2022-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2022-0623"
    },
    {
        "id": 27442,
        "title": "Energy-aware Multiple Access Using Deep Reinforcement Learning",
        "authors": "Hamid Reza Mazandarani, Siavash Khorsandi",
        "published": "2021-5-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icee52715.2021.9544417"
    },
    {
        "id": 27443,
        "title": "A Deep Reinforcement Learning based approach for movement training of neuro-musculoskeletal systems",
        "authors": "Raghu Sesha Iyengar, Kapardi Mallampalli, Mohan Raghavan",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMechanisms behind neural control of movement have been an active area of research. Goal-directed movement is a common experimental paradigm used to understand these mechanisms and relevant neural pathways. In this paper, we attempt to build an anatomically and physiologically realistic model of spinal cord along with the relevant circuitry and interface it with a musculoskeletal model of an upper limb, using the NEUROiD platform. The neuronal model (simulated on NEURON) and the musculoskeletal model (simulated on OpenSim) are cosimulated on NEUROiD. We then use Deep Reinforcement Learning to obtain a functionally equivalent model of the supraspinal components and the descending cortical activations feeding into the last-order interneurons and motoneurons. Uniplanar goal directed movement of the elbow joint was used as the goal for the learning algorithm. Key aspects of our work are: (1) Our solution converges naturally to the triphasic response observed in goal directed tasks (2) Gradually increasing the complexity of task helped in faster learning (3) In response to corticospinal inputs, our model could produce movements on which it was not explicitly trained, but were close to the trained movements. Being able to generate movements on which the model was not explicitly trained, implies that the movement repertoire that a biomimetic model needs to learn, could be much smaller than the complete set of movements it can execute. We hope that this will lead to building larger and complex biomimetic systems, one block at a time.",
        "link": "http://dx.doi.org/10.1101/2021.03.28.437396"
    },
    {
        "id": 27444,
        "title": "Ship Trajectory Tracking Using Improved Simulated Annealing and Reinforcement Learning",
        "authors": "Yuewen Yu",
        "published": "2018-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icinfa.2018.8812464"
    },
    {
        "id": 27445,
        "title": "Assembly Combinatorial Optimisation with Deep Reinforcement Learning",
        "authors": "B. Forget, K. Shirvan, M. Radaideh",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.13182/t123-33259"
    },
    {
        "id": 27446,
        "title": "Voltage Control-Based Ancillary Service Using Deep Reinforcement Learning",
        "authors": "Oleh Lukianykhin, Tetiana Bogodorova",
        "published": "2021-4-18",
        "citations": 2,
        "abstract": "Ancillary services rely on operating reserves to support an uninterrupted electricity supply that meets demand. One of the hidden reserves of the grid is in thermostatically controlled loads. To efficiently exploit these reserves, a new realization of control of voltage in the allowable range to follow the set power reference is proposed. The proposed approach is based on the deep reinforcement learning (RL) algorithm. Double DQN is utilized because of the proven state-of-the-art level of performance in complex control tasks, native handling of continuous environment state variables, and model-free application of the trained DDQN to the real grid. To evaluate the deep RL control performance, the proposed method was compared with a classic proportional control of the voltage change according to the power reference setup. The solution was validated in setups with a different number of thermostatically controlled loads (TCLs) in a feeder to show its generalization capabilities. In this article, the particularities of deep reinforcement learning application in the power system domain are discussed along with the results achieved by such an RL-powered demand response solution. The tuning of hyperparameters for the RL algorithm was performed to achieve the best performance of the double deep Q-network (DDQN) algorithm. In particular, the influence of a learning rate, a target network update step, network hidden layer size, batch size, and replay buffer size were assessed. The achieved performance is roughly two times better than the competing approach of optimal control selection within the considered time interval of the simulation. The decrease in deviation of the actual power consumption from the reference power profile is demonstrated. The benefit in costs is estimated for the presented voltage control-based ancillary service to show the potential impact.",
        "link": "http://dx.doi.org/10.3390/en14082274"
    },
    {
        "id": 27447,
        "title": "Human-in-the-loop reinforcement learning",
        "authors": "Huanghuang Liang, Lu Yang, Hong Cheng, Wenzhe Tu, Mengjie Xu",
        "published": "2017-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2017.8243575"
    },
    {
        "id": 27448,
        "title": "Evaluating Reinforcement Learning Methods for Bundle Routing Control",
        "authors": "Gandhimathi Velusamy, Ricardo Lent",
        "published": "2019-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccaaw.2019.8904909"
    },
    {
        "id": 27449,
        "title": "Decentralized Multi-Agent Reinforcement Learning for Continuous-Space Stochastic Games",
        "authors": "Awni Altabaa, Bora Yongacoglu, Serdar Yüksel",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10155828"
    },
    {
        "id": 27450,
        "title": "Energy and Comfort Aware Operation of Multi-Zone Hvac System Through Preference-Inspired Deep Reinforcement Learning",
        "authors": "Can Cui, Jing Xue",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4545929"
    },
    {
        "id": 27451,
        "title": "Few-Shot System Identification for Reinforcement Learning",
        "authors": "Karim Farid, Nourhan Sakr",
        "published": "2021-7-16",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acirs52449.2021.9519314"
    },
    {
        "id": 27452,
        "title": "A joint task caching and computation offloading scheme based on Deep Reinforcement Learning",
        "authors": "Huizi Tian, Lin Zhu, Long Tan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nConsidering the dynamic variability of the vehicular edge environment and the limited edge servers resources, this paper proposes a joint task caching and computation offloading scheme based on deep reinforcement learning (DRL). Considering that the motion trajectories of different vehicles overlap and their task requests may be the same, this paper designs a vehicle-edge-cloud computing framework to fully use the cache resources of vehicles, edge servers, and clouds to reduce task processing delays and energy consumption. Secondly, this paper adopts a method of partial offloading and collaboration between edge servers, which fully utilizes the computing resources of vehicles, edge servers, and the cloud, reducing the burden of vehicles and edge servers. In addition, this paper proposes a DRL-based task offloading scheme to obtain better task caching and offloading strategies. The simulation results show that the scheme proposed in this article performs better compared to other schemes and effectively reduces the latency and energy consumption of task processing.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3972282/v1"
    },
    {
        "id": 27453,
        "title": "Caac: Effective Reinforcement Learning for Sparse Reward Environments",
        "authors": "Kun Liu, Libing Wu, Zhuangzhuang Zhang, Chao Ma, Na Lu, Xuejiang Wei",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4418679"
    },
    {
        "id": 27454,
        "title": "Credit-of-Q-value for Multi-Agent Reinforcement Learning",
        "authors": "Shuaibin Li, Xiu Li, Jinqiang Cui",
        "published": "2022-7-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9902237"
    },
    {
        "id": 27455,
        "title": "Adaptive Tsallis Entropy Regularization for Efficient Reinforcement Learning",
        "authors": "Kyungjae Lee",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictc55196.2022.9952774"
    },
    {
        "id": 27456,
        "title": "RL-GRIT: Reinforcement Learning for Grammar Inference",
        "authors": "Walt Woods",
        "published": "2021-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spw53761.2021.00031"
    },
    {
        "id": 27457,
        "title": "Active Object Searching on Mobile Robot Using Reinforcement Learning",
        "authors": "Nuo Xu",
        "published": "2021-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cds52072.2021.00058"
    },
    {
        "id": 27458,
        "title": "Adjusting the game difficulty by changing AI behaviors with Reinforcement Learning",
        "authors": "Louis Schmidt, Taichi Watanabe, Koji Mikami",
        "published": "2020-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nicoint50878.2020.00030"
    },
    {
        "id": 27459,
        "title": "Aufgabenorientierte Konfiguration autonomer Robotersysteme zur industriellen Anwendung von Reinforcement Learning",
        "authors": "M. Röhler, J. Berg, G. Reinhart",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.51202/9783181023518-753"
    },
    {
        "id": 27460,
        "title": "A State Representation for Reinforcement Learning and Decision-Making in the Orbitofrontal Cortex",
        "authors": "Nicolas W. Schuck, Robert Wilson, Yael Niv",
        "published": "No Date",
        "citations": 6,
        "abstract": "AbstractDespite decades of research, the exact ways in which the orbitofrontal cortex (OFC) influences cognitive function have remained mysterious. Anatomically, the OFC is characterized by remarkably broad connectivity to sensory, limbic and subcortical areas, and functional studies have implicated the OFC in a plethora of functions ranging from facial processing to value-guided choice. Notwithstanding such diversity of findings, much research suggests that one important function of the OFC is to support decision making and reinforcement learning. Here, we describe a novel theory that posits that OFC’s specific role in decision-making is to provide an up-to-date representation of task-related information, called a state representation. This representation reflects a mapping between distinct task states and sensory as well as unobservable information. We summarize evidence supporting the existence of such state representations in rodent and human OFC and argue that forming these state representations provides a crucial scaffold that allows animals to efficiently perform decision making and reinforcement learning in high-dimensional and partially observable environments. Finally, we argue that our theory offers an integrating framework for linking the diversity of functions ascribed to OFC and is in line with its wide ranging connectivity.",
        "link": "http://dx.doi.org/10.1101/210591"
    },
    {
        "id": 27461,
        "title": "Enhanced Reinforcement Learning with Targeted Dropout",
        "authors": "Mark Jovic A. Daday, Kristoffer Franz Mari R. Millado",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icd47981.2019.9105750"
    },
    {
        "id": 27462,
        "title": "Intermittent absence of control during reinforcement learning interferes with Pavlovian bias in action selection",
        "authors": "Gábor Csifcsák, Eirik Melsæter, Matthias Mittner",
        "published": "No Date",
        "citations": 3,
        "abstract": "The ability to control the occurrence of rewarding and punishing events is crucial for our well-being. Two ways to optimize performance are to follow heuristics like Pavlovian biases to approach reward and avoid loss, or to rely more on slowly accumulated stimulus-action associations. Although reduced control over outcomes has been linked to suboptimal decision-making in clinical conditions associated with learned helplessness, it is unclear how uncontrollability of the environment is related to the arbitration between different response strategies.This study directly tested whether a behavioral manipulation designed to induce learned helplessness in healthy adults (intermittent loss of control over feedback in a reinforcement learning task; “yoking”) would modulate the magnitude of Pavlovian bias and the neurophysiological signature of cognitive control (frontal midline theta power) in healthy adults. Using statistical analysis and computational modeling of behavioral data and electroencephalographic signals, we found stronger Pavlovian influences and alterations in frontal theta activity in the yoked group. However, these effects were not accompanied by reduced performance in experimental blocks with regained control, indicating that our behavioral manipulation was not potent enough for inducing helplessness and impaired coping ability with task demands.We conclude that the level of contingency between instrumental choices and rewards/punishments modulates Pavlovian bias during value-based decision-making, probably via interfering with the implementation of cognitive control. These findings might have implications for understanding the mechanisms underlying helplessness in various psychiatric conditions.",
        "link": "http://dx.doi.org/10.31234/osf.io/jpq6f"
    },
    {
        "id": 27463,
        "title": "Reinforcement Learning Based Dynamic Inverse Attitude Control of Near-space Vehicle",
        "authors": "Yaohua Shen, Mou Chen",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9189285"
    },
    {
        "id": 27464,
        "title": "A Reinforcement Learning-based Sequence Generation Algorithm for Password Guessing",
        "authors": "Zheng Chen, Xuliang Zhang",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10000814"
    },
    {
        "id": 27465,
        "title": "State Constraints and Safety Consideration",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_9"
    },
    {
        "id": 27466,
        "title": "Reinforcement Learning for Hybrid Disassembly Line Balancing Problems",
        "authors": "Jiacun Wang, Guipeng Xi, Xiwang Guo, Shixin Liu, Shujin Qin, Henry Han",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4551988"
    },
    {
        "id": 27467,
        "title": "Game Playing Agent for 2048 using Deep Reinforcement Learning",
        "authors": "Varun Kaundinya, Shubham Jain, Sumanth Saligram, C K Vanamala, Avinash B",
        "published": "2018-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21467/proceedings.1.57"
    },
    {
        "id": 27468,
        "title": "Reinforcement Learning Based Proactive Monitor Channel Allocation",
        "authors": "Tuo Wu",
        "published": "2021-3-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iaeac50856.2021.9391117"
    },
    {
        "id": 27469,
        "title": "Grid-Supportive Load Frequency Control using Deep Reinforcement Learning",
        "authors": "Faisal Albeladi, Masoud Barati",
        "published": "2023-4-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/kpec58008.2023.10215451"
    },
    {
        "id": 27470,
        "title": "Decentralized Safe Reinforcement Learning for Voltage Control",
        "authors": "Wenqi Cui, Jiayi Li, Baosen Zhang",
        "published": "2022-6-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867766"
    },
    {
        "id": 27471,
        "title": "Reinforcement Learning Tuned PI Controller for Two Tank Interacting Hybrid System",
        "authors": "Billa Harikrishna, Kanagalakshmi S",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/silcon59133.2023.10404691"
    },
    {
        "id": 27472,
        "title": "A Reinforcement Learning Based on Book Recommendation System",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ajcis.2023.061303"
    },
    {
        "id": 27473,
        "title": "An Intelligent Interactive Conflict Solver Incorporating Air Traffic Controllers’ Preferences Using Reinforcement Learning",
        "authors": "",
        "published": "2019-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnsurv.2019.8735305"
    },
    {
        "id": 27474,
        "title": "Deep reinforcement learning in stock portfolios",
        "authors": "Yue Quan",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "This paper investigates stock portfolios by application of Deep Reinforcement Learning (DRL) Models to achieve an optimal tactical asset allocation. The research problem is described as an optimization scenario that seeks to maximize the portfolio risk adjusted returns for a given portfolio asset allocation. The problem is set up with an initial capital investment which is invested in a set of assets. The initial strategic allocation is determined, which in our case is the equal weight allocation, and all of the capital is invested in the set of assets. At each point in time, the assets are reallocated according to the allocation which will increase the portfolio value. Two DRL models are implemented. The performance of the DRL models is compared with the uniform weights portfolio. The results show that, generally, two DRL models have higher cumulative returns.",
        "link": "http://dx.doi.org/10.54254/2755-2721/6/20230866"
    },
    {
        "id": 27475,
        "title": "Energy efficient online control of a water distribution network based on Deep Reinforcement Learning",
        "authors": "Baltasar Beferull-Lozano, Jyotirmoy Bhardwaj, Helge Liltvedt",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Data is obtained via Deep Reinforcement Learning Environment</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23896902"
    },
    {
        "id": 27476,
        "title": "Deep Reinforcement Learning Based End-to-End Stock Trading Strategy",
        "authors": "Yonghao Wang, Qinke Peng, Tian Han, Haozhou Li, Yiqing Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4384871"
    },
    {
        "id": 27477,
        "title": "Multiagent-Based Deep Reinforcement Learning Framework for Multi-Asset Adaptive Trading and Portfolio Management",
        "authors": "LI CHEN CHENG, Jian-Shiou Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4385481"
    },
    {
        "id": 27478,
        "title": "A Reinforcement Learning-Based Neighborhood Search Operator for Multi-Modal Optimization and its Applications",
        "authors": "Jiale Hong, Bo Shen, Anqi Pan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4423357"
    },
    {
        "id": 27479,
        "title": "Behaviourism: Learning through Imitation and Reinforcement",
        "authors": "Susan Young",
        "published": "2023-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003331193-4"
    },
    {
        "id": 27480,
        "title": "Applications of deep reinforcement learning  Alphago",
        "authors": "Yingchen Liu",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "With the progress of the times, the field of artificial intelligence (AI) has become one of the hottest fields in the 21st century. Currently, artificial intelligence is successfully used in the retail, financial, and medical industries. Especially in 2016, Google's DeepMind used deep reinforcement learning to train AlphaGo and defeated Lee Sedol, which propelled the field into the public eye. Most people are aware of artificial intelligence, but few understand it. This article will focus on analyzing the literature \"Mastering the game of Go with deep neural networks and tree search\" and other related articles to introduce the basics of deep reinforcement learning and AlphaGo. Finally, readers will understand how artificial intelligence can successfully imitate humans and defeat humans in Go.",
        "link": "http://dx.doi.org/10.54254/2755-2721/5/20230668"
    },
    {
        "id": 27481,
        "title": "A Machine of Few Words: Interactive Speaker Recognition with Reinforcement Learning",
        "authors": "Mathieu Seurin, Florian Strub, Philippe Preux, Olivier Pietquin",
        "published": "2020-10-25",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2892"
    },
    {
        "id": 27482,
        "title": "Changes in the Dopaminergic circuitry and Adult Neurogenesis linked to Reinforcement Learning in Corvids",
        "authors": "Pooja Parishar, Madhumita Rajagopalan, Soumya Iyengar",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe caudolateral nidopallium (NCL, an analogue of the prefrontal cortex) is known to be involved in learning, memory, and discrimination in crows, whereas the involvement of other brain regions in these phenomena are unknown. However, recent studies on pigeons have demonstrated that besides NCL, basal ganglia-thalamocortical loops connected to this region play are also crucial for learning. The present study demonstrates that besides NCL, other parts of the caudal nidopallium (NC), avian basal ganglia, and intriguingly, vocal control regions in house crows (Corvus splendens), are involved in visual discrimination. We have also found that training on the visual discrimination task can be correlated to neurite pruning in mature dopaminoceptive neurons and immature doublecortin-positive neurons in the NC of house crows. Furthermore, there is an increase in the incorporation of new neurons throughout NC and the medial striatum which can also be linked to learning. For the first time, our results demonstrate that a combination of structural changes in mature and immature neurons and adult neurogenesis are linked to learning in corvids.",
        "link": "http://dx.doi.org/10.1101/2023.08.31.555829"
    },
    {
        "id": 27483,
        "title": "Deep Reinforcement Learning for Alleviation of Unbalanced Active Powers Using Distributed Batteries in LV Residential Distribution System",
        "authors": "Watcharakorn Pinthurat, Branislav Hredzak",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The high penetration and uneven distribution of single-phase rooftop PVs and load demands in power systems may lead to unbalanced active powers, adversely impacting power quality and system reliability. This paper introduces a strategy based on multi-agent deep reinforcement learning to address these unbalanced active powers. The approach involves deploying single-phase battery systems throughout the LV residential distribution system, subsidized by the utility. Initially, the unbalanced active powers are framed as a Markov game, which is then addressed using a multi-agent deep deterministic policy gradient algorithm. The strategy relies on local measurements, with agents' experiences centrally shared during training for cooperative tasks. Notably, information about the phase connections of the battery systems becomes unnecessary. The strategy learns from historical data, gradually mastering the process. Real data from rooftop PVs and demands in a four-wire LV residential distribution system validate the effectiveness of the proposed approach. Acting as adaptive agents, the battery systems collaboratively operate by adjusting active powers to minimize neutral current at the point of common connection.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24499090.v2"
    },
    {
        "id": 27484,
        "title": "Risk-Sensitive Policy with Distributional Reinforcement Learning",
        "authors": "Thibaut Théate, Damien Ernst",
        "published": "2023-6-30",
        "citations": 2,
        "abstract": "Classical reinforcement learning (RL) techniques are generally concerned with the design of decision-making policies driven by the maximisation of the expected outcome. Nevertheless, this approach does not take into consideration the potential risk associated with the actions taken, which may be critical in certain applications. To address that issue, the present research work introduces a novel methodology based on distributional RL to derive sequential decision-making policies that are sensitive to the risk, the latter being modelled by the tail of the return probability distribution. The core idea is to replace the Q function generally standing at the core of learning schemes in RL by another function, taking into account both the expected return and the risk. Named the risk-based utility function U, it can be extracted from the random return distribution Z naturally learnt by any distributional RL algorithm. This enables the spanning of the complete potential trade-off between risk minimisation and expected return maximisation, in contrast to fully risk-averse methodologies. Fundamentally, this research yields a truly practical and accessible solution for learning risk-sensitive policies with minimal modification to the distributional RL algorithm, with an emphasis on the interpretability of the resulting decision-making process.",
        "link": "http://dx.doi.org/10.3390/a16070325"
    },
    {
        "id": 27485,
        "title": "Reinforcement Learning Quantum Local Search",
        "authors": "Chen-Yu Liu, Hsi-Sheng Goan",
        "published": "2023-9-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/qce57702.2023.10226"
    },
    {
        "id": 27486,
        "title": "Group and Socially Aware Multi-Agent Reinforcement Learning",
        "authors": "Manav Vallecha, Rahul Kala",
        "published": "2022-6-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/med54222.2022.9837206"
    },
    {
        "id": 27487,
        "title": "Informative Path Planning for Mobile Sensing with Reinforcement Learning",
        "authors": "Yongyong Wei, Rong Zheng",
        "published": "2020-7",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infocom41043.2020.9155528"
    },
    {
        "id": 27488,
        "title": "Hierarchical Control of Multi-Agent Systems using Online Reinforcement Learning",
        "authors": "He Bai, Jemin George, Aranya Chakrabortty",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147329"
    },
    {
        "id": 27489,
        "title": "Improving Pairs Trading Strategies via Reinforcement Learning",
        "authors": "Cheng Wang, Patrik Sandas, Peter Beling",
        "published": "2021-5-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icapai49758.2021.9462067"
    },
    {
        "id": 27490,
        "title": "Cooperative Perception with Deep Reinforcement Learning for Connected Vehicles",
        "authors": "Shunsuke Aoki, Takamasa Higuchi, Onur Altintas",
        "published": "2020-10-19",
        "citations": 53,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv47402.2020.9304570"
    },
    {
        "id": 27491,
        "title": "Recent Advances in Reinforcement Learning in Finance",
        "authors": "Ben M. Hambly, Renyuan Xu, Huining Yang",
        "published": "No Date",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3971071"
    },
    {
        "id": 27492,
        "title": "DeepWalk: Omnidirectional Bipedal Gait by Deep Reinforcement Learning",
        "authors": "Diego Rodriguez, Sven Behnke",
        "published": "2021-5-30",
        "citations": 29,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9561717"
    },
    {
        "id": 27493,
        "title": "Reinforcement Learning Control for Robot Arm Grasping Based on Improved DDPG",
        "authors": "Guangjun Qi, Yuan Li",
        "published": "2021-7-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9550413"
    },
    {
        "id": 27494,
        "title": "Evolutionary Reinforcement Learning for the Coordination of Swarm UAVs",
        "authors": "Umut Can Altin",
        "published": "2020-10-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu49456.2020.9302227"
    },
    {
        "id": 27495,
        "title": "Scalable evolutionary hierarchical reinforcement learning",
        "authors": "Sasha Abramowitz, Geoff Nitschke",
        "published": "2022-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3520304.3528937"
    },
    {
        "id": 27496,
        "title": "Monte Carlo Methods",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_4"
    },
    {
        "id": 27497,
        "title": "Deep Reinforcement Learning Based Dynamic Multichannel Access in HetNets",
        "authors": "Shaoyang Wang, Tiejun Lv",
        "published": "2019-4",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc.2019.8885857"
    },
    {
        "id": 27498,
        "title": "Development of Apple Detection System and Reinforcement Learning for Apple Manipulator",
        "authors": "Nikita Andriyanov",
        "published": "2023-2-1",
        "citations": 5,
        "abstract": "Modern deep learning systems make it possible to develop increasingly intelligent solutions in various fields of science and technology. The electronics of single board computers facilitate the control of various robotic solutions. At the same time, the implementation of such tasks does not require a large amount of resources. However, deep learning models still require a high level of computing power. Thus, the effective control of an intelligent robot manipulator is possible when a computationally complex deep learning model on GPU graphics devices and a mechanics control unit on a single-board computer work together. In this regard, the study is devoted to the development of a computer vision model for estimation of the coordinates of objects of interest, as well as the subsequent recalculation of coordinates relative to the control of the manipulator to form a control action. In addition, in the simulation environment, a reinforcement learning model was developed to determine the optimal path for picking apples from 2D images. The detection efficiency on the test images was 92%, and in the laboratory it was possible to achieve 100% detection of apples. In addition, an algorithm has been trained that provides adequate guidance to apples located at a distance of 1 m along the Z axis. Thus, the original neural network used to recognize apples was trained using a big image dataset, algorithms for estimating the coordinates of apples were developed and investigated, and the use of reinforcement learning was suggested to optimize the picking policy.",
        "link": "http://dx.doi.org/10.3390/electronics12030727"
    },
    {
        "id": 27499,
        "title": "Traffic light control with reinforcement learning",
        "authors": "Taoyu Pan",
        "published": "2024-2-26",
        "citations": 0,
        "abstract": "Urban traffic signal optimization is important for alleviating congestion in urban transportation systems. This study proposes a real-time traffic light control algorithm based on deep Q learning with a reward function that accounts for queue lengths, delays, travel times, and throughput. The model dynamically decides phase changes based on current traffic conditions. The training of the deep Q network involves an offline stage from pre-generated data with fixed signal timing and an online stage using real-time traffic data. A deep Q network structure with aphase gate component is used to simplify the model's learning task under different phases. Amemory palace\" mechanism is used to address sample imbalance during the training process. Both synthetic and real-world traffic flow data are used to validate our approach under an urban road intersection scenario in Hangzhou, China. Results demonstrate significant performance improvements of the proposed method in reducing vehicle waiting time (57.1% to 100%), queue lengths (40.9% to 100%), and total travel time (16.8% to 68.0%) compared to traditional fixed signal timing plans.",
        "link": "http://dx.doi.org/10.54254/2755-2721/43/20230804"
    },
    {
        "id": 27500,
        "title": "Deep Reinforcement Learning Based Power Allocation for D2D Network",
        "authors": "Zhengran Bi, Wenan Zhou",
        "published": "2020-5",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2020-spring48590.2020.9129537"
    },
    {
        "id": 27501,
        "title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-based Robot Navigation",
        "authors": "Thomas Chaffre, Julien Moras, Adrien Chan-Hon-Tong, Julien Marzat",
        "published": "2020",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009821603140323"
    },
    {
        "id": 27502,
        "title": "Effect of Interaction Design of Reinforcement Learning Agents on Human Satisfaction in Partially Observable Domains",
        "authors": "Divya Srivastava, Spencer Frazier, Mark Riedl, Karen Feigh",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010240101740181"
    },
    {
        "id": 27503,
        "title": "Replacing the Reinforcement Learning (RL) to the Auto Reinforcement Learning (AutoRL) Algorithms to Find the Optimal Structure of Business Processes in the Bank",
        "authors": "Andrey A. Bugaenko",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-90318-3_2"
    },
    {
        "id": 27504,
        "title": "Delay of reinforcement versus rate of reinforcement in Pavlovian conditioning.",
        "authors": "Joseph M. Austen, David J. Sanderson",
        "published": "2019-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1037/xan0000199"
    },
    {
        "id": 27505,
        "title": "Monte-Carlo and Temporal-Difference for Control",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-12"
    },
    {
        "id": 27506,
        "title": "A Deep Reinforcement Learning Approach to Collision Avoidance",
        "authors": "Barton J. Bacon",
        "published": "2022-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2022-0623"
    },
    {
        "id": 27507,
        "title": "Energy-aware Multiple Access Using Deep Reinforcement Learning",
        "authors": "Hamid Reza Mazandarani, Siavash Khorsandi",
        "published": "2021-5-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icee52715.2021.9544417"
    },
    {
        "id": 27508,
        "title": "A Deep Reinforcement Learning based approach for movement training of neuro-musculoskeletal systems",
        "authors": "Raghu Sesha Iyengar, Kapardi Mallampalli, Mohan Raghavan",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMechanisms behind neural control of movement have been an active area of research. Goal-directed movement is a common experimental paradigm used to understand these mechanisms and relevant neural pathways. In this paper, we attempt to build an anatomically and physiologically realistic model of spinal cord along with the relevant circuitry and interface it with a musculoskeletal model of an upper limb, using the NEUROiD platform. The neuronal model (simulated on NEURON) and the musculoskeletal model (simulated on OpenSim) are cosimulated on NEUROiD. We then use Deep Reinforcement Learning to obtain a functionally equivalent model of the supraspinal components and the descending cortical activations feeding into the last-order interneurons and motoneurons. Uniplanar goal directed movement of the elbow joint was used as the goal for the learning algorithm. Key aspects of our work are: (1) Our solution converges naturally to the triphasic response observed in goal directed tasks (2) Gradually increasing the complexity of task helped in faster learning (3) In response to corticospinal inputs, our model could produce movements on which it was not explicitly trained, but were close to the trained movements. Being able to generate movements on which the model was not explicitly trained, implies that the movement repertoire that a biomimetic model needs to learn, could be much smaller than the complete set of movements it can execute. We hope that this will lead to building larger and complex biomimetic systems, one block at a time.",
        "link": "http://dx.doi.org/10.1101/2021.03.28.437396"
    },
    {
        "id": 27509,
        "title": "Ship Trajectory Tracking Using Improved Simulated Annealing and Reinforcement Learning",
        "authors": "Yuewen Yu",
        "published": "2018-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icinfa.2018.8812464"
    },
    {
        "id": 27510,
        "title": "Assembly Combinatorial Optimisation with Deep Reinforcement Learning",
        "authors": "B. Forget, K. Shirvan, M. Radaideh",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.13182/t123-33259"
    },
    {
        "id": 27511,
        "title": "Voltage Control-Based Ancillary Service Using Deep Reinforcement Learning",
        "authors": "Oleh Lukianykhin, Tetiana Bogodorova",
        "published": "2021-4-18",
        "citations": 2,
        "abstract": "Ancillary services rely on operating reserves to support an uninterrupted electricity supply that meets demand. One of the hidden reserves of the grid is in thermostatically controlled loads. To efficiently exploit these reserves, a new realization of control of voltage in the allowable range to follow the set power reference is proposed. The proposed approach is based on the deep reinforcement learning (RL) algorithm. Double DQN is utilized because of the proven state-of-the-art level of performance in complex control tasks, native handling of continuous environment state variables, and model-free application of the trained DDQN to the real grid. To evaluate the deep RL control performance, the proposed method was compared with a classic proportional control of the voltage change according to the power reference setup. The solution was validated in setups with a different number of thermostatically controlled loads (TCLs) in a feeder to show its generalization capabilities. In this article, the particularities of deep reinforcement learning application in the power system domain are discussed along with the results achieved by such an RL-powered demand response solution. The tuning of hyperparameters for the RL algorithm was performed to achieve the best performance of the double deep Q-network (DDQN) algorithm. In particular, the influence of a learning rate, a target network update step, network hidden layer size, batch size, and replay buffer size were assessed. The achieved performance is roughly two times better than the competing approach of optimal control selection within the considered time interval of the simulation. The decrease in deviation of the actual power consumption from the reference power profile is demonstrated. The benefit in costs is estimated for the presented voltage control-based ancillary service to show the potential impact.",
        "link": "http://dx.doi.org/10.3390/en14082274"
    },
    {
        "id": 27512,
        "title": "Human-in-the-loop reinforcement learning",
        "authors": "Huanghuang Liang, Lu Yang, Hong Cheng, Wenzhe Tu, Mengjie Xu",
        "published": "2017-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2017.8243575"
    },
    {
        "id": 27513,
        "title": "Evaluating Reinforcement Learning Methods for Bundle Routing Control",
        "authors": "Gandhimathi Velusamy, Ricardo Lent",
        "published": "2019-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccaaw.2019.8904909"
    },
    {
        "id": 27514,
        "title": "Decentralized Multi-Agent Reinforcement Learning for Continuous-Space Stochastic Games",
        "authors": "Awni Altabaa, Bora Yongacoglu, Serdar Yüksel",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10155828"
    },
    {
        "id": 27515,
        "title": "Energy and Comfort Aware Operation of Multi-Zone Hvac System Through Preference-Inspired Deep Reinforcement Learning",
        "authors": "Can Cui, Jing Xue",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4545929"
    },
    {
        "id": 27516,
        "title": "Recent Advances in Reinforcement Learning in Finance",
        "authors": "Ben M. Hambly, Renyuan Xu, Huining Yang",
        "published": "No Date",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3971071"
    },
    {
        "id": 27517,
        "title": "Improving Pairs Trading Strategies via Reinforcement Learning",
        "authors": "Cheng Wang, Patrik Sandas, Peter Beling",
        "published": "2021-5-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icapai49758.2021.9462067"
    },
    {
        "id": 27518,
        "title": "DeepWalk: Omnidirectional Bipedal Gait by Deep Reinforcement Learning",
        "authors": "Diego Rodriguez, Sven Behnke",
        "published": "2021-5-30",
        "citations": 29,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9561717"
    },
    {
        "id": 27519,
        "title": "An Intelligent Interactive Conflict Solver Incorporating Air Traffic Controllers’ Preferences Using Reinforcement Learning",
        "authors": "",
        "published": "2019-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnsurv.2019.8735305"
    },
    {
        "id": 27520,
        "title": "Deep reinforcement learning in stock portfolios",
        "authors": "Yue Quan",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "This paper investigates stock portfolios by application of Deep Reinforcement Learning (DRL) Models to achieve an optimal tactical asset allocation. The research problem is described as an optimization scenario that seeks to maximize the portfolio risk adjusted returns for a given portfolio asset allocation. The problem is set up with an initial capital investment which is invested in a set of assets. The initial strategic allocation is determined, which in our case is the equal weight allocation, and all of the capital is invested in the set of assets. At each point in time, the assets are reallocated according to the allocation which will increase the portfolio value. Two DRL models are implemented. The performance of the DRL models is compared with the uniform weights portfolio. The results show that, generally, two DRL models have higher cumulative returns.",
        "link": "http://dx.doi.org/10.54254/2755-2721/6/20230866"
    },
    {
        "id": 27521,
        "title": "Caac: Effective Reinforcement Learning for Sparse Reward Environments",
        "authors": "Kun Liu, Libing Wu, Zhuangzhuang Zhang, Chao Ma, Na Lu, Xuejiang Wei",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4418679"
    },
    {
        "id": 27522,
        "title": "Credit-of-Q-value for Multi-Agent Reinforcement Learning",
        "authors": "Shuaibin Li, Xiu Li, Jinqiang Cui",
        "published": "2022-7-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9902237"
    },
    {
        "id": 27523,
        "title": "Adaptive Tsallis Entropy Regularization for Efficient Reinforcement Learning",
        "authors": "Kyungjae Lee",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictc55196.2022.9952774"
    },
    {
        "id": 27524,
        "title": "RL-GRIT: Reinforcement Learning for Grammar Inference",
        "authors": "Walt Woods",
        "published": "2021-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spw53761.2021.00031"
    },
    {
        "id": 27525,
        "title": "Active Object Searching on Mobile Robot Using Reinforcement Learning",
        "authors": "Nuo Xu",
        "published": "2021-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cds52072.2021.00058"
    },
    {
        "id": 27526,
        "title": "Adjusting the game difficulty by changing AI behaviors with Reinforcement Learning",
        "authors": "Louis Schmidt, Taichi Watanabe, Koji Mikami",
        "published": "2020-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nicoint50878.2020.00030"
    },
    {
        "id": 27527,
        "title": "Aufgabenorientierte Konfiguration autonomer Robotersysteme zur industriellen Anwendung von Reinforcement Learning",
        "authors": "M. Röhler, J. Berg, G. Reinhart",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.51202/9783181023518-753"
    },
    {
        "id": 27528,
        "title": "A State Representation for Reinforcement Learning and Decision-Making in the Orbitofrontal Cortex",
        "authors": "Nicolas W. Schuck, Robert Wilson, Yael Niv",
        "published": "No Date",
        "citations": 6,
        "abstract": "AbstractDespite decades of research, the exact ways in which the orbitofrontal cortex (OFC) influences cognitive function have remained mysterious. Anatomically, the OFC is characterized by remarkably broad connectivity to sensory, limbic and subcortical areas, and functional studies have implicated the OFC in a plethora of functions ranging from facial processing to value-guided choice. Notwithstanding such diversity of findings, much research suggests that one important function of the OFC is to support decision making and reinforcement learning. Here, we describe a novel theory that posits that OFC’s specific role in decision-making is to provide an up-to-date representation of task-related information, called a state representation. This representation reflects a mapping between distinct task states and sensory as well as unobservable information. We summarize evidence supporting the existence of such state representations in rodent and human OFC and argue that forming these state representations provides a crucial scaffold that allows animals to efficiently perform decision making and reinforcement learning in high-dimensional and partially observable environments. Finally, we argue that our theory offers an integrating framework for linking the diversity of functions ascribed to OFC and is in line with its wide ranging connectivity.",
        "link": "http://dx.doi.org/10.1101/210591"
    },
    {
        "id": 27529,
        "title": "Enhanced Reinforcement Learning with Targeted Dropout",
        "authors": "Mark Jovic A. Daday, Kristoffer Franz Mari R. Millado",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icd47981.2019.9105750"
    },
    {
        "id": 27530,
        "title": "Intermittent absence of control during reinforcement learning interferes with Pavlovian bias in action selection",
        "authors": "Gábor Csifcsák, Eirik Melsæter, Matthias Mittner",
        "published": "No Date",
        "citations": 3,
        "abstract": "The ability to control the occurrence of rewarding and punishing events is crucial for our well-being. Two ways to optimize performance are to follow heuristics like Pavlovian biases to approach reward and avoid loss, or to rely more on slowly accumulated stimulus-action associations. Although reduced control over outcomes has been linked to suboptimal decision-making in clinical conditions associated with learned helplessness, it is unclear how uncontrollability of the environment is related to the arbitration between different response strategies.This study directly tested whether a behavioral manipulation designed to induce learned helplessness in healthy adults (intermittent loss of control over feedback in a reinforcement learning task; “yoking”) would modulate the magnitude of Pavlovian bias and the neurophysiological signature of cognitive control (frontal midline theta power) in healthy adults. Using statistical analysis and computational modeling of behavioral data and electroencephalographic signals, we found stronger Pavlovian influences and alterations in frontal theta activity in the yoked group. However, these effects were not accompanied by reduced performance in experimental blocks with regained control, indicating that our behavioral manipulation was not potent enough for inducing helplessness and impaired coping ability with task demands.We conclude that the level of contingency between instrumental choices and rewards/punishments modulates Pavlovian bias during value-based decision-making, probably via interfering with the implementation of cognitive control. These findings might have implications for understanding the mechanisms underlying helplessness in various psychiatric conditions.",
        "link": "http://dx.doi.org/10.31234/osf.io/jpq6f"
    },
    {
        "id": 27531,
        "title": "Reinforcement Learning Based Dynamic Inverse Attitude Control of Near-space Vehicle",
        "authors": "Yaohua Shen, Mou Chen",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9189285"
    },
    {
        "id": 27532,
        "title": "A Reinforcement Learning-based Sequence Generation Algorithm for Password Guessing",
        "authors": "Zheng Chen, Xuliang Zhang",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10000814"
    },
    {
        "id": 27533,
        "title": "Monte Carlo Methods",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_4"
    },
    {
        "id": 27534,
        "title": "Reinforcement Learning for Hybrid Disassembly Line Balancing Problems",
        "authors": "Jiacun Wang, Guipeng Xi, Xiwang Guo, Shixin Liu, Shujin Qin, Henry Han",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4551988"
    },
    {
        "id": 27535,
        "title": "Game Playing Agent for 2048 using Deep Reinforcement Learning",
        "authors": "Varun Kaundinya, Shubham Jain, Sumanth Saligram, C K Vanamala, Avinash B",
        "published": "2018-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21467/proceedings.1.57"
    },
    {
        "id": 27536,
        "title": "Reinforcement Learning Based Proactive Monitor Channel Allocation",
        "authors": "Tuo Wu",
        "published": "2021-3-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iaeac50856.2021.9391117"
    },
    {
        "id": 27537,
        "title": "Development of Apple Detection System and Reinforcement Learning for Apple Manipulator",
        "authors": "Nikita Andriyanov",
        "published": "2023-2-1",
        "citations": 5,
        "abstract": "Modern deep learning systems make it possible to develop increasingly intelligent solutions in various fields of science and technology. The electronics of single board computers facilitate the control of various robotic solutions. At the same time, the implementation of such tasks does not require a large amount of resources. However, deep learning models still require a high level of computing power. Thus, the effective control of an intelligent robot manipulator is possible when a computationally complex deep learning model on GPU graphics devices and a mechanics control unit on a single-board computer work together. In this regard, the study is devoted to the development of a computer vision model for estimation of the coordinates of objects of interest, as well as the subsequent recalculation of coordinates relative to the control of the manipulator to form a control action. In addition, in the simulation environment, a reinforcement learning model was developed to determine the optimal path for picking apples from 2D images. The detection efficiency on the test images was 92%, and in the laboratory it was possible to achieve 100% detection of apples. In addition, an algorithm has been trained that provides adequate guidance to apples located at a distance of 1 m along the Z axis. Thus, the original neural network used to recognize apples was trained using a big image dataset, algorithms for estimating the coordinates of apples were developed and investigated, and the use of reinforcement learning was suggested to optimize the picking policy.",
        "link": "http://dx.doi.org/10.3390/electronics12030727"
    },
    {
        "id": 27538,
        "title": "Deep Reinforcement Learning Based Dynamic Multichannel Access in HetNets",
        "authors": "Shaoyang Wang, Tiejun Lv",
        "published": "2019-4",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc.2019.8885857"
    },
    {
        "id": 27539,
        "title": "Energy efficient online control of a water distribution network based on Deep Reinforcement Learning",
        "authors": "Baltasar Beferull-Lozano, Jyotirmoy Bhardwaj, Helge Liltvedt",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Data is obtained via Deep Reinforcement Learning Environment</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23896902"
    },
    {
        "id": 27540,
        "title": "Deep Reinforcement Learning Based End-to-End Stock Trading Strategy",
        "authors": "Yonghao Wang, Qinke Peng, Tian Han, Haozhou Li, Yiqing Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4384871"
    },
    {
        "id": 27541,
        "title": "Multiagent-Based Deep Reinforcement Learning Framework for Multi-Asset Adaptive Trading and Portfolio Management",
        "authors": "LI CHEN CHENG, Jian-Shiou Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4385481"
    },
    {
        "id": 27542,
        "title": "A Reinforcement Learning-Based Neighborhood Search Operator for Multi-Modal Optimization and its Applications",
        "authors": "Jiale Hong, Bo Shen, Anqi Pan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4423357"
    },
    {
        "id": 27543,
        "title": "Behaviourism: Learning through Imitation and Reinforcement",
        "authors": "Susan Young",
        "published": "2023-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003331193-4"
    },
    {
        "id": 27544,
        "title": "Applications of deep reinforcement learning  Alphago",
        "authors": "Yingchen Liu",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "With the progress of the times, the field of artificial intelligence (AI) has become one of the hottest fields in the 21st century. Currently, artificial intelligence is successfully used in the retail, financial, and medical industries. Especially in 2016, Google's DeepMind used deep reinforcement learning to train AlphaGo and defeated Lee Sedol, which propelled the field into the public eye. Most people are aware of artificial intelligence, but few understand it. This article will focus on analyzing the literature \"Mastering the game of Go with deep neural networks and tree search\" and other related articles to introduce the basics of deep reinforcement learning and AlphaGo. Finally, readers will understand how artificial intelligence can successfully imitate humans and defeat humans in Go.",
        "link": "http://dx.doi.org/10.54254/2755-2721/5/20230668"
    },
    {
        "id": 27545,
        "title": "Changes in the Dopaminergic circuitry and Adult Neurogenesis linked to Reinforcement Learning in Corvids",
        "authors": "Pooja Parishar, Madhumita Rajagopalan, Soumya Iyengar",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe caudolateral nidopallium (NCL, an analogue of the prefrontal cortex) is known to be involved in learning, memory, and discrimination in crows, whereas the involvement of other brain regions in these phenomena are unknown. However, recent studies on pigeons have demonstrated that besides NCL, basal ganglia-thalamocortical loops connected to this region play are also crucial for learning. The present study demonstrates that besides NCL, other parts of the caudal nidopallium (NC), avian basal ganglia, and intriguingly, vocal control regions in house crows (Corvus splendens), are involved in visual discrimination. We have also found that training on the visual discrimination task can be correlated to neurite pruning in mature dopaminoceptive neurons and immature doublecortin-positive neurons in the NC of house crows. Furthermore, there is an increase in the incorporation of new neurons throughout NC and the medial striatum which can also be linked to learning. For the first time, our results demonstrate that a combination of structural changes in mature and immature neurons and adult neurogenesis are linked to learning in corvids.",
        "link": "http://dx.doi.org/10.1101/2023.08.31.555829"
    },
    {
        "id": 27546,
        "title": "Deep Reinforcement Learning for Alleviation of Unbalanced Active Powers Using Distributed Batteries in LV Residential Distribution System",
        "authors": "Watcharakorn Pinthurat, Branislav Hredzak",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The high penetration and uneven distribution of single-phase rooftop PVs and load demands in power systems may lead to unbalanced active powers, adversely impacting power quality and system reliability. This paper introduces a strategy based on multi-agent deep reinforcement learning to address these unbalanced active powers. The approach involves deploying single-phase battery systems throughout the LV residential distribution system, subsidized by the utility. Initially, the unbalanced active powers are framed as a Markov game, which is then addressed using a multi-agent deep deterministic policy gradient algorithm. The strategy relies on local measurements, with agents' experiences centrally shared during training for cooperative tasks. Notably, information about the phase connections of the battery systems becomes unnecessary. The strategy learns from historical data, gradually mastering the process. Real data from rooftop PVs and demands in a four-wire LV residential distribution system validate the effectiveness of the proposed approach. Acting as adaptive agents, the battery systems collaboratively operate by adjusting active powers to minimize neutral current at the point of common connection.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24499090.v2"
    },
    {
        "id": 27547,
        "title": "Scalable evolutionary hierarchical reinforcement learning",
        "authors": "Sasha Abramowitz, Geoff Nitschke",
        "published": "2022-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3520304.3528937"
    },
    {
        "id": 27548,
        "title": "Reinforcement Learning Control for Robot Arm Grasping Based on Improved DDPG",
        "authors": "Guangjun Qi, Yuan Li",
        "published": "2021-7-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9550413"
    },
    {
        "id": 27549,
        "title": "Informative Path Planning for Mobile Sensing with Reinforcement Learning",
        "authors": "Yongyong Wei, Rong Zheng",
        "published": "2020-7",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infocom41043.2020.9155528"
    },
    {
        "id": 27550,
        "title": "Group and Socially Aware Multi-Agent Reinforcement Learning",
        "authors": "Manav Vallecha, Rahul Kala",
        "published": "2022-6-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/med54222.2022.9837206"
    },
    {
        "id": 27551,
        "title": "Traffic light control with reinforcement learning",
        "authors": "Taoyu Pan",
        "published": "2024-2-26",
        "citations": 0,
        "abstract": "Urban traffic signal optimization is important for alleviating congestion in urban transportation systems. This study proposes a real-time traffic light control algorithm based on deep Q learning with a reward function that accounts for queue lengths, delays, travel times, and throughput. The model dynamically decides phase changes based on current traffic conditions. The training of the deep Q network involves an offline stage from pre-generated data with fixed signal timing and an online stage using real-time traffic data. A deep Q network structure with aphase gate component is used to simplify the model's learning task under different phases. Amemory palace\" mechanism is used to address sample imbalance during the training process. Both synthetic and real-world traffic flow data are used to validate our approach under an urban road intersection scenario in Hangzhou, China. Results demonstrate significant performance improvements of the proposed method in reducing vehicle waiting time (57.1% to 100%), queue lengths (40.9% to 100%), and total travel time (16.8% to 68.0%) compared to traditional fixed signal timing plans.",
        "link": "http://dx.doi.org/10.54254/2755-2721/43/20230804"
    },
    {
        "id": 27552,
        "title": "Risk-Sensitive Policy with Distributional Reinforcement Learning",
        "authors": "Thibaut Théate, Damien Ernst",
        "published": "2023-6-30",
        "citations": 2,
        "abstract": "Classical reinforcement learning (RL) techniques are generally concerned with the design of decision-making policies driven by the maximisation of the expected outcome. Nevertheless, this approach does not take into consideration the potential risk associated with the actions taken, which may be critical in certain applications. To address that issue, the present research work introduces a novel methodology based on distributional RL to derive sequential decision-making policies that are sensitive to the risk, the latter being modelled by the tail of the return probability distribution. The core idea is to replace the Q function generally standing at the core of learning schemes in RL by another function, taking into account both the expected return and the risk. Named the risk-based utility function U, it can be extracted from the random return distribution Z naturally learnt by any distributional RL algorithm. This enables the spanning of the complete potential trade-off between risk minimisation and expected return maximisation, in contrast to fully risk-averse methodologies. Fundamentally, this research yields a truly practical and accessible solution for learning risk-sensitive policies with minimal modification to the distributional RL algorithm, with an emphasis on the interpretability of the resulting decision-making process.",
        "link": "http://dx.doi.org/10.3390/a16070325"
    },
    {
        "id": 27553,
        "title": "Reinforcement Learning Tuned PI Controller for Two Tank Interacting Hybrid System",
        "authors": "Billa Harikrishna, Kanagalakshmi S",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/silcon59133.2023.10404691"
    },
    {
        "id": 27554,
        "title": "State Constraints and Safety Consideration",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_9"
    },
    {
        "id": 27555,
        "title": "A Machine of Few Words: Interactive Speaker Recognition with Reinforcement Learning",
        "authors": "Mathieu Seurin, Florian Strub, Philippe Preux, Olivier Pietquin",
        "published": "2020-10-25",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2020-2892"
    },
    {
        "id": 27556,
        "title": "A Reinforcement Learning Based on Book Recommendation System",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ajcis.2023.061303"
    },
    {
        "id": 27557,
        "title": "Grid-Supportive Load Frequency Control using Deep Reinforcement Learning",
        "authors": "Faisal Albeladi, Masoud Barati",
        "published": "2023-4-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/kpec58008.2023.10215451"
    },
    {
        "id": 27558,
        "title": "Decentralized Safe Reinforcement Learning for Voltage Control",
        "authors": "Wenqi Cui, Jiayi Li, Baosen Zhang",
        "published": "2022-6-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867766"
    },
    {
        "id": 27559,
        "title": "Hierarchical Control of Multi-Agent Systems using Online Reinforcement Learning",
        "authors": "He Bai, Jemin George, Aranya Chakrabortty",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147329"
    },
    {
        "id": 27560,
        "title": "Evolutionary Reinforcement Learning for the Coordination of Swarm UAVs",
        "authors": "Umut Can Altin",
        "published": "2020-10-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu49456.2020.9302227"
    },
    {
        "id": 27561,
        "title": "Deep Reinforcement Learning Based Power Allocation for D2D Network",
        "authors": "Zhengran Bi, Wenan Zhou",
        "published": "2020-5",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2020-spring48590.2020.9129537"
    },
    {
        "id": 27562,
        "title": "Cooperative Perception with Deep Reinforcement Learning for Connected Vehicles",
        "authors": "Shunsuke Aoki, Takamasa Higuchi, Onur Altintas",
        "published": "2020-10-19",
        "citations": 53,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv47402.2020.9304570"
    },
    {
        "id": 27563,
        "title": "Reinforcement Learning Quantum Local Search",
        "authors": "Chen-Yu Liu, Hsi-Sheng Goan",
        "published": "2023-9-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/qce57702.2023.10226"
    },
    {
        "id": 27564,
        "title": "Few-Shot System Identification for Reinforcement Learning",
        "authors": "Karim Farid, Nourhan Sakr",
        "published": "2021-7-16",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acirs52449.2021.9519314"
    },
    {
        "id": 27565,
        "title": "A joint task caching and computation offloading scheme based on Deep Reinforcement Learning",
        "authors": "Huizi Tian, Lin Zhu, Long Tan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nConsidering the dynamic variability of the vehicular edge environment and the limited edge servers resources, this paper proposes a joint task caching and computation offloading scheme based on deep reinforcement learning (DRL). Considering that the motion trajectories of different vehicles overlap and their task requests may be the same, this paper designs a vehicle-edge-cloud computing framework to fully use the cache resources of vehicles, edge servers, and clouds to reduce task processing delays and energy consumption. Secondly, this paper adopts a method of partial offloading and collaboration between edge servers, which fully utilizes the computing resources of vehicles, edge servers, and the cloud, reducing the burden of vehicles and edge servers. In addition, this paper proposes a DRL-based task offloading scheme to obtain better task caching and offloading strategies. The simulation results show that the scheme proposed in this article performs better compared to other schemes and effectively reduces the latency and energy consumption of task processing.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3972282/v1"
    },
    {
        "id": 27566,
        "title": "Reinforcement Learning Algorithms: An Overview and Classification",
        "authors": "Fadi AlMahamid, Katarina Grolinger",
        "published": "2021-9-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccece53047.2021.9569056"
    },
    {
        "id": 27567,
        "title": "Deep Reinforcement Learning: Reservoir Optimization from Pixels",
        "authors": "Ruslan Miftakhov, Abdulaziz Al-Qasim, Igor Efremov",
        "published": "2020-1-13",
        "citations": 18,
        "abstract": "\nThe application of Artificial Intelligence (AI) methods in the petroleum industry gain traction in recent years. In this paper, Deep Reinforcement Learning (RL) by pixel data is used to maximize the Net Present Value (NPV) of waterflooding by changing the water injection rate. This work is the first step towards showing that learning from pixel information offers many benefits, for example, understanding the reservoir's physics directly by measuring changes in pressure and saturation distribution without taking into consideration reservoir petrophysical property and the total amount of wells with corresponding locations.\nThe optimization is tested on the 2D model, which is a vertical section of the SPE 10 model. It has been shown that RL is able to optimize waterflooding in a 2D compressible reservoir with the 2-phase flow (oil-water) by means of pixel data. In the first few thousands of updates, optimization remains in the baseline NPV since it takes more time to converge from raw pixel data than to use classical well production/injection rate information.\nThe reservoir NPV increased by 15 percent as a result of optimization, where the optimum scenario results in less watercut and more stable production. Additionally, we evaluated the impact of choosing the action set for optimization and examined two cases where water injection well can change injection pressure with a step of 200 psi and 600 psi. The results show that in the second case, AI optimization is exploiting the limitation of the reservoir simulator and tries to imitate a cycled injection regime, which results in a 7% higher NPV than the first case.",
        "link": "http://dx.doi.org/10.2523/iptc-20151-ms"
    },
    {
        "id": 27568,
        "title": "Linguistic Lyapunov reinforcement learning control for robotic manipulators",
        "authors": "Abhishek Kumar, Rajneesh Sharma",
        "published": "2018-1",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2017.06.064"
    },
    {
        "id": 27569,
        "title": "Reinforcement Learning and Adaptive Control",
        "authors": "Girish Chowdhary, Girish Joshi, Aaron Havens",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4471-5102-9_100064-1"
    },
    {
        "id": 27570,
        "title": "Optimization of High-Speed Train Operation Control Based on Soft Actor-Critic Deep Reinforcement Learning Algorithm",
        "authors": "Huiqin Pei, Zhuyuan Lan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4573607"
    },
    {
        "id": 27571,
        "title": "Continuous-Time Mean-Variance Portfolio Selection: A Reinforcement Learning Framework",
        "authors": "Haoran Wang, Xunyu Zhou",
        "published": "No Date",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3382932"
    },
    {
        "id": 27572,
        "title": "Group value learned through interactions with members: A reinforcement learning account",
        "authors": "Leor M Hackel, Drew Kogon, David Amodio, Wendy Wood",
        "published": "No Date",
        "citations": 1,
        "abstract": "How do group-based interaction tendencies form through encounters with individual group members? In three experiments, in which participants interacted with group members in a reinforcement learning task presented as a money sharing game, participants formed instrumental reward associations with individual group members through direct interaction and feedback. Results revealed that individual-level reward learning generalized to a group-based representation, as indicated in self-reported group attitudes, trait impressions, and the tendency to choose subsequent interactions with novel members of the group. Moreover, group-based reward values continued to predict interactions with novel members after controlling for explicit attitudes and impressions, suggesting that instrumental learning contributes to an implicit form of group-based choice. Experiment 3 further demonstrated that group-based reward effects on interaction choices persisted even when group reward value was no longer predicted of positive outcomes, consistent with a habit-like expression of group bias. These results demonstrate a novel process of prejudice formation based on instrumental reward learning from direct interactions with individual group members. We discuss implications for existing theories of prejudice, the role of habit in intergroup bias, and intervention strategies to reduce prejudice.",
        "link": "http://dx.doi.org/10.31234/osf.io/25bc6"
    },
    {
        "id": 27573,
        "title": "Better Chinese Sentence Segmentation with Reinforcement Learning",
        "authors": "Srivatsan Srinivasan, Chris Dyer",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-acl.25"
    },
    {
        "id": 27574,
        "title": "Hierarchical Reinforcement Learning for Decision Support in Health Care",
        "authors": "Caroline Strickland, Daniel Lizotte",
        "published": "2021-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/594757db.0d86cb14"
    },
    {
        "id": 27575,
        "title": "Auto-scaling Resources for Cloud Applications using Reinforcement learning",
        "authors": "Indu John, Aiswarya Sreekantan, Shalabh Bhatnagar",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ghci47972.2019.9071835"
    },
    {
        "id": 27576,
        "title": "An Adaptive Dual-Level Reinforcement Learning Approach for Optimal Trade Execution",
        "authors": "Soohan Kim, Jimyeong Kim, Hong Kee Sul, Youngjoon Hong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4470977"
    },
    {
        "id": 27577,
        "title": "A Deep Reinforcement Learning Approach to The Ancient Indian Game - Chowka Bhara",
        "authors": "Annapurna P Patil, SANJAY RAGHAVENDRA, Shruthi Srinarasi, Reshma Ram",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement Learning (RL) is the study of how Artificial Intelligence (AI) agents learn to make their own decisions in an environment to maximize the cumulative reward received. Although there has been notable progress in the application of RL for games, the category of ancient Indian games has remained almost untouched. Chowka Bhara is one such ancient Indian board game. This work aims at developing a Q-Learning-based RL Chowka Bhara player whose strategies and methodologies are obtained from three Strategic Players viz. Fast Player, Random Player, and Balanced Player. It is observed through the experimental results that the Q-Learning Player outperforms all three Strategic Players.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16780414"
    },
    {
        "id": 27578,
        "title": "Feature Transformation and Simulation of Short Term Price Variability in Reinforcement Learning for Portfolio Management",
        "authors": "",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22360/springsim.2020.ais.002"
    },
    {
        "id": 27579,
        "title": "Full-Body Control of an Aerial Manipulator for Advance Physical Interaction Using Fuzzy Reinforcement Learning: A Road Map",
        "authors": "Mohsen Zahmatkesh",
        "published": "No Date",
        "citations": 0,
        "abstract": "A detailed literature review is performed in this study to address solutions for the full-body design and control of an aerial manipulator. Deep Reinforcement Learning methods are growing to be utilized recently to cope with various uncertainties. The pros and cons of these theories will be explained as well as introducing the advantages of Fuzzy Reinforcement Learning methods. State-of-the-Art, possible challenges, potential approaches, and a summary of desired precision devices are discussed in this study.",
        "link": "http://dx.doi.org/10.20944/preprints202302.0210.v1"
    },
    {
        "id": 27580,
        "title": "Deep Reinforcement Learning for Optimal Experimental Design in Biology",
        "authors": "Neythen J. Treloar, Nathan Braniff, Brian Ingalls, Chris P. Barnes",
        "published": "No Date",
        "citations": 4,
        "abstract": "AbstractThe field of optimal experimental design uses mathematical techniques to determine experiments that are maximally informative from a given experimental setup. Here we apply a technique from artificial intelligence—reinforcement learning—to the optimal experimental design task of maximizing confidence in estimates of model parameter values. We show that a reinforcement learning approach performs favourably in comparison with a one-step ahead optimisation algorithm and a model predictive controller for the inference of bacterial growth parameters in a simulated chemostat. Further, we demonstrate the ability of reinforcement learning to train over a distribution of parameters, indicating that this approach is robust to parametric uncertainty.1Author summaryBiological systems are often complex and typically exhibit non-linear behaviour, making accurate model parametrisation difficult. Optimal experimental design tools help address this problem by identifying experiments that are predicted to provide maximally accurate parameter estimates. In this work we use reinforcement learning, an artificial intelligence method, to determine such experiments. Our simulation studies show that this approach allows uncertainty in model parameterisation to be directly incorporated into the search for optimal experiments, opening a practical avenue for training an experimental controller without confident knowledge of the system’s parameter values. We present this method as complementary to existing optimisation approaches and we anticipate that artificial intelligence has a fundamental role to play in the future of optimal experimental design.",
        "link": "http://dx.doi.org/10.1101/2022.05.09.491138"
    },
    {
        "id": 27581,
        "title": "Effectuation entwickeln",
        "authors": "Martin Sterzel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-39251-2"
    },
    {
        "id": 27582,
        "title": "Reinforcement Learning for the Traveling Salesman Problem: Performance Comparison of Three Algorithms",
        "authors": "Jiaying Wang, Chenglong Xiao, Shanshan Wang, Yaqi Ruan",
        "published": "No Date",
        "citations": 0,
        "abstract": "TSP is one of the most famous problems in graph theory, as well as one\nof the typical NP-hard problems in combinatorial optimization. Its\napplications range from how to plan the most reasonable and efficient\nroad traffic to how to better set up nodes in the Internet environment\nto facilitate information flow, among others. Reinforcement learning has\nbeen widely regarded as an effective tool for solving combinatorial\noptimization problems. This paper attempts to solve the TSP problem\nusing different reinforcement learning algorithms and evaluated the\nperformance of three RL algorithms (Q-learning, Sarsa, and Double\nQ-Learning) under different reward functions, ε-greedy decay strategies,\nand running times. The results show that the Double Q-Learning algorithm\nis the best algorithm, as it could produce results closest to the\noptimal solutions, and by analyzing the results, better reward\nstrategies and epsilon-greedy decay strategies are obtained.",
        "link": "http://dx.doi.org/10.22541/au.168389655.51789479/v1"
    },
    {
        "id": 27583,
        "title": "Deep Reinforcement Learning for Multiparameter Optimization in de novo Drug Design",
        "authors": "Niclas Ståhl, Göran Falkman, Alexander Karlsson, Gunnar Mathiason, Jonas Boström",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>In medicinal chemistry\nprograms it is key to design and make compounds that are efficacious and safe.\nThis is a long, complex and difficult multi-parameter optimization process,\noften including several properties with orthogonal trends. New methods for the\nautomated design of compounds against profiles of multiple properties are thus\nof great value. Here we present a fragment-based reinforcement learning\napproach based on an actor-critic model, for the generation of novel molecules\nwith optimal properties. The actor and the critic are both modelled with\nbidirectional long short-term memory (LSTM) networks. The AI method learns how\nto generate new compounds with desired properties by starting from an initial\nset of lead molecules and then improve these by replacing some of their\nfragments. A balanced binary tree based on the similarity of fragments is used\nin the generative process to bias the output towards structurally similar\nmolecules. The method is demonstrated by a case study showing that 93% of the\ngenerated molecules are chemically valid, and a third satisfy the targeted\nobjectives, while there were none in the initial set.</p>",
        "link": "http://dx.doi.org/10.26434/chemrxiv.7990910"
    },
    {
        "id": 27584,
        "title": "Demonstration-Guided Deep Reinforcement Learning for Coordinated Ramp Metering and Perimeter Control in Large Scale Networks",
        "authors": "Zijian Hu, Wei Ma",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4380256"
    },
    {
        "id": 27585,
        "title": "Expert Initialized Hybrid Model-Based and Model-Free Reinforcement Learning",
        "authors": "Jeppe Langaa, Christoffer Sloth",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178306"
    },
    {
        "id": 27586,
        "title": "Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones",
        "authors": "Ugurkan Ates",
        "published": "2020-10-15",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asyu50717.2020.9259811"
    },
    {
        "id": 27587,
        "title": "Performance of Reinforcement Learning on Traditional Video Games",
        "authors": "Yuanxi Sun",
        "published": "2021-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiam54119.2021.00063"
    },
    {
        "id": 27588,
        "title": "Exploration-Enhanced Multi-Agent Reinforcement Learning for Distributed Pv-Ess Scheduling with Incomplete Data",
        "authors": "Jian Hou, Yutong Li, Gangfeng Yan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4512889"
    },
    {
        "id": 27589,
        "title": "Hippocampal Contribution to Probabilistic Feedback Learning: Modeling Observation- and Reinforcement-based Processes",
        "authors": "Virginie Patt, Daniela Palombo, Michael Esterman, Mieke Verfaellie",
        "published": "No Date",
        "citations": 0,
        "abstract": "Simple probabilistic reinforcement learning is recognized as a striatum-based learning system, but in recent years, has also been associated with hippocampal involvement. The present study examined whether such involvement may be attributed to observation-based learning processes, running in parallel to striatum-based reinforcement learning. A computational model of observation-based learning (OL), mirroring classic models of reinforcement-based learning (RL), was constructed and applied to the neuroimaging dataset of Palombo, Hayes, Reid, &amp; Verfaellie (2019). Hippocampal contributions to value-based learning: Converging evidence from fMRI and amnesia. Cognitive, Affective &amp; Behavioral Neuroscience, 19(3), 523–536. Results suggested that observation-based learning processes may indeed take place concomitantly to reinforcement learning and involve activation of the hippocampus and central orbitofrontal cortex (cOFC). However, rather than independent mechanisms running in parallel, the brain correlates of the OL and RL prediction errors indicated collaboration between systems, with direct implication of the hippocampus in computations of the discrepancy between the expected and actual reinforcing values of actions. These findings are consistent with previous accounts of a role for the hippocampus in encoding the strength of observed stimulus-outcome associations, with updating of such associations through striatal reinforcement-based computations. Additionally, enhanced negative RL prediction error signaling was found in the anterior insula with greater use of OL over RL processes. This result may suggest an additional mode of collaboration between the OL and RL systems, implicating the error monitoring network.",
        "link": "http://dx.doi.org/10.31234/osf.io/qhb3a"
    },
    {
        "id": 27590,
        "title": "Reinforcement Learning for Minimizing Communication Delay in Edge Computing",
        "authors": "Kolichala Rajashekar",
        "published": "2022-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdcs54860.2022.00128"
    },
    {
        "id": 27591,
        "title": "Language Inference with Multi-head Automata through Reinforcement Learning",
        "authors": "Alper Sekerci, Ozlem Salehi",
        "published": "2020-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207156"
    },
    {
        "id": 27592,
        "title": "Analysis on Deep Reinforcement Learning with Flappy Brid Gameplay",
        "authors": "Zhixuan He",
        "published": "2022-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icict55905.2022.00025"
    },
    {
        "id": 27593,
        "title": "Inverse Reinforcement Learning Control for Building Energy Management",
        "authors": "Sourav Dey, Thibault Marzullo, Gregor Henze",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4330892"
    },
    {
        "id": 27594,
        "title": "Application of Phase and Time-Optimized Parallel Deep Reinforcement Learning Model to Sequential Junctions",
        "authors": "ERHAN TURAN, Beşir DANDIL, Engin AVCI",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4494609"
    },
    {
        "id": 27595,
        "title": "Reinforcement Learning for Shared Driving",
        "authors": "Sangjin Ko, Reza Langari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.12.032"
    },
    {
        "id": 27596,
        "title": "Fathoming the Mandela Effect: Deploying Reinforcement Learning to Untangle the Multiverse",
        "authors": "A’aeshah Alhakamy",
        "published": "2023-3-10",
        "citations": 0,
        "abstract": "Multiverse is a hypothetical idea that other universes can exist beyond our own. Various scientific theories have suggested scenarios such as the existence of bubble universes that constantly expand or string theory that attempts to merge gravity with other forces. Thus, a multiverse is a complex theoretical phenomenon that can best be conceived through computer simulation. Albeit within the multiverse, the causality of the Mandela effect is entirely possible. To examine the behavior of the multiverse as a representative ensemble, each universe as a specific ensemble element needs to be generated. Our universe generation is based on unique universes for two binary attributes of a population of n=303. The maximum possible universes this could produce within the multiverse is in the exponent of 182. To computationally confine the simulation to the scope of this study, the sample count of the multiverse is nmultiverse=606. Parameters representing the existence of each multiverse are implemented through the μ and σ values of each universe’s attributes. By using a developed reinforcement learning algorithm, we generate a multiverse yielding various universes. The computer gains consciousness of the parameters that can represent the expanse of possibility to exist for multiple universes. Furthermore, for each universe, a heart attack prediction model is performed to understand the universe’s environment and behavior. We test the Mandela effect or déjà vu of each universe by comparing error test losses with the training size of order M. Our model can measure the behavior of environments in different regions referred to as specific ensemble elements. By explicitly exploiting the attributes of each universe, we can get a better idea of the possible outcomes for the creation of other specific ensemble elements, as seen in the multiverse space planes.",
        "link": "http://dx.doi.org/10.3390/sym15030699"
    },
    {
        "id": 27597,
        "title": "Reinforcement Learning for Improving Flappy Bird Game",
        "authors": "Shiyao Wei",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "Currently, Artificial Intelligence becomes popularity among the human daily life, like games, Internet and so on. Authors has shown that in the game filed the Artificial Intelligence always have better performance than human beings, so in this article, the author wants to use AI to carry out on an old- fashion fame called Flappy Bird. This study aims to determine the specific method why AI has better performance than human beings. In this context, the author based on the process of experiments: It mainly used reinforcement learning model (Acting based on feedback from the environment, through continuous interaction with the environment, trial, and error, to ultimately accomplish a specific purpose or to maximize the overall benefits of the action) and supervised learning model (the process of making the machine learn a large amount of sample data with labels, training a model and making the model get the corresponding output according to the input) to improve the Flappy Bird and both two method are belonging to the machine learning. In addition, this study alters the layout of the game, including pipe, appearance of agent, and background of the game in order to make a more fashionable game. Furthermore, this study increases the number of agents, which makes it easier for agent to achieve higher score. Last but not the least, author establish a archive point, which means if the player face operation mistake and lead to game over, they bird will relive before passing the last pipe.",
        "link": "http://dx.doi.org/10.54097/hset.v34i.5479"
    },
    {
        "id": 27598,
        "title": "Deep Reinforcement Learning for the Computation Offloading in MIMO-based Edge Computing",
        "authors": "Abdeladim Sadiki, Jamal Bentahar, Rachida Dssouli, Abdeslam En-Nouaary",
        "published": "No Date",
        "citations": 1,
        "abstract": "Multi-access Edge Computing (MEC) has recently emerged as a potential technology to serve the needs of mobile devices (MDs) in 5G and 6G cellular networks. By offloading tasks to high-performance servers installed at the edge of the wireless networks, resource-limited MDs can cope with the proliferation of the recent computationally-intensive applications. In this paper, we study the computation offloading problem in a massive multiple-input multiple-output (MIMO)-based MEC system where the base stations are equipped with a large number of antennas. Our objective is to minimize the power consumption and offloading delay at the MDs under the stochastic system environment. To this end, we formulate the problem as a Markov Decision Process (MDP) and propose two Deep Reinforcement Learning (DRL) strategies to learn the optimal offloading policy without any prior knowledge of the environment dynamics. First, a Deep Q-Network (DQN) strategy to solve the curse of the state space explosion is analyzed. Then, a more general Proximal Policy Optimization (PPO) strategy to solve the problem of discrete action space is introduced. Simulation results show that the proposed DRL-based strategies outperform the baseline and state-of-the-art algorithms. Moreover, our PPO algorithm exhibits stable performance and efficient offloading results compared to the benchmark DQN strategy.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16869119.v1"
    },
    {
        "id": 27599,
        "title": "Enviroment Representations with Bisimulation Metrics for Hierarchical Reinforcement Learning",
        "authors": "Chao Zhang",
        "published": "2023-1-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccrd56364.2023.10080162"
    },
    {
        "id": 27600,
        "title": "Reinforcement Learning for Provisioning OTN Leased Lines",
        "authors": "Ashwin Gumaste, Joao Pedro, Harald Bock",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ofc49934.2023.10116266"
    },
    {
        "id": 27601,
        "title": "Model-Based Algorithms",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_3"
    },
    {
        "id": 27602,
        "title": "Optimizing and Extending the Functionality of EXARL for Scalable Reinforcement Learning [Slides]",
        "authors": "Sai Chenna, Katherine Cosburn, Uchenna Ezeobi, Maxim Moraru",
        "published": "2021-8-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1812639"
    },
    {
        "id": 27603,
        "title": "Adaptive Selection of Informative Path Planning Strategies via Reinforcement Learning",
        "authors": "Taeyeong Choi, Grzegorz Cielniak",
        "published": "2021-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecmr50962.2021.9568796"
    },
    {
        "id": 27604,
        "title": "Policy Gradient Algorithms",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_7"
    },
    {
        "id": 27605,
        "title": "Multiagent-Based Deep Reinforcement Learning Framework for Multi-Asset Adaptive Trading and Portfolio Management",
        "authors": "LI CHEN CHENG, Jian-Shiou Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4488079"
    },
    {
        "id": 27606,
        "title": "Model dependent reinforcement learning algorithm for reservoir operation stochastic optimization",
        "authors": "Li Wenwu",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15406/ijh.2018.02.00129"
    },
    {
        "id": 27607,
        "title": "Modeling individual variation in visual search with reinforcement learning",
        "authors": "Ben Lonnqvist, Micha Elsner, Amelia R. Hunt, Alasdair D F Clarke",
        "published": "No Date",
        "citations": 0,
        "abstract": "Experiments on the efficiency of human search sometimes reveal large differences between individual participants. We argue that reward-driven task-specific learning may account for some of this variation. In a computational reinforcement learning model of this process, a wide variety of strategies emerge, despite all simulated participants having the same visual acuity. We conduct a visual search experiment, and replicate previous findings that participant preferences about where to search are highly varied, with a distribution comparable to the simulated results. Thus, task-specific learning is an under-explored mechanism by which large inter-participant differences can arise.",
        "link": "http://dx.doi.org/10.31234/osf.io/suj28"
    },
    {
        "id": 27608,
        "title": "Fairac: A Multi-Objective Reinforcement Learning Framework for Ensuring Fairness in Dynamic Recommender Systems",
        "authors": "Mohammad  Amir Rezaei Gazik, Mehdy Roayaei",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4202337"
    },
    {
        "id": 27609,
        "title": "Real-Time Reinforcement Learning Control in Poor Experimental Conditions",
        "authors": "Jorge Val, Rafal Wisniewski, Carsten Skovmose Kallesoe",
        "published": "2021-6-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc54610.2021.9654896"
    },
    {
        "id": 27610,
        "title": "Embedding Reinforcement Learning in Simulation",
        "authors": "Ayman AboElHassan, Soumaya Yacout",
        "published": "2021-9-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/etfa45728.2021.9613611"
    },
    {
        "id": 27611,
        "title": "The Use of Deep Reinforcement Learning in Tactical Asset Allocation",
        "authors": "Musonda Katongo, Ritabrata Bhattacharyya",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3812609"
    },
    {
        "id": 27612,
        "title": "Sample Complexity of Model-Based Robust Reinforcement Learning",
        "authors": "Kishan Panaganti, Dileep Kalathil",
        "published": "2021-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683162"
    },
    {
        "id": 27613,
        "title": "An Improved Deep Reinforcement Learning Algorithm for Autonomous Underwater Vehicles",
        "authors": "Jing Zhang, Huimin Jiang, Jingwen Qin, Guilin Zhang, Wendong Gai",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4494670"
    },
    {
        "id": 27614,
        "title": "Markov Decision Processes",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_2"
    },
    {
        "id": 27615,
        "title": "Improvements to DQN",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_8"
    },
    {
        "id": 27616,
        "title": "Reinforcement Learning -based Autonomous Multilayer Network Operation",
        "authors": "Sima Barzegar, Marc Ruiz, Luis Velasco",
        "published": "2020-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecoc48923.2020.9333268"
    },
    {
        "id": 27617,
        "title": "Intact reinforcement learning in healthy ageing",
        "authors": "Wei-Hsiang Lin, Aaron M. Clarke, Karin S. Pilz, Michael H. Herzog, Marina Kunchulia",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractWhat does age in ageing? Results in reinforcement learning (RL) are mixed. Some studies found deteriorated performance in older participants compared to younger controls whereas other studies did not. Daniel et al. (2020) suggested that task demand can explain these differences, with less demanding tasks showing no effect of age. Here, we increased the task demand of previous studies turning them into a classic navigation task. We extracted 4 behavioral parameters and 2 parameters (learning and exploration rates) of a classic Q-learning model. Except for one specific parameter, all other parameters showed no group differences, i.e., RL turned out to be intact in older individuals also with higher task demands. It is important to publish such null results to avoid the stigmatizing impression of an overall performance deficit among older people.",
        "link": "http://dx.doi.org/10.1101/2023.05.25.542104"
    },
    {
        "id": 27618,
        "title": "On the Relative Stability of Deep Distributional Reinforcement Learning",
        "authors": "Rex Liu, Bowen He, Michael Frank",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1309-0"
    },
    {
        "id": 27619,
        "title": "Explicit Reinforcement Learning Safety Layer for Computationally Efficient Inverter-Based Voltage Regulation",
        "authors": "Xingyu Zhao, Qianwen Xu",
        "published": "2023-5-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10156201"
    },
    {
        "id": 27620,
        "title": "Deep Reinforcement Learning with Dual Targeting Algorithm",
        "authors": "Naoki Kodama, Taku Harada, Kazuteru Miyazaki",
        "published": "2019-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2019.8851690"
    },
    {
        "id": 27621,
        "title": "Explainable Deep Reinforcement Learning for Aircraft Separation Assurance",
        "authors": "Wei Guo, Peng Wei",
        "published": "2022-9-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dasc55683.2022.9925786"
    },
    {
        "id": 27622,
        "title": "Transfer Reinforcement Learning: Feature Transferability in Ship Collision Avoidance",
        "authors": "Xinrui Wang, Yan Jin",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "Abstract\nThe integration of artificial intelligence into engineering work has become increasingly prevalent. Engineering work processes can be highly complex, and learning from scratch requires large computation resources. Transfer learning has emerged as a promising technique for improving learning efficiency by leveraging knowledge gained from related tasks to the target task. To achieve optimal performance, one of the key challenges is to figure out how transferrable the features are among different work processes and within training networks. Simulation-based ship collision avoidance is used for case studies due to its inherent complexity and diversity. Two transfer reinforcement learning methods, feature extraction, and finetuning, are implemented and evaluated against the baseline. Instead of introducing large-scaled pre-trained models as the backbone, a light CNN model pre-trained in a related base case has been proven to transfer essential features to target cases. Simplified ship dynamics is introduced into the training process to make it more realistic and applicable, and the delay caused by the large moment of inertia is addressed by modifying the model-environment interaction mechanism. Work process features for the ship collision avoidance process are concluded from crucial aspects. The effects on transferability are displayed by experimental results discussed from the feature category and similarity perspective.",
        "link": "http://dx.doi.org/10.1115/detc2023-116709"
    },
    {
        "id": 27623,
        "title": "Out-of-Distribution Detection with Confidence Deep Reinforcement Learning",
        "authors": "Di Wang",
        "published": "2023-10-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccci58712.2023.10290768"
    },
    {
        "id": 27624,
        "title": "Optimal Continuous Control of Refrigerator for Electricity Cost Minimization - Hierarchical Reinforcement Learning Approach",
        "authors": "Bongseok Kim, Jihwan An, Min Kyu Sim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4528070"
    },
    {
        "id": 27625,
        "title": "A Research on Manipulator Path Tracking based on Deep Reinforcement Learning",
        "authors": "Pengyu Zhang, Jie Zhang, Jiangming Kan",
        "published": "No Date",
        "citations": 0,
        "abstract": "We propose a deep reinforcement learning based manipulator path tracking method to solve the computationally difficult and non-unique problem of manipulator path tracking methods based on inverse kinematics. By transforming the path tracking task into a sequence decision problem, our method adopts an end-to-end learning method for closed-loop control and avoids the process of finding the inverse solution. We first explored the feasibility of the deep reinforcement learning method in the path tracking of the manipulator. After verifying the feasibility, the path tracking of the multi-degree-of-freedom(multi-DOF) manipulator was realized by combining the maximum entropy deep reinforcement learning algorithm. The experimental results show that our method has a good effect on the path tracking of the manipulator, which not only avoids the process of finding the inverse kinematics solution, but also requires no dynamic model. Therefore, we believe that our method has great significance in the study of manipulator path tracking.",
        "link": "http://dx.doi.org/10.20944/preprints202305.1862.v1"
    },
    {
        "id": 27626,
        "title": "Scalable Multi-Agent Reinforcement Learning for Residential Load Scheduling Under Data Governance",
        "authors": "Zhaoming Qin, Nanqing Dong, Di Liu, Junwei Cao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4545912"
    },
    {
        "id": 27627,
        "title": "A Deep Reinforcement Learning Approach to The Ancient Indian Game - Chowka Bhara",
        "authors": "Annapurna P Patil, SANJAY RAGHAVENDRA, Shruthi Srinarasi, Reshma Ram",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement Learning (RL) is the study of how Artificial Intelligence (AI) agents learn to make their own decisions in an environment to maximize the cumulative reward received. Although there has been notable progress in the application of RL for games, the category of ancient Indian games has remained almost untouched. Chowka Bhara is one such ancient Indian board game. This work aims at developing a Q-Learning-based RL Chowka Bhara player whose strategies and methodologies are obtained from three Strategic Players viz. Fast Player, Random Player, and Balanced Player. It is observed through the experimental results that the Q-Learning Player outperforms all three Strategic Players.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16780414.v1"
    },
    {
        "id": 27628,
        "title": "A STUDY OF EMERGENT COOPERATIVE BEHAVIORS OF MULTI-AGENT SYSTEMS USING REINFORCEMENT LEARNING",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.48009/1_iis_2023_116"
    },
    {
        "id": 27629,
        "title": "A Reinforcement Learning-based Approach to Testing GUI of Moblie Applications",
        "authors": "Chuanqi Tao, Yuemeng Gao, Hongjing Guo, Jerry Gao",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nWith the popularity of mobile devices, the software market of mobile applications has been booming in recent years. Android applications occupy a vast market share. However, the applications inevitably con- tain defects. Defects may affect the user experience and even cause severe economic losses. This paper proposes ATAC and ATPPO, which apply reinforcement learning to Android GUI testing to mitigate the state explosion problem. The article designs a new reward function and a new state representation. It also constructs two GUI testing models(ATAC and ATPPO) based on A2C and PPO algorithms to save memory space and accelerate training speed. Empirical studies on twenty open-source applications from GitHub demonstrate that: (1) ATAC performs best in 16 of 20 apps in code coverage and defects more exceptions; (2) ATPPO can get higher code coverage in 15 of 20 apps and defects more exceptions; (3)Compared with state-of-art tools Monkey and ARES, ATAC, and ATPPO shows higher code cov- erage and detects more errors. ATAC and ATPPO not only can cover more code coverage but also can effectively detect more exceptions. This paper also introduces Finite-State Machine into the reinforcement learning framework to avoid falling into the local optimal state, which provides high-level guidance for further improving the test efficiency.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2565153/v1"
    },
    {
        "id": 27630,
        "title": "Asset Allocation Based On Reinforcement Learning",
        "authors": "Yaoming Li, Junfeng Wu, Yun Chen",
        "published": "2020-7-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indin45582.2020.9442214"
    },
    {
        "id": 27631,
        "title": "Multi-agent deep reinforcement learning (MADRL) meets multi-user MIMO systems",
        "authors": "Heunchul Lee, Jaeseong Jeong",
        "published": "2021-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685914"
    },
    {
        "id": 27632,
        "title": "Reinforcement Learning for Accident Risk-Adaptive V2X Networking",
        "authors": "Seungmo Kim, Byung-Jun Kim",
        "published": "2020-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2020-fall49728.2020.9348445"
    },
    {
        "id": 27633,
        "title": "Aircraft Line Maintenance Scheduling Using Simulation and Reinforcement Learning",
        "authors": "Simon Widmer, Syed Shaukat, Cheng-Lung Wu",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408075"
    },
    {
        "id": 27634,
        "title": "Joint Modeling of Reaction Times and Choice Improves Parameter Identifiability in Reinforcement Learning Models",
        "authors": "Ian C. Ballard, Samuel M. McClure",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractBackgroundReinforcement learning models provide excellent descriptions of learning in multiple species across a variety of tasks. Many researchers are interested in relating parameters of reinforcement learning models to neural measures, psychological variables or experimental manipulations. We demonstrate that parameter identification is difficult because a range of parameter values provide approximately equal quality fits to data. This identification problem has a large impact on power: we show that a researcher who wants to detect a medium sized correlation (r= .3) with 80% power between a variable and learning rate must collect 60% more subjects than specified by a typical power analysis in order to account for the noise introduced by model fitting.New MethodWe derive a Bayesian optimal model fitting technique that takes advantage of information contained in choices and reaction times to constrain parameter estimates.ResultsWe show using simulation and empirical data that this method substantially improves the ability to recover learning rates.Comparison with Existing MethodsWe compare this method against the use of Bayesian priors. We show in simulations that the combined use of Bayesian priors and reaction times confers the highest parameter identifiability. However, in real data where the priors may have been misspecified, the use of Bayesian priors interferes with the ability of reaction time data to improve parameter identifiability.ConclusionsWe present a simple technique that takes advantage of readily available data to substantially improve the quality of inferences that can be drawn from parameters of reinforcement learning models.Highlights–Parameters of reinforcement learning models are particularly difficult to estimate–Incorporating reaction times into model fitting improves parameter identifiability–Bayesian weighting of choice and reaction times improves the power of analyses assessing learning rate",
        "link": "http://dx.doi.org/10.1101/306720"
    },
    {
        "id": 27635,
        "title": "A Knowledge-Guided Process Planning Approach with Reinforcement Learning",
        "authors": "Lijun Zhang, Hongjin Wu, Yelin Chen, Xuesong Wang, Yibing Peng",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWith the wide application of computer-aided technologies such as CAD and CAM in the manufacturing industry, more and more process documents and design documents generate multi-source process knowledge and expert experience. However, due to the diverse and complex representation of process knowledge, more effective methods are needed to mine a large amount of multi-source information and the explicit and implicit relationships between knowledge. Effective knowledge reuse in process planning still needs to be improved. This paper proposes a reinforcement learning approach that combines knowledge graphs and process decision-making activities in process planning to exploit the learning potential of process knowledge graphs. Firstly, a reinforcement learning environment for process planning is introduced to model the process planning problem as a sequential recommendation of process knowledge. Secondly, this paper designs in detail the state representation that combines process sequences and potential relationships between processes. This paper also creates a composite reward function that combines the process planning environment. In addition, a new algorithm is proposed for learning the proposed model more efficiently. Experimental results show that the network structure proposed in this paper has more accurate recommendation results than other methods. Finally, this paper takes flange as an example to verify the feasibility and effectiveness of the proposed method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3969456/v1"
    },
    {
        "id": 27636,
        "title": "Peer Review #1 of \"Heterogeneous mission planning for a single unmanned aerial vehicle (UAV) with attention-based deep reinforcement learning (v0.2)\"",
        "authors": "",
        "published": "2022-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1119v0.2/reviews/1"
    },
    {
        "id": 27637,
        "title": "A Dynamic Chemical Production Scheduling Method based on Reinforcement Learning",
        "authors": "Zhenyu Wu, Yin Wang, Limin Jia",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055985"
    },
    {
        "id": 27638,
        "title": "Reinforcement Learning, Bit by Bit",
        "authors": "Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, Zheng Wen",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1561/9781638282556"
    },
    {
        "id": 27639,
        "title": "Adaptive Event-based Reinforcement Learning Control",
        "authors": "Fancheng Meng, Aimin An, Erchao Li, Shuo Yang",
        "published": "2019-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc.2019.8832922"
    },
    {
        "id": 27640,
        "title": "Adaptive Adversarial Training for Meta Reinforcement Learning",
        "authors": "Shiqi Chen, Zhengyu Chen, Donglin Wang",
        "published": "2021-7-18",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9534316"
    },
    {
        "id": 27641,
        "title": "Multi-agent Reinforcement Learning Approach for Scheduling Cluster Tools with Condition Based Chamber Cleaning Operations",
        "authors": "Cheolhui Hong, Tae-Eog Lee",
        "published": "2018-12",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00143"
    },
    {
        "id": 27642,
        "title": "Reinforcement Learning for Virtually-Coupled Trains Using Q-Learning with Continuous Actions",
        "authors": "Zhengda Liu, Shigen Gao, Hang Zhang, Hairong Dong",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240437"
    },
    {
        "id": 27643,
        "title": "Enhancing HVAC control systems through transfer learning with deep reinforcement learning agents",
        "authors": "Kevlyn Kadamala, Des Chambers, Enda Barrett",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.segy.2024.100131"
    },
    {
        "id": 27644,
        "title": "Learning Accurate Long-term Dynamics for Model-based Reinforcement Learning",
        "authors": "Nathan Lambert, Albert Wilcox, Howard Zhang, Kristofer S. J. Pister, Roberto Calandra",
        "published": "2021-12-14",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683134"
    },
    {
        "id": 27645,
        "title": "Learning-Based Resource Management for Maritime Communications",
        "authors": "Liang Xiao, Helin Yang, Weihua Zhuang, Minghui Min",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-32138-2_4"
    },
    {
        "id": 27646,
        "title": "Bridging the Reality Gap of Reinforcement Learning based Traffic Signal Control using Domain Randomization and Meta Learning",
        "authors": "Arthur Müller, Matthia Sabatelli",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10421987"
    },
    {
        "id": 27647,
        "title": "Adaptive User Scheduling and Resource Allocation in Wireless Federated Learning Networks: A Deep Reinforcement Learning Approach",
        "authors": "Changxiang Wu, Yijing Ren, Daniel K. C. So",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279678"
    },
    {
        "id": 27648,
        "title": "Reinforcement learning predicts frustration-related motor invigoration",
        "authors": "Bowen Fung, Xin Sui, Colin Camerer, Dean Mobbs",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2019.1020-0"
    },
    {
        "id": 27649,
        "title": "Multiagent Reinforcement Learning",
        "authors": "Jonathan P. How, Dong-Ki Kim, Samir Wadhwania",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4471-5102-9_100066-1"
    },
    {
        "id": 27650,
        "title": "Setting Up ML Agents Toolkit",
        "authors": "Abhilash Majumder",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6503-1_3"
    },
    {
        "id": 27651,
        "title": "Vocal Learning: Shaping by Social Reinforcement",
        "authors": "Daniel Y. Takahashi",
        "published": "2019-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cub.2019.01.001"
    },
    {
        "id": 27652,
        "title": "Generalized Deep Reinforcement Learning for Trading",
        "authors": "Junyoung Sim, Benjamin Kirk",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "This paper proposes generalized deep reinforcement learning with multivariate state space, discrete rewards, and adaptive synchronization for trading any stock held in the S&P 500. Specifically, the proposed trading model observes the daily historical data of a stock held in the S&P 500 and multiple market-indicating securities (SPY, IEF, EUR=X, GSG), selects a trading action, and observes a discrete reward that is based on the correctness of the selected action and independent of the volatility of stocks. The proposed trading model’s reward-maximizing behavior is optimized by using a standard deep q-network (DQN) with adaptive synchronization that stabilizes and enables to track learning performance on generalizing new experiences from each stock. The proposed trading model was trained on the top 50 holdings of the S&P 500 and tested on the top 100 holdings of the S&P 500 starting from 2006 to 2022. Experimental results suggest that the proposed trading model significantly outperforms the 100% long-strategy benchmark in terms of annualized return, Sharpe ratio, and maximum drawdown.",
        "link": "http://dx.doi.org/10.47611/jsrhs.v12i1.4316"
    },
    {
        "id": 27653,
        "title": "Conclusions and Future Directions",
        "authors": "",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119699057.ch6"
    },
    {
        "id": 27654,
        "title": "Reinforcement learning in queues",
        "authors": "U. Ayesta",
        "published": "2022-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11134-022-09844-w"
    },
    {
        "id": 27655,
        "title": "Accelerated Reinforcement Learning for Sentence Generation by Vocabulary Prediction",
        "authors": "Kazuma Hashimoto, Yoshimasa Tsuruoka",
        "published": "2019",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/n19-1315"
    },
    {
        "id": 27656,
        "title": "Rapid Locomotion via Reinforcement Learning",
        "authors": "Gabriel B. Margolis, Ge Yang, Kartik Paigwar, Tao Chen, Pulkit Agrawal",
        "published": "2022-6-27",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2022.xviii.022"
    },
    {
        "id": 27657,
        "title": "Distilling Reinforcement Learning Tricks for Video Games",
        "authors": "Anssi Kanervisto, Christian Scheller, Yanick Schraner, Ville Hautamaki",
        "published": "2021-8-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog52621.2021.9618997"
    },
    {
        "id": 27658,
        "title": "Ramping and phasic dopamine activity accounts for efficient cognitive resource allocation during reinforcement learning",
        "authors": "Minryung R. Song, Sang Wan Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractDopamine activity may transition between two patterns: phasic responses to reward-predicting cues and ramping activity arising when an agent approaches the reward. However, when and why dopamine activity transitions between these modes is not understood. We hypothesize that the transition between ramping and phasic patterns reflects resource allocation which addresses the task dimensionality problem during reinforcement learning (RL). By parsimoniously modifying a standard temporal difference (TD) learning model to accommodate a mixed presentation of both experimental and environmental stimuli, we simulated dopamine transitions and compared it with experimental data from four different studies. The results suggested that dopamine transitions from ramping to phasic patterns as the agent narrows down candidate stimuli for the task; the opposite occurs when the agent needs to re-learn candidate stimuli due to a value change. These results lend insight into how dopamine deals with the tradeoff between cognitive resource and task dimensionality during RL.",
        "link": "http://dx.doi.org/10.1101/381103"
    },
    {
        "id": 27659,
        "title": "Einführung",
        "authors": "Andreas Folkers",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-28886-0_1"
    },
    {
        "id": 27660,
        "title": "Deep Reinforcement Learning based Energy Scheduling for Edge Computing",
        "authors": "Qinglin Yang, Peng Li",
        "published": "2020-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smartcloud49737.2020.00041"
    },
    {
        "id": 27661,
        "title": "AMF Optimal Placement based on Deep Reinforcement Learning in Heterogeneous Radio Access Network",
        "authors": "hao jin, Wenzhe Pang, Chenglin Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTo support various service requirements such as massive Machine Type Communications, Ultra-Reliable and Low-Latency Communications in 5G scenario, Network Function Virtualization (NFV) plays an important role in the 5G network architecture to manage and orchestrate network services. As the key network function responsible for mobility management, Access and Mobility Management Function (AMF) can be deployed flexibly at the edge of the radio access network to improve the performance of mobility management based on NFV. In this paper, the optimal placement of AMF is addressed based on Deep Reinforcement Learning (DRL) in a heterogeneous radio access network, which aims to minimize the network utility including the average delay of mobility management requests at AMF, the average wired hops to relay the requests and the cost of AMF instances. By considering time-varying features including user mobility and the arrival rate of user mobility management requests, an AMF optimal placement approach is proposed for the long term optimization. Simulation results show that the performance of the proposed DRL based AMF optimal placement approach outperforms that of the baselines.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-14323/v1"
    },
    {
        "id": 27662,
        "title": "Approximation in Quantum Computing",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1_5"
    },
    {
        "id": 27663,
        "title": "Adaptive Data Replication Optimization Based on Reinforcement Learning",
        "authors": "Chee Keong Wee, Richi Nayak",
        "published": "2020-12-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci47803.2020.9308306"
    },
    {
        "id": 27664,
        "title": "Model-Free Reinforcement Learning-Based Control for Continuous-Time Systems",
        "authors": "Kyriakos G. Vamvoudakis",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-44184-5_100065"
    },
    {
        "id": 27665,
        "title": "Portfolio Optimization using Reinforcement Learning",
        "authors": "Dhruv Joshi",
        "published": "2022-10-1",
        "citations": 0,
        "abstract": "Reinforcement Learning has contributed to the automation of various tasks, including electronic algorithmic trading. The main benefit of Reinforcement Learning in financial applications is the relaxation of assumptions on market models and hand-picked features, allowing a data-driven, automated process. The performance of Reinforcement Learning agents on the portfolio management problem is measured. A finite universe of financial instruments such as stocks is selected and the trained agent is constructing an internal representation (model) of the market, allowing it to determine how to optimally allocate funds of a finite budget to those assets. The agent is trained on the actual market prices. The performance metrics are then compared with those of standard portfolio management algorithms on a dataset that has not been used before. A successful agent can be used as a consulting software for portfolio managers or it can be used for low-frequency algorithmic trading. Moreover, it can allow us to identify the missing pieces of the existing models and suggest directions to improve them.",
        "link": "http://dx.doi.org/10.55041/ijsrem16672"
    },
    {
        "id": 27666,
        "title": "FWA-RL: Fireworks Algorithm with Policy Gradient for Reinforcement Learning",
        "authors": "Maiyue Chen, Ying Tan",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec53210.2023.10254081"
    },
    {
        "id": 27667,
        "title": "Challenging Risk-Neutrality, Reinforcement Learning for Options Pricing in Indian Options market",
        "authors": "Dhruv Mahajan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3917860"
    },
    {
        "id": 27668,
        "title": "Refinement Of Reinforcement Learning Algorithms Guided By Counterexamples",
        "authors": "Briti Gangopadhyay, Somi Vishnoi, Pallab Dasgupta",
        "published": "2022-6-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wintechcon55229.2022.9832063"
    },
    {
        "id": 27669,
        "title": "Deep Reinforcement Learning for Stabilization of Large-scale Probabilistic Boolean Networks",
        "authors": "Sotiris Moschoyiannis, Evangelos Chatzaroulas, Vytenis Sliogeris, Yuhu Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe ability to direct a Probabilistic Boolean Network (PBN) to a desired state is important to applications such as targeted therapeutics in cancer biology. Reinforcement Learning (RL) has been proposed as a framework that solves a discrete-time optimal control problem cast as a Markov Decision Process. We focus on an integrative framework powered by a model-free deep RL method that can address different flavours of the control problem (e.g., withorwithout control inputs; attractor stateora subset of the state space as the target domain). The method is agnostic to the distribution of probabilities for the next state, hence it does not use the probability transition matrix. The time complexity is onlylinearon the time steps, or interactions between the agent (deep RL) and the environment (PBN), during training. Indeed, we explore thescalabilityof the deep RL approach to (set) stabilization of large-scale PBNs and demonstrate successful control on large networks, including a metastatic melanoma PBN with200 nodes.",
        "link": "http://dx.doi.org/10.1101/2022.10.21.513276"
    },
    {
        "id": 27670,
        "title": "Mutual Reinforcement Learning with Heterogenous Agents",
        "authors": "Cameron Reid, Snehasis Mukhopadhyay",
        "published": "2021-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smartcomp52413.2021.00081"
    },
    {
        "id": 27671,
        "title": "Evolvable Motion-planning Method using Deep Reinforcement Learning",
        "authors": "Kaichiro Nishi, Nobuaki Nakasu",
        "published": "2021-5-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9561602"
    },
    {
        "id": 27672,
        "title": "Monte-Carlo and Temporal-Difference for Prediction",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-11"
    },
    {
        "id": 27673,
        "title": "Tag-Aware Recommender System Based on Deep Reinforcement Learning",
        "authors": "Zhiruo Zhao, Lei Cao, Xiliang Chen, Zhixiong Xu",
        "published": "No Date",
        "citations": 2,
        "abstract": "Recently, the application of deep reinforcement learning in recommender system is flourishing and stands out by overcoming drawbacks of traditional methods and achieving high recommendation quality. The dynamics, long-term returns and sparse data issues in recommender system have been effectively solved. But the application of deep reinforcement learning brings problems of interpretability, overfitting, complex reward function design, and user cold start. This paper proposed a tag-aware recommender system based on deep reinforcement learning without complex function design, taking advantage of tags to make up for the interpretability problems existing in recommender system. Our experiment is carried out on MovieLens dataset. The result shows that, DRL based recommender system is superior than traditional algorithms in minimum error and the application of tags has little effect on accuracy when making up for interpretability. In addition, DRL based recommender system has excellent performance on user cold start problems.",
        "link": "http://dx.doi.org/10.20944/preprints202101.0176.v1"
    },
    {
        "id": 27674,
        "title": "ML-based Reinforcement Learning Approach for Power Management in SoCs",
        "authors": "David Akselrod",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/socc46988.2019.1570548498"
    },
    {
        "id": 27675,
        "title": "Image Captioning: From Encoder-Decoder to Reinforcement Learning",
        "authors": "Yu Tang",
        "published": "2022-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icispc57208.2022.00009"
    },
    {
        "id": 27676,
        "title": "Reinforcement Learning based Control of a Quadruped Robot",
        "authors": "Ancy A, V R Jisha",
        "published": "2022-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indicon56171.2022.10039800"
    },
    {
        "id": 27677,
        "title": "Model Free DEAP Controller Learned by Reinforcement Learning DDPG Algorithm",
        "authors": "Jakub Bernat, Dawid Apanasiewicz",
        "published": "2020-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/argencon49523.2020.9505344"
    },
    {
        "id": 27678,
        "title": "Deep Reinforcement Learning solution for Scheduling critical notifications in a Digital Twin cluster",
        "authors": "Mira Vrbaski, Miodrag Bolic, Shikharesh Majumdar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nA Scheduling approach for a Critical Monitoring System in a Digital Twin (DT) cluster based on Deep Reinforcement Learning (DRL) is presented. Recent advances in the DRL field inspired us to research how to build a solution that learns to manage the cloud container’s resources directly from experience. The paper presents a multi-objective Scheduling approach for containerized microservice Critical Notification system applications based on DRL (SCN-DRL), where Neural Networks (NN) are used RL Agents. This paper implements, compares and evaluates three Neural Networks (NN) that: a) provide scheduling of the DT cluster’s notification jobs, b) outperform state-of-art heuristics, and c) keep steady performance when notification workload increases from 10 to 90%. Furthermore, resilience to resource container failures is a critical component of the distributed system; our proposed research shows that SCN-DRL is resilient to sudden resource drops by 10%.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3667938/v1"
    },
    {
        "id": 27679,
        "title": "Deep Reinforcement Learning for Alleviation of Unbalanced Active Powers Using Distributed Batteries in LV Residential Distribution System",
        "authors": "Watcharakorn Pinthurat, Branislav Hredzak",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>The high penetration and uneven distribution of single-phase rooftop PVs and load demands in power systems may lead to unbalanced active powers, adversely impacting power quality and system reliability. This paper introduces a strategy based on multi-agent deep reinforcement learning to address these unbalanced active powers. The approach involves deploying single-phase battery systems throughout the LV residential distribution system, subsidized by the utility. Initially, the unbalanced active powers are framed as a Markov game, which is then addressed using a multi-agent deep deterministic policy gradient algorithm. The strategy relies on local measurements, with agents' experiences centrally shared during training for cooperative tasks. Notably, information about the phase connections of the battery systems becomes unnecessary. The strategy learns from historical data, gradually mastering the process. Real data from rooftop PVs and demands in a four-wire LV residential distribution system validate the effectiveness of the proposed approach. Acting as adaptive agents, the battery systems collaboratively operate by adjusting active powers to minimize neutral current at the point of common connection.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24499090"
    },
    {
        "id": 27680,
        "title": "Deep Reinforcement Learning for Mobile Robot Navigation",
        "authors": "Martin Gromniak, Jonas Stenzel",
        "published": "2019-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acirs.2019.8935944"
    },
    {
        "id": 27681,
        "title": "Stabilized Platform Attitude Control Based on Deep Reinforcement Learning Using Disturbance Observer-Based",
        "authors": "Aiqing Huo, Xue Jiang, Shuhan Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn order to address the difficulties of attitude control for stabilized platform in rotary steerable drilling, including instability, difficult to control, and severe friction, we proposed a Disturbance Observer-Based Deep Deterministic Policy Gradient (DDPG_DOB) control algorithm. The stabilized platform in rotary steering drilling was taken as a research object. On the basis of building a stabilized platform controlled object model and a LuGre friction model, DDPG algorithm is used to design a deep reinforcement learning controller. After the overall framework of the stabilized platform control system was given, appropriate state vectors were selected, a reward function satisfying the system requirement was designed, an Actor-Critic network structure was constructed and the network parameters was updated. Moreover considering the non-linear friction disturbance that causes steady-state errors, oscillations, and hysteresis phenomena in the stabilized platform control system, a DDPG algorithm based on the disturbance observer was proposed to eliminate the effects of friction disturbance so that to enhance robustness and anti-interference ability of the stabilized platform control system. Experimental results show that the DDPG_DOB control method had good set-point control performance and tracking effect. The tracking error of the tool face angle can be maintained within ± 8.7% and the DDPG_DOB control method can effectively suppress friction interference and improve the nonlinear hysteresis phenomenon when the system is affected by friction interference,enhancing the robustness of the system.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2905841/v1"
    },
    {
        "id": 27682,
        "title": "Pairs Trading Using Clustering and Deep Reinforcement Learning",
        "authors": "Raktim Roychoudhury, Rahul Bhagtani, Aditya Daftari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4504599"
    },
    {
        "id": 27683,
        "title": "Efficient Distributional Reinforcement Learning with Kullback-Leibler Divergence Regularization",
        "authors": "Renxing Li, Zhiwei Shang, Chunhua Zheng, Huiyun Li, Qing Liang, Yunduan Cui",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this article, we address the issues of stability and data-efficiency in reinforcement learning (RL). A novel RL approach, Kullback–Leibler divergence-regularized distributional RL (KLC51) is proposed to integrate the advantages of both stability in the distributional RL and data-efficiency in the Kullback-Leibler (KL) divergence-regularized RL in one framework. KLC51 derived the  Bellman equation and the TD errors regularized by KL divergence in a distributional perspective and explored the approximated strategies of properly mapping the corresponding Boltzmann softmax term into distributions. Evaluated by several benchmark tasks with different complexity, the proposed method clearly illustrates the positive effect of the KL divergence regularization to the distributional RL including exclusive exploration behaviors and smooth value function update, and successfully demonstrates its significant superiority in both learning stability and data-efficiency compared with the related baseline approaches.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19679454.v1"
    },
    {
        "id": 27684,
        "title": "Optimization of job shop scheduling problem based on deep reinforcement learning",
        "authors": "Dongping Qiao, Lvqi Duan, HongLei Li, Yanqiu Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAiming at the optimization problem of minimizing the maximum completion time in job shop scheduling, a deep reinforcement learning optimization algorithm is proposed. First, a deep reinforcement learning scheduling environment is built based on the disjunctive graph model, and three channels of state characteristics are established. The action space consists of 20 designed combination scheduling rules. The reward function is designed based on the proportional relationship between the total work of the scheduled operation and the current maximum completion time. The deep convolutional neural network is used to construct action network and target network, and the state features are used as inputs to output the Q value of each action. Then, the action is selected by using the action validity exploration and exploitation strategy. Finally, the immediate reward is calculated and the scheduling environment is updated. Experiments are carried out using benchmark instances to verify the algorithm. The results show that it can balance solution quality and computation time effectively, and the trained agent has good generalization ability to the scheduling problem in the non-zero initial state.\nKeywords\nDeep reinforcement learning · Job shop scheduling · Scheduling rules · Agent",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2148871/v1"
    },
    {
        "id": 27685,
        "title": "MERMAID: An Open Source Automated Hit-to-Lead Method Based on Deep Reinforcement Learning",
        "authors": "Daiki Erikawa, Nobuaki Yasuo, Masakazu Sekijima",
        "published": "No Date",
        "citations": 0,
        "abstract": "The hit-to-lead process makes the physicochemical properties of the hit compounds that show the desired type of activity obtained in the screening assay more drug-like. Deep learning-based molecular generative models are expected to contribute to the hit-to-lead process.The simplified molecular input line entry system (SMILES), which is a string of alphanumeric characters representing the chemical structure of a molecule, is one of the most commonly used representations of molecules, and molecular generative models based on SMILES have achieved significant success. However, in contrast to molecular graphs, during the process of generation, SMILES are not considered as valid SMILES. Further, it is quite difficult to generate molecules starting from a certain molecule, thus making it difficult to apply SMILES to the hit-to-lead process.In this study, we have developed a SMILES-based generative model that can be generated starting from a certain compound. This method generates partial SMILES and inserts it into the original SMILES using Monte Carlo Tree Search and a Recurrent Neural Network.We validated our method using a molecule dataset obtained from the ZINC database and successfully generated molecules that were both well optimized for the objectives of the quantitative estimate of drug-likeness (QED) and penalized octanol-water partition coefficient (PLogP) optimization.The source code is available at https: //github.com/sekijima-lab/mermaid.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.14450313.v1"
    },
    {
        "id": 27686,
        "title": "Reinforcement Learning-based Anomaly Detection for PHM applications",
        "authors": "Samir Khan, Takehisa Yairi, Shinichi Nakasuka, Seiji Tsutsumi",
        "published": "2022-3-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aero53065.2022.9843543"
    },
    {
        "id": 27687,
        "title": "A Novel Deep Reinforcement Learning Algorithm for Online Antenna Tuning",
        "authors": "Eren Balevi, Jeffrey G. Andrews",
        "published": "2019-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9013308"
    },
    {
        "id": 27688,
        "title": "Enhancing Hybrid System Based on Reinforcement Learning",
        "authors": "",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22266/ijies2024.0229.50"
    },
    {
        "id": 27689,
        "title": "Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning Approach",
        "authors": "Soroush Saghafian",
        "published": "2023-10-4",
        "citations": 0,
        "abstract": "A main research goal in various studies is to use an observational data set and provide a new set of counterfactual guidelines that can yield causal improvements. Dynamic Treatment Regimes (DTRs) are widely studied to formalize this process and enable researchers to find guidelines that are both personalized and dynamic. However, available methods in finding optimal DTRs often rely on assumptions that are violated in real-world applications (e.g., medical decision making or public policy), especially when (a) the existence of unobserved confounders cannot be ignored, and (b) the unobserved confounders are time varying (e.g., affected by previous actions). When such assumptions are violated, one often faces ambiguity regarding the underlying causal model that is needed to be assumed to obtain an optimal DTR. This ambiguity is inevitable because the dynamics of unobserved confounders and their causal impact on the observed part of the data cannot be understood from the observed data. Motivated by a case study of finding superior treatment regimes for patients who underwent transplantation in our partner hospital (Mayo Clinic) and faced a medical condition known as new-onset diabetes after transplantation, we extend DTRs to a new class termed Ambiguous Dynamic Treatment Regimes (ADTRs), in which the causal impact of treatment regimes is evaluated based on a “cloud” of potential causal models. We then connect ADTRs to Ambiguous Partially Observable Markov Decision Processes (APOMDPs) proposed by Saghafian (2018) , and consider unobserved confounders as latent variables but with ambiguous dynamics and causal effects on observed variables. Using this connection, we develop two reinforcement learning methods termed Direct Augmented V-Learning (DAV-Learning) and Safe Augmented V-Learning (SAV-Learning), which enable using the observed data to effectively learn an optimal treatment regime. We establish theoretical results for these learning methods, including (weak) consistency and asymptotic normality. We further evaluate the performance of these learning methods both in our case study (using clinical data) and in simulation experiments (using synthetic data). We find promising results for our proposed approaches, showing that they perform well even compared with an imaginary oracle who knows both the true causal model (of the data-generating process) and the optimal regime under that model. Finally, we highlight that our approach enables a two-way personalization; obtained treatment regimes can be personalized based on both patients’ characteristics and physicians’ preferences.This paper was accepted by David Simchi-Levi, data science.Supplemental Material: The data files and online appendix are available at https://doi.org/10.1287/mnsc.2022.00883 .",
        "link": "http://dx.doi.org/10.1287/mnsc.2022.00883"
    },
    {
        "id": 27690,
        "title": "Safe Reinforcement Learning via Probabilistic Timed Computation Tree Logic",
        "authors": "Li Qian, Jing Liu",
        "published": "2020-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9207384"
    },
    {
        "id": 27691,
        "title": "Reinforcement learning based adaptive metaheuristics",
        "authors": "Michele Tessari, Giovanni Iacca",
        "published": "2022-7-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3520304.3533983"
    },
    {
        "id": 27692,
        "title": "Case Studies in ML Agents",
        "authors": "Abhilash Majumder",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6503-1_7"
    },
    {
        "id": 27693,
        "title": "A Reinforcement Learning Algorithm based on Neural Network for Economic Dispatch",
        "authors": "Liying Yu, Ning Li",
        "published": "2020-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9188641"
    },
    {
        "id": 27694,
        "title": "Policy Gradient Methods",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_9"
    },
    {
        "id": 27695,
        "title": "Curiosity-Driven Exploration",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_13"
    },
    {
        "id": 27696,
        "title": "Deep reinforcement learning for partial differential equation control",
        "authors": "Amir-massoud Farahmand, Saleh Nabi, Daniel N. Nikovski",
        "published": "2017-5",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2017.7963427"
    },
    {
        "id": 27697,
        "title": "Deep Reinforcement Learning for Controlling the Groundwater in Slopes",
        "authors": "Aynaz Biniyaz, Behnam Azmoon, Zhen Liu",
        "published": "2022-3-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1061/9780784484036.065"
    },
    {
        "id": 27698,
        "title": "On Deep Reinforcement Learning for Spacecraft Guidance",
        "authors": "Kirk Hovell, Steve Ulrich",
        "published": "2020-1-6",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2020-1600"
    },
    {
        "id": 27699,
        "title": "Deep Reinforcement Learning Based Grid-Forming Inverter",
        "authors": "Ebrahim Balouji, Karl Bäckstrüm, Tomas McKelvey",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ias54024.2023.10406664"
    },
    {
        "id": 27700,
        "title": "Recurrent neural networks that learn multi-step visual routines with reinforcement learning",
        "authors": "Sami Mollard, Catherine Wacongne, Sander Bohte, Pieter Roelfsema",
        "published": "No Date",
        "citations": 1,
        "abstract": "Many problems can be decomposed into series of subproblems that are solved sequentially. When the subproblems are solved, relevant intermediate results have to be stored and propagated from one subproblem to the next, until the overarching goal has been completed. The same applies for visual tasks that can be decomposed into sequences of elemental visual operations: experimental evidence suggests that for visual tasks, intermediate results are stored in working memory as an enhancement of neural activity in the visual cortex. The focus of enhanced activity is then available for subsequent subroutines to act upon. However, it remains unknown how those dynamics can emerge in neural networks that are trained with only rewards, as animals are. Here, we propose a new recurrent architecture capable of solving composite visual tasks in a reinforcement learning context. We trained neural networks on three visual tasks for which electrophysiological recordings of monkeys' visual cortex are available and updated weights following RELEARNN, a biologically plausible four-factor Hebbian learning rule, which is local both in time and space. The networks learned an abstract, general rule and selected the appropriate sequence of elemental operations, solely based on the characteristics of the visual stimuli and the reward structure. The activity of the units of the neural network resembled the activity of neurons in the visual cortex of monkeys solving the same tasks. Relevant information that needed to be exchanged between subroutines was maintained as a focus of enhanced activity and passed on to the subsequent subroutines. Our results demonstrate how a biologically plausible learning rule can train a recurrent neural network on multistep visual tasks.",
        "link": "http://dx.doi.org/10.1101/2023.07.03.547198"
    },
    {
        "id": 27701,
        "title": "Deep reinforcement learning for power system: An overview",
        "authors": "",
        "published": "2019",
        "citations": 98,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2019.00920"
    },
    {
        "id": 27702,
        "title": "Intrinsic fluctuations of reinforcement learning promote cooperation",
        "authors": "Wolfram Barfuss, Janusz M. Meylahn",
        "published": "2023-1-24",
        "citations": 5,
        "abstract": "AbstractIn this work, we ask for and answer what makes classical temporal-difference reinforcement learning with $$\\epsilon$$\nϵ\n-greedy strategies cooperative. Cooperating in social dilemma situations is vital for animals, humans, and machines. While evolutionary theory revealed a range of mechanisms promoting cooperation, the conditions under which agents learn to cooperate are contested. Here, we demonstrate which and how individual elements of the multi-agent learning setting lead to cooperation. We use the iterated Prisoner’s dilemma with one-period memory as a testbed. Each of the two learning agents learns a strategy that conditions the following action choices on both agents’ action choices of the last round. We find that next to a high caring for future rewards, a low exploration rate, and a small learning rate, it is primarily intrinsic stochastic fluctuations of the reinforcement learning process which double the final rate of cooperation to up to 80%. Thus, inherent noise is not a necessary evil of the iterative learning process. It is a critical asset for the learning of cooperation. However, we also point out the trade-off between a high likelihood of cooperative behavior and achieving this in a reasonable amount of time. Our findings are relevant for purposefully designing cooperative algorithms and regulating undesired collusive effects.",
        "link": "http://dx.doi.org/10.1038/s41598-023-27672-7"
    },
    {
        "id": 27703,
        "title": "Peer Review #2 of \"Heterogeneous mission planning for a single unmanned aerial vehicle (UAV) with attention-based deep reinforcement learning (v0.2)\"",
        "authors": "",
        "published": "2022-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj-cs.1119v0.2/reviews/2"
    },
    {
        "id": 27704,
        "title": "A Survey of Reinforcement Learning in Intrusion Detection",
        "authors": "Zheni Utic, Kandethody Ramachandran",
        "published": "2022-5-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaic53980.2022.9897058"
    },
    {
        "id": 27705,
        "title": "A Reinforcement Learning based Eye-Gaze Behavior Tracking",
        "authors": "R Deepalakshmi, J Amudha",
        "published": "2021-10-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcat52182.2021.9587480"
    },
    {
        "id": 27706,
        "title": "Unmanned Aerial Vehicle Trajectory Planning via Staged Reinforcement Learning",
        "authors": "Chenyang Xi, Xinfu Liu",
        "published": "2020-9",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icuas48674.2020.9213983"
    },
    {
        "id": 27707,
        "title": "Review of Deep Reinforcement Learning for Robot Manipulation",
        "authors": "Hai Nguyen, Hung La",
        "published": "2019-2",
        "citations": 107,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/irc.2019.00120"
    },
    {
        "id": 27708,
        "title": "Self-Optimizing Path Tracking Controller for Intelligent Vehicles Based on Reinforcement Learning",
        "authors": "Jichang Ma, Hui Xie, Kang Song, Hao Liu",
        "published": "2021-12-27",
        "citations": 4,
        "abstract": "The path tracking control system is a crucial component for autonomous vehicles; it is challenging to realize accurate tracking control when approaching a wide range of uncertain situations and dynamic environments, particularly when such control must perform as well as, or better than, human drivers. While many methods provide state-of-the-art tracking performance, they tend to emphasize constant PID control parameters, calibrated by human experience, to improve tracking accuracy. A detailed analysis shows that PID controllers inefficiently reduce the lateral error under various conditions, such as complex trajectories and variable speed. In addition, intelligent driving vehicles are highly non-linear objects, and high-fidelity models are unavailable in most autonomous systems. As for the model-based controller (MPC or LQR), the complex modeling process may increase the computational burden. With that in mind, a self-optimizing, path tracking controller structure, based on reinforcement learning, is proposed. For the lateral control of the vehicle, a steering method based on the fusion of the reinforcement learning and traditional PID controllers is designed to adapt to various tracking scenarios. According to the pre-defined path geometry and the real-time status of the vehicle, the interactive learning mechanism, based on an RL framework (actor–critic—a symmetric network structure), can realize the online optimization of PID control parameters in order to better deal with the tracking error under complex trajectories and dynamic changes of vehicle model parameters. The adaptive performance of velocity changes was also considered in the tracking process. The proposed controlling approach was tested in different path tracking scenarios, both the driving simulator platforms and on-site vehicle experiments have verified the effects of our proposed self-optimizing controller. The results show that the approach can adaptively change the weights of PID to maintain a tracking error (simulation: within ±0.071 m; realistic vehicle: within ±0.272 m) and steering wheel vibration standard deviations (simulation: within ±0.04°; realistic vehicle: within ±80.69°); additionally, it can adapt to high-speed simulation scenarios (the maximum speed is above 100 km/h and the average speed through curves is 63–76 km/h).",
        "link": "http://dx.doi.org/10.3390/sym14010031"
    },
    {
        "id": 27709,
        "title": "Effective Radio Resource Allocation for IoT Random Access by Using Reinforcement Learning",
        "authors": "Yen-Wen Chen Yen-Wen Chen, Ji-Zheng You Yen-Wen Chen",
        "published": "2022-9",
        "citations": 0,
        "abstract": "\n                        <p>Emerging intelligent and highly interactive services result in the mass deployment of internet of things (IoT) devices. They are dominating wireless communication networks compared to human-held devices. Random access performance is one of the most critical issues in providing quick responses to various IoT services. In addition to the anchor carrier, the non-anchor carrier can be flexibly allocated to support the random access procedure in release 14 of the 3rd generation partnership project. However, arranging more non-anchor carriers for the use of random access will squeeze the data transmission bandwidth in a narrowband physical uplink shared channel. In this paper, we propose the prediction-based random access resource allocation (PRARA) scheme to properly allocated the non-anchor carrier by applying reinforcement learning. The simulation results show that the proposed PRARA can improve the random access performance and effectively use the radio resource compared to the rule-based scheme. </p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/160792642022092305015"
    },
    {
        "id": 27710,
        "title": "Exploring Applications of Deep Reinforcement Learning for Real-world Autonomous Driving Systems",
        "authors": "Victor Talpaert, Ibrahim Sobh, B. Kiran, Patrick Mannion, Senthil Yogamani, Ahmad El-Sallab, Patrick Perez",
        "published": "2019",
        "citations": 27,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007520305640572"
    },
    {
        "id": 27711,
        "title": "Exploring Applications of Deep Reinforcement Learning for Real-world Autonomous Driving Systems",
        "authors": "Victor Talpaert, Ibrahim Sobh, B. Kiran, Patrick Mannion, Senthil Yogamani, Ahmad El-Sallab, Patrick Perez",
        "published": "2019",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007520300002108"
    },
    {
        "id": 27712,
        "title": "Cellular Learning Automata Versus Multi-agent Reinforcement Learning",
        "authors": "Reza Vafashoar, Hossein Morshedlou, Alireza Rezvanian, Mohammad Reza Meybodi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-53141-6_8"
    },
    {
        "id": 27713,
        "title": "An Introduction to Deep\n                  Reinforcement Learning",
        "authors": "Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau",
        "published": "2018",
        "citations": 533,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1561/2200000071"
    },
    {
        "id": 27714,
        "title": "Optimization of Energy Consumption in 5G Networks Using Learning Algorithms in Reinforcement Learning",
        "authors": "Daffa Dean Naufal, Harry Ramza, Emilia Roza",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "The 5G network is an evolution of the 4G LTE (Long Term Evolution) fast internet network that is widely adopted in smart phones or gadgets. 5G networks offer faster wireless internet for various purposes. This research is a literature review of several articles related to machine learning, specifically regarding energy consumption optimization with 5G networks and reinforcement learning algorithms.The results show that various techniques have evolved to overcome the complexity of large energy intake including integration with 5G networks and algorithms have been completed by many researchers. Related to electricity consumption, it was found that during 5G use cases, in a low site visitor load scenario and while reducing power intake takes precedence over QoS, power savings can be made by 80% with 50 ms latency, 75% with 20 ms and 10 ms latency, and 20% with 1 ms latency. If QoS is prioritized, then power savings reach a maximum of five percent with minimum impact in terms of latency. Moreover, with regards to power performance, it has been observed that DQN-assisted motion can offer improvements.",
        "link": "http://dx.doi.org/10.57152/malcom.v3i2.959"
    },
    {
        "id": 27715,
        "title": "RL-Chord: CLSTM-Based Melody Harmonization Using Deep Reinforcement Learning",
        "authors": "Shulei Ji, Xinyu Yang, Jing Luo, Juan Li",
        "published": "2024",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3248793"
    },
    {
        "id": 27716,
        "title": "Plume Tracing via Model-Free Reinforcement Learning Method",
        "authors": "Hangkai Hu, Shiji Song, C. L. Phillip Chen",
        "published": "2019-8",
        "citations": 41,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2018.2885374"
    },
    {
        "id": 27717,
        "title": "Treatment Recommendation with Preference-based Reinforcement Learning",
        "authors": "Nan Xu, Nitin Kamra, Yan Liu",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ickg52313.2021.00025"
    },
    {
        "id": 27718,
        "title": "Home Energy Recommendation System (HERS): A Deep Reinforcement Learning Method for Electricity Optimization in Smart Homes",
        "authors": "Salman Sadiq Shuvo, Yasin Yilmaz",
        "published": "No Date",
        "citations": 0,
        "abstract": "Aras activity dataset<div>NYISO dynamic electricity price</div><div>A2C implementation in Python</div><div><br></div><div>Article under review in IEEE Transactions on Smart Grid </div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.16574711"
    },
    {
        "id": 27719,
        "title": "Hyperheuristic Method Based on Deep Reinforcement Learning",
        "authors": "Hitoshi Iima, Yoshiyuki Nakamura",
        "published": "2022-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iiaiaai55812.2022.00068"
    },
    {
        "id": 27720,
        "title": "Adaptive Power Control using Reinforcement Learning in 5G Mobile Networks",
        "authors": "Hyebin Park, Yujin Lim",
        "published": "2020-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin48656.2020.9016566"
    },
    {
        "id": 27721,
        "title": "Reinforcement Learning with Symmetry Augmentation for Portfolio Management",
        "authors": "Amine Mohamed Aboussalah, Chi-Guhn Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3748132"
    },
    {
        "id": 27722,
        "title": "Graph convolution with topology refinement for Automatic Reinforcement Learning",
        "authors": "Jianghui Sang, Yongli Wang",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126621"
    },
    {
        "id": 27723,
        "title": "Pruning replay buffer for efficient training of deep reinforcement learning",
        "authors": "Gavin An, Sai Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) is a type of machine learning that develops artificial intelligence by training an algorithm through multiple generations to understand what strategies to use in various situations. RL has applications in virtually every field, from transportation to research. However, RL is limited in that it is very resource intensive, partially because of the necessity of a large replay buffer, which contains the data learned from each episode. This study provides knowledge on replay buffer reward mechanics to inform the creation of new pruning methods for improving RL efficiency. Specifically, we develop a novel approach designed to reduce storage complexity of the replay buffer and training data and thus improve model efficiency. We create three algorithms, Threshold Replay Buffer Pruning (TRBP), Cluster Replay Buffer Pruning (CRBP), and Inverse Threshold Replay Buffer Pruning (ITRBP), for this purpose, testing three contradicting theories on reward mechanics. We hypothesized that TRBP’s theory would be the most conducive to real-world conditions, which our results corroborated. These results indicated that TRBP can achieve a 2-fold reduction in replay buffer size with only a 5% reduction in score, while CRBP and ITRBP performed much worse. This supported the hypothesis that TRBP’s reward thesis is the most accurate out of the three algorithms, as well as demonstrated that TRBP is a potentially effective replay buffer pruning algorithm.",
        "link": "http://dx.doi.org/10.59720/23-068"
    },
    {
        "id": 27724,
        "title": "Parametric Circuit Optimization with Reinforcement Learning",
        "authors": "Changcheng Tang, Zuochang Ye, Yan Wang",
        "published": "2018-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isvlsi.2018.00045"
    },
    {
        "id": 27725,
        "title": "Penalized Bootstrapping for Reinforcement Learning in Robot Control",
        "authors": "Christopher Gebauer, Maren Bennewitz",
        "published": "2020-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5121/csit.2020.101114"
    },
    {
        "id": 27726,
        "title": "Negative Reinforcement in Social Learning Theory",
        "authors": "Maxine Notice, Jinsook Song, Janet Robertson",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-49425-8_47"
    },
    {
        "id": 27727,
        "title": "Understanding brain agents and academy",
        "authors": "Abhilash Majumder",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6503-1_4"
    },
    {
        "id": 27728,
        "title": "Bounds for off-policy prediction in reinforcement learning",
        "authors": "Ajin George Joseph, Shalabh Bhatnagar",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7966359"
    },
    {
        "id": 27729,
        "title": "Reinforcement Learning-Based Path Tracking With Application of Quadruped Robot",
        "authors": "Yikang Ouyang, Haotian Bai, Yunxiao Shan",
        "published": "2021-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728351"
    },
    {
        "id": 27730,
        "title": "Reinforcement Learning for Guiding the E Theorem Prover",
        "authors": "Jack McKeown, Geoff Sutcliffe",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "Automated Theorem Proving (ATP) systems search for aproof in a rapidly growing space of possibilities. Heuristicshave a profound impact on search, and ATP systems makeheavy use of heuristics. This work uses reinforcement learn-ing to learn a metaheuristic that decides which heuristic to useat each step of a proof search in the E ATP system. Proximalpolicy optimization is used to dynamically select a heuristicfrom a fixed set, based on the current state of E. The approachis evaluated on its ability to reduce the number of inferencesteps used in successful proof searches, as an indicator of in-telligent search.",
        "link": "http://dx.doi.org/10.32473/flairs.36.133334"
    },
    {
        "id": 27731,
        "title": "Curiosity-driven Exploration for Cooperative Multi-Agent Reinforcement Learning",
        "authors": "Fanchao Xu, Tomoyuki Kaneko",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191336"
    },
    {
        "id": 27732,
        "title": "Buffer-Based Reinforcement Learning for Adaptive Streaming",
        "authors": "Yue Zhang, Yao Liu",
        "published": "2017-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdcs.2017.146"
    },
    {
        "id": 27733,
        "title": "English synchronous real-time translation method based on reinforcement learning",
        "authors": "Xin Ke",
        "published": "2022-2-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11276-022-02910-4"
    },
    {
        "id": 27734,
        "title": "An Imputation Reinforcement Learning Agent For Power Transformers Load Study",
        "authors": "Therence Houngbadji",
        "published": "2022-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powercon53406.2022.9929886"
    },
    {
        "id": 27735,
        "title": "DYNAMIC SELECTION OF P-NORM IN LINEAR ADAPTIVE FILTERING VIA ONLINE KERNEL-BASED REINFORCEMENT LEARNING",
        "authors": "Minh Vu, Yuki Akiyama, Konstantinos Slavakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This study addresses the problem of selecting dynamically, at each time instance, the “optimal” p-norm to combat outliers in linear adaptive filtering without any knowledge on the potentially timevarying probability density function of the outliers. To this end, an online and data-driven framework is designed via kernel-based reinforcement learning (KBRL). Novel Bellman mappings on reproducing kernel Hilbert spaces (RKHSs) are introduced that need no knowledge on transition probabilities of Markov decision processes, and are nonexpansive with respect to the underlying Hilbertian norm. An approximate policy-iteration framework is finally offered via the introduction of a finite-dimensional affine superset of the fixed-point set of the proposed Bellman mappings. The well-known “curse of dimensionality” in RKHSs is addressed by building a basis of vectors via an approximate linear dependency criterion. Numerical tests on synthetic data demonstrate that the proposed framework selects always the “optimal” p-norm for the outlier scenario at hand, outperforming at the same time several non-RL and KBRL schemes.</p>\n<p><br></p>\n<p>-------</p>\n<p><br></p>\n<p>© 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21376059.v1"
    },
    {
        "id": 27736,
        "title": "Energy efficient online control of a water distribution network based on Deep Reinforcement Learning",
        "authors": "Baltasar Beferull-Lozano, Jyotirmoy Bhardwaj, Helge Liltvedt",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Data is obtained via Deep Reinforcement Learning Environment</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23896902.v1"
    },
    {
        "id": 27737,
        "title": "Physics Informed Intrinsic Rewards in Reinforcement Learning",
        "authors": "Jiazhou Jiang, Minyue Fu, Zhiyong Chen",
        "published": "2022-11-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/anzcc56036.2022.9966956"
    },
    {
        "id": 27738,
        "title": "Competitive Algorithms and Reinforcement Learning for NOMA in IoT Networks",
        "authors": "Zoubeir Mlika, Soumaya Cherkaoui",
        "published": "2021-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc42927.2021.9500285"
    },
    {
        "id": 27739,
        "title": "Cultural reinforcement learning: a framework for modeling cumulative culture on a limited channel",
        "authors": "Ben Prystawski, Dilip Arumugam, Noah D. Goodman",
        "published": "No Date",
        "citations": 0,
        "abstract": "Humans' capacity for cumulative culture is remarkable: we can build up vast bodies of knowledge over generations. Communication, particularly via language, is a key component of this process. Previous work has described language as enabling posterior passing, where one Bayesian agent transmits a posterior distribution to the next. In practice, we cannot exactly copy our beliefs into the minds of others---we must communicate over the limited channel language provides. In this paper, we analyze cumulative culture as Bayesian reinforcement learning with communication over a rate-limited channel. We implement an agent that solves a crafting task and communicates to the next agent by approximating the optimal rate-distortion trade-off. Our model produces documented effects, such as the benefits of abstraction and selective social learning. It also suggests a new hypothesis: selective social learning can be harmful in tasks where initial exploration is required.",
        "link": "http://dx.doi.org/10.31234/osf.io/q4tz8"
    },
    {
        "id": 27740,
        "title": "Path planning based on reinforcement learning",
        "authors": "Jin Lin",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "With the wide application of mobile robots in industry, path planning has always been a difficult problem for mobile robots. Reinforcement learning algorithms such as Q-learning play a huge role in path planning. Traditional Q-learning algorithm mainly uses - greedy search policy. But for a fixed search factor -greedy. For example, the problems of slow convergence speed, time-consuming and many continuous action transformations (such as the number of turns during robot movement) are not conducive to the stability requirements of mobile robots in industrial transportation. Especially for the transportation of dangerous chemicals, continuous transformation of turns will increase the risk of objects toppling. This paper proposes a new method based on - greedy 's improved dynamic search strategy is used to improve the stability of mobile robots in motion planning. The experiment shows that the dynamic search strategy converges faster, consumes less time, has less continuous transformation times of action, and has higher motion stability in the test environment.",
        "link": "http://dx.doi.org/10.54254/2755-2721/5/20230728"
    },
    {
        "id": 27741,
        "title": "Mimicking Electronic Gaming Machine Player Behavior Using Reinforcement Learning",
        "authors": "Gaurav Jariwala, Vlado Keselj",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/594757db.6b6b324c"
    },
    {
        "id": 27742,
        "title": "Distributed Disaggregated Communications via Reinforcement Learning and Backpressure (D2CRaB)",
        "authors": "Mu-Cheng Wang, Paul C. Hershey",
        "published": "2023-4-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/syscon53073.2023.10131166"
    },
    {
        "id": 27743,
        "title": "A Robust Routing Strategy based on Deep Reinforcement Learning for Mege Satellite Constellations",
        "authors": "Ke Chu, Sixi Cheng, Zhu Lidong (GE)",
        "published": "No Date",
        "citations": 0,
        "abstract": "The development of mega constellations inevitably brings various\nproblems for the development of routing techniques. Most of the existing\nwork considers end-to-end delay and load balancing problems, while the\nanalysis of routing strategies in case of link performance degradation\nis neglected, and an optimization approach applicable to mega satellite\nnetworks is not developed. In this letter, we propose a robust routing\nstrategy based on deep reinforcement learning (RRS-DRL) that regards the\nAge of Information (AoI) of packets as an optimization target, and\nensures the effectiveness of message transmission throughout the\nnetwork. Extensive simulation results show that our proposed RRS-DRL\nalgorithm obtains a lower average AoI across the network and better\nutilization of the resources than the traditional shortest path\nalgorithm, significantly increasing the robustness of the constellation.",
        "link": "http://dx.doi.org/10.22541/au.167773143.34984973/v1"
    },
    {
        "id": 27744,
        "title": "Distributed Approximating Global Optimality with Local Reinforcement Learning in HetNets",
        "authors": "Yawen Fan, Husheng Li",
        "published": "2017-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/glocom.2017.8254853"
    },
    {
        "id": 27745,
        "title": "Reinforcement Learning for Finance",
        "authors": "Samit Ahlawat",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-8835-1"
    },
    {
        "id": 27746,
        "title": "Exploiting Environment Configurability in Reinforcement Learning",
        "authors": "Alberto Maria Metelli",
        "published": "2022-11-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3233/faia361"
    },
    {
        "id": 27747,
        "title": "Curiosity-Driven Reinforcement Learning with Homeostatic Regulation",
        "authors": "Ildefons Magrans de Abril, Ryota Kanai",
        "published": "2018-7",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489075"
    },
    {
        "id": 27748,
        "title": "SARSA(0) Reinforcement Learning over Fully Homomorphic Encryption",
        "authors": "Jihoon Suh, Takashi Tanaka",
        "published": "2021-3-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/siceiscs51787.2021.9495321"
    },
    {
        "id": 27749,
        "title": "A Universal Offline Reinforcement Learning Model for Adaptive Traffic Signal Control at Heterogeneous Intersections",
        "authors": "Jiaming Lu, Ying Zeng, Feng Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4665766"
    },
    {
        "id": 27750,
        "title": "Batch(Offline) Reinforcement Learning for Recommender System",
        "authors": "Mohammad Amir Rezaei Gazik, Mehdy Roayaei",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icee59167.2023.10334722"
    },
    {
        "id": 27751,
        "title": "Deep Reinforcement Learning based Dynamic Edge/Fog Network Slicing",
        "authors": "H. H. Esmat, B. Lorenzo",
        "published": "2020-12",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom42002.2020.9322631"
    },
    {
        "id": 27752,
        "title": "Reinforcement Learning-Based Hybrid Multi-Objective Optimization Algorithm Design",
        "authors": "Herbert Palm, Lorin Arndt",
        "published": "2023-5-22",
        "citations": 1,
        "abstract": "The multi-objective optimization (MOO) of complex systems remains a challenging task in engineering domains. The methodological approach of applying MOO algorithms to simulation-enabled models has established itself as a standard. Despite increasing in computational power, the effectiveness and efficiency of such algorithms, i.e., their ability to identify as many Pareto-optimal solutions as possible with as few simulation samples as possible, plays a decisive role. However, the question of which class of MOO algorithms is most effective or efficient with respect to which class of problems has not yet been resolved. To tackle this performance problem, hybrid optimization algorithms that combine multiple elementary search strategies have been proposed. Despite their potential, no systematic approach for selecting and combining elementary Pareto search strategies has yet been suggested. In this paper, we propose an approach for designing hybrid MOO algorithms that uses reinforcement learning (RL) techniques to train an intelligent agent for dynamically selecting and combining elementary MOO search strategies. We present both the fundamental RL-Based Hybrid MOO (RLhybMOO) methodology and an exemplary implementation applied to mathematical test functions. The results indicate a significant performance gain of intelligent agents over elementary and static hybrid search strategies, highlighting their ability to effectively and efficiently select algorithms.",
        "link": "http://dx.doi.org/10.3390/info14050299"
    },
    {
        "id": 27753,
        "title": "Z-Number-Based Data Aggregation with the Least Uncertainty Using Reinforcement Learning",
        "authors": "Yuhang Chang, Yunjia Zhang, Yaxian Tang, Bingyi Kang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4552862"
    },
    {
        "id": 27754,
        "title": "Trial and Error Experience Replay Based Deep Reinforcement Learning",
        "authors": "Cheng Zhang, Liang Ma",
        "published": "2019-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smartcloud.2019.00045"
    },
    {
        "id": 27755,
        "title": "Episodic memory contributions to working memory-supported reinforcement learning",
        "authors": "Soobin Han Hong, Amy R. Zou, Aspen H. Yoo, Anne Collins",
        "published": "No Date",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) frameworks have been extremely successful at capturing how biological agents learn to make rewarding choices. However, there is also increasing evidence that multiple cognitive processes, including working memory (WM) and episodic memory (EM), support such learning in parallel with value-based mechanisms such as RL. Here, we investigate EM’s role in a context where both RL and WM are known to strongly support learning. We develop two new experimental paradigms to isolate EM’s contributions, using trial-unique signals (Experiment 1) and temporal context effects (Experiment 2) to tag EM. As predicted, our results across both experiments consistently showed a weak role of EM in learning alongside RL and WM. However, surprisingly, we showed that EM’s contributions did not improve overall behavior; instead, participants appeared to only encode in or retrieve from EM part of a past trial’s information (the stimulus-action choice, without outcome), leading to characteristic error patterns. Across both experiments, computational modeling confirmed a small contribution of traces of past stimulus-action (association) events stored in EM to learning behavior. Our results shed light on the format of EM traces and how they support decision-making.",
        "link": "http://dx.doi.org/10.31234/osf.io/64hxe"
    },
    {
        "id": 27756,
        "title": "Energy Storage Arbitrage in Day-Ahead Electricity Market Using Deep Reinforcement Learning",
        "authors": "Tim Zonjee, Shahab Shariat Torbaghan",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powertech55446.2023.10202674"
    },
    {
        "id": 27757,
        "title": "Reinforcement Learning for Vapor Compression Cycle Control",
        "authors": "Tech Logg Ding, Stuart Norris, Alison Subiantoro",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4157083"
    },
    {
        "id": 27758,
        "title": "Decision Making For Multi-Robot Fixture Planning Using Multi Agent Reinforcement Learning",
        "authors": "Ethan Canzini, Marc Auledas Noguera, Simon Pope, Ashutosh Tiwari",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Within the realm of flexible manufacturing, fixture layout planning allows manufacturers to rapidly deploy optimal fixturing plans that can reduce surface deformation that leads to crack propagation in components during manufacturing tasks. The role of fixture layout planning has evolved from being performed by experienced engineers to computational methods due to the number of possible configurations for components. Current optimisation methods commonly fall into sub-optimal positions due to the existence of local optima, with data-driven machine learning techniques relying on costly to collect labelled training data. In this paper, we present a framework for multi-agent reinforcement learning with team decision theory to find optimal fixturing plans for manufacturing tasks. We demonstrate our approach on two representative aerospace components with complex geometries across a set of drilling tasks, illustrating the capabilities of our method; we will compare this against state of the art methods to showcase our method’s improvement at finding optimal fixturing plans with 3 times the improvement in deformation control within tolerance bounds.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24171534"
    },
    {
        "id": 27759,
        "title": "The Recurrent Reinforcement Learning Crypto Agent",
        "authors": "Gabriel Borrageiro, Nick Firoozye, Paolo Barucca",
        "published": "2022",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3166599"
    },
    {
        "id": 27760,
        "title": "Control of Flexible Manipulator Based on Reinforcement Learning",
        "authors": "Leilei Cui, Weidong Chen, Hesheng Wang, Jingchuan Wang",
        "published": "2018-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2018.8623788"
    },
    {
        "id": 27761,
        "title": "Influence of surprise on reinforcement learning in younger and older adults",
        "authors": "Christoph Koch, Ondrej Zika, Rasmus Bruckner, Nicolas W Schuck",
        "published": "No Date",
        "citations": 1,
        "abstract": "Surprise is a key component of many learning experiences, and yet its precise computational role, and how it changes with age, remain debated.One major challenge is that surprise often occurs jointly with other variables, such as uncertainty, outcome magnitude and outcome probability. To assess how humans learn from surprising events, and whether aging affects this process, we studied choices while participants learned from stationary asymmetric outcome distributions, which decouple outcome magnitude, probability, uncertainty, and surprise.A total of 102 participants (51 older, aged 50 -- 73; 51 younger, 19 -- 30 years) chose between three bandits, one of which had a bimodal outcome distribution. Behavioral analyses showed that both age-groups learned the average of the bimodal bandit less well. A trial-by-trial analysis indicated that participants performed choice reversals immediately following large absolute prediction errors, consistent with heightened sensitivity to surprise.This effect was stronger in older adults.Computational models indicated that learning rates in younger as well as older adults were influenced by surprise, rather than uncertainty. Our work bridges between behavioral economics research that has focused on how outcomes with low probability affect choice in older adults, and reinforcement learning work that has investigated age differences in the effects of uncertainty and suggests that older adults overly adapt to surprising events, even when accounting for probability and uncertainty effects.",
        "link": "http://dx.doi.org/10.31234/osf.io/unx5y"
    },
    {
        "id": 27762,
        "title": "Collaborative hunting in artificial agents with deep reinforcement learning",
        "authors": "Kazushi Tsutsui, Ryoya Tanaka, Kazuya Takeda, Keisuke Fujii",
        "published": "No Date",
        "citations": 0,
        "abstract": "ABSTRACTCollaborative hunting, in which predators play different and complementary roles to capture prey, has been traditionally believed as an advanced hunting strategy requiring large brains that involve high level cognition. However, recent findings that collaborative hunting have also been documented in smaller-brained vertebrates have placed this previous belief under strain. Here, we demonstrate that decisions underlying collaborative hunts do not necessarily rely on sophisticated cognitive processes using computational multi-agent simulation based on deep reinforcement learning. We found that apparently elaborate coordination can be achieved through a relatively simple decision process of mapping between observations and actions via distance-dependent internal representations formed by prior experience. Furthermore, we confirmed that this decision rule of predators is robust against unknown prey controlled by humans. Our results of computational ecology emphasize that collaborative hunting can emerge in various intra- and inter-specific interactions in nature, and provide insights into the evolution of sociality.",
        "link": "http://dx.doi.org/10.1101/2022.10.10.511517"
    },
    {
        "id": 27763,
        "title": "2DoF Robotic Arm using Reinforcement Learning",
        "authors": "",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "The 2DoF Robo-Arm Reinforcement Learning aims to develop an intelligent system that can learn to control a robotic arm with two degrees of freedom using reinforcement learning techniques. The concept here involves the use of a simulated environment in which the robotic arm can interact with different objects and learn to perform tasks such as reaching, grasping, and moving objects. The idea here seeks to improve the efficiency and effectiveness of robotic arm control in industrial and manufacturing applications, enabling them to perform complex tasks with good accuracy and speed. The overview of the proposed system’s objectives, methodology, and results, demonstrating the potential of this technology to control Robotic Arm and also its use in Automation are discussed here",
        "link": "http://dx.doi.org/10.30534/ijeter/2023/0211122023"
    },
    {
        "id": 27764,
        "title": "Indirect RL with Function Approximation",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_6"
    },
    {
        "id": 27765,
        "title": "Reinforcement Learning Models for Abstractive Text Summarization",
        "authors": "Sergiu Buciumas",
        "published": "2019-4-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3299815.3314486"
    },
    {
        "id": 27766,
        "title": "Congestion-Aware Routing in Dynamic IoT Networks: A Reinforcement Learning Approach",
        "authors": "Hossam Farag, Cedomir Stefanovic",
        "published": "2021-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685191"
    },
    {
        "id": 27767,
        "title": "Einstieg in Deep Reinforcement Learning",
        "authors": "Alexander Zai, Brandon Brown",
        "published": "2020-10-12",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446466081"
    },
    {
        "id": 27768,
        "title": "Drone mapping through multi-agent reinforcement learning",
        "authors": "Riccardo Zanol, Federico Chiariotti, Andrea Zanella",
        "published": "2019-4",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc.2019.8885873"
    },
    {
        "id": 27769,
        "title": "Model-Based Reinforcement Learning via Proximal Policy Optimization",
        "authors": "Yuewen Sun, Xin Yuan, Wenzhang Liu, Changyin Sun",
        "published": "2019-11",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac48633.2019.8996875"
    },
    {
        "id": 27770,
        "title": "Accelerated Reinforcement Learning for Temporal Logic Control Objectives",
        "authors": "Yiannis Kantaros",
        "published": "2022-10-23",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981759"
    },
    {
        "id": 27771,
        "title": "Superstitious learning of abstract order from random reinforcement",
        "authors": "Yuhao Jin, Greg Jensen, Jacqueline Gottlieb, Vincent P. Ferrera",
        "published": "No Date",
        "citations": 1,
        "abstract": "ABSTRACTSurvival depends on identifying learnable features of the environment that predict reward, and avoiding others that are random and unlearnable. However, humans and other animals often infer spurious associations among unrelated events, raising the question of how well they can distinguish learnable patterns from unlearnable events. Here, we tasked monkeys with discovering the serial order of two pictorial sets: a “learnable” set in which the stimuli were implicitly ordered and monkeys were rewarded for choosing the higher-rank stimulus and an “unlearnable” set in which stimuli were unordered and feedback was random regardless of the choice. We replicated prior results that monkeys reliably learned the implicit order of the learnable set. Surprisingly, the monkeys behaved as though some ordering also existed in the unlearnable set, showing consistent choice preference that transferred to novel untrained pairs in this set, even under a preference-discouraging reward schedule that gave rewards more frequently to the stimulus that was selected less often. In simulations, a model-free RL algorithm (Q-learning) displayed a degree of consistent ordering among the unlearnable set but, unlike the monkeys, failed to do so under the preference, discouraging reward schedule. Our results suggest that monkeys infer abstract structures from objectively random events using heuristics that extend beyond stimulus-outcome conditional learning to more cognitive model-based learning mechanisms.",
        "link": "http://dx.doi.org/10.1101/2022.02.02.478909"
    },
    {
        "id": 27772,
        "title": "Deep Reinforcement Learning for Demand-Aware Joint VNF Placement-and-Routing",
        "authors": "Shaoyang Wang, Tiejun Lv",
        "published": "2019-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcwkshps45667.2019.9024688"
    },
    {
        "id": 27773,
        "title": "Deep Reinforcement Learning for the Computation Offloading in MIMO-based Edge Computing",
        "authors": "Abdeladim Sadiki, Jamal Bentahar, Rachida Dssouli, Abdeslam En-Nouaary",
        "published": "No Date",
        "citations": 1,
        "abstract": "Multi-access Edge Computing (MEC) has recently emerged as a potential technology to serve the needs of mobile devices (MDs) in 5G and 6G cellular networks. By offloading tasks to high-performance servers installed at the edge of the wireless networks, resource-limited MDs can cope with the proliferation of the recent computationally-intensive applications. In this paper, we study the computation offloading problem in a massive multiple-input multiple-output (MIMO)-based MEC system where the base stations are equipped with a large number of antennas. Our objective is to minimize the power consumption and offloading delay at the MDs under the stochastic system environment. To this end, we formulate the problem as a Markov Decision Process (MDP) and propose two Deep Reinforcement Learning (DRL) strategies to learn the optimal offloading policy without any prior knowledge of the environment dynamics. First, a Deep Q-Network (DQN) strategy to solve the curse of the state space explosion is analyzed. Then, a more general Proximal Policy Optimization (PPO) strategy to solve the problem of discrete action space is introduced. Simulation results show that the proposed DRL-based strategies outperform the baseline and state-of-the-art algorithms. Moreover, our PPO algorithm exhibits stable performance and efficient offloading results compared to the benchmark DQN strategy.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16869119"
    },
    {
        "id": 27774,
        "title": "Efficient Distributional Reinforcement Learning with Kullback-Leibler Divergence Regularization",
        "authors": "Renxing Li, Zhiwei Shang, Chunhua Zheng, Huiyun Li, Qing Liang, Yunduan Cui",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this article, we address the issues of stability and data-efficiency in reinforcement learning (RL). A novel RL approach, Kullback–Leibler divergence-regularized distributional RL (KLC51) is proposed to integrate the advantages of both stability in the distributional RL and data-efficiency in the Kullback-Leibler (KL) divergence-regularized RL in one framework. KLC51 derived the  Bellman equation and the TD errors regularized by KL divergence in a distributional perspective and explored the approximated strategies of properly mapping the corresponding Boltzmann softmax term into distributions. Evaluated by several benchmark tasks with different complexity, the proposed method clearly illustrates the positive effect of the KL divergence regularization to the distributional RL including exclusive exploration behaviors and smooth value function update, and successfully demonstrates its significant superiority in both learning stability and data-efficiency compared with the related baseline approaches.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19679454"
    },
    {
        "id": 27775,
        "title": "Reinforcement Learning in Process Scheduling",
        "authors": "Darothi Sarkar, Swati Singal",
        "published": "2023-1-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/confluence56041.2023.10048821"
    },
    {
        "id": 27776,
        "title": "Imminent Collision Mitigation with Reinforcement Learning and Vision",
        "authors": "Horia Porav, Paul Newman",
        "published": "2018-11",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2018.8569222"
    },
    {
        "id": 27777,
        "title": "Deep Reinforcement Learning on HVAC Control",
        "authors": "Ivars Namatēvs",
        "published": "2018-12-14",
        "citations": 6,
        "abstract": "Due to increase of computing power and innovative approaches of an end-to-end reinforcement learning (RL) that feed data from high-dimensional sensory inputs, it is now plausible to combine RL and Deep learning to perform Smart Building Energy Control (SBEC) systems. Deep reinforcement learning (DRL) revolutionizes existing Q-learning algorithm to Deep Q-learning (DQL) profited by artificial neural networks. Deep Neural Network (DNN) is well trained to calculate the Q-function. To create comprehensive SBEC system it is crucial to choose appropriate mathematical background and benchmark the best framework of a model based predictive control to manage the building heating, ventilation, and air condition (HVAC) system. The main contribution of this paper is to explore a state-of-the-art DRL methodology to smart building control.",
        "link": "http://dx.doi.org/10.7250/itms-2018-0004"
    },
    {
        "id": 27778,
        "title": "Heterogeneous 360 Degree Videos in Metaverse: Differentiated Reinforcement Learning Approaches",
        "authors": "Wenhan Yu, Jun Zhao",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436796"
    },
    {
        "id": 27779,
        "title": "Task Allocation for Mobile Crowdsensing with Deep Reinforcement Learning",
        "authors": "Xi Tao, Wei Song",
        "published": "2020-5",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc45663.2020.9120489"
    },
    {
        "id": 27780,
        "title": "Reinforcement Learning Approaches in Social Robotics",
        "authors": "Neziha Akalin, Amy Loutfi",
        "published": "2021-2-11",
        "citations": 50,
        "abstract": "This article surveys reinforcement learning approaches in social robotics. Reinforcement learning is a framework for decision-making problems in which an agent interacts through trial-and-error with its environment to discover an optimal behavior. Since interaction is a key component in both reinforcement learning and social robotics, it can be a well-suited approach for real-world interactions with physically embodied social robots. The scope of the paper is focused particularly on studies that include social physical robots and real-world human-robot interactions with users. We present a thorough analysis of reinforcement learning approaches in social robotics. In addition to a survey, we categorize existent reinforcement learning approaches based on the used method and the design of the reward mechanisms. Moreover, since communication capability is a prominent feature of social robots, we discuss and group the papers based on the communication medium used for reward formulation. Considering the importance of designing the reward function, we also provide a categorization of the papers based on the nature of the reward. This categorization includes three major themes: interactive reinforcement learning, intrinsically motivated methods, and task performance-driven methods. The benefits and challenges of reinforcement learning in social robotics, evaluation methods of the papers regarding whether or not they use subjective and algorithmic measures, a discussion in the view of real-world reinforcement learning challenges and proposed solutions, the points that remain to be explored, including the approaches that have thus far received less attention is also given in the paper. Thus, this paper aims to become a starting point for researchers interested in using and applying reinforcement learning methods in this particular research field.",
        "link": "http://dx.doi.org/10.3390/s21041292"
    },
    {
        "id": 27781,
        "title": "Deep Reinforcement Learning-Based Traffic Engineering in SD-WANs",
        "authors": "Zehua Guo",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-4874-9_2"
    },
    {
        "id": 27782,
        "title": "An Efficient Computing of Correlated Equilibrium for Cooperative Q‐Learning‐Based Multi‐Robot Planning",
        "authors": "",
        "published": "2020-11-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119699057.ch4"
    },
    {
        "id": 27783,
        "title": "A micro-genesis account of longer-form reinforcement learning in structured and unstructured environments",
        "authors": "Benjamin James Dyson, Ahad Asad",
        "published": "2021-6-23",
        "citations": 1,
        "abstract": "AbstractWe explored the possibility that in order for longer-form expressions of reinforcement learning (win-calmness, loss-restlessness) to manifest across tasks, they must first develop because of micro-transactions within tasks. We found no evidence of win-calmness or loss-restlessness when wins could not be maximised (unexploitable opponents), nor when the threat of win minimisation was presented (exploiting opponents), but evidence of win-calmness (but not loss-restlessness) when wins could be maximised (exploitable opponents).",
        "link": "http://dx.doi.org/10.1038/s41539-021-00098-4"
    },
    {
        "id": 27784,
        "title": "Path Planning for Mobile Robot Based on Deep Reinforcement Learning and Fuzzy Control",
        "authors": "Chunling Liu, Jun Xu, Kaiwen Guo",
        "published": "2022-10-28",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml57342.2022.10009792"
    },
    {
        "id": 27785,
        "title": "Deep Reinforcement Learning for Resource Allocation in Blockchain-Based Federated Learning",
        "authors": "Yueyue Dai, Huijiong Yang, Huiran Yang",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279529"
    },
    {
        "id": 27786,
        "title": "Deep Reinforcement Learning for Autonomous Ground Vehicle Exploration Without A-Priori Maps",
        "authors": "Shathushan Sivashangaran, Azim Eskandarian",
        "published": "2023",
        "citations": 1,
        "abstract": "Autonomous Ground Vehicles (AGVs) are essential tools for a wide range of applications stemming from their ability to operate in hazardous environments with minimal human operator input. Effective motion planning is paramount for successful operation of AGVs. Conventional motion planning algorithms are dependent on prior knowledge of environment characteristics and offer limited utility in information poor, dynamically altering environments such as areas where emergency hazards like fire and earthquake occur, and unexplored subterranean environments such as tunnels and lava tubes on Mars. We propose a Deep Reinforcement Learning (DRL) framework for intelligent AGV exploration without a-priori maps utilizing Actor-Critic DRL algorithms to learn policies in continuous and high-dimensional action spaces directly from raw sensor data. The DRL architecture comprises feedforward neural networks for the critic and actor representations in which the actor network strategizes linear and angular velocity control actions given current state inputs, that are evaluated by the critic network which learns and estimates Q-values to maximize an accumulated reward. Three off-policy DRL algorithms, DDPG, TD3 and SAC, are trained and compared in two environments of varying complexity, and further evaluated in a third with no prior training or knowledge of map characteristics. The agent is shown to learn optimal policies at the end of each training period to chart quick, collision-free exploration trajectories, and is extensible, capable of adapting to an unknown environment without changes to network architecture or hyperparameters. The best algorithm is further evaluated in a realistic 3D environment.",
        "link": "http://dx.doi.org/10.54364/aaiml.2023.1170"
    },
    {
        "id": 27787,
        "title": "Learning to Interrupt: A Hierarchical Deep Reinforcement Learning Framework for Efficient Exploration",
        "authors": "Tingguang Li, Jin Pan, Delong Zhu, Max Q.-H. Meng",
        "published": "2018-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio.2018.8665177"
    },
    {
        "id": 27788,
        "title": "A Design of Reinforcement Learning Accelerator Based on Deep Q-learning Network",
        "authors": "Yufei Nai, Zhenghan Fang, Limeng Zhao",
        "published": "2022-10-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccasit55263.2022.9986639"
    },
    {
        "id": 27789,
        "title": "Intelligent Anti-jamming based on Deep Reinforcement Learning and Transfer Learning",
        "authors": "Siavash Barqi Janiar, Ping Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tvt.2024.3359426"
    },
    {
        "id": 27790,
        "title": "Towards sentiment aided dialogue policy learning for multi-intent conversations using hierarchical reinforcement learning",
        "authors": "Tulika Saha, Sriparna Saha, Pushpak Bhattacharyya",
        "published": "2020-7-2",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1371/journal.pone.0235367"
    },
    {
        "id": 27791,
        "title": "Gradient Monitored Reinforcement Learning",
        "authors": "Mohammed Sharafath Abdul Hameed, Gavneet Singh Chadha, Andreas Schwung, Steven X. Ding",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3119853"
    },
    {
        "id": 27792,
        "title": "Maritime platform defense with deep reinforcement learning",
        "authors": "Jared Markowitz, Ryan Sheffield, Galen Mullins",
        "published": "2022-6-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2618808"
    },
    {
        "id": 27793,
        "title": "Research on Cooperative Control of Traffic Signals based on Deep Reinforcement Learning",
        "authors": "Lingling Fan, Yusong Yang, Honghai Ji, Shuangshuang Xiong",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10167232"
    },
    {
        "id": 27794,
        "title": "Sparsely Ensembled Convolutional Neural Network Classifiers via Reinforcement Learning",
        "authors": "Roman Olegovich Malashin",
        "published": "2021-4-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3468891.3468906"
    },
    {
        "id": 27795,
        "title": "Q-Learning Algorithm with Double-Agent Reinforcement Learning for Smart Traffic Controller",
        "authors": "Jalu Reswara, Nana Sutisna, Infall Syafalni, Trio Adiono",
        "published": "2023-8-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mwscas57524.2023.10406051"
    },
    {
        "id": 27796,
        "title": "ADAM &amp; RAL: Adaptive Memory Learning and Reinforcement Active Learning for Network Monitoring",
        "authors": "Sarah Wassermann, Thibaut Cuvelier, Pavol Mulinka, Pedro Casas",
        "published": "2019-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/cnsm46954.2019.9012675"
    },
    {
        "id": 27797,
        "title": "Deep Learning with Reinforcement Learning on Order Books",
        "authors": "Koti S. Jaddu, Paul A. Bilokon",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3905/jfds.2024.1.149"
    },
    {
        "id": 27798,
        "title": "Action Guidance-Based Deep Interactive Reinforcement Learning for AUV Path Planning",
        "authors": "Dong Jiang, Zheng Fang, Chunxi Cheng, Bo He, Guangliang Li",
        "published": "2022-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlcr57210.2022.00037"
    },
    {
        "id": 27799,
        "title": "A Preliminary Study on the Relationship Between Iterative Learning Control and Reinforcement Learning",
        "authors": "Yueqing Zhang, Bing Chu, Zhan Shu",
        "published": "2019",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2019.12.669"
    },
    {
        "id": 27800,
        "title": "Preserving model privacy for Federated Reinforcement Learning in Complementary Environments",
        "authors": "Pengyu Xuan, Aiguo Chen, Zexin Sha",
        "published": "2023-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3587716.3587815"
    },
    {
        "id": 27801,
        "title": "Solving the VRP Using Transformer-Based Deep Reinforcement Learning",
        "authors": "Xue-Lian Ren, Ai-Xiang Chen",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmlc58545.2023.10327956"
    },
    {
        "id": 27802,
        "title": "Towards Smart Educational Recommendations with Reinforcement Learning in Classroom",
        "authors": "Su Liu, Ye Chen, Hui Huang, Liang Xiao, Xiaojun Hei",
        "published": "2018-12",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tale.2018.8615217"
    },
    {
        "id": 27803,
        "title": "Balancing Learning and Engagement in Game-Based Learning Environments with Multi-objective Reinforcement Learning",
        "authors": "Robert Sawyer, Jonathan Rowe, James Lester",
        "published": "2017",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-61425-0_27"
    },
    {
        "id": 27804,
        "title": "Reinforcement Learning-Based Model-Free Controller for Feedback Stabilization of Robotic Systems",
        "authors": "Rupam Singh, Bharat Bhushan",
        "published": "2023-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3137548"
    },
    {
        "id": 27805,
        "title": "Transfer learning for occupancy-based HVAC control: A data-driven approach using unsupervised learning of occupancy profiles and deep reinforcement learning",
        "authors": "Mohammad Esrafilian-Najafabadi, Fariborz Haghighat",
        "published": "2023-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.enbuild.2023.113637"
    },
    {
        "id": 27806,
        "title": "An inverse reinforcement learning framework with the Q-learning mechanism for the metaheuristic algorithm",
        "authors": "Fuqing Zhao, Qiaoyun Wang, Ling Wang",
        "published": "2023-4",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.110368"
    },
    {
        "id": 27807,
        "title": "The influence of different environments on reinforcement learning",
        "authors": "Siyue Zheng",
        "published": "2022-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvidliccea56201.2022.9824796"
    },
    {
        "id": 27808,
        "title": "Model-free mean-field reinforcement learning: Mean-field MDP and mean-field Q-learning",
        "authors": "René Carmona, Mathieu Laurière, Zongjun Tan",
        "published": "2023-12-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1214/23-aap1949"
    },
    {
        "id": 27809,
        "title": "Multi-sensor based strategy learning with deep reinforcement learning for unmanned ground vehicle",
        "authors": "Mingyu Luo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ijin.2023.11.003"
    },
    {
        "id": 27810,
        "title": "Applying reinforcement learning to estimating apartment reference rents",
        "authors": "Jian Wang, Murtaza Das, Stephen Tappert",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-25456-7_10"
    },
    {
        "id": 27811,
        "title": "Uncertainty-based Meta-Reinforcement Learning for Robust Radar Tracking",
        "authors": "Julius Ott, Lorenzo Servadei, Gianfranco Mauro, Thomas Stadelmayer, Avik Santra, Robert Wille",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla55696.2022.00232"
    },
    {
        "id": 27812,
        "title": "Reinforcement Learning Path Planning based on Step Batch Q-Learning Algorithm",
        "authors": "Zhiqian Yin, Wei Cao, Tao Song, Xu Yang, Tianhao Zhang",
        "published": "2022-6-24",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaica54878.2022.9844553"
    },
    {
        "id": 27813,
        "title": "English information teaching resource sharing based on deep reinforcement learning",
        "authors": "Chao Han",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijceell.2024.135269"
    },
    {
        "id": 27814,
        "title": "A Biped Robot Learning to Walk like Human by Reinforcement Learning",
        "authors": "Yi Liu, Honglei An, Hongxu Ma",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3573834.3574484"
    },
    {
        "id": 27815,
        "title": "Artificial Intelligence Based on Modular Reinforcement Learning and Unsupervised Learning",
        "authors": "Shawan Taha Mohammed, Angelo Mihaltan, Kemal Acar, Hendrik Laux, Gerd Ascheid",
        "published": "2021-9-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367808150-10"
    },
    {
        "id": 27816,
        "title": "Moody Learners - Explaining Competitive Behaviour of Reinforcement Learning Agents",
        "authors": "Pablo Barros, Ana Tanevska, Francisco Cruz, Alessandra Sciutti",
        "published": "2020-10-26",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl-epirob48136.2020.9278125"
    },
    {
        "id": 27817,
        "title": "Federated sequential decision making: Bayesian optimization, reinforcement learning, and beyond",
        "authors": "Zhongxiang Dai, Flint Xiaofeng Fan, Cheston Tan, Trong Nghia Hoang, Bryan Kian Hsiang Low, Patrick Jaillet",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-44-319037-7.00023-5"
    },
    {
        "id": 27818,
        "title": "Swarm Reinforcement Learning Method Based on Hierarchical Q-Learning",
        "authors": "Yasuaki Kuroe, Kenya Takeuchi, Yutaka Maeda",
        "published": "2021-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9659877"
    },
    {
        "id": 27819,
        "title": "Multi-missile Path Planning algorithm based on Reinforcement Learning",
        "authors": "Xianlong Ma, Wei Yin, Zhiqiang Gao, Weijun Hu",
        "published": "2023-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3587716.3587720"
    },
    {
        "id": 27820,
        "title": "Reinforcement learning of occupant behavior model for cross-building transfer learning to various HVAC control systems",
        "authors": "Zhipeng Deng, Qingyan Chen",
        "published": "2021-5",
        "citations": 49,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.enbuild.2021.110860"
    },
    {
        "id": 27821,
        "title": "English Language Learning Pattern Matching Based on Distributed Reinforcement Learning",
        "authors": "Hua Zhao",
        "published": "2022-9-7",
        "citations": 0,
        "abstract": "The rapid development of a new generation of information technology, the promotion of network technology, and the emergence of complex and diverse requirements for control objects make the structure of language learning models more and more distributed. Distributed learning theory emphasizes the central position of learners in the learning process and the universality of learning scenes. This paper explores the significance and value of various learning modes to improve students’ learning effect. By analyzing the research data and explaining various effective language learning models, this paper aims to establish a theoretical framework of English language learning models and explore more effective language model matching schemes. This paper analyzes the adaptive multiagent, reward function, Markov model, probability function model, etc. and conducts experiments on the basis of the designed model. The linear correlation parameters of the model and the English language pattern matching efficiency are analyzed and judged on several important indicators. Because the algorithm designed in this paper has a good effect on the control of error, the error reduction rate has reached 85.6%.",
        "link": "http://dx.doi.org/10.1155/2022/7876504"
    },
    {
        "id": 27822,
        "title": "Meta Q-network: a combination of reinforcement learning and meta learning",
        "authors": "Min Lu, Yi Wang, Wenfeng Wang",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijans.2022.125303"
    },
    {
        "id": 27823,
        "title": "Nonlinear Approximate Optimal Control Based on Integral Reinforcement Learning",
        "authors": "Fenming Tian, Fei Liu",
        "published": "2022-8-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls55054.2022.9858347"
    },
    {
        "id": 27824,
        "title": "Online Optimal Investment Portfolio Model Based on Deep Reinforcement Learning",
        "authors": "Ni GAO, Yiyue HE, Yuming JIAO, Zhuo CHANG",
        "published": "2021-2-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3457682.3457685"
    },
    {
        "id": 27825,
        "title": "Cryptocurrency Portfolio Management Using Reinforcement Learning",
        "authors": "Vatsal Khandor, Sanay Shah, Parth Kalkotwar, Saurav Tiwari, Sindhu Nair",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "Portfolio management is the science of choosing the best investment policies\nand strategies with the aim of getting maximum returns. Simply, it means managing the\nassets/stocks of a company, organization, or individual and taking into account the\nrisks, and increasing the profit. This paper proposes portfolio management using a bot\nleveraging a reinforcement learning environment specifically for cryptocurrencies\nwhich are a hot topic in the current world of technology. The reinforcement Learning\nEnvironment gives the reward/penalty to the agent, which helps it train itself during the\ntraining process and make decisions based on the trial-and-error method. Dense and\nCNN networks are used for training the agent to taking the decision to either buy, hold\nor sell the coin. Various technical indicators, like MACD, SMA, etc., are also included\nin the dataset while making the decisions. The bot is trained on 3-year hourly data of\nBitcoin, and results demonstrate that the Dense and CNN network models show a good\namount of profit against a starting balance of 1,000, indicating that reinforcement\nlearning environments can be efficacious and can be incorporated into the trading\nenvironments.",
        "link": "http://dx.doi.org/10.2174/9789815079210123010018"
    },
    {
        "id": 27826,
        "title": "Obstacle avoidance and navigation utilizing reinforcement learning with reward shaping",
        "authors": "Daniel Zhang, Colleen P. Bailey",
        "published": "2020-4-21",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2558212"
    },
    {
        "id": 27827,
        "title": "Meta Q-network: a combination of reinforcement learning and meta learning",
        "authors": "Min Lu, Yi Wang, Wenfeng Wang",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijans.2022.10050329"
    },
    {
        "id": 27828,
        "title": "Enhancing Offline Coverage Path Planning with Deep Reinforcement Learning",
        "authors": "Yaoning Lian, Shaowu Yang, Songchang Jin, Yuxi Zheng, Dianxi Shi",
        "published": "2023-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlccim60412.2023.00056"
    },
    {
        "id": 27829,
        "title": "Deep Reinforcement Learning Based Pushing and Grasping Model with Frequency Domain Mapping and Supervised Learning",
        "authors": "Weiliang Cao, Zhenwei Cao, Yong Song",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oncon60463.2023.10430838"
    },
    {
        "id": 27830,
        "title": "Toward an adaptive deep reinforcement learning agent for maritime platform defense",
        "authors": "Jared Markowitz, Edward W. Staley",
        "published": "2023-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2663831"
    },
    {
        "id": 27831,
        "title": "Evaluating multi-agent reinforcement learning on heterogeneous platforms",
        "authors": "Samuel Wiggins, Yuan Meng, Rajgopal Kannan, Viktor Prasanna",
        "published": "2023-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2668689"
    },
    {
        "id": 27832,
        "title": "Learning to Send Reinforcements: Coordinating Multi-Agent Dynamic Police Patrol Dispatching and Rescheduling via Reinforcement Learning",
        "authors": "Waldy Joe, Hoong Chuin Lau",
        "published": "2023-8",
        "citations": 0,
        "abstract": "We address the problem of coordinating multiple agents in a dynamic police patrol scheduling via a Reinforcement Learning (RL) approach. Our approach utilizes Multi-Agent Value Function Approximation (MAVFA) with a rescheduling heuristic to learn dispatching and rescheduling policies jointly. Often, police operations are divided into multiple sectors for more effective and efficient operations. In a dynamic setting, incidents occur throughout the day across different sectors, disrupting initially-planned patrol schedules. To maximize policing effectiveness, police agents from different sectors cooperate by sending reinforcements to support one another in their incident response and even routine patrol. This poses an interesting research challenge on how to make such complex decision of dispatching and rescheduling involving multiple agents in a coordinated fashion within an operationally reasonable time. Unlike existing Multi-Agent RL (MARL) approaches which solve similar problems by either decomposing the problem or action into multiple components, our approach learns the dispatching and rescheduling policies jointly without any decomposition step. In addition, instead of directly searching over the joint action space, we incorporate an iterative best response procedure as a decentralized optimization heuristic and an explicit coordination mechanism for a scalable and coordinated decision-making. We evaluate our approach against the commonly adopted two-stage approach and conduct a series of ablation studies to ascertain the effectiveness of our proposed learning and coordination mechanisms.",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/18"
    },
    {
        "id": 27833,
        "title": "Vehicle emission control on road with temporal traffic information using deep reinforcement learning",
        "authors": "Zhenyi Xu, Yang Cao, Yu Kang, Zhenyi Zhao",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003190691-11"
    },
    {
        "id": 27834,
        "title": "Target-Network Update Linked with Learning Rate Decay Based on Mutual Information and Reward in Deep Reinforcement Learning",
        "authors": "Chayoung Kim",
        "published": "2023-9-28",
        "citations": 0,
        "abstract": "In this study, a target-network update of deep reinforcement learning (DRL) based on mutual information (MI) and rewards is proposed. In DRL, updating the target network from the Q network was used to reduce training diversity and contribute to the stability of learning. If it is not properly updated, the overall update rate is reduced to mitigate this problem. Simply slowing down is not recommended because it reduces the speed of the decaying learning rate. Some studies have been conducted to improve the issues with the t-soft update based on the Student’s-t distribution or a method that does not use the target-network. However, there are certain situations in which using the Student’s-t distribution might fail or force it to use more hyperparameters. A few studies have used MI in deep neural networks to improve the decaying learning rate and directly update the target-network by replaying experiences. Therefore, in this study, the MI and reward provided in the experience replay of DRL are combined to improve both the decaying learning rate and the target-network updating. Utilizing rewards is appropriate for use in environments with intrinsic symmetry. It has been confirmed in various OpenAI gymnasiums that stable learning is possible while maintaining an improvement in the decaying learning rate.",
        "link": "http://dx.doi.org/10.3390/sym15101840"
    },
    {
        "id": 27835,
        "title": "Reinforcement Learning Based Online Active Learning for Human Activity Recognition",
        "authors": "Yulai Cui, Shruthi Kashinath Hiremath, Thomas Ploetz",
        "published": "2022-9-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3544794.3558457"
    },
    {
        "id": 27836,
        "title": "Rapidly Learning Bayesian Networks for Complex System Diagnosis: A Reinforcement Learning Directed Greedy Search Approach",
        "authors": "Wenfeng Zhang, Wenquan Feng, Hongbo Zhao, Qi Zhao",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2019.2952143"
    },
    {
        "id": 27837,
        "title": "A Reinforcement Learning Approach for Ensemble Machine Learning Models in Peak Electricity Forecasting",
        "authors": "Warut Pannakkong, Vu Thanh Vinh, Nguyen Ngoc Minh Tuyen, Jirachai Buddhakulsomsiri",
        "published": "2023-7-1",
        "citations": 2,
        "abstract": "Electricity peak load forecasting plays an important role in electricity generation capacity planning to ensure reliable power supplies. To achieve high forecast accuracy, multiple machine learning models have been implemented to forecast the monthly peak load in Thailand over the past few years, yielding promising results. One approach to further improve forecast accuracy is to effectively select the most accurate forecast value for each period from among the forecast values generated by these models. This article presents a novel reinforcement learning approach using the double deep Q-network (Double DQN), which acts as a model selector from a pool of available models. The monthly electricity peak load data of Thailand from 2004 to 2017 are used to demonstrate the effectiveness of the proposed method. A hyperparameter tuning methodology using a fractional factorial design is implemented to significantly reduce the number of required experimental runs. The results indicate that the proposed selection model using Double DQN outperforms all tested individual machine learning models in terms of mean square error.",
        "link": "http://dx.doi.org/10.3390/en16135099"
    },
    {
        "id": 27838,
        "title": "Deep reinforcement learning to study spatial navigation, learning and memory in artificial and biological agents",
        "authors": "Edgar Bermudez-Contreras",
        "published": "2021-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00422-021-00862-0"
    },
    {
        "id": 27839,
        "title": "Structured Online Learning‐Based Control of Continuous‐Time Nonlinear Systems",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch5"
    },
    {
        "id": 27840,
        "title": "Towards learning behavior modeling of military logistics agent utilizing profit sharing reinforcement learning algorithm",
        "authors": "Xiong Li, Wei Pu, Xiaodong Zhao",
        "published": "2021-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.asoc.2021.107784"
    },
    {
        "id": 27841,
        "title": "Constraints Penalized Q-learning for Safe Offline Reinforcement Learning",
        "authors": "Haoran Xu, Xianyuan Zhan, Xiangyu Zhu",
        "published": "2022-6-28",
        "citations": 3,
        "abstract": "We study the problem of safe offline reinforcement learning (RL), the goal is to learn a policy that maximizes long-term reward while satisfying safety constraints given only offline data, without further interaction with the environment. This problem is more appealing for real world RL applications, in which data collection is costly or dangerous. Enforcing constraint satisfaction is non-trivial, especially in offline settings, as there is a potential large discrepancy between the policy distribution and the data distribution, causing errors in estimating the value of safety constraints. We show that naïve approaches that combine techniques from safe RL and offline RL can only learn sub-optimal solutions. We thus develop a simple yet effective algorithm, Constraints Penalized Q-Learning (CPQ), to solve the problem. Our method admits the use of data generated by mixed behavior policies. We present a theoretical analysis and demonstrate empirically that our approach can learn robustly across a variety of benchmark control tasks, outperforming several baselines.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i8.20855"
    },
    {
        "id": 27842,
        "title": "Deep Reinforcement Learning for Nash Equilibrium of Differential Games",
        "authors": "Zhenyu Li, Yazhong Luo",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2024.3351631"
    },
    {
        "id": 27843,
        "title": "Experienced Gray Wolf Optimization Through Reinforcement Learning and Neural Networks",
        "authors": "E. Emary, Hossam M. Zawbaa, Crina Grosan",
        "published": "2018-3",
        "citations": 144,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2016.2634548"
    },
    {
        "id": 27844,
        "title": "Online mobile learning resource recommendation method based on deep reinforcement learning",
        "authors": "Pingyang Li, Juan Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijisd.2023.10056670"
    },
    {
        "id": 27845,
        "title": "Utilized System Model Using Channel State Information Network with Gated Recurrent Units (CsiNet-GRUs)",
        "authors": "Hany Helmy, Sherif El Diasty, Hazem Shatila",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "MIMO: multiple-input multiple-output technology uses multiple antennas to use reflected signals to provide channel robustness and throughput gains. It is advantageous in several applications like cellular systems, and users are distributed over a wide coverage area in various applications such as mobile systems, improving channel state information (CSI) processing efficiency in massive MIMO systems. This chapter proposes two channel-based deep learning methods to enhance the performance in a massive MIMO system and compares our proposed technique to the previous methods. The proposed technique is based on the channel state information network combined with the gated recurrent unit’s technique CsiNet-GRUs, which increases recovery efficiency. Besides, a fair balance between compression ratio (CR) and complexity is given using correlation time in training samples. The simulation results show that the proposed CsiNet-GRUs technique fulfills performance improvement compared with the existing literature techniques, namely CS-based methods Conv-LSTM CsiNet, LASSO, Tval3, and CsiNet.",
        "link": "http://dx.doi.org/10.5772/intechopen.111650"
    },
    {
        "id": 27846,
        "title": "Batch Reinforcement Learning on a RoboCup SSL Keepaway Strategy Learning Problem",
        "authors": "Franco Ollino, Miguel A. Solís, Héctor Allende",
        "published": "2019-3-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/5b03f636"
    },
    {
        "id": 27847,
        "title": "Advances in Value-based, Policy-based, and Deep Learning-based Reinforcement Learning",
        "authors": "Haewon Byeon",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140838"
    },
    {
        "id": 27848,
        "title": "Collision-Free UAV Navigation with a Monocular Camera Using Deep Reinforcement Learning",
        "authors": "Yun Chen, Nuria Gonzalez-Prelcic, Robert W. Heath",
        "published": "2020-9",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp49062.2020.9231577"
    },
    {
        "id": 27849,
        "title": "Learning and Generalization of Dynamic Movement Primitives by Hierarchical Deep Reinforcement Learning from Demonstration",
        "authors": "Wonchul Kim, Chungkeun Lee, H. Jin Kim",
        "published": "2018-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2018.8594476"
    },
    {
        "id": 27850,
        "title": "Coupling Effect of Exploration Rate and Learning Rate  for Optimized Scaled Reinforcement Learning",
        "authors": "Smriti Gupta, Sabita Pal, Kundan Kumar, Kuntal Ghosh",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42979-023-02114-3"
    },
    {
        "id": 27851,
        "title": "Session details: Session 3B: Efficient and Secure Deep Learning and Reinforcement Learning in Embedded Systems",
        "authors": "Yanzhi Wang",
        "published": "2020-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3422942"
    },
    {
        "id": 27852,
        "title": "Adaptive Online-Learning Volt-Var Control for Smart Inverters Using Deep Reinforcement Learning",
        "authors": "Kirstin Beyer, Robert Beckmann, Stefan Geißendörfer, Karsten von Maydell, Carsten Agert",
        "published": "2021-4-3",
        "citations": 15,
        "abstract": "The increasing penetration of the power grid with renewable distributed generation causes significant voltage fluctuations. Providing reactive power helps balancing the voltage in the grid. This paper proposes a novel adaptive volt-var control algorithm on the basis of deep reinforcement learning. The learning agent is an online-learning deep deterministic policy gradient that is applicable under real-time conditions in smart inverters for reactive power management. The algorithm only uses input data from the grid connection point of the inverter itself; thus, no additional communication devices are needed and it can be applied individually to any inverter in the grid. The proposed volt-var control is successfully simulated at various grid connection points in a 21-bus low-voltage distribution test feeder. The resulting voltage behavior is analyzed and a systematic voltage reduction is observed both in a static grid environment and a dynamic environment. The proposed algorithm enables flexible adaption to changing environments through continuous exploration during the learning process and, thus, contributes to a decentralized, automated voltage control in future power grids.",
        "link": "http://dx.doi.org/10.3390/en14071991"
    },
    {
        "id": 27853,
        "title": "Distributed deep reinforcement learning for simulation control",
        "authors": "Suraj Pawar, Romit Maulik",
        "published": "2021-6-1",
        "citations": 7,
        "abstract": "Abstract\nSeveral applications in the scientific simulation of physical systems can be formulated as control/optimization problems. The computational models for such systems generally contain hyperparameters, which control solution fidelity and computational expense. The tuning of these parameters is non-trivial and the general approach is to manually ‘spot-check’ for good combinations. This is because optimal hyperparameter configuration search becomes intractable when the parameter space is large and when they may vary dynamically. To address this issue, we present a framework based on deep reinforcement learning (RL) to train a deep neural network agent that controls a model solve by varying parameters dynamically. First, we validate our RL framework for the problem of controlling chaos in chaotic systems by dynamically changing the parameters of the system. Subsequently, we illustrate the capabilities of our framework for accelerating the convergence of a steady-state computational fluid dynamics solver by automatically adjusting the relaxation factors of the discretized Navier–Stokes equations during run-time. The results indicate that the run-time control of the relaxation factors by the learned policy leads to a significant reduction in the number of iterations for convergence compared to the random selection of the relaxation factors. Our results point to potential benefits for learning adaptive hyperparameter learning strategies across different geometries and boundary conditions with implications for reduced computational campaign expenses\n4\n\n\n4\nData and codes available at https://github.com/Romit-Maulik/PAR-RL.\n.",
        "link": "http://dx.doi.org/10.1088/2632-2153/abdaf8"
    },
    {
        "id": 27854,
        "title": "Reinforcement Learning Meets Cognitive Situation Management: A Review of Recent Learning Approaches from the Cognitive Situation Management Perspective",
        "authors": "Andrea Salfinger",
        "published": "2020-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cogsima49017.2020.9216026"
    },
    {
        "id": 27855,
        "title": "Key node identification of satellite time-varying network with deep reinforcement learning",
        "authors": "Liyang Wang, Yun Li, Ye Ren, Lina Yu",
        "published": "2022-10-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml57342.2022.10009841"
    },
    {
        "id": 27856,
        "title": "Deep Reinforcement Learning with Copy-oriented Context Awareness and Weighted Rewards for Abstractive Summarization",
        "authors": "Caidong Tan",
        "published": "2023-3-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3590003.3590019"
    },
    {
        "id": 27857,
        "title": "Anomaly Detection in Structural Health Monitoring with Ensemble Learning and Reinforcement Learning",
        "authors": "Nan Huang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2024.0150112"
    },
    {
        "id": 27858,
        "title": "Learning to Drive (L2D) as a Low-Cost Benchmark for Real-World Reinforcement Learning",
        "authors": "Ari Viitala, Rinu Boney, Yi Zhao, Alexander Ilin, Juho Kannala",
        "published": "2021-12-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icar53236.2021.9659342"
    },
    {
        "id": 27859,
        "title": "Automatic berthing using supervised learning and reinforcement learning",
        "authors": "Shoma Shimizu, Kenta Nishihara, Yoshiki Miyauchi, Kouki Wakita, Rin Suyama, Atsuo Maki, Shinichi Shirakawa",
        "published": "2022-12",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2022.112553"
    },
    {
        "id": 27860,
        "title": "Exploiting Generalization in the Subspaces for Faster Model-Based Reinforcement Learning",
        "authors": "Maryam Hashemzadeh, Reshad Hosseini, Majid Nili Ahmadabadi",
        "published": "2019-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2018.2869978"
    },
    {
        "id": 27861,
        "title": "Learning at Variable Attentional Load Requires Cooperation of Working Memory, Meta-learning, and Attention-augmented Reinforcement Learning",
        "authors": "Thilo Womelsdorf, Marcus R. Watson, Paul Tiesinga",
        "published": "2021-11-12",
        "citations": 6,
        "abstract": "Abstract\nFlexible learning of changing reward contingencies can be realized with different strategies. A fast learning strategy involves using working memory of recently rewarded objects to guide choices. A slower learning strategy uses prediction errors to gradually update value expectations to improve choices. How the fast and slow strategies work together in scenarios with real-world stimulus complexity is not well known. Here, we aim to disentangle their relative contributions in rhesus monkeys while they learned the relevance of object features at variable attentional load. We found that learning behavior across six monkeys is consistently best predicted with a model combining (i) fast working memory and (ii) slower reinforcement learning from differently weighted positive and negative prediction errors as well as (iii) selective suppression of nonchosen feature values and (iv) a meta-learning mechanism that enhances exploration rates based on a memory trace of recent errors. The optimal model parameter settings suggest that these mechanisms cooperate differently at low and high attentional loads. Whereas working memory was essential for efficient learning at lower attentional loads, enhanced weighting of negative prediction errors and meta-learning were essential for efficient learning at higher attentional loads. Together, these findings pinpoint a canonical set of learning mechanisms and suggest how they may cooperate when subjects flexibly adjust to environments with variable real-world attentional demands.",
        "link": "http://dx.doi.org/10.1162/jocn_a_01780"
    },
    {
        "id": 27862,
        "title": "Model-Free Reinforcement Learning for Fully Cooperative Consensus Problem of Nonlinear Multiagent Systems",
        "authors": "Hong Wang, Man Li",
        "published": "2022-4",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2020.3042508"
    },
    {
        "id": 27863,
        "title": "Real-Time Pose Recognition for Billiard Players Using Deep Learning",
        "authors": "Zhikang Chen, Wei Qi Yan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "In this book chapter, the authors propose a method for player pose recognition in billiards matches by combining keypoint extraction and an optimized transformer. Given that those human pose analysis methods usually require high labour costs, the authors explore deep learning methods to achieve real-time, high-precision pose recognition. Firstly, they utilize human key point detection technology to extract the key points of players from real-time videos and generate key points. Then, the key point data is input into the transformer model for pose analysis and recognition. In addition, the authors design a human skeletal alignment method for comparison with standard poses. The experimental results show that the method performs well in recognizing players' poses in billiards matches and provides real-time and timely feedback on players' pose information. This research project provides a new and efficient tool for training billiard players and opens up new possibilities for applying deep learning in sports analytics. In addition, one of these contributions is the creation of a dataset for pose recognition.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch010"
    },
    {
        "id": 27864,
        "title": "Reinforcement Learning-Based Assistive Framework for Disabled Persons",
        "authors": "Muhammad Talha, Muhammad Aamir Panhwar, Kamran Ali Memon, Abbasi A, Ghazala Panhwar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nRobot programming by demonstration (PbD) enables human users to teach the robot and increases its capabilities by interactive teaching without having to manually program the robot. PbD combined with intelligent machine learning algothrims can help us to develop autonomous robots for various industrial and domestic tasks. One such task is the pouring of liquids from bottle into the cup/glass. In this paper the first step is to teach the robot liquid pouring task by the human user in order to facilitate physically disabled people in making various types of drinks and then dataset is created from the user taken demonstrations. In training stage the dataset obtained is feed to the decision-making algorithm based on reinforcement learning. The algorithm enables the robot to learn to pour different liquids under different pouring conditions with the help of minimum number of human user demonstrations needed for the learning of task. The acquired results show that the robot can learn to pour different liquids and is able to accurately adapt to different pouring conditions by using reward-based system and online feedback. Furthermore the results show that the proposed framework can work with different types of liquid and container sizes without any further need for reprogramming or demonstrations.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-939770/v1"
    },
    {
        "id": 27865,
        "title": "Desynchronizing of noisy neuron networks using reinforcement learning",
        "authors": "Meili Lu, Xile Wei",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ner.2017.8008349"
    },
    {
        "id": 27866,
        "title": "Solving Permutation Flowshop Problem with Deep Reinforcement Learning",
        "authors": "Ruyuan Pan, Xingye Dong, Sheng Han",
        "published": "2020-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/phm-besancon49106.2020.00068"
    },
    {
        "id": 27867,
        "title": "Inverse Reinforcement Learning for Generalized Labeled Multi-Bernoulli Multi-Target Tracking",
        "authors": "Ryan W. Thomas, Jordan D. Larson",
        "published": "2021-3-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aero50100.2021.9438310"
    },
    {
        "id": 27868,
        "title": "Speeding Up Reinforcement Learning by Exploiting Causality in Reward Sequences",
        "authors": "Hongming Li, Jose Principe",
        "published": "2021-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533910"
    },
    {
        "id": 27869,
        "title": "Intranet User-Level Security Traffic Management with Deep Reinforcement Learning",
        "authors": "Qiuqing Jin, Liming Wang",
        "published": "2019-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2019.8852447"
    },
    {
        "id": 27870,
        "title": "MADDPG: an efficient multi-agent reinforcement learning algorithm",
        "authors": "Xinyu Song",
        "published": "2022-6-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2637060"
    },
    {
        "id": 27871,
        "title": "State‐dependent Problems",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch8"
    },
    {
        "id": 27872,
        "title": "Safe Multi-Agent Deep Reinforcement Learning for Dynamic Virtual Network Allocation",
        "authors": "Akito Suzuki, Shigeaki Harada",
        "published": "2020-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom42002.2020.9348210"
    },
    {
        "id": 27873,
        "title": "Resource Allocation Strategy of Internet of Vehicles Using Reinforcement Learning in Edge Computing Environment",
        "authors": "Yihong li, Zhengli liu, Qi Tao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBecause the traditional computing model can no longer meet the particularity of Internet of Vehicles tasks, aiming at its characteristics of high bandwidth, low latency and high reliability, this paper proposes a resource allocation strategy for Internet of Vehicles using reinforcement learning in edge cloud computing environment. First, a multi-layer resource allocation model for Internet of Vehicles is proposed, which uses the cooperation mode of edge cloud computing servers and roadside units to dynamically coordinate edge computing and content caching. Then, based on the construction of communication model, calculation model and cache model, make full use of idle resources in Internet of Vehicles to minimize network delay under the condition of limited energy consumption. Finally, the optimization goal is solved by two-layer deep Q network model, and the best resource allocation plan is obtained. The simulation results based on the Internet of Vehicles model show that the computational energy consumption and system delay of proposed strategy do not exceed 400J and 600ms respectively. Besides, the overall effect of resource allocation is better than other comparison strategies and it has certain application prospects.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-974516/v1"
    },
    {
        "id": 27874,
        "title": "A Deep Reinforcement Learning Approach to IRS-assisted MU-MIMO Communication Systems",
        "authors": "Dariel Pereira-Ruisánchez, Óscar Fresnedo, Darian Pérez-Adán, Luis Castedo",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>The deep reinforcement learning (DRL)-based deep deterministic policy gradient (DDPG) framework is proposed to solve the joint optimization of the IRS phase-shift matrix and the precoding matrix in an IRS-assisted multi-stream multi-user MIMO communication.<br></div><div><br></div><div>The combination of multiple-input multiple-output(MIMO)  communications  and  intelligent  reflecting  surfaces(IRSs) is foreseen as a key enabler of beyond 5G (B5G) and 6Gsystems. In this work, we develop an innovative deep reinforcement learning (DRL)-based approach to the joint optimization of the MIMO precoders and the IRS phase-shift matrices that is proved to be efficient in high dimensional systems. The proposed approach is termed deep deterministic policy gradient (DDPG)and maximizes the sum rate of an IRS-assisted multi-stream(MS) multi-user MIMO (MU-MIMO) system by learning the best matrix configuration through online trial-and-error interactions. The proposed approach is formulated in terms of continuous state and action spaces, and a sum-rate-based reward function. The computational complexity is reduced by using artificial neural networks (ANNs) for function approximations and it is shown that the proposed solution scales better than other state-of-the-art methods, while reaching a competitive performance.<br></div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.17868425"
    },
    {
        "id": 27875,
        "title": "Inference-based Reinforcement Learning and its Application to Dynamic Resource Allocation",
        "authors": "Paschalis Tsiaflakis, Werner Coomans",
        "published": "2022-8-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco55093.2022.9909777"
    },
    {
        "id": 27876,
        "title": "Leveraging Application Permissions for Android Ransomware Detection: Deep Reinforcement Learning Approach",
        "authors": "Sekione  Reward Jeremiah, Haotian Chen, Stefanos Gritzalis, Jong Hyuk Park",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4740534"
    },
    {
        "id": 27877,
        "title": "A Reinforcement Learning Based Service Scheduling Algorithm for Internet of Drones",
        "authors": "Cong Pu",
        "published": "2022-5-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccworkshops53468.2022.9814662"
    },
    {
        "id": 27878,
        "title": "Reinforcement Learning-based Power Management Architecture for Optimal DVFS in SoCs",
        "authors": "David Akselrod",
        "published": "2021-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/socc52499.2021.9739607"
    },
    {
        "id": 27879,
        "title": "Continual Reinforcement Learning with Multi-Timescale Successor Features",
        "authors": "Raymond Chua, Blake Richards, Doina Precup, Christos Kaplanis",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2022.1229-0"
    },
    {
        "id": 27880,
        "title": "Nucleus Basalis Links Sensory Stimuli with Delayed Reinforcement to Support Learning",
        "authors": "W. Guo, D.B. Polley",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3248007"
    },
    {
        "id": 27881,
        "title": "Optimized Path Planning in Reinforcement Learning by Backtracking",
        "authors": "Qing Tan",
        "published": "2019-7-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32474/ctcsa.2019.01.000116"
    },
    {
        "id": 27882,
        "title": "Deep Reinforcement Learning based Intelligent Traffic Control System",
        "authors": "Paras Narendranath, Dhanaraj Venkatramana Kidiyoor, Sheela SV",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3834969"
    },
    {
        "id": 27883,
        "title": "Modeling Driver Behavior using Adversarial Inverse Reinforcement Learning",
        "authors": "Moritz Sackmann, Henrik Bey, Ulrich Hofmann, Jorn Thielecke",
        "published": "2022-6-5",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv51971.2022.9827292"
    },
    {
        "id": 27884,
        "title": "Deep Reinforcement Learning for Optimizing Finance Portfolio Management",
        "authors": "Yuh-Jong Hu, Shang-Jen Lin",
        "published": "2019-2",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aicai.2019.8701368"
    },
    {
        "id": 27885,
        "title": "Hardware Realization of Polar Decoder Using Reinforcement Learning Algorithm",
        "authors": "Albert Raj A, Calin A, Latha T, Subbulakshmi D, Vijila J",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nError control coding is nearly ubiquitous in our information-based society. In recent days, one amongst the error correcting techniques referred to as the polar codes which attracts the researchers as it represents one of the foremost breakthroughs in 5G standard. It is built by polarization effect of polarization matrix and it is one of the capacity achieving algorithms. It is proved that Successive List decoding (SCL) algorithm improves the efficiency of Polar codes. However, when the codelength increases the latency also increases. It is expected that Reinforcement learning algorithm (RLA) will be able to reduce the latency of the decoder. Therefore, in this article, Markov decision-process algorithm is proposed. RLA uses this Markov decision process when the decoding probabilities are unknown. The same is implemented in the hardware architecture. The implementation result shows that, this method reduces decoding latency to 33% without sacrificing a frame error rate. Experiment result shows that the hardware complexity is also reduced when compared to SCL decoding algorithm. This project is developed using System Generator (Xilinx), with a target device of FPGA-Virtex6.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1222830/v1"
    },
    {
        "id": 27886,
        "title": "State-space segmentation for faster training reinforcement learning",
        "authors": "Jongrae Kim",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2022.09.352"
    },
    {
        "id": 27887,
        "title": "Autonomous Surface Vessel Obstacle Avoidance Based on Hierarchical Reinforcement Learning With Potential Field Method",
        "authors": "Chang Zhou, Lei Wang, Huacheng He, Shangyu Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.0000710v"
    },
    {
        "id": 27888,
        "title": "Accelerated Deep Reinforcement Learning for Wireless Coded Caching",
        "authors": "Zhe Zhang, Meixia Tao",
        "published": "2019-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccchina.2019.8855915"
    },
    {
        "id": 27889,
        "title": "Focus on Scene Text Using Deep Reinforcement Learning",
        "authors": "Haobin Wang, Shuangping Huang, Lianwen Jin",
        "published": "2018-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr.2018.8545022"
    },
    {
        "id": 27890,
        "title": "Deep Reinforcement Learning-based anomaly detection for Video Surveillance",
        "authors": "Sabrina Aberkane, Mohamed Elarbi-Boudihir",
        "published": "2022-6-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31449/inf.v46i2.3603"
    },
    {
        "id": 27891,
        "title": "DATA-EFFICIENT MODEL-BASED REINFORCEMENT LEARNING FOR ROBOT CONTROL",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2316/j.2021.206-0528"
    },
    {
        "id": 27892,
        "title": "Exploring the behavioural spectrum with efficiency vs. fairness goals in Multi-Agent Reinforcement Learning",
        "authors": "Zafeiris Kokkinogenis, Margarida Silva, Jeremy Pitt, Rosaldo Rossetti",
        "published": "No Date",
        "citations": 0,
        "abstract": "The concept of fairness has been studied in philosophy and economics for thousands of years, so human actors in social systems have had plenty of time to ``learn'' what does, and does not, work. Yet, only recently. However, it is a relatively new question how software agents in a multi-agent system can use Reinforcement Learning models to develop an architecture that promotes equality or equity in the distribution of rewards to the agents within the system. Recent significant contributions have focused on optimising for efficiency based on the assumption that efficiency and fairness are opposites to be traded off against each other, but actually, the result of mixing fair and efficient policies is unknown in multi-agent reinforcement learning settings. In this work, we experiment with fair and efficient behaviours jointly, based on an extension of the state-of-the-art model in fairness SOTO that intertwines efficient and equitable recommendations. We analyse the fair versus efficient behavioural spectrum in the Matthew Effect and Traffic Light Control problems, finding some solutions that outperform the baseline SOTO and others that outperform a selfish baseline with comparable architectural design. We conclude it is possible to optimise for fairness and efficiency  and this is important when computation of the reward distribution has to be paid for from the rewards themselves.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19388147.v1"
    },
    {
        "id": 27893,
        "title": "State Recognition and Reinforcement Learning for Two-Wheel Mobile Robot",
        "authors": "Yoshihiro Yasutake",
        "published": "2018-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscit.2018.8587946"
    },
    {
        "id": 27894,
        "title": "LSI element placement method based on deep reinforcement learning",
        "authors": "A. V. Mozhzhukhina",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.54092/25421085_2022_2_31"
    },
    {
        "id": 27895,
        "title": "The social transmission of empathy relies on observational reinforcement learning",
        "authors": "Yuqing Zhou, Shihui Han, Pyungwon Kang, Philippe N. Tobler, Grit Hein",
        "published": "No Date",
        "citations": 0,
        "abstract": "Theories of moral development propose that empathy is transmitted across individuals, yet the mechanism through which empathy is socially transmitted remains unclear. We conducted three studies to investigate whether, and if so, how observing empathic responses in others affects the empathy of the observer. Our results show that observing empathic or non-empathic responses generates learning signals that respectively increases or decreases empathy ratings of the observer and alters empathy-related responses in the anterior insula (AI), i.e., the same region that correlated with empathy baseline ratings, as well as its functional connectivity with the temporal-parietal junction (TPJ). Together, our findings provide a neurocomputational mechanism for the social transmission of empathy that accounts for changes in individual empathic responses in empathic and non-empathic social environments.",
        "link": "http://dx.doi.org/10.31234/osf.io/wknuh"
    },
    {
        "id": 27896,
        "title": "Reinforcement Learning Motivated Feedforward Control Approach for Disturbance Rejection and Tracking",
        "authors": "I. Faktorovich, C. Bohn, J. Vogelsang",
        "published": "2021-6-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc54610.2021.9655160"
    },
    {
        "id": 27897,
        "title": "Local Coordination in Multi-Agent Reinforcement Learning",
        "authors": "Fanchao Xu, Tomoyuki Kaneko",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/taai54685.2021.00036"
    },
    {
        "id": 27898,
        "title": "Hierarchical Reinforcement Learning for Efficient and Effective Automated Penetration Testing of Large Networks",
        "authors": "Mohamed Chahine Ghanem, Thomas Chen, Erivelton Nepomuceno",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nPenetration testing (PT) is a method for assessing and evaluating the security of digital assets by planning, generating, and executing possible attacks that aim to discover and exploit vulnerabilities. In large networks, penetration testing becomes repetitive, complex and resource-consuming despite the use of automated tools.\nThis paper investigates reinforcement learning (RL) to make penetration testing more intelligent, targeted, and efficient. The proposed approach called Intelligent Automated Penetration Testing Framework (IAPTF) utilizes model-based RL to automate sequential decision making. Penetration testing tasks are treated as a partially observed Markov decision process (POMDP) which is solved with an external POMDP-solver using different algorithms to identify the most efficient options.\nA major difficulty encountered was solving large POMDPs resulting from large networks. This was overcome by representing networks hierarchically as a group of clusters and treating each cluster separately.\nThis approach is tested through simulations of networks of various sizes. The results show that IAPTF with hierarchical network modeling outperforms previous approaches as well as human performance in terms of time, number of tested vectors and accuracy, especially in large networks. Another advantage of IAPTF is the ease of repetition for retesting similar networks, which is often encountered in real penetration testing. The results suggest that IAPTF is a promising approach to offload work from and ultimately replace human pen-testing.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1686285/v1"
    },
    {
        "id": 27899,
        "title": "Quantum Computing &amp;amp;Amp; Reinforcement Learning: Partners in Achieving Artificial General Intelligence",
        "authors": "Akintunde Agunbiade",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4197291"
    },
    {
        "id": 27900,
        "title": "Inductive biases in theory-based reinforcement learning",
        "authors": "Thomas Pouncy, Samuel J. Gershman",
        "published": "2022-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cogpsych.2022.101509"
    },
    {
        "id": 27901,
        "title": "MARLYC: Multi-Agent Reinforcement Learning Yaw Control",
        "authors": "Elie KADOCHE, Sébastien Gourvénec, Maxime Pallud, Tanguy Levent",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4507479"
    },
    {
        "id": 27902,
        "title": "ReCEIF: Reinforcement Learning-Controlled Effective Ingress Filtering",
        "authors": "Hauke Heseding, Martina Zitterbart",
        "published": "2022-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lcn53696.2022.9843478"
    },
    {
        "id": 27903,
        "title": "Underwater Acoustic Channel Prediction in delay-Doppler Domain based on Reinforcement Learning",
        "authors": "Sixu Wang, Wei Li, Rongrong Guo",
        "published": "2022-10-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oceans47191.2022.9977194"
    },
    {
        "id": 27904,
        "title": "PSS Control of Multi Machine Power System Using Reinforcement Learning",
        "authors": "Xingwang Xie",
        "published": "2021-6-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icras52289.2021.9476268"
    },
    {
        "id": 27905,
        "title": "Collaborative Multi-Agent Reinforcement Learning of Caching Optimization in Small-Cell Networks",
        "authors": "Xianzhe Xu, Meixia Tao",
        "published": "2018-12",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/glocom.2018.8647341"
    },
    {
        "id": 27906,
        "title": "Reinforcement learning-guided control strategies for CAR T-cell activation and expansion",
        "authors": "Nigel Reuel, Sakib Ferdous, Ibne Farabi Shihab, Ratul Chowdhury",
        "published": "No Date",
        "citations": 0,
        "abstract": "Reinforcement learning (RL), a subset of machine learning (ML), can\npotentially optimize and control biomanufacturing processes, such as\nimproved production of therapeutic cells. Here, the process of CAR-T\ncell activation by antigen presenting beads and their subsequent\nexpansion is formulated in-silico. The simulation is used as an\nenvironment to train RL-agents to dynamically control the number of\nbeads in culture with the objective of maximizing the population of\nrobust effector cells at the end of the culture. We make periodic\ndecisions of incremental bead addition or complete removal. The\nsimulation is designed to operate in OpenAI Gym which enables testing of\ndifferent environments, cell types, agent algorithms and state-inputs to\nthe RL-agent. Agent training is demonstrated with three different\nalgorithms (PPO, A2C and DQN) each sampling three different state input\ntypes (tabular, image, mixed); PPO-tabular performs best for this\nsimulation environment. Using this approach, training of the RL-agent on\ndifferent cell types is demonstrated, resulting in unique control\nstrategies for each type. Sensitivity to input-noise (sensor\nperformance), number of control step interventions, and advantage of\npre-trained agents are also evaluated. Therefore, we present a general\ncomputational framework to maximize the population of robust effector\ncells in CAR-T cell therapy production.",
        "link": "http://dx.doi.org/10.22541/au.169347990.09869065/v1"
    },
    {
        "id": 27907,
        "title": "The Concept of Criticality in Reinforcement Learning",
        "authors": "Yitzhak Spielberg, Amos Azaria",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai.2019.00043"
    },
    {
        "id": 27908,
        "title": "Adaptive Identification of Supply Chain Disruptions Through Reinforcement Learning",
        "authors": "Hamed Aboutorab, Omar  Khadeer Hussain, Morteza Saberi, Farookh  Khadeer Hussain, Daniel Prior",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4374893"
    },
    {
        "id": 27909,
        "title": "Analysis on Deep Reinforcement Learning in Industrial Robotic Arm",
        "authors": "Hengyue Guan",
        "published": "2020-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichci51889.2020.00094"
    },
    {
        "id": 27910,
        "title": "Exploration with Multiple Random ε-Buffers in Off-Policy Deep Reinforcement Learning",
        "authors": " Kim,  Park",
        "published": "2019-11-1",
        "citations": 2,
        "abstract": "In terms of deep reinforcement learning (RL), exploration is highly significant in achieving better generalization. In benchmark studies, ε-greedy random actions have been used to encourage exploration and prevent over-fitting, thereby improving generalization. Deep RL with random ε-greedy policies, such as deep Q-networks (DQNs), can demonstrate efficient exploration behavior. A random ε-greedy policy exploits additional replay buffers in an environment of sparse and binary rewards, such as in the real-time online detection of network securities by verifying whether the network is “normal or anomalous.” Prior studies have illustrated that a prioritized replay memory attributed to a complex temporal difference error provides superior theoretical results. However, another implementation illustrated that in certain environments, the prioritized replay memory is not superior to the randomly-selected buffers of random ε-greedy policy. Moreover, a key challenge of hindsight experience replay inspires our objective by using additional buffers corresponding to each different goal. Therefore, we attempt to exploit multiple random ε-greedy buffers to enhance explorations for a more near-perfect generalization with one original goal in off-policy RL. We demonstrate the benefit of off-policy learning from our method through an experimental comparison of DQN and a deep deterministic policy gradient in terms of discrete action, as well as continuous control for complete symmetric environments.",
        "link": "http://dx.doi.org/10.3390/sym11111352"
    },
    {
        "id": 27911,
        "title": "A New Approach for Drone Tracking with Drone Using Proximal Policy Optimization Based Distributed Deep Reinforcement Learning",
        "authors": "Ziya Tan, Mehmet Karakose",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4393763"
    },
    {
        "id": 27912,
        "title": "An overview of reinforcement learning techniques",
        "authors": "Damjan Pecioski, Viktor Gavriloski, Simona Domazetovska, Anastasija Ignjatovska",
        "published": "2023-6-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/meco58584.2023.10155066"
    },
    {
        "id": 27913,
        "title": "Model-Free Cooperative Optimal Output Regulation for Linear Discrete-Time Multi-Agent Systems Using Reinforcement Learning",
        "authors": "Beining Wu, Wei Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this paper, an off-policy model-free algorithm is presented for solving the cooperative optimal output regulation problem for linear discrete-time multi-agent systems. First, an adaptive distributed observer is designed for each follower to estimate the leader's information. Then, a distributed feedback-feedforward controller is developed for each follower to solve the cooperative optimal output regulation problem utilizing the follower's state information and the adaptive distributed observer. Based on reinforcement learning method, an adaptive algorithm is presented to find the optimal feedback gains via online data collecting from system trajectory. By designing a Sylvester map, the solution to the regulator equations is calculated via data collected from the optimal feedback gain design steps, and the feedforward control gain is found. Finally, an off-policy model-free algorithm is proposed to design the distributed feedback-feedforward controller for each follower to solve the cooperative optimal output regulation problem. A numerical example is given to verify the effectiveness of this proposed approach.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2797557/v1"
    },
    {
        "id": 27914,
        "title": "Stability-Certified Reinforcement Learning Via Spectral Normalization",
        "authors": "Ryoichi Takase, Nobuyuki Yoshikawa, Toshisada Mariyama, Takeshi Tsuchiya",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4156886"
    },
    {
        "id": 27915,
        "title": "Cross-modal Bidirectional Translation via Reinforcement Learning",
        "authors": "Jinwei Qi, Yuxin Peng",
        "published": "2018-7",
        "citations": 22,
        "abstract": "The inconsistent distribution and representation of image and text make it quite challenging to measure their similarity, and construct correlation between them. Inspired by neural machine translation to establish a corresponding relationship between two entirely different languages, we attempt to treat images as a special kind of language to provide visual descriptions, so that translation can be conduct between bilingual pair of image and text to effectively explore cross-modal correlation. Thus, we propose Cross-modal Bidirectional Translation (CBT) approach, and further explore the utilization of reinforcement learning to improve the translation process. First, a cross-modal translation mechanism is proposed, where image and text are treated as bilingual pairs, and cross-modal correlation can be effectively captured in both feature spaces of image and text by bidirectional translation training. Second, cross-modal reinforcement learning is proposed to perform a bidirectional game between image and text, which is played as a round to promote the bidirectional translation process. Besides, both inter-modality and intra-modality reward signals can be extracted to provide complementary clues for boosting cross-modal correlation learning. Experiments are conducted to verify the performance of our proposed approach on cross-modal retrieval, compared with 11 state-of-the-art methods on 3 datasets.",
        "link": "http://dx.doi.org/10.24963/ijcai.2018/365"
    },
    {
        "id": 27916,
        "title": "Reinforcement Learning Approach for Hybrid WiFi-VLC Networks",
        "authors": "Abdulmajeed M. Alenezi, Khairi A. Hamdi",
        "published": "2020-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2020-spring48590.2020.9128892"
    },
    {
        "id": 27917,
        "title": "Deep reinforcement learning for robotic grasping with tactile sensor\n\t\t\t\t\t\tfeedback",
        "authors": "Bahador Beigomi, Zheng H. Zhu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17118/11143/21095"
    },
    {
        "id": 27918,
        "title": "Real Options Models Without Threshold Behavior: A Reinforcement Learning Approach",
        "authors": "Farzan Faninam, Kuno  J.M. Huisman, Peter  M. Kort, Juan  C. Vera",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4449006"
    },
    {
        "id": 27919,
        "title": "Investigating the Practicality of Existing Reinforcement Learning Algorithms: A Performance Comparison",
        "authors": "Olivia Dizon-Paradis, Stephen Wormald, Daniel Capecci, Avanti Bhandarkar, Damon Woodard",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p><strong>Abstract---</strong>Reinforcement learning (RL) has become more popular due to promising results in applications such as chat-bots, healthcare, and autonomous driving. However, one significant challenge in current RL research is the difficulty in understanding which RL algorithms, if any, are practical for a given use case. Few RL algorithms are rigorously tested, and hence understood, for their practical implications. Although there are a number of performance comparisons in literature, many use few environments and do not consider real-world limitations such as run-time and memory usage. Furthermore, many works do not make their code publicly accessible for others to use. This paper addresses this gap by presenting the most comprehensive performance comparison on the practicality of RL algorithms known to date. Specifically, this paper focuses on discrete, model-free deep RL algorithms for their practicality in real-world problems where efficient implementations are necessary. In total, fourteen RL algorithms were trained on twenty-three environments (468 environment instances), which collectively required 224 GB and 766 days CPU time to run all experiments, and 1.7 GB to store all models. Overall, the results indicate several shortcomings in RL algorithms' exploration efficiency, memory/sample efficiency, and space/time complexity. Based on these shortcomings, numerous opportunities for future works were identified to improve the capabilities of modern algorithms. This paper’s findings will help researchers and practitioners improve and employ RL algorithms in time-sensitive and resource-constrained applications such as economics, cybersecurity, and Internet of Things (IoT).  </p>\n<p><br></p>\n<p><strong>Impact Statement---</strong>Reinforcement learning (RL) technologies are commonly used in autonomous driving, chat-bot, and business analytic applications. They learn how to adapt to unforeseen situations, reducing the load on human drivers, support teams, and analysts. Although there are a variety of theoretical works in RL literature, very few algorithms are tested and evaluated to facilitate their use in real-life scenarios. The performance comparison introduced in this paper addresses these limitations. The performance analysis framework, re-implemented source code, and findings identified in this study could increase the adoption and speed development of RL technologies in more real-life applications. Moreover, the open challenges, recommendations, and practical implications identified in this paper could facilitate collaboration and development of new technologies among researchers and practitioners in industry and academia.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23739099.v1"
    },
    {
        "id": 27920,
        "title": "Valence biases in reinforcement learning and autobiographical memory",
        "authors": "Susan Benear, Michael Evans, Gail Rosenbaum, Catherine Hartley",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1394-0"
    },
    {
        "id": 27921,
        "title": "Identifying critical nodes via link equations and deep reinforcement learning",
        "authors": "Peiyu Chen, Wenhui Fan",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126871"
    },
    {
        "id": 27922,
        "title": "Reinforcement learning in synthetic gene circuits",
        "authors": "Adrian Racovita, Alfonso Jaramillo",
        "published": "2020-8-28",
        "citations": 5,
        "abstract": "Synthetic gene circuits allow programming in DNA the expression of a phenotype at a given environmental condition. The recent integration of memory systems with gene circuits opens the door to their adaptation to new conditions and their re-programming. This lays the foundation to emulate neuromorphic behaviour and solve complex problems similarly to artificial neural networks. Cellular products such as DNA or proteins can be used to store memory in both digital and analog formats, allowing cells to be turned into living computing devices able to record information regarding their previous states. In particular, synthetic gene circuits with memory can be engineered into living systems to allow their adaptation through reinforcement learning. The development of gene circuits able to adapt through reinforcement learning moves Sciences towards the ambitious goal: the bottom-up creation of a fully fledged living artificial intelligence.",
        "link": "http://dx.doi.org/10.1042/bst20200008"
    },
    {
        "id": 27923,
        "title": "Individualized sepsis treatment using reinforcement learning",
        "authors": "Suchi Saria",
        "published": "2018-11",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1038/s41591-018-0253-x"
    },
    {
        "id": 27924,
        "title": "Nav-Q: Quantum Deep Reinforcement Learning for Collision-Free Navigation of Self-Driving Cars",
        "authors": "Akash Sinha, Antonio Macaluso, Matthias Klusch",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe task of collision-free navigation (CFN) of self-driving cars is an NP-hard problem usually tackled using Deep Reinforcement Learning (DRL). While DRL methods have proven to be effective, their implementation requires substantial computing resources and extended training periods to develop a robust agent. On the other hand, quantum reinforcement learning has recently demonstrated faster convergence and improved stability in simple, non-real-world environments.\nIn this work, we propose Nav-Q, the first quantum-supported DRL algorithm for CFN of self-driving cars, that leverages quantum computation for improving the training performance without the requirement for onboard quantum hardware. Nav-Q is based on the actor-critic approach, where the critic is implemented using a hybrid quantum-classical algorithm suitable for near-term quantum devices. We assess the performance of Nav-Q using the CARLA driving simulator, a de facto standard benchmark for evaluating state-of-the-art DRL methods. Our empirical evaluations showcase that Nav-Q surpasses its classical counterpart in terms of training stability and, in certain instances, with respect to the convergence rate. Furthermore, we assess Nav-Q in relation to effective dimension, unveiling that the incorporation of a quantum component results in a model with greater descriptive power compared to classical baselines. Finally, we evaluate the performance of Nav-Q using noisy quantum simulation, observing that the quantum noise deteriorates the training performances but enhances the exploratory tendencies of the agent during training.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3796117/v1"
    },
    {
        "id": 27925,
        "title": "Artificial Intelligence Gamers Based on Deep Reinforcement Learning",
        "authors": "Haolin Wang",
        "published": "2024-1-26",
        "citations": 0,
        "abstract": "This study investigates the design and implementation of Artificial Intelligence (AI) game players based on deep reinforcement learning, offering a novel approach to autonomous decision-making and strategy acquisition in intelligent games. Initially, the fundamental principles and algorithms of deep reinforcement learning are introduced, along with the fusion of deep learning and reinforcement learning. Subsequently, existing research is reviewed, and the pros and cons of current methodologies are examined, highlighting the underlying issues and challenges. The utilization of AI players in mainstream games is then introduced, and the influence of AI players on contemporary games is analyzed. Through this analysis of AI players in mainstream games, the strengths and weaknesses of current AI players are identified, and recommendations for optimizing them are provided. This study holds significant implications for guiding the design and development of intelligent game players, while also enriching the application of deep reinforcement learning within the gaming domain.",
        "link": "http://dx.doi.org/10.54097/p9tv1494"
    },
    {
        "id": 27926,
        "title": "Deep Reinforcement Learning for Traffic Light Optimization",
        "authors": "Mustafa Coskun, Abdelkader Baggag, Sanjay Chawla",
        "published": "2018-11",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdmw.2018.00088"
    },
    {
        "id": 27927,
        "title": "Reinforcement Learning for Efficient Scheduling in Complex Semiconductor Equipment",
        "authors": "Doug Suerich, Terry Young",
        "published": "2020-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asmc49169.2020.9185293"
    },
    {
        "id": 27928,
        "title": "Deep Residual Reinforcement Learning (Extended Abstract)",
        "authors": "Shangtong Zhang, Wendelin Boehmer, Shimon Whiteson",
        "published": "2021-8",
        "citations": 1,
        "abstract": "We revisit residual algorithms in both model-free and model-based reinforcement learning settings.\n\nWe propose the bidirectional target network technique to stabilize residual algorithms, yielding a residual version of DDPG that significantly outperforms vanilla DDPG in commonly used benchmarks.\n\nMoreover, we find the residual algorithm an effective approach to the distribution mismatch problem in model-based planning.\n\nCompared with the existing TD(k) method, our residual-based method makes weaker assumptions about the model and yields a greater performance boost.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/668"
    },
    {
        "id": 27929,
        "title": "Utterance Censorship of Online Reinforcement Learning Chatbot",
        "authors": "Yixuan Chai, Guohua Liu",
        "published": "2018-11",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai.2018.00063"
    },
    {
        "id": 27930,
        "title": "Deep Hierarchical Reinforcement Learning for Autonomous Driving with Distinct Behaviors",
        "authors": "Jianyu Chen, Zining Wang, Masayoshi Tomizuka",
        "published": "2018-6",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2018.8500368"
    },
    {
        "id": 27931,
        "title": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning",
        "authors": "Xiao-Yang Liu",
        "published": "No Date",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4253139"
    },
    {
        "id": 27932,
        "title": "Do model-based and model-free reinforcement learning correspond to goal-directed and habitual actions, respectively? A systematic review.",
        "authors": "Xuan Yee",
        "published": "No Date",
        "citations": 0,
        "abstract": "The idea that model-based reinforcement learning (RL) corresponds to goal-directed actions and model-free RL corresponds to habitual actions is deeply entrenched in psychology. To test this hypothesis, we will first review and evaluate the evidence base from behavioral and neural studies. Behaviorally, positive correlations between behavioral parameters that index model-based/model-free RL and goal-directed/habitual actions would support the hypothesis. Neurally, overlapping neural substrates that underlie the model-based/goal-directed constructs and between the model-free/habitual constructs, as well as a dissociation between the neural substrates underlying both groups of constructs, would support the hypothesis. We will then discuss alternative classes of computational theories beyond the model-based/model-free framework that purport to describe goal-directed and habitual behaviour, and compare these theories against the model-based/model-free framework as well as against each other. Some of the alternative theories covered in this review include dichotomy-based frameworks (e.g., value-based vs. value-free), hierarchical frameworks (e.g., action sequences or active inference), and biological frameworks (e.g., actor-critic models). We then outline potential approaches to synthesize the findings and future avenues of research. Overall, we find that model-based RL maps onto certain facets of goal-directed actions but not necessarily onto other facets. On the other hand, there is much evidence suggesting that model-free RL does not track habitual actions.",
        "link": "http://dx.doi.org/10.31234/osf.io/qm3gp"
    },
    {
        "id": 27933,
        "title": "Deep Reinforcement Learning Based Airport Departure Metering",
        "authors": "Hasnain Ali, Pham Duc Thinh, Sameer Alam",
        "published": "2021-9-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9565012"
    },
    {
        "id": 27934,
        "title": "Adversarial Manipulation of Reinforcement Learning Policies in Autonomous Agents",
        "authors": "Yonghong Huang, Shih-han Wang",
        "published": "2018-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489741"
    },
    {
        "id": 27935,
        "title": "Planning for Negotiations in Autonomous Driving using Reinforcement Learning",
        "authors": "Roi Reshef",
        "published": "2022-10-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981988"
    },
    {
        "id": 27936,
        "title": "Reinforcement Learning Based Transmitter-Receiver Selection for Distributed MIMO Radars",
        "authors": "Petteri Pulkkinen, Tuomas Aittomaki, Visa Koivunen",
        "published": "2020-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radar42522.2020.9114644"
    },
    {
        "id": 27937,
        "title": "Disrupted reinforcement learning during post-error slowing in ADHD",
        "authors": "Andre Chevrier, Mehereen Bhaijiwala, Jonathan Lipszyc, Douglas Cheyne, Simon Graham, Russell Schachar",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractADHD is associated with altered dopamine regulated reinforcement learning on prediction errors. Despite evidence of categorically altered error processing in ADHD, neuroimaging advances have largely investigated models of normal reinforcement learning in greater detail. Further, although reinforcement leaning critically relies on ventral striatum exerting error magnitude related thresholding influences on substantia nigra (SN) and dorsal striatum, these thresholding influences have never been identified with neuroimaging. To identify such thresholding influences, we propose that error magnitude related activities must first be separated from opposite activities in overlapping neural regions during error detection. Here we separate error detection from magnitude related adjustment (post-error slowing) during inhibition errors in the stop signal task in typically developing (TD) and ADHD adolescents using fMRI. In TD, we predicted that: 1) deactivation of dorsal striatum on error detection interrupts ongoing processing, and should be proportional to right frontoparietal response phase activity that has been observed in the SST; 2) deactivation of ventral striatum on post-error slowing exerts thresholding influences on, and should be proportional to activity in dorsal striatum. In ADHD, we predicted that ventral striatum would instead correlate with heightened amygdala responses to errors. We found deactivation of dorsal striatum on error detection correlated with response-phase activity in both groups. In TD, post-error slowing deactivation of ventral striatum correlated with activation of dorsal striatum. In ADHD, ventral striatum correlated with heightened amygdala activity. Further, heightened activities in locus coeruleus (norepinephrine), raphe nucleus (serotonin) and medial septal nuclei (acetylcholine), which all compete for control of DA, and are altered in ADHD, exhibited altered correlations with SN. All correlations in TD were replicated in healthy adults. Results in TD are consistent with dopamine regulated reinforcement learning on post-error slowing. In ADHD, results are consistent with heightened activities in the amygdala and non-dopaminergic neurotransmitter nuclei preventing reinforcement learning.",
        "link": "http://dx.doi.org/10.1101/449975"
    },
    {
        "id": 27938,
        "title": "Acquiring musculoskeletal skills with curriculum-based reinforcement learning",
        "authors": "Alberto Silvio Chiappa, Pablo Tano, Nisheet Patel, Abigaïl Ingster, Alexandre Pouget, Alexander Mathis",
        "published": "No Date",
        "citations": 0,
        "abstract": "Efficient, physiologically-detailed musculoskeletal simulators and powerful learning algorithms provide new computational tools to tackle the grand challenge of understanding biological motor control. Our winning solution for the first NeurIPS MyoChallenge leverages an approach mirroring human learning and showcases reinforcement and curriculum learning as mechanisms to find motor control policies in complex object manipulation tasks. Analyzing the policy against data from human subjects reveals insights into efficient control of complex biological systems. Overall, our work highlights the new possibilities emerging at the interface of musculoskeletal physics engines, reinforcement learning and neuroscience.",
        "link": "http://dx.doi.org/10.1101/2024.01.24.577123"
    },
    {
        "id": 27939,
        "title": "Meta-Reinforcement Learning with Transformer Networks for Space Guidance Applications",
        "authors": "Lorenzo Federici, Roberto Furfaro",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2024-2061"
    },
    {
        "id": 27940,
        "title": "Reinforcement Learning for Maritime Communications",
        "authors": "Liang Xiao, Helin Yang, Weihua Zhuang, Minghui Min",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-32138-2"
    },
    {
        "id": 27941,
        "title": "DeepBike: A Deep Reinforcement Learning Based Model for Large-scale Online Bike Share Rebalancing",
        "authors": "Zhuoli Yin, Zhaoyu Kou, Hua Cai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBike share systems (BSSs), as a potentially environment-friendly mobility mode, are being deployed globally. To address spatially and temporally imbalanced bike and dock demands, BSS operators need to redistribute bikes among stations using a fleet of rebalancing vehicles in real-time. However, existing studies mainly generate BSS rebalancing solutions for small-scale BSSs or subsets of BSSs, while deploying small-size rebalancing fleets. How to produce online rebalancing solutions for large-scale BSS with multiple rebalancing vehicles to minimize customer loss is critical for system operation yet remains unsolved. To address this gap, we proposed a deep reinforcement learning based model — DeepBike — that trains deep Q-network (DQN) to learn the optimal strategy for dynamic bike share rebalancing. DeepBike uses real-time states of rebalancing vehicles, stations and predicted demands as inputs to output the long-term quality values of rebalancing actions of each rebalancing vehicle. Rebalancing vehicles could work asynchronously as each individually runs the DQN. We compared the performance of the proposed DeepBike against baseline models for dynamic bike share rebalancing based on historical trip records from Divvy BSS in Chicago, which possesses more than 500 stations and 16 rebalancing vehicles. The evaluation results show that our proposed DeepBike model was able to better reduce customer loss by 111.09% and 57.6% than the mixed integer programming and heuristic-based models, respectively, and increased overall net profits by 101.26% and 220.01%, respectively. The DeepBike model is effective for large-scale dynamic bike share rebalancing problems and has the potential to improve the operation of shared mobility systems.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3998473/v1"
    },
    {
        "id": 27942,
        "title": "Container Caching Optimization based on Explainable Deep Reinforcement Learning",
        "authors": "Divyashree Jayaram, Saad Jeelani, Genya Ishigaki",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437757"
    },
    {
        "id": 27943,
        "title": "Reinforcement Learning for Cyber-Physical Systems",
        "authors": "Xing Liu, Hansong Xu, Weixian Liao, Wei Yu",
        "published": "2019-11",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icii.2019.00063"
    },
    {
        "id": 27944,
        "title": "Polya Decision Processes: A New History-Dependent Framework for Reinforcement Learning",
        "authors": "Masahiro Kohjima",
        "published": "2022-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc51059.2022.9992504"
    },
    {
        "id": 27945,
        "title": "A Survey on Reinforcement Learning for Combinatorial Optimization",
        "authors": "Yunhao Yang, Andrew Whinston",
        "published": "2023-7-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aic57670.2023.10263956"
    },
    {
        "id": 27946,
        "title": "EdgeAlign: platform for deep reinforcement learning based pairwise DNA sequence alignment compatible with embedded edge devices",
        "authors": "Aryan Lall, Siddharth Tallur",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nSequence alignment is an essential component of bioinformatics, for identifying regions of similarity that may indicate functional, structural, or evolutionary relationships between the sequences. Genome-based diagnostics relying on DNA sequencing have benefited hugely from the boom in computing power in recent decades, particularly due to cloud-computing and the rise of graphics processing units (GPUs) and other advanced computing platforms for running advanced algorithms. Translating the success of such breakthroughs in diagnostics to affordable solutions for low-cost healthcare requires development of algorithms that can operate on the edge instead of in the cloud, using low-cost and low-power electronic systems such as microcontrollers and field programmable gate arrays (FPGAs). In this work, we present EdgeAlign, a deep reinforcement learning based framework for performing pairwise DNA sequence alignment on stand-alone edge devices. EdgeAlign uses deep reinforcement learning to train a deep Q-network (DQN) agent for performing sequence alignment on fixed length sub-sequences, using a sliding window that is scanned over the length of the entire sequence. The hardware resource-consumption for implementing this scheme is thus independent of the lengths of the sequences to be aligned, and is further optimized using a novel AutoML based method for neural network model size reduction. Unlike other algorithms for sequence alignment reported in literature, the model demonstrated in this work is highly compact and deployed on two edge devices (NVIDIA® Jetson NanoTM Developer Kit and Digilent Arty A7-100T, containing Xilinx® XC7A35T Artix®-7 FPGA) for demonstration of alignment for sequences from the publicly available Influenza sequences at the National Center for Biotechnology Information (NCBI) Virus Data Hub.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2175890/v1"
    },
    {
        "id": 27947,
        "title": "Optimal Adaptive Prediction Intervals for Electricity Load Forecasting in Distribution Systems via Reinforcement Learning",
        "authors": "Yufan Zhang, Honglin Wen, Qiuwei Wu, Qian Ai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Prediction intervals (PIs) offer an effective tool for quantifying uncertainty of loads in distribution systems. The traditional central PIs cannot adapt well to skewed distributions, and their offline training fashion is vulnerable to the unforeseen change in future load patterns. Therefore, we propose an optimal PI estimation approach, which is online and adaptive to different data distributions by adaptively determining symmetric or asymmetric probability proportion pairs for quantiles of PIs’ bounds. It relies on the online learning ability of reinforcement learning (RL) to integrate the two online tasks, i.e., the adaptive selection of probability proportion pairs and quantile predictions, both of which are modeled by neural networks. As such, the quality of quantiles-formed PI can guide the selection process of optimal probability proportion pairs, which forms a closed loop to improve PIs’ quality. Furthermore, to improve the learning efficiency of quantile forecasts, a prioritized experience replay (PER) strategy is proposed for online quantile regression processes. Case studies on both load and net load demonstrate that the proposed method can better adapt to data distribution compared with online central PIs method. Compared with offline-trained methods, it obtains PIs with better quality and is more robust against concept drift.",
        "link": "http://dx.doi.org/10.36227/techrxiv.17925911.v1"
    },
    {
        "id": 27948,
        "title": "Optimal Adaptive Prediction Intervals for Electricity Load Forecasting in Distribution Systems via Reinforcement Learning",
        "authors": "Yufan Zhang, Honglin Wen, Qiuwei Wu, Qian Ai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Prediction intervals (PIs) offer an effective tool for quantifying uncertainty of loads in distribution systems. The traditional central PIs cannot adapt well to skewed distributions, and their offline training fashion is vulnerable to the unforeseen change in future load patterns. Therefore, we propose an optimal PI estimation approach, which is online and adaptive to different data distributions by adaptively determining symmetric or asymmetric probability proportion pairs for quantiles of PIs’ bounds. It relies on the online learning ability of reinforcement learning (RL) to integrate the two online tasks, i.e., the adaptive selection of probability proportion pairs and quantile predictions, both of which are modeled by neural networks. As such, the quality of quantiles-formed PI can guide the selection process of optimal probability proportion pairs, which forms a closed loop to improve PIs’ quality. Furthermore, to improve the learning efficiency of quantile forecasts, a prioritized experience replay (PER) strategy is proposed for online quantile regression processes. Case studies on both load and net load demonstrate that the proposed method can better adapt to data distribution compared with online central PIs method. Compared with offline-trained methods, it obtains PIs with better quality and is more robust against concept drift.",
        "link": "http://dx.doi.org/10.36227/techrxiv.17925911"
    },
    {
        "id": 27949,
        "title": "Robust Lane Change Decision Making for Autonomous Vehicles: An Observation Adversarial Reinforcement Learning Approach",
        "authors": "Xiangkun He, Haohan Yang, Zhongxu Hu, Chen Lv",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning holds the promise of allowing autonomous vehicles to learn complex decision making behaviors through interacting with other traffic participants. However, many real-world driving tasks involve unpredictable perception errors or measurement noises which may mislead an autonomous vehicle into making unsafe decisions, even cause catastrophic failures. In light of these risks, to ensure safety under perception uncertainty, autonomous vehicles are required to be able to cope with the worst case observation perturbations. Therefore, this paper proposes a novel observation adversarial reinforcement learning approach for robust lane change decision making of autonomous vehicles. A constrained observation-robust Markov decision process is presented to model lane change decision making behaviors of autonomous vehicles under policy constraints and observation uncertainties.  Meanwhile, a black-box attack technique based on Bayesian optimization is implemented to approximate the optimal adversarial observation perturbations efficiently. Furthermore, a constrained observation-robust actor-critic algorithm is advanced to optimize autonomous driving lane change policies while keeping the variations of the policies attacked by the optimal adversarial observation perturbations within bounds. Finally, the robust lane change decision making approach is evaluated in three stochastic mixed traffic flows based on different densities. The results demonstrate that the proposed method can not only enhance the performance of an autonomous vehicle but also improve the robustness of lane change policies against adversarial observation perturbations.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21229319"
    },
    {
        "id": 27950,
        "title": "Deep Contextual Bandit and Reinforcement Learning for IRS-assisted MU-MIMO Systems",
        "authors": "Dariel Pereira-Ruisánchez, Óscar Fresnedo, Darian Pérez-Adán, Luis Castedo",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> The combination of multiple-input multiple-output (MIMO) and intelligent reflecting surfaces (IRSs) is foreseen as a key enabler of beyond 5G (B5G) and 6G. In this work, two different approaches are considered for the joint optimization of the IRS phase-shift matrix and MIMO precoders of an IRS-assisted multi-stream (MS) multi-user MIMO (MU-MIMO) system with the aim of maximizing the system sum-rate for every channel realization. The first one is a novel contextual bandit (CB) approach with continuous state and action spaces called deep contextual bandit-oriented deep deterministic policy gradient (DCB-DDPG). The second is an innovative deep reinforcement learning (DRL) formulation where the states, actions and rewards are selected such that the Markov decision process (MDP) property of reinforcement learning (RL) is properly met. Both proposals perform remarkably better than state-of-the-art heuristic methods in high multi-user interference scenarios. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19787551.v1"
    },
    {
        "id": 27951,
        "title": "Robust Market Making via Adversarial Reinforcement Learning",
        "authors": "Thomas Spooner, Rahul Savani",
        "published": "2020-7",
        "citations": 21,
        "abstract": "We show that adversarial reinforcement learning (ARL) can be used to produce market marking agents that are robust to adversarial and adaptively-chosen market conditions. To apply ARL, we turn the well-studied single-agent model of Avellaneda and Stoikov [2008] into a discrete-time zero-sum game between a market maker and adversary.  The adversary acts as a proxy for other market participants that would like to profit at the market maker's expense.  We empirically compare two conventional single-agent RL agents with ARL, and show that our ARL approach leads to: 1) the emergence of risk-averse behaviour without constraints or domain-specific penalties; 2) significant improvements in performance across a set of standard metrics, evaluated with or without an adversary in the test environment, and; 3) improved robustness to model uncertainty. We empirically demonstrate that our ARL method consistently converges, and we prove for several special cases that the profiles that we converge to correspond to Nash equilibria in a simplified single-stage game.",
        "link": "http://dx.doi.org/10.24963/ijcai.2020/633"
    },
    {
        "id": 27952,
        "title": "Causal Meta-Reinforcement Learning for Multimodal Remote Sensing Data Classification",
        "authors": "Wei Zhang, Xuesong Wang, Haoyu Wang, Yuhu Cheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "Multimodal remote sensing data classification can enhance the model’s ability to distinguish land features through multimodal data fusion. In this context, how to help models understand the relationship between multimodal data and target tasks is the focus of researchers. Inspired by human feedback learning mechanisms, causal reasoning mechanisms, and knowledge induction mechanisms, this paper integrates causal learning, reinforcement learning, and meta learning into a unified remote sensing data classification framework and proposed the causal meta-reinforcement learning (CMRL). First, based on feedback learning mechanisms, we have overcome the limitations of traditional implicit optimization of fusion features and customized a reinforcement learning environment for multimodal remote sensing data classification tasks. Through feedback interactive learning between agents and the environment, we help them understand the complex relationships between multimodal data and labels, thereby achieving full mining of multimodal complementary information. Second, based on the causal inference mechanism we designed causal distribution prediction action, classification reward, and causal intervention reward, capturing pure causal factors in multimodal data and cutting off false statistical associations between non-causal factors and class labels. Finally, based on the knowledge induction mechanism, we designed a bi-layer optimization mechanism based on meta-learning. By constructing a meta training task and meta validation task simulation model in the generalization scenario of unseen data, we helped the model induce cross-task shared knowledge, thereby improving its generalization ability for unseen multimodal data. The experimental results on multiple sets of multimodal datasets show that the proposed method achieved state-of-the-art performance in multimodal remote sensing data classification tasks.",
        "link": "http://dx.doi.org/10.20944/preprints202402.1296.v1"
    },
    {
        "id": 27953,
        "title": "Biologically inspired reinforcement learning for mobile robot collision avoidance",
        "authors": "Myung Seok Shim, Peng Li",
        "published": "2017-5",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7966242"
    },
    {
        "id": 27954,
        "title": "Spontaneous emergence of eyes in reinforcement learning agents",
        "authors": "Zongfu Yu",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3000685"
    },
    {
        "id": 27955,
        "title": "Networked Multi-Agent Reinforcement Learning in Continuous Spaces",
        "authors": "Kaiqing Zhang, Zhuoran Yang, Tamer Basar",
        "published": "2018-12",
        "citations": 62,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc.2018.8619581"
    },
    {
        "id": 27956,
        "title": "Combinatorial Reinforcement Learning of Linear Assignment Problems",
        "authors": "Sascha Hamzehi, Klaus Bogenberger, Philipp Franeck, Bernd Kaltenhauser",
        "published": "2019-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8916920"
    },
    {
        "id": 27957,
        "title": "FoLaR: Foggy Latent Representations for Reinforcement Learning with Partial Observability",
        "authors": "Hardik Meisheri, Harshad Khadilkar",
        "published": "2021-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9534127"
    },
    {
        "id": 27958,
        "title": "Reinforcement Learning Based User-Guided Motion Planning for Human-Robot Collaboration",
        "authors": "Tian Yu, Qing Chang",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4045878"
    },
    {
        "id": 27959,
        "title": "Pixel to Stroke Sketch Generation Using Reinforcement Learning",
        "authors": "Haizhou Wang, Conrad Tucker",
        "published": "2019-8-18",
        "citations": 0,
        "abstract": "Abstract\nMany engineering design tasks involve creating early conceptual sketches that do not require exact dimensions. Although some previous works focus on automatically generating sketches from reference images, many of them output exactly the same objects as the reference images. There are also models that generate sketches from scratch, which can be divided into pixel-based and stroke-based methods. Pixel-based methods generate sketches as a whole, without any information of the strokes, while stroke-based methods generate sketches by outputting strokes in a sequential manner. Pixel-based methods are frequently used to generate realistic color images. Although the pixel-based methods are more popular, stroke-based methods have the advantages to scale to a larger dimension without losing high fidelity. An image generated from stroke-based methods has only strokes on the canvas, resulting in no random noise in the blank areas of the canvas. However, one challenge in the engineering design community is that most of the sketches are saved as pixel-based images. Furthermore, many non-pixel-based methods rely on stroke-based training data, making them ill-suited for generating design conceptual sketches. In order to overcome these limitations, the authors proposed an agent that can learn from pixel-based images and generate stroke-based images. An advantage of such an agent is the ability to utilize pixel-based training data that is abundant in design repositories, to train stroke-based methods that are typically constrained by the lack of access to stroke-based training data.",
        "link": "http://dx.doi.org/10.1115/detc2019-98481"
    },
    {
        "id": 27960,
        "title": "ID-RDRL: A Deep Reinforcement Learning-based Feature Selection Intrusion Detection Model",
        "authors": "Kezhou Ren, Yifan Zeng, Zhiqin Cao, Yingchao Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nNetwork assaults pose significant security concerns to network services; hence, new technical solutions must be used to enhance the efficacy of intrusion detection systems. Existing approaches pay insufficient attention to data preparation and inadequately identify unknown network threats. This paper presents a network intrusion detection model (ID-RDRL) based on RFE feature extraction and reinforcement learning. ID-RDRL filters the optimum subset of features using the RFE feature selection technique, feeds them into a neural network to extract feature information and then trains a classifier using DRL to recognize network intrusions. We utilized CSE-CIC-IDS2018 as a dataset and conducted tests to evaluate the model's performance, which is comprised of a comprehensive collection of actual network traffic. The experimental results demonstrate that the proposed ID-RDRL model can select the optimal subset of features, remove approximately 80 percent of redundant features, and learn the selected features through DRL to enhance the IDS performance for network attack identification. In a complicated network environment, it has promising application potential in IDS.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1765453/v1"
    },
    {
        "id": 27961,
        "title": "Reinforcement Learning using Kalman Filters",
        "authors": "Kei Takahata, Takao Miura",
        "published": "2019-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccicc46617.2019.9146066"
    },
    {
        "id": 27962,
        "title": "Reinforcement Learning",
        "authors": "Wolfgang Ertel",
        "published": "2017",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-58487-4_10"
    },
    {
        "id": 27963,
        "title": "Online Aircraft System Identification using Parameter Informed Reinforcement Learning",
        "authors": "Nathan Schaff, Richard J. Prazenica",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2024-0344"
    },
    {
        "id": 27964,
        "title": "Shielded Deep Reinforcement Learning for Multi-Sensor Spacecraft Imaging",
        "authors": "Islam Nazmy, Andrew Harris, Morteza Lahijanian, Hanspeter Schaub",
        "published": "2022-6-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867762"
    },
    {
        "id": 27965,
        "title": "Direct Expected Quadratic Utility Maximization for Mean-Variance Controlled Reinforcement Learning",
        "authors": "Masahiro Kato, Kei Nakagawa",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3818994"
    },
    {
        "id": 27966,
        "title": "Research on Efficient Reinforcement Learning for Adaptive Frequency-Agility Radar",
        "authors": "Xinzhi Li, Shengbo Dong",
        "published": "2021-11-27",
        "citations": 1,
        "abstract": "Modern radar jamming scenarios are complex and changeable. In order to improve the adaptability of frequency-agile radar under complex environmental conditions, reinforcement learning (RL) is introduced into the radar anti-jamming research. There are two aspects of the radar system that do not obey with the Markov decision process (MDP), which is the basic theory of RL: Firstly, the radar cannot confirm the interference rules of the jammer in advance, resulting in unclear environmental boundaries; secondly, the radar has frequency-agility characteristics, which does not meet the sequence change requirements of the MDP. As the existing RL algorithm is directly applied to the radar system, there would be problems, such as low sample utilization rate, poor computational efficiency and large error oscillation amplitude. In this paper, an adaptive frequency agile radar anti-jamming efficient RL model is proposed. First, a radar-jammer system model based on Markov game (MG) established, and the Nash equilibrium point determined and set as a dynamic environment boundary. Subsequently, the state and behavioral structure of RL model is improved to be suitable for processing frequency-agile data. Experiments that our proposal effectively the anti-jamming performance and efficiency of frequency-agile radar.",
        "link": "http://dx.doi.org/10.3390/s21237931"
    },
    {
        "id": 27967,
        "title": "Growspace: A Reinforcement Learning Environment for Plant Architecture",
        "authors": "Yasmeen Hitti, Ionelia Buzatu, Manuel Del Verme, Florian Golemo, Audrey Durand, Mark Lefsrud",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4329504"
    },
    {
        "id": 27968,
        "title": "Integrated Operations Strategies for Shared and Privately-Owned Autonomous Vehicles: A Deep Reinforcement Learning Framework",
        "authors": "Hanwen Dai, Fang He, Xi Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4525591"
    },
    {
        "id": 27969,
        "title": "Design of Reinforcement Learning Algorathm for Single Inverted Pendulum Swing Control",
        "authors": "Yue Chao, Liu Yongxin, Wang Linglin",
        "published": "2018-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2018.8623253"
    },
    {
        "id": 27970,
        "title": "Reinforcement Learning for Image-Based Visual Servo Control",
        "authors": "Ashwin P. Dani, Shubhendu Bhasin",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383782"
    },
    {
        "id": 27971,
        "title": "Equilibrium Inverse Reinforcement Learning for Ride-hailing Vehicle Network",
        "authors": "Takuma Oda",
        "published": "2021-4-19",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3442381.3449935"
    },
    {
        "id": 27972,
        "title": "Reinforcement-Learning-Based Localization of Hippocampus for Alzheimer’s Disease Detection",
        "authors": "Aditya Raj, Golrokh Mirzaei",
        "published": "2023-10-24",
        "citations": 0,
        "abstract": "Alzheimer’s disease (AD) is a progressive neurodegenerative disorder primarily impacting memory and cognitive functions. The hippocampus serves as a key biomarker associated with AD. In this study, we present an end-to-end automated approach for AD detection by introducing a reinforcement-learning-based technique to localize the hippocampus within structural MRI images. Subsequently, this localized hippocampus serves as input for a deep convolutional neural network for AD classification. We model the agent–environment interaction using a Deep Q-Network (DQN), encompassing both a convolutional Target Net and Policy Net. Furthermore, we introduce an integrated loss function that combines cross-entropy and contrastive loss to effectively train the classifier model. Our approach leverages a single optimal slice extracted from each subject’s 3D sMRI, thereby reducing computational complexity while maintaining performance comparable to volumetric data analysis methods. To evaluate the effectiveness of our proposed localization and classification framework, we compare its performance to the results achieved by supervised models directly trained on ground truth hippocampal regions as input. The proposed approach demonstrates promising performance in terms of classification accuracy, F1-score, precision, and recall. It achieves an F1-score within an error margin of 3.7% and 1.1% and an accuracy within an error margin of 6.6% and 1.6% when compared to the supervised models trained directly on ground truth masks, all while achieving the highest recall score.",
        "link": "http://dx.doi.org/10.3390/diagnostics13213292"
    },
    {
        "id": 27973,
        "title": "Reinforcement Learning Technique for Parameterization in Powertrain Controls",
        "authors": "Gautham Sidharthan, Vivek Venkobarao",
        "published": "2021-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4271/2021-26-0045"
    },
    {
        "id": 27974,
        "title": "Overview of Cyber-Physical Systems and Cybersecurity",
        "authors": "Chong Li",
        "published": "2019-2-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781351006620-2"
    },
    {
        "id": 27975,
        "title": "Deep Reinforcement Learning for Pairs Trading:Evidence from Soybean Commodities",
        "authors": "Xiangyu Zong, Jianhe Liu, Chensong Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3831581"
    },
    {
        "id": 27976,
        "title": "An opponent striatal circuit for distributional reinforcement learning",
        "authors": "Adam S. Lowet, Qiao Zheng, Melissa Meng, Sara Matias, Jan Drugowitsch, Naoshige Uchida",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMachine learning research has achieved large performance gains on a wide range of tasks by expanding the learning target from mean rewards to entire probability distributions of rewards — an approach known as distributional reinforcement learning (RL)1. The mesolimbic dopamine system is thought to underlie RL in the mammalian brain by updating a representation of mean value in the striatum2,3, but little is known about whether, where, and how neurons in this circuit encode information about higher-order moments of reward distributions4. To fill this gap, we used high-density probes (Neuropixels) to acutely record striatal activity from well-trained, water-restricted mice performing a classical conditioning task in which reward mean, reward variance, and stimulus identity were independently manipulated. In contrast to traditional RL accounts, we found robust evidence for abstract encoding of variance in the striatum. Remarkably, chronic ablation of dopamine inputs disorganized these distributional representations in the striatum without interfering with mean value coding. Two-photon calcium imaging and optogenetics revealed that the two major classes of striatal medium spiny neurons — D1 and D2 MSNs — contributed to this code by preferentially encoding the right and left tails of the reward distribution, respectively. We synthesize these findings into a new model of the striatum and mesolimbic dopamine that harnesses the opponency between D1 and D2 MSNs5–15to reap the computational benefits of distributional RL.",
        "link": "http://dx.doi.org/10.1101/2024.01.02.573966"
    },
    {
        "id": 27977,
        "title": "Teaching Eigensolvers the Path to the Fastest Solution Utilizing Deep Reinforcement Learning [Slides]",
        "authors": "Phillip Romero, Christian Negre, Anders Niklasson, Manish Bhattarai",
        "published": "2023-3-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1968184"
    },
    {
        "id": 27978,
        "title": "Robust Risk-Aware Reinforcement Learning",
        "authors": "Sebastian Jaimungal, Silvana M. Pesenti, Ye Sheng Wang, Hariom Tatsat",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3910498"
    },
    {
        "id": 27979,
        "title": "Time Your Hedge With Deep Reinforcement Learning",
        "authors": "Eric Benhamou, David Saltiel, Sandrine Ungari, Abhishek Mukhopadhyay",
        "published": "No Date",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3693614"
    },
    {
        "id": 27980,
        "title": "Optimistic reinforcement learning by forward Kullback–Leibler divergence optimization",
        "authors": "Taisuke Kobayashi",
        "published": "2022-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.04.021"
    },
    {
        "id": 27981,
        "title": "Diversifying experiences in multi agent reinforcement learning",
        "authors": "N.A.V Suryanarayanan, Iba Hitoshi",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwcia47330.2019.8955073"
    },
    {
        "id": 27982,
        "title": "Deep reinforcement learning for diagnosing various types of cancer by TP53 mutation patterns",
        "authors": "Armaqan Rahmani, Behrouz Minaei-Bidgoli, Meysam Ahangaran",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nOne of the key challenges for classifying multiple cancer types is the complexity of Tumor Protein p53 mutation patterns and its individual effects on tumors. However, far too little attention has been paid to Deep reinforcement Learning on TP53 mutation patterns because of its extremely difficult result interpretations. We introduce a critic network by a long-short term memory, which is appropriated for discriminating the noise samples from a Feedback Generative Adversarial Network and analyzing the actor network. The correlation and analysis of the results in a belief network demonstrates significant relations between mutations and disease risk in cancer subtypes identification. In other words, the results indicate statically significant differences between the primary and secondary subtype groups of the most probable tumor.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-744748/v1"
    },
    {
        "id": 27983,
        "title": "Density Independent Self-organized Support for Q-Value Function Interpolation in Reinforcement Learning",
        "authors": "Antonin Calba, Alain Dutech, Jeremy Fix",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2021.es2021-62"
    },
    {
        "id": 27984,
        "title": "HRL2E: Hierarchical Reinforcement Learning with Low-level Ensemble",
        "authors": "You Qin, Zhi Wang, Chunlin Chen",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892189"
    },
    {
        "id": 27985,
        "title": "Data-Driven Post-Prognostics Production Planning and Control Through Reinforcement Learning",
        "authors": "Kevin Wesendrup, Bernd Hellingrath",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4293396"
    },
    {
        "id": 27986,
        "title": "Intersecting reinforcement learning and deep factor methods for optimizing locality and globality in forecasting: a review",
        "authors": "João Sousa, Roberto Henriques",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the current context of big data, the operational forecasting problems are more and more frequently involving the prediction of collections of multivariate, high-dimensional, related time series. The challenge of forecasting groups of time series can be tackled by fitting a single function to all series (global approach) or assuming each series to be a separate prediction problem and fitting one function to each problem (local approach). Although some global models show promising results against local benchmarks, there is still unclarity on how to best control generalization and structural assumptions by calibrating the exploitation of global patterns with local components. This is heavily reliant on the level of complexity from the dependency relationships exhibited between the series within a collection (e.g. indirect latent, local covariate relationships or noise effects) and other dimensions, such as the heterogeneity of the series and its length.\nFor these reasons, there is an acknowledged need to invest further in developing scalable and data-driven hybrid models. The literature proposes Deep Learning (DL) based models built from different data generating processes (such as Auto-Regressive) which combine global models with classical local models (global-local frameworks). These methods frequently capture globality through factorization models or latent deep components and locality via classical local models or networks. These methods present non-linear modelling capabilities but are still either limited by solid theoretical assumptions (e.g. linearity of the underlying data) or predominantly one-dimensional, i.e., the future predictions for a single dimension mainly depend on past values from that same dimension.\nSimultaneously, dynamic Reinforcement Learning (RL) based models have been widely explored to optimize the balance of global and local signals in general regression problems, frequently through Q-learning algorithms. These derive from the equilibrium of cooperative strategies or the societal value in multi-agent learning systems. Similar models have been successfully applied to time series forecasting problems (such as stock market predictions), addressing the risk of method generalization under varying conditions.\nThis paper conducted a concise literature review focused on these two research streams (global-local frameworks and RL based models) to optimize the balance between globality and locality in forecasting collections of time series. It focuses on their evolution across time and hints at opportunities to close some of the research gaps by intersecting both propositions. We followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and achieved a selection of 118 publications since 2000. The main findings revealed that global models have achieved strong expressiveness in capturing the most complex structural patterns while still enabling probabilistic outcomes to be delivered through uncertainty estimates. On the other hand, RL based methods depict great benefits in mitigating the risks of generalization by imprinting contextual diversity when predicting each step ahead for each series. Within those, the adoption of other computational learning or evolutionary-based methods (e.g. Genetic Programming) to improve the parametrization of the learning policies is also highlighted as an area of future work yet to be uncovered.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2570163/v1"
    },
    {
        "id": 27987,
        "title": "Track-To-Learn: A general framework for tractography with deep reinforcement learning",
        "authors": "Antoine Théberge, Christian Desrosiers, Maxime Descoteaux, Pierre-Marc Jodoin",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractDiffusion MRI tractography is currently the only non-invasive tool able to assess the white-matter structural connectivity of a brain. Since its inception, it has been widely documented that tractography is prone to producing erroneous tracks while missing true positive connections. Anatomical priors have been conceived and implemented in classical algorithms to try and tackle these issues, yet problems still remain and the conception and validation of these priors is very challenging. Recently, supervised learning algorithms have been proposed to learn the tracking procedure implicitly from data, without relying on anatomical priors. However, these methods rely on labelled data that is very hard to obtain. To remove the need for such data but still leverage the expressiveness of neural networks, we introduce Track-To-Learn: A general framework to pose tractography as a deep reinforcement learning problem. Deep reinforcement learning is a type of machine learning that does not depend on ground-truth data but rather on the concept of “reward”. We implement and train algorithms to maximize returns from a reward function based on the alignment of streamlines with principal directions extracted from diffusion data. We show that competitive results can be obtained on known data and that the algorithms are able to generalize far better to new, unseen data, than prior machine learning-based tractography algorithms. To the best of our knowledge, this is the first successful use of deep reinforcement learning for tractography.",
        "link": "http://dx.doi.org/10.1101/2020.11.16.385229"
    },
    {
        "id": 27988,
        "title": "Reward boosts reinforcement-based motor learning",
        "authors": "Pierre Vassiliadis, Gerard Derosiere, Cecile Dubuc, Aegryan Lete, Frederic Crevecoeur, Friedhelm C Hummel, Julie Duque",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractBesides relying heavily on sensory and reinforcement feedback, motor skill learning may also depend on the level of motivation experienced during training. Yet, how motivation by reward modulates motor learning remains unclear. In 90 healthy subjects, we investigated the net effect of motivation by reward on motor learning while controlling for the sensory and reinforcement feedback received by the participants. Reward improved motor skill learning beyond performance-based reinforcement feedback. Importantly, the beneficial effect of reward involved a specific potentiation of reinforcement-related adjustments in motor commands, which concerned primarily the most relevant motor component for task success and persisted on the following day in the absence of reward. We propose that the long-lasting effects of motivation on motor learning may entail a form of associative learning resulting from the repetitive pairing of the reinforcement feedback and reward during training, a mechanism that may be exploited in future rehabilitation protocols.",
        "link": "http://dx.doi.org/10.1101/2021.03.23.436627"
    },
    {
        "id": 27989,
        "title": "Quantum Deep Recurrent Reinforcement Learning",
        "authors": "Samuel Yen-Chi Chen",
        "published": "2023-6-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096981"
    },
    {
        "id": 27990,
        "title": "Softmax exploration strategies for multiobjective reinforcement learning",
        "authors": "Peter Vamplew, Richard Dazeley, Cameron Foale",
        "published": "2017-11",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2016.09.141"
    },
    {
        "id": 27991,
        "title": "Reinforcement Learning in a Large Scale Photonic Network",
        "authors": "Sheler Maktoobi, Louis Andreoli, Laurent Larger, Maxime Jacquot, Daniel Brunner",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1364/cleopr.2018.w1d.4"
    },
    {
        "id": 27992,
        "title": "Deep Reinforcement Learning for Efficient Digital Pap Smear Analysis",
        "authors": "Carlos Julio Macancela, Manuel Eugenio Morocho Cayamcela, Oscar Chang",
        "published": "No Date",
        "citations": 1,
        "abstract": "In August 2020, the World Health Assembly launched a global initiative to eliminate cervical cancer by 2030, setting three primary targets. One key goal is to achieve a 70% screening coverage rate for cervical cancer, primarily relying on the precise analysis of Papanicolaou (Pap) or digital Pap smears. However, the responsibility of reviewing Pap smear samples to identify potentially cancerous cells primarily falls on pathologists, a task known to be exceptionally challenging and time-consuming. This paper proposes a solution to address the shortage of pathologists for cervical cancer screening. It leverages the OpenAI-GYM API to create a deep reinforcement learning environment utilizing liquid-based Pap smear images. By employing the Proximal Policy Optimization algorithm, autonomous agents navigate Pap smear images, identifying cells with the aid of rewards, penalties, and accumulated experiences. Furthermore, the use of a pre-trained convolutional neuronal network like Res-Net54 enhances the classification of detected cells based on their potential for malignancy. The ultimate goal of this study is to develop a highly efficient, automated Papanicolaou analysis system, ultimately reducing the need for human intervention in regions with limited pathologists.",
        "link": "http://dx.doi.org/10.20944/preprints202310.1409.v1"
    },
    {
        "id": 27993,
        "title": "Revisiting Optimal Execution of Portfolio Transactions: A Dynamic Programming and Reinforcement Learning Approach",
        "authors": "Jens Pedersen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4508553"
    },
    {
        "id": 27994,
        "title": "Cross-Domain Transfer in Reinforcement Learning Using Target Apprentice",
        "authors": "Girish Joshi, Girish Chowdhary",
        "published": "2018-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2018.8462977"
    },
    {
        "id": 27995,
        "title": "Temporal stimulus segmentation by reinforcement learning in populations of spiking neurons",
        "authors": "Luisa Le Donne, Robert Urbanczik, Walter Senn, Giancarlo La Camera",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractLearning to detect, identify or select stimuli is an essential requirement of many behavioral tasks. In real life situations, relevant and non-relevant stimuli are often embedded in a continuous sensory stream, presumably represented by different segments of neural activity. Here, we introduce a neural circuit model that can learn to identify action-relevant stimuli embedded in a spatio-temporal stream of spike trains, while learning to ignore stimuli that are not behaviorally relevant. The model uses a biologically plausible plasticity rule and learns from the reinforcement of correct decisions taken at the right time. Learning is fully online; it is successful for a wide spectrum of stimulus-encoding strategies; it scales well with population size; and can segment cortical spike patterns recorded from behaving animals. Altogether, these results provide a biologically plausible theory of reinforcement learning in the absence of prior information on the relevance and timing of input stimuli.",
        "link": "http://dx.doi.org/10.1101/2020.12.22.424037"
    },
    {
        "id": 27996,
        "title": "Adaptive Slope Locomotion with Deep Reinforcement Learning",
        "authors": "William Jones, Tamir Blum, Kazuya Yoshida",
        "published": "2020-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sii46433.2020.9025928"
    },
    {
        "id": 27997,
        "title": "Reinforcement Learning for Wind Turbine Load Control : How AI can drive tomorrow‘s wind turbines",
        "authors": "Nico Westerbeck",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25368/2023.43"
    },
    {
        "id": 27998,
        "title": "Procedural Content Generation: Better Benchmarks for Transfer Reinforcement Learning",
        "authors": "Matthias Muller-Brockhausen, Mike Preuss, Aske Plaat",
        "published": "2021-8-17",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog52621.2021.9619000"
    },
    {
        "id": 27999,
        "title": "Safe Reinforcement Learning for Sepsis Treatment",
        "authors": "Yan Jia, John Burden, Tom Lawton, Ibrahim Habli",
        "published": "2020-11",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichi48887.2020.9374367"
    },
    {
        "id": 28000,
        "title": "A model-based approach to meta-Reinforcement Learning: Transformers and tree search",
        "authors": "Brieuc Pinon, Raphaël Jungers, Jean-Charles Delvenne",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-117"
    },
    {
        "id": 28001,
        "title": "Safe Multi-Agent Deep Reinforcement Learning for Dynamic Virtual Network Allocation",
        "authors": "Akito Suzuki, Shigeaki Harada",
        "published": "2020-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom42002.2020.9348210"
    },
    {
        "id": 28002,
        "title": "Individualized sepsis treatment using reinforcement learning",
        "authors": "Suchi Saria",
        "published": "2018-11",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1038/s41591-018-0253-x"
    },
    {
        "id": 28003,
        "title": "Polya Decision Processes: A New History-Dependent Framework for Reinforcement Learning",
        "authors": "Masahiro Kohjima",
        "published": "2022-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc51059.2022.9992504"
    },
    {
        "id": 28004,
        "title": "A Survey on Reinforcement Learning for Combinatorial Optimization",
        "authors": "Yunhao Yang, Andrew Whinston",
        "published": "2023-7-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aic57670.2023.10263956"
    },
    {
        "id": 28005,
        "title": "Identifying critical nodes via link equations and deep reinforcement learning",
        "authors": "Peiyu Chen, Wenhui Fan",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126871"
    },
    {
        "id": 28006,
        "title": "Reinforcement learning in synthetic gene circuits",
        "authors": "Adrian Racovita, Alfonso Jaramillo",
        "published": "2020-8-28",
        "citations": 5,
        "abstract": "Synthetic gene circuits allow programming in DNA the expression of a phenotype at a given environmental condition. The recent integration of memory systems with gene circuits opens the door to their adaptation to new conditions and their re-programming. This lays the foundation to emulate neuromorphic behaviour and solve complex problems similarly to artificial neural networks. Cellular products such as DNA or proteins can be used to store memory in both digital and analog formats, allowing cells to be turned into living computing devices able to record information regarding their previous states. In particular, synthetic gene circuits with memory can be engineered into living systems to allow their adaptation through reinforcement learning. The development of gene circuits able to adapt through reinforcement learning moves Sciences towards the ambitious goal: the bottom-up creation of a fully fledged living artificial intelligence.",
        "link": "http://dx.doi.org/10.1042/bst20200008"
    },
    {
        "id": 28007,
        "title": "Meta-Reinforcement Learning with Transformer Networks for Space Guidance Applications",
        "authors": "Lorenzo Federici, Roberto Furfaro",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2024-2061"
    },
    {
        "id": 28008,
        "title": "Acquiring musculoskeletal skills with curriculum-based reinforcement learning",
        "authors": "Alberto Silvio Chiappa, Pablo Tano, Nisheet Patel, Abigaïl Ingster, Alexandre Pouget, Alexander Mathis",
        "published": "No Date",
        "citations": 0,
        "abstract": "Efficient, physiologically-detailed musculoskeletal simulators and powerful learning algorithms provide new computational tools to tackle the grand challenge of understanding biological motor control. Our winning solution for the first NeurIPS MyoChallenge leverages an approach mirroring human learning and showcases reinforcement and curriculum learning as mechanisms to find motor control policies in complex object manipulation tasks. Analyzing the policy against data from human subjects reveals insights into efficient control of complex biological systems. Overall, our work highlights the new possibilities emerging at the interface of musculoskeletal physics engines, reinforcement learning and neuroscience.",
        "link": "http://dx.doi.org/10.1101/2024.01.24.577123"
    },
    {
        "id": 28009,
        "title": "Deep Reinforcement Learning for Optimizing Finance Portfolio Management",
        "authors": "Yuh-Jong Hu, Shang-Jen Lin",
        "published": "2019-2",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aicai.2019.8701368"
    },
    {
        "id": 28010,
        "title": "Modeling Driver Behavior using Adversarial Inverse Reinforcement Learning",
        "authors": "Moritz Sackmann, Henrik Bey, Ulrich Hofmann, Jorn Thielecke",
        "published": "2022-6-5",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv51971.2022.9827292"
    },
    {
        "id": 28011,
        "title": "DeepBike: A Deep Reinforcement Learning Based Model for Large-scale Online Bike Share Rebalancing",
        "authors": "Zhuoli Yin, Zhaoyu Kou, Hua Cai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBike share systems (BSSs), as a potentially environment-friendly mobility mode, are being deployed globally. To address spatially and temporally imbalanced bike and dock demands, BSS operators need to redistribute bikes among stations using a fleet of rebalancing vehicles in real-time. However, existing studies mainly generate BSS rebalancing solutions for small-scale BSSs or subsets of BSSs, while deploying small-size rebalancing fleets. How to produce online rebalancing solutions for large-scale BSS with multiple rebalancing vehicles to minimize customer loss is critical for system operation yet remains unsolved. To address this gap, we proposed a deep reinforcement learning based model — DeepBike — that trains deep Q-network (DQN) to learn the optimal strategy for dynamic bike share rebalancing. DeepBike uses real-time states of rebalancing vehicles, stations and predicted demands as inputs to output the long-term quality values of rebalancing actions of each rebalancing vehicle. Rebalancing vehicles could work asynchronously as each individually runs the DQN. We compared the performance of the proposed DeepBike against baseline models for dynamic bike share rebalancing based on historical trip records from Divvy BSS in Chicago, which possesses more than 500 stations and 16 rebalancing vehicles. The evaluation results show that our proposed DeepBike model was able to better reduce customer loss by 111.09% and 57.6% than the mixed integer programming and heuristic-based models, respectively, and increased overall net profits by 101.26% and 220.01%, respectively. The DeepBike model is effective for large-scale dynamic bike share rebalancing problems and has the potential to improve the operation of shared mobility systems.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3998473/v1"
    },
    {
        "id": 28012,
        "title": "Exploring the behavioural spectrum with efficiency vs. fairness goals in Multi-Agent Reinforcement Learning",
        "authors": "Zafeiris Kokkinogenis, Margarida Silva, Jeremy Pitt, Rosaldo Rossetti",
        "published": "No Date",
        "citations": 0,
        "abstract": "The concept of fairness has been studied in philosophy and economics for thousands of years, so human actors in social systems have had plenty of time to ``learn'' what does, and does not, work. Yet, only recently. However, it is a relatively new question how software agents in a multi-agent system can use Reinforcement Learning models to develop an architecture that promotes equality or equity in the distribution of rewards to the agents within the system. Recent significant contributions have focused on optimising for efficiency based on the assumption that efficiency and fairness are opposites to be traded off against each other, but actually, the result of mixing fair and efficient policies is unknown in multi-agent reinforcement learning settings. In this work, we experiment with fair and efficient behaviours jointly, based on an extension of the state-of-the-art model in fairness SOTO that intertwines efficient and equitable recommendations. We analyse the fair versus efficient behavioural spectrum in the Matthew Effect and Traffic Light Control problems, finding some solutions that outperform the baseline SOTO and others that outperform a selfish baseline with comparable architectural design. We conclude it is possible to optimise for fairness and efficiency  and this is important when computation of the reward distribution has to be paid for from the rewards themselves.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19388147.v1"
    },
    {
        "id": 28013,
        "title": "Collaborative Multi-Agent Reinforcement Learning of Caching Optimization in Small-Cell Networks",
        "authors": "Xianzhe Xu, Meixia Tao",
        "published": "2018-12",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/glocom.2018.8647341"
    },
    {
        "id": 28014,
        "title": "PSS Control of Multi Machine Power System Using Reinforcement Learning",
        "authors": "Xingwang Xie",
        "published": "2021-6-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icras52289.2021.9476268"
    },
    {
        "id": 28015,
        "title": "Artificial Intelligence Gamers Based on Deep Reinforcement Learning",
        "authors": "Haolin Wang",
        "published": "2024-1-26",
        "citations": 0,
        "abstract": "This study investigates the design and implementation of Artificial Intelligence (AI) game players based on deep reinforcement learning, offering a novel approach to autonomous decision-making and strategy acquisition in intelligent games. Initially, the fundamental principles and algorithms of deep reinforcement learning are introduced, along with the fusion of deep learning and reinforcement learning. Subsequently, existing research is reviewed, and the pros and cons of current methodologies are examined, highlighting the underlying issues and challenges. The utilization of AI players in mainstream games is then introduced, and the influence of AI players on contemporary games is analyzed. Through this analysis of AI players in mainstream games, the strengths and weaknesses of current AI players are identified, and recommendations for optimizing them are provided. This study holds significant implications for guiding the design and development of intelligent game players, while also enriching the application of deep reinforcement learning within the gaming domain.",
        "link": "http://dx.doi.org/10.54097/p9tv1494"
    },
    {
        "id": 28016,
        "title": "Reinforcement Learning for Maritime Communications",
        "authors": "Liang Xiao, Helin Yang, Weihua Zhuang, Minghui Min",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-32138-2"
    },
    {
        "id": 28017,
        "title": "Container Caching Optimization based on Explainable Deep Reinforcement Learning",
        "authors": "Divyashree Jayaram, Saad Jeelani, Genya Ishigaki",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437757"
    },
    {
        "id": 28018,
        "title": "Causal Meta-Reinforcement Learning for Multimodal Remote Sensing Data Classification",
        "authors": "Wei Zhang, Xuesong Wang, Haoyu Wang, Yuhu Cheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "Multimodal remote sensing data classification can enhance the model’s ability to distinguish land features through multimodal data fusion. In this context, how to help models understand the relationship between multimodal data and target tasks is the focus of researchers. Inspired by human feedback learning mechanisms, causal reasoning mechanisms, and knowledge induction mechanisms, this paper integrates causal learning, reinforcement learning, and meta learning into a unified remote sensing data classification framework and proposed the causal meta-reinforcement learning (CMRL). First, based on feedback learning mechanisms, we have overcome the limitations of traditional implicit optimization of fusion features and customized a reinforcement learning environment for multimodal remote sensing data classification tasks. Through feedback interactive learning between agents and the environment, we help them understand the complex relationships between multimodal data and labels, thereby achieving full mining of multimodal complementary information. Second, based on the causal inference mechanism we designed causal distribution prediction action, classification reward, and causal intervention reward, capturing pure causal factors in multimodal data and cutting off false statistical associations between non-causal factors and class labels. Finally, based on the knowledge induction mechanism, we designed a bi-layer optimization mechanism based on meta-learning. By constructing a meta training task and meta validation task simulation model in the generalization scenario of unseen data, we helped the model induce cross-task shared knowledge, thereby improving its generalization ability for unseen multimodal data. The experimental results on multiple sets of multimodal datasets show that the proposed method achieved state-of-the-art performance in multimodal remote sensing data classification tasks.",
        "link": "http://dx.doi.org/10.20944/preprints202402.1296.v1"
    },
    {
        "id": 28019,
        "title": "Biologically inspired reinforcement learning for mobile robot collision avoidance",
        "authors": "Myung Seok Shim, Peng Li",
        "published": "2017-5",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7966242"
    },
    {
        "id": 28020,
        "title": "Deep Reinforcement Learning for Traffic Light Optimization",
        "authors": "Mustafa Coskun, Abdelkader Baggag, Sanjay Chawla",
        "published": "2018-11",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdmw.2018.00088"
    },
    {
        "id": 28021,
        "title": "Deep Residual Reinforcement Learning (Extended Abstract)",
        "authors": "Shangtong Zhang, Wendelin Boehmer, Shimon Whiteson",
        "published": "2021-8",
        "citations": 1,
        "abstract": "We revisit residual algorithms in both model-free and model-based reinforcement learning settings.\n\nWe propose the bidirectional target network technique to stabilize residual algorithms, yielding a residual version of DDPG that significantly outperforms vanilla DDPG in commonly used benchmarks.\n\nMoreover, we find the residual algorithm an effective approach to the distribution mismatch problem in model-based planning.\n\nCompared with the existing TD(k) method, our residual-based method makes weaker assumptions about the model and yields a greater performance boost.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/668"
    },
    {
        "id": 28022,
        "title": "Reinforcement Learning for Efficient Scheduling in Complex Semiconductor Equipment",
        "authors": "Doug Suerich, Terry Young",
        "published": "2020-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asmc49169.2020.9185293"
    },
    {
        "id": 28023,
        "title": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning",
        "authors": "Xiao-Yang Liu",
        "published": "No Date",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4253139"
    },
    {
        "id": 28024,
        "title": "Utterance Censorship of Online Reinforcement Learning Chatbot",
        "authors": "Yixuan Chai, Guohua Liu",
        "published": "2018-11",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai.2018.00063"
    },
    {
        "id": 28025,
        "title": "Deep Hierarchical Reinforcement Learning for Autonomous Driving with Distinct Behaviors",
        "authors": "Jianyu Chen, Zining Wang, Masayoshi Tomizuka",
        "published": "2018-6",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2018.8500368"
    },
    {
        "id": 28026,
        "title": "Disrupted reinforcement learning during post-error slowing in ADHD",
        "authors": "Andre Chevrier, Mehereen Bhaijiwala, Jonathan Lipszyc, Douglas Cheyne, Simon Graham, Russell Schachar",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractADHD is associated with altered dopamine regulated reinforcement learning on prediction errors. Despite evidence of categorically altered error processing in ADHD, neuroimaging advances have largely investigated models of normal reinforcement learning in greater detail. Further, although reinforcement leaning critically relies on ventral striatum exerting error magnitude related thresholding influences on substantia nigra (SN) and dorsal striatum, these thresholding influences have never been identified with neuroimaging. To identify such thresholding influences, we propose that error magnitude related activities must first be separated from opposite activities in overlapping neural regions during error detection. Here we separate error detection from magnitude related adjustment (post-error slowing) during inhibition errors in the stop signal task in typically developing (TD) and ADHD adolescents using fMRI. In TD, we predicted that: 1) deactivation of dorsal striatum on error detection interrupts ongoing processing, and should be proportional to right frontoparietal response phase activity that has been observed in the SST; 2) deactivation of ventral striatum on post-error slowing exerts thresholding influences on, and should be proportional to activity in dorsal striatum. In ADHD, we predicted that ventral striatum would instead correlate with heightened amygdala responses to errors. We found deactivation of dorsal striatum on error detection correlated with response-phase activity in both groups. In TD, post-error slowing deactivation of ventral striatum correlated with activation of dorsal striatum. In ADHD, ventral striatum correlated with heightened amygdala activity. Further, heightened activities in locus coeruleus (norepinephrine), raphe nucleus (serotonin) and medial septal nuclei (acetylcholine), which all compete for control of DA, and are altered in ADHD, exhibited altered correlations with SN. All correlations in TD were replicated in healthy adults. Results in TD are consistent with dopamine regulated reinforcement learning on post-error slowing. In ADHD, results are consistent with heightened activities in the amygdala and non-dopaminergic neurotransmitter nuclei preventing reinforcement learning.",
        "link": "http://dx.doi.org/10.1101/449975"
    },
    {
        "id": 28027,
        "title": "EdgeAlign: platform for deep reinforcement learning based pairwise DNA sequence alignment compatible with embedded edge devices",
        "authors": "Aryan Lall, Siddharth Tallur",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nSequence alignment is an essential component of bioinformatics, for identifying regions of similarity that may indicate functional, structural, or evolutionary relationships between the sequences. Genome-based diagnostics relying on DNA sequencing have benefited hugely from the boom in computing power in recent decades, particularly due to cloud-computing and the rise of graphics processing units (GPUs) and other advanced computing platforms for running advanced algorithms. Translating the success of such breakthroughs in diagnostics to affordable solutions for low-cost healthcare requires development of algorithms that can operate on the edge instead of in the cloud, using low-cost and low-power electronic systems such as microcontrollers and field programmable gate arrays (FPGAs). In this work, we present EdgeAlign, a deep reinforcement learning based framework for performing pairwise DNA sequence alignment on stand-alone edge devices. EdgeAlign uses deep reinforcement learning to train a deep Q-network (DQN) agent for performing sequence alignment on fixed length sub-sequences, using a sliding window that is scanned over the length of the entire sequence. The hardware resource-consumption for implementing this scheme is thus independent of the lengths of the sequences to be aligned, and is further optimized using a novel AutoML based method for neural network model size reduction. Unlike other algorithms for sequence alignment reported in literature, the model demonstrated in this work is highly compact and deployed on two edge devices (NVIDIA® Jetson NanoTM Developer Kit and Digilent Arty A7-100T, containing Xilinx® XC7A35T Artix®-7 FPGA) for demonstration of alignment for sequences from the publicly available Influenza sequences at the National Center for Biotechnology Information (NCBI) Virus Data Hub.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2175890/v1"
    },
    {
        "id": 28028,
        "title": "Robust Lane Change Decision Making for Autonomous Vehicles: An Observation Adversarial Reinforcement Learning Approach",
        "authors": "Xiangkun He, Haohan Yang, Zhongxu Hu, Chen Lv",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning holds the promise of allowing autonomous vehicles to learn complex decision making behaviors through interacting with other traffic participants. However, many real-world driving tasks involve unpredictable perception errors or measurement noises which may mislead an autonomous vehicle into making unsafe decisions, even cause catastrophic failures. In light of these risks, to ensure safety under perception uncertainty, autonomous vehicles are required to be able to cope with the worst case observation perturbations. Therefore, this paper proposes a novel observation adversarial reinforcement learning approach for robust lane change decision making of autonomous vehicles. A constrained observation-robust Markov decision process is presented to model lane change decision making behaviors of autonomous vehicles under policy constraints and observation uncertainties.  Meanwhile, a black-box attack technique based on Bayesian optimization is implemented to approximate the optimal adversarial observation perturbations efficiently. Furthermore, a constrained observation-robust actor-critic algorithm is advanced to optimize autonomous driving lane change policies while keeping the variations of the policies attacked by the optimal adversarial observation perturbations within bounds. Finally, the robust lane change decision making approach is evaluated in three stochastic mixed traffic flows based on different densities. The results demonstrate that the proposed method can not only enhance the performance of an autonomous vehicle but also improve the robustness of lane change policies against adversarial observation perturbations.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21229319"
    },
    {
        "id": 28029,
        "title": "Deep Contextual Bandit and Reinforcement Learning for IRS-assisted MU-MIMO Systems",
        "authors": "Dariel Pereira-Ruisánchez, Óscar Fresnedo, Darian Pérez-Adán, Luis Castedo",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> The combination of multiple-input multiple-output (MIMO) and intelligent reflecting surfaces (IRSs) is foreseen as a key enabler of beyond 5G (B5G) and 6G. In this work, two different approaches are considered for the joint optimization of the IRS phase-shift matrix and MIMO precoders of an IRS-assisted multi-stream (MS) multi-user MIMO (MU-MIMO) system with the aim of maximizing the system sum-rate for every channel realization. The first one is a novel contextual bandit (CB) approach with continuous state and action spaces called deep contextual bandit-oriented deep deterministic policy gradient (DCB-DDPG). The second is an innovative deep reinforcement learning (DRL) formulation where the states, actions and rewards are selected such that the Markov decision process (MDP) property of reinforcement learning (RL) is properly met. Both proposals perform remarkably better than state-of-the-art heuristic methods in high multi-user interference scenarios. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19787551.v1"
    },
    {
        "id": 28030,
        "title": "Optimal Adaptive Prediction Intervals for Electricity Load Forecasting in Distribution Systems via Reinforcement Learning",
        "authors": "Yufan Zhang, Honglin Wen, Qiuwei Wu, Qian Ai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Prediction intervals (PIs) offer an effective tool for quantifying uncertainty of loads in distribution systems. The traditional central PIs cannot adapt well to skewed distributions, and their offline training fashion is vulnerable to the unforeseen change in future load patterns. Therefore, we propose an optimal PI estimation approach, which is online and adaptive to different data distributions by adaptively determining symmetric or asymmetric probability proportion pairs for quantiles of PIs’ bounds. It relies on the online learning ability of reinforcement learning (RL) to integrate the two online tasks, i.e., the adaptive selection of probability proportion pairs and quantile predictions, both of which are modeled by neural networks. As such, the quality of quantiles-formed PI can guide the selection process of optimal probability proportion pairs, which forms a closed loop to improve PIs’ quality. Furthermore, to improve the learning efficiency of quantile forecasts, a prioritized experience replay (PER) strategy is proposed for online quantile regression processes. Case studies on both load and net load demonstrate that the proposed method can better adapt to data distribution compared with online central PIs method. Compared with offline-trained methods, it obtains PIs with better quality and is more robust against concept drift.",
        "link": "http://dx.doi.org/10.36227/techrxiv.17925911.v1"
    },
    {
        "id": 28031,
        "title": "Optimal Adaptive Prediction Intervals for Electricity Load Forecasting in Distribution Systems via Reinforcement Learning",
        "authors": "Yufan Zhang, Honglin Wen, Qiuwei Wu, Qian Ai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Prediction intervals (PIs) offer an effective tool for quantifying uncertainty of loads in distribution systems. The traditional central PIs cannot adapt well to skewed distributions, and their offline training fashion is vulnerable to the unforeseen change in future load patterns. Therefore, we propose an optimal PI estimation approach, which is online and adaptive to different data distributions by adaptively determining symmetric or asymmetric probability proportion pairs for quantiles of PIs’ bounds. It relies on the online learning ability of reinforcement learning (RL) to integrate the two online tasks, i.e., the adaptive selection of probability proportion pairs and quantile predictions, both of which are modeled by neural networks. As such, the quality of quantiles-formed PI can guide the selection process of optimal probability proportion pairs, which forms a closed loop to improve PIs’ quality. Furthermore, to improve the learning efficiency of quantile forecasts, a prioritized experience replay (PER) strategy is proposed for online quantile regression processes. Case studies on both load and net load demonstrate that the proposed method can better adapt to data distribution compared with online central PIs method. Compared with offline-trained methods, it obtains PIs with better quality and is more robust against concept drift.",
        "link": "http://dx.doi.org/10.36227/techrxiv.17925911"
    },
    {
        "id": 28032,
        "title": "Reinforcement Learning for Cyber-Physical Systems",
        "authors": "Xing Liu, Hansong Xu, Weixian Liao, Wei Yu",
        "published": "2019-11",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icii.2019.00063"
    },
    {
        "id": 28033,
        "title": "Online mobile learning resource recommendation method based on deep reinforcement learning",
        "authors": "Pingyang Li, Juan Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijisd.2023.10056670"
    },
    {
        "id": 28034,
        "title": "Utilized System Model Using Channel State Information Network with Gated Recurrent Units (CsiNet-GRUs)",
        "authors": "Hany Helmy, Sherif El Diasty, Hazem Shatila",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "MIMO: multiple-input multiple-output technology uses multiple antennas to use reflected signals to provide channel robustness and throughput gains. It is advantageous in several applications like cellular systems, and users are distributed over a wide coverage area in various applications such as mobile systems, improving channel state information (CSI) processing efficiency in massive MIMO systems. This chapter proposes two channel-based deep learning methods to enhance the performance in a massive MIMO system and compares our proposed technique to the previous methods. The proposed technique is based on the channel state information network combined with the gated recurrent unit’s technique CsiNet-GRUs, which increases recovery efficiency. Besides, a fair balance between compression ratio (CR) and complexity is given using correlation time in training samples. The simulation results show that the proposed CsiNet-GRUs technique fulfills performance improvement compared with the existing literature techniques, namely CS-based methods Conv-LSTM CsiNet, LASSO, Tval3, and CsiNet.",
        "link": "http://dx.doi.org/10.5772/intechopen.111650"
    },
    {
        "id": 28035,
        "title": "Batch Reinforcement Learning on a RoboCup SSL Keepaway Strategy Learning Problem",
        "authors": "Franco Ollino, Miguel A. Solís, Héctor Allende",
        "published": "2019-3-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/5b03f636"
    },
    {
        "id": 28036,
        "title": "Advances in Value-based, Policy-based, and Deep Learning-based Reinforcement Learning",
        "authors": "Haewon Byeon",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140838"
    },
    {
        "id": 28037,
        "title": "Collision-Free UAV Navigation with a Monocular Camera Using Deep Reinforcement Learning",
        "authors": "Yun Chen, Nuria Gonzalez-Prelcic, Robert W. Heath",
        "published": "2020-9",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp49062.2020.9231577"
    },
    {
        "id": 28038,
        "title": "Learning and Generalization of Dynamic Movement Primitives by Hierarchical Deep Reinforcement Learning from Demonstration",
        "authors": "Wonchul Kim, Chungkeun Lee, H. Jin Kim",
        "published": "2018-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2018.8594476"
    },
    {
        "id": 28039,
        "title": "Coupling Effect of Exploration Rate and Learning Rate  for Optimized Scaled Reinforcement Learning",
        "authors": "Smriti Gupta, Sabita Pal, Kundan Kumar, Kuntal Ghosh",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42979-023-02114-3"
    },
    {
        "id": 28040,
        "title": "Session details: Session 3B: Efficient and Secure Deep Learning and Reinforcement Learning in Embedded Systems",
        "authors": "Yanzhi Wang",
        "published": "2020-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3422942"
    },
    {
        "id": 28041,
        "title": "Adaptive Online-Learning Volt-Var Control for Smart Inverters Using Deep Reinforcement Learning",
        "authors": "Kirstin Beyer, Robert Beckmann, Stefan Geißendörfer, Karsten von Maydell, Carsten Agert",
        "published": "2021-4-3",
        "citations": 15,
        "abstract": "The increasing penetration of the power grid with renewable distributed generation causes significant voltage fluctuations. Providing reactive power helps balancing the voltage in the grid. This paper proposes a novel adaptive volt-var control algorithm on the basis of deep reinforcement learning. The learning agent is an online-learning deep deterministic policy gradient that is applicable under real-time conditions in smart inverters for reactive power management. The algorithm only uses input data from the grid connection point of the inverter itself; thus, no additional communication devices are needed and it can be applied individually to any inverter in the grid. The proposed volt-var control is successfully simulated at various grid connection points in a 21-bus low-voltage distribution test feeder. The resulting voltage behavior is analyzed and a systematic voltage reduction is observed both in a static grid environment and a dynamic environment. The proposed algorithm enables flexible adaption to changing environments through continuous exploration during the learning process and, thus, contributes to a decentralized, automated voltage control in future power grids.",
        "link": "http://dx.doi.org/10.3390/en14071991"
    },
    {
        "id": 28042,
        "title": "Distributed deep reinforcement learning for simulation control",
        "authors": "Suraj Pawar, Romit Maulik",
        "published": "2021-6-1",
        "citations": 7,
        "abstract": "Abstract\nSeveral applications in the scientific simulation of physical systems can be formulated as control/optimization problems. The computational models for such systems generally contain hyperparameters, which control solution fidelity and computational expense. The tuning of these parameters is non-trivial and the general approach is to manually ‘spot-check’ for good combinations. This is because optimal hyperparameter configuration search becomes intractable when the parameter space is large and when they may vary dynamically. To address this issue, we present a framework based on deep reinforcement learning (RL) to train a deep neural network agent that controls a model solve by varying parameters dynamically. First, we validate our RL framework for the problem of controlling chaos in chaotic systems by dynamically changing the parameters of the system. Subsequently, we illustrate the capabilities of our framework for accelerating the convergence of a steady-state computational fluid dynamics solver by automatically adjusting the relaxation factors of the discretized Navier–Stokes equations during run-time. The results indicate that the run-time control of the relaxation factors by the learned policy leads to a significant reduction in the number of iterations for convergence compared to the random selection of the relaxation factors. Our results point to potential benefits for learning adaptive hyperparameter learning strategies across different geometries and boundary conditions with implications for reduced computational campaign expenses\n4\n\n\n4\nData and codes available at https://github.com/Romit-Maulik/PAR-RL.\n.",
        "link": "http://dx.doi.org/10.1088/2632-2153/abdaf8"
    },
    {
        "id": 28043,
        "title": "Reinforcement Learning Meets Cognitive Situation Management: A Review of Recent Learning Approaches from the Cognitive Situation Management Perspective",
        "authors": "Andrea Salfinger",
        "published": "2020-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cogsima49017.2020.9216026"
    },
    {
        "id": 28044,
        "title": "Key node identification of satellite time-varying network with deep reinforcement learning",
        "authors": "Liyang Wang, Yun Li, Ye Ren, Lina Yu",
        "published": "2022-10-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml57342.2022.10009841"
    },
    {
        "id": 28045,
        "title": "Deep Reinforcement Learning with Copy-oriented Context Awareness and Weighted Rewards for Abstractive Summarization",
        "authors": "Caidong Tan",
        "published": "2023-3-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3590003.3590019"
    },
    {
        "id": 28046,
        "title": "Anomaly Detection in Structural Health Monitoring with Ensemble Learning and Reinforcement Learning",
        "authors": "Nan Huang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2024.0150112"
    },
    {
        "id": 28047,
        "title": "Learning to Drive (L2D) as a Low-Cost Benchmark for Real-World Reinforcement Learning",
        "authors": "Ari Viitala, Rinu Boney, Yi Zhao, Alexander Ilin, Juho Kannala",
        "published": "2021-12-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icar53236.2021.9659342"
    },
    {
        "id": 28048,
        "title": "Automatic berthing using supervised learning and reinforcement learning",
        "authors": "Shoma Shimizu, Kenta Nishihara, Yoshiki Miyauchi, Kouki Wakita, Rin Suyama, Atsuo Maki, Shinichi Shirakawa",
        "published": "2022-12",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2022.112553"
    },
    {
        "id": 28049,
        "title": "Real-Time Pose Recognition for Billiard Players Using Deep Learning",
        "authors": "Zhikang Chen, Wei Qi Yan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "In this book chapter, the authors propose a method for player pose recognition in billiards matches by combining keypoint extraction and an optimized transformer. Given that those human pose analysis methods usually require high labour costs, the authors explore deep learning methods to achieve real-time, high-precision pose recognition. Firstly, they utilize human key point detection technology to extract the key points of players from real-time videos and generate key points. Then, the key point data is input into the transformer model for pose analysis and recognition. In addition, the authors design a human skeletal alignment method for comparison with standard poses. The experimental results show that the method performs well in recognizing players' poses in billiards matches and provides real-time and timely feedback on players' pose information. This research project provides a new and efficient tool for training billiard players and opens up new possibilities for applying deep learning in sports analytics. In addition, one of these contributions is the creation of a dataset for pose recognition.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch010"
    },
    {
        "id": 28050,
        "title": "Design of Nonlinear Iterative Learning Control Based on Deep Reinforcement Learning Algorithm",
        "authors": "Jia Shi, Kechao Wen, Xinghai Xu, Xiongzhe Hu, Dijun Luo",
        "published": "2021-5-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls52934.2021.9455494"
    },
    {
        "id": 28051,
        "title": "Air Learning: a deep reinforcement learning gym for autonomous aerial robot visual navigation",
        "authors": "Srivatsan Krishnan, Behzad Boroujerdian, William Fu, Aleksandra Faust, Vijay Janapa Reddi",
        "published": "2021-9",
        "citations": 15,
        "abstract": "AbstractWe introduce Air Learning, an open-source simulator, and a gym environment for deep reinforcement learning research on resource-constrained aerial robots. Equipped with domain randomization, Air Learning exposes a UAV agent to a diverse set of challenging scenarios. We seed the toolset with point-to-point obstacle avoidance tasks in three different environments and Deep Q Networks (DQN) and Proximal Policy Optimization (PPO) trainers. Air Learning assesses the policies’ performance under various quality-of-flight (QoF) metrics, such as the energy consumed, endurance, and the average trajectory length, on resource-constrained embedded platforms like a Raspberry Pi. We find that the trajectories on an embedded Ras-Pi are vastly different from those predicted on a high-end desktop system, resulting in up to$$40\\%$$40%longer trajectories in one of the environments. To understand the source of such discrepancies, we use Air Learning to artificially degrade high-end desktop performance to mimic what happens on a low-end embedded system. We then propose a mitigation technique that uses the hardware-in-the-loop to determine the latency distribution of running the policy on the target platform (onboard compute on aerial robot). A randomly sampled latency from the latency distribution is then added as an artificial delay within the training loop. Training the policy with artificial delays allows us to minimize the hardware gap (discrepancy in the flight time metric reduced from 37.73% to 0.5%). Thus, Air Learning with hardware-in-the-loop characterizes those differences and exposes how the onboard compute’s choice affects the aerial robot’s performance. We also conduct reliability studies to assess the effect of sensor failures on the learned policies. All put together, Air Learning enables a broad class of deep RL research on UAVs. The source code is available at: https://github.com/harvard-edge/AirLearning.",
        "link": "http://dx.doi.org/10.1007/s10994-021-06006-6"
    },
    {
        "id": 28052,
        "title": "Data-Driven Dynamic Multiobjective Optimal Control: An Aspiration-Satisfying Reinforcement Learning Approach",
        "authors": "Majid Mazouchi, Yongliang Yang, Hamidreza Modares",
        "published": "2022-11",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3072571"
    },
    {
        "id": 28053,
        "title": "Incorporating Observation Uncertainty into Reinforcement Learning-Based Spacecraft Guidance Schemes",
        "authors": "",
        "published": "2022-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2022-1765.vid"
    },
    {
        "id": 28054,
        "title": "Reinforcement learning and stochastic optimisation",
        "authors": "Sebastian Jaimungal",
        "published": "2022-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00780-021-00467-2"
    },
    {
        "id": 28055,
        "title": "Partitioning Distributed Compute Jobs with Reinforcement Learning and Graph Neural Networks",
        "authors": "Christopher  William Falke Parsonson, Zacharaya Shabka, Alessandro Ottino, Georgios Zervas",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4365914"
    },
    {
        "id": 28056,
        "title": "Vehicle Crash Simulation Models for Reinforcement Learning driven crash-detection algorithm calibration",
        "authors": "Shahabaz Afraj, Ondřej Vaculín, Dennis Böhmländer, Luděk Hynčík",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe development of finite element vehicle models for crash simulations is a highly complex task. The main aim of these models is to simulate a variety of crash scenarios and assess all the safety systems for their respective performances. These vehicle models possess a substantial amount of data pertaining to the vehicle's geometry, structure, materials, etc., and are used to estimate a large set of system and component level characteristics using crash simulations. It is understood that even the most well-developed simulation models are prone to deviations in estimation when compared to real-world physical test results. This is generally due to our inability to model the chaos and uncertainties introduced in the real world. Such unavoidable deviations render the use of virtual simulations ineffective for the calibration process of the algorithms that activate the restraint systems in the event of a crash (crash-detection algorithm). In the scope of this research, authors hypothesize the possibility of accounting for such variations introduced in the real world by creating a feedback loop between real-world crash tests and crash simulations. To accomplish this, a Reinforcement Learning (RL) compatible virtual surrogate model is used, which is adapted from crash simulation models. Hence, a conceptual methodology is illustrated in this paper for developing an RL-compatible model that can be trained using the results of crash simulations and crash tests. As the calibration of the crash-detection algorithm is fundamentally dependent upon the crash pulses, the scope of the expected output is limited to advancing the ability to estimate crash pulses. Furthermore, the real-time implementation of the methodology is illustrated using an actual vehicle model.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3004299/v1"
    },
    {
        "id": 28057,
        "title": "3D Generative Design for Non-Experts: Multiview Perceptual Similarity with Agent-Based Reinforcement Learning",
        "authors": "Robert Stuart-Smith, Patrick Danahy",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5151/sigradi2022-sigradi2022_53"
    },
    {
        "id": 28058,
        "title": "Scalability of Multiagent Reinforcement Learning",
        "authors": "Yunkai Zhuang, Yujing Hu, Hao Wang",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789813208742_0001"
    },
    {
        "id": 28059,
        "title": "Deep Reinforcement Learning in Competing Cognitive Tactical Networks",
        "authors": "Yi Chen",
        "published": "2023-2-12",
        "citations": 0,
        "abstract": "These instructions give you guidelines for preparing papers for DRP. Use this document as a template if you are using Microsoft Word 6.0 or later. Otherwise, use this document as an instruction set. The electronic file of your paper will be formatted further at DRP. Paper titles should be written in uppercase and lowercase letters, not all uppercase. Avoid writing long formulas with subscripts in the title; short formulas that identify the elements are fine (e.g., \"Nd-Fe-B\"). Do not write “(Invited)” in the title. Full names of authors are preferred in the author field, but are not required. Put a space between authors’ initials. The abstract must be a concise yet comprehensive reflection of what is in your article. In particular, the abstract must be self-contained, without abbreviations, footnotes, or references. It should be a microcosm of the full article. The abstract must be between 100 - 300 words. Be sure that you adhere to these limits; otherwise, you will need to edit your abstract accordingly. The abstract must be written as one paragraph, and should not contain displayed mathematical equations or tabular material. The abstract should include three or four different keywords or phrases, as this will help readers to find it. It is important to avoid over-repetition of such phrases as this can result in a page being rejected by search engines. Ensure that your abstract reads well and is grammatically correct.",
        "link": "http://dx.doi.org/10.54097/hset.v32i.5186"
    },
    {
        "id": 28060,
        "title": "Intelligent Steam Turbine Start-Up Control Based on Deep Reinforcement Learning",
        "authors": "Guangya Zhu, Ding Guo, Jinxing Li, Yonghui Xie, Di Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4716978"
    },
    {
        "id": 28061,
        "title": "Financial Trading with Feature Preprocessing and Recurrent Reinforcement Learning",
        "authors": "Lin Li",
        "published": "2021-11-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iske54062.2021.9755374"
    },
    {
        "id": 28062,
        "title": "Data-Driven MPC for Linear Systems using Reinforcement Learning",
        "authors": "Zhongqi Sun, Qian Wang, Junan Pan, Yuanqing Xia",
        "published": "2021-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728233"
    },
    {
        "id": 28063,
        "title": "Co-training by Experience Replay for Reinforcement Learning",
        "authors": "Yuyang Huang",
        "published": "2023-4-1",
        "citations": 0,
        "abstract": " In this paper, to improve the efficiency of the reinforcement learning model to explore the environment and get better results, a new method which involves the co-training process in reinforcement learning by sharing the experience pool of each agent in the training process has been developed. In this method, agents can gain a better understanding of the environment since agents use different policies to make action and explore the environment. At the same time, this paper designed an agent called Hard Memory Collector by modifying the value function and combining this agent and a normal agent for co-training. As an experimental result on the ViZDoom platform, the model achieved better results than the original Duel DQN network in terms of score, steps used per game and loss value.",
        "link": "http://dx.doi.org/10.54097/hset.v39i.6585"
    },
    {
        "id": 28064,
        "title": "Sequential Decision Problems",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch1"
    },
    {
        "id": 28065,
        "title": "Direct Lookahead Policies",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch19"
    },
    {
        "id": 28066,
        "title": "Exact Dynamic Programming",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch14"
    },
    {
        "id": 28067,
        "title": "Control of an Insect-Scale Flyer in Complex Flow Using Deep Reinforcement Learning",
        "authors": "Seungpyo Hong, Sejin Kim, Innyoung Kim, Donghyun You",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4432236"
    },
    {
        "id": 28068,
        "title": "Combining heuristics with counterfactual play in reinforcement learning.",
        "authors": "Erik Peterson, Necati Müyesser, Kyle Dunovan, Tim Verstynen",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2018.1151-0"
    },
    {
        "id": 28069,
        "title": "Multi-agent Robust Time Differential Reinforcement Learning Over Communicated Networks",
        "authors": "Jiahong Li, Nan Ma, Xiangmin Han",
        "published": "2018-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2018.8483961"
    },
    {
        "id": 28070,
        "title": "Reinforcement Learning Environment for Tactical Networks",
        "authors": "Thies Mohlenhof, Norman Jansen, Wiam Rachid",
        "published": "2021-5-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmcis52405.2021.9486411"
    },
    {
        "id": 28071,
        "title": "Maze Search Using Reinforcement Learning by a Mobile Robot",
        "authors": "Makoto Katoh",
        "published": "2018-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32474/arme.2018.01.000110"
    },
    {
        "id": 28072,
        "title": "A Full Freedom Pose Measurement Method of Industrial Robot Based on Reinforcement Learning Algorithm",
        "authors": "Xinghua Lu, Yunsheng Chen, Ziyue Yuan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn order to improve the efficiency of robot operation in the field of industrial automation, a full freedom pose measurement method of industrial robot based on reinforcement learning algorithm is proposed. According to the characteristics of two-wheel independent driving industrial robot, the attitude of the robot in three kinds of moving modes in unconstrained space is calculated. The algorithm for measuring the full degree of freedom of industrial robot is given by the multi-agent method of population particle optimization combined with the reinforcement learning algorithm (PSO-QL). The experimental results show that the proposed method has the advantages of low accuracy, high measurement efficiency, high success rate of grabbing and avoiding obstacles and good application effect.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-410169/v1"
    },
    {
        "id": 28073,
        "title": "Modelling global public health strategies in COVID-19 pandemic using deep reinforcement learning",
        "authors": "Kwak Gloria Hyunjung, Lowell Ling, Pan Hui",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nRationale: Unprecedented public health measures have been used during this coronavirus 2019 (COVID-19) pandemic but with a cost to economic and social disruption. It is a challenge to implement timely and appropriate public health interventions.Objectives: This study evaluates the timing and intensity of public health policies in each country and territory in the COVID-19 pandemic, and whether machine learning can help them to find better global health strategies.Methods: Population and COVID-19 epidemiological data between 21st January 2020 to 7th April 2020 from 183 countries and 78 territories were included with the implemented public health interventions. We used deep reinforcement learning, and the model was trained to try to find the optimal public health strategies with maximizing total reward on controlling spread of COVID-19. The results proposed by the model were analyzed against the actual timing and intensity of lockdown and travel restrictions.Measurements and Main Results: Early implementation of the actual lockdown and travel restriction policies were associated with gradually groups of less severe crisis severity, relative to local index case date in each country or territory, not to 31st December 2019. However, our model suggested to initiate at least minimal intensity of lockdown or travel restriction even before index cases in each country and territory. In addition, the model mostly recommended a combination of lockdown and travel restrictions and higher intensity policies than the implemented policies by government, but did not always encourage rapid full lockdown and full border closures.Conclusion: Compared to actual government implementation, our model mostly recommended earlier and higher intensity of lockdown and travel restrictions. Machine learning may be used as a decision support tool for implementation of public health interventions during COVID-19 and future pandemics.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-31226/v1"
    },
    {
        "id": 28074,
        "title": "The Basics of Feedback Control Systems",
        "authors": "Changsheng Hua",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-33034-7_2"
    },
    {
        "id": 28075,
        "title": "Reinforcement Learning Enables Field-Development Policy Optimization",
        "authors": "Chris Carpenter",
        "published": "2021-9-1",
        "citations": 1,
        "abstract": "This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 201254, “Reinforcement Learning for Field-Development Policy Optimization,” by Giorgio De Paola, SPE, and Cristina Ibanez-Llano, Repsol, and Jesus Rios, IBM, et al., prepared for the 2020 SPE Annual Technical Conference and Exhibition, originally scheduled to be held in Denver, Colorado, 5–7 October. The paper has not been peer reviewed.\nA field-development plan consists of a sequence of decisions. Each action taken affects the reservoir and conditions any future decision. The presence of uncertainty associated with this process, however, is undeniable. The novelty of the approach proposed by the authors in the complete paper is the consideration of the sequential nature of the decisions through the framework of dynamic programming (DP) and reinforcement learning (RL). This methodology allows moving the focus from a static field-development plan optimization to a more-dynamic framework that the authors call field-development policy optimization. This synopsis focuses on the methodology, while the complete paper also contains a real-field case of application of the methodology.\n\nMethodology\nDeep RL (DRL). RL is considered an important learning paradigm in artificial intelligence (AI) but differs from supervised or unsupervised learning, the most commonly known types currently studied in the field of machine learning. During the last decade, RL has attracted greater attention because of success obtained in applications related to games and self-driving cars resulting from its combination with deep-learning architectures such as DRL, which has allowed RL to scale on to previously unsolvable problems and, therefore, solve much larger sequential decision problems.\nRL, also referred to as stochastic approximate dynamic programming, is a goal-directed sequential-learning-from-interaction paradigm. The learner or agent is not told what to do but instead has to learn which actions or decisions yield a maximum reward through interaction with an uncertain environment without losing too much reward along the way. This way of learning from interaction to achieve a goal must be achieved in balance with the exploration and exploitation of possible actions. Another key characteristic of this type of problem is its sequential nature, where the actions taken by the agent affect the environment itself and, therefore, the subsequent data it receives and the subsequent actions to be taken.\nMathematically, such problems are formulated in the framework of the Markov decision process (MDP) that primarily arises in the field of optimal control. An RL problem consists of two principal parts: the agent, or decision-making engine, and the environment, the interactive world for an agent (in this case, the reservoir). Sequentially, at each timestep, the agent takes an action (e.g., changing control rates or deciding a well location) that makes the environment (reservoir) transition from one state to another. Next, the agent receives a reward (e.g., a cash flow) and an observation of the state of the environment (partial or total) before taking the next action.\nAll relevant information informing the agent of the state of the system is assumed to be included in the last state observed by the agent (Markov property). If the agent observes the full environment state once it has acted, the MDP is said to be fully observable; otherwise, a partially observable Markov decision process (POMDP) results. The agent’s objective is to learn policy mapping from states (MDPs) or histories (POMDPs) to actions such that the agent’s cumulated (discounted) reward in the long run is maximized.\n",
        "link": "http://dx.doi.org/10.2118/0921-0046-jpt"
    },
    {
        "id": 28076,
        "title": "Constrained Expectation-Maximization Methods for Effective Reinforcement Learning",
        "authors": "Gang Chen, Yiming Peng, Mengjie Zhang",
        "published": "2018-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8488990"
    },
    {
        "id": 28077,
        "title": "Deep Reinforcement Learning for Network Provisioning in Elastic Optical Networks",
        "authors": "Junior Momo Ziazet, Brigitte Jaumard",
        "published": "2022-5-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45855.2022.9839228"
    },
    {
        "id": 28078,
        "title": "Improving Algorithm Conflict Resolution Manoeuvres with Reinforcement Learning",
        "authors": "Marta Ribeiro, Joost Ellerbroek, Jacco Hoekstra",
        "published": "2022-12-19",
        "citations": 2,
        "abstract": "Future high traffic densities with drone operations are expected to exceed the number of aircraft that current air traffic control procedures can control simultaneously. Despite extensive research on geometric CR methods, at higher densities, their performance is hindered by the unpredictable emergent behaviour from surrounding aircraft. In response, research has shifted its attention to creating automated tools capable of generating conflict resolution (CR) actions adapted to the environment and not limited by man-made rules. Several works employing reinforcement learning (RL) methods for conflict resolution have been published recently. Although proving that they have potential, at their current development, the results of the practical implementation of these methods do not reach their expected theoretical performance. Consequently, RL applications cannot yet match the efficacy of geometric CR methods. Nevertheless, these applications can improve the set of rules that geometrical CR methods use to generate a CR manoeuvre. This work employs an RL method responsible for deciding the parameters that a geometric CR method uses to generate the CR manoeuvre for each conflict situation. The results show that this hybrid approach, combining the strengths of geometric CR and RL methods, reduces the total number of losses of minimum separation. Additionally, the large range of different optimal solutions found by the RL method shows that the rules of geometric CR method must be expanded, catering for different conflict geometries.",
        "link": "http://dx.doi.org/10.3390/aerospace9120847"
    },
    {
        "id": 28079,
        "title": "Meta Reinforcement Learning Based Computation Offloading Strategy for Vehicular Networks",
        "authors": "chao Yang, Yangshui Gao, Hongwei Ding, Rencan Nie, Bo Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDeep learning (DL) and reinforcement learning (RL) based methods can efficiently generate offloading strategies for computational offloading problems in mobile edge computing (MEC) environments. However, the rapid movement of vehicles in the vehicular network causes dynamic changes in the network environment, and DL or RL methods require additional training samples and multiple gradient updates before the model converges, which is time-consuming. In this letter, we propose a meta reinforcement learning-based computation task offloading and resource allocation (MRLOA) algorithm for vehicular networks. Specifically, the MRLOA can converge quickly for a new task with a small number of experience and gradient updates based on a pre-trained meta policy. Simulation results show that our proposed algorithm can more quickly adapt to new computational offloading tasks in the vehicular network environment compared to traditional baseline algorithms.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1614949/v1"
    },
    {
        "id": 28080,
        "title": "Object-Oriented State Abstraction in Reinforcement Learning for Video Games",
        "authors": "Yu Chen, Huizhuo Yuan, Yujun Li",
        "published": "2019-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8848099"
    },
    {
        "id": 28081,
        "title": "Calibrating Dead Reckoning with Deep Reinforcement Learning",
        "authors": "Sangmin Lee, Hwangnam Kim",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apcc55198.2022.9943735"
    },
    {
        "id": 28082,
        "title": "Similar Assembly State Discriminator for Reinforcement Learning-Based Robotic Connector Assembly",
        "authors": "Jun-Wan Yun, Minwoo Na, Yuhyeon Hwang, Jae-Bok Song",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4622927"
    },
    {
        "id": 28083,
        "title": "Virtual Autonomous Vehicle Using Deep Reinforcement Learning",
        "authors": "Vanita Jain, Aditya Chaudhry, Manas Batra, Prakhar Gupta",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3607824"
    },
    {
        "id": 28084,
        "title": "Autonomous Surface Vessel Obstacle Avoidance Based on Hierarchical Reinforcement Learning With Potential Field Method",
        "authors": "Chang Zhou, Lei Wang, Huacheng He, Shangyu Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.0005152v"
    },
    {
        "id": 28085,
        "title": "Batch RL, Experience-Replay, DQN, LSPI, Gradient TD",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-13"
    },
    {
        "id": 28086,
        "title": "Application of Reinforcement Learning for Electric Power System",
        "authors": "Yingqi Zhou",
        "published": "2021-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iaecst54258.2021.9695746"
    },
    {
        "id": 28087,
        "title": "A reinforcement learning algorithm shapes maternal care in mice",
        "authors": "Yunyao Xie, Longwen Huang, Alberto Corona, Alexa H. Pagliaro, Stephen D. Shea",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe neural substrates for processing classical rewards such as food or drugs of abuse are well-understood. In contrast, the mechanisms by which organisms perceive social contact as rewarding and subsequently modify their interactions are unclear. Here we tracked the gradual emergence of a repetitive and highly-stereotyped parental behavior and show that trial-by-trial performance correlates with the history of midbrain dopamine (DA) neuron activity. We used a novel behavior paradigm to manipulate the subject’s expectation of imminent pup contact and show that DA signals conform to reward prediction error, a fundamental component of reinforcement learning (RL). Finally, closed-loop optogenetic inactivation of DA neurons at the onset of pup contact dramatically slowed emergence of parental care. We conclude that this prosocial behavior is shaped by an RL mechanism in which social contact itself is the primary reward.One-Sentence SummaryMaternal interactions with offspring are shaped by a dopaminergic reinforcement learning mechanism.",
        "link": "http://dx.doi.org/10.1101/2022.03.21.485130"
    },
    {
        "id": 28088,
        "title": "Critical Discussion and Outlook",
        "authors": "Schirin Bär",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-39179-9_8"
    },
    {
        "id": 28089,
        "title": "Hybrid IRS-Assisted Secure Satellite Downlink Communications: A Fast Deep Reinforcement Learning Approach",
        "authors": "Quynh Ngo, Tran Khoa Phan, Abdun Mahmood, Wei Xiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper studies a secure satellite-terrestrial communication system assisted by a hybrid intelligent reflecting surface (IRS). The hybrid IRS, which composes of active and passive reflecting elements, is deployed to enhance the secure communication from the satellite to multiple users against multiple eavesdroppers. A joint design optimization problem for the satellite beamforming and the hybrid IRS interaction is formulated to maximize the system worst-case secrecy rate under time-varying channel conditions. With high system dynamic and complexity, deep reinforcement learning (DRL) is employed to solve the non-convex optimization problem. We propose a fast DRL algorithm, namely deep PDS-DPG, to obtain the robust secure beamforming design for satellite and hybrid IRS. Numerical results show a better learning efficiency of the proposed algorithm as to the state-of-the-art deep deterministic policy gradient (DDPG) algorithm with comparable system secrecy performance. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20478438"
    },
    {
        "id": 28090,
        "title": "Monopoly Using Reinforcement Learning",
        "authors": "Edupuganti Arun, Harikrishna Rajesh, Debarka Chakrabarti, Harikiran Cherala, Koshy George",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon.2019.8929523"
    },
    {
        "id": 28091,
        "title": "A Reinforcement Learning Approach for Mobile Beamforming",
        "authors": "Anastasios Dimas, Konstantinos Diamantaras, Athina Petropulu",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf44664.2019.9048888"
    },
    {
        "id": 28092,
        "title": "Superstitious learning of abstract order from random reinforcement",
        "authors": "Yuhao Jin, Greg Jensen, Jacqueline Gottlieb, Vincent Ferrera",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2022.1067-0"
    },
    {
        "id": 28093,
        "title": "Automatic Generation Control with Competing GENCOs-A Reinforcement Learning Based Approach",
        "authors": "Devika Jay, K.S Swarup",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npsc.2018.8771789"
    },
    {
        "id": 28094,
        "title": "Model-Free Indirect RL: Monte Carlo",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_3"
    },
    {
        "id": 28095,
        "title": "Reinforcement Learning with Explainability for Traffic Signal Control",
        "authors": "Stefano Giovanni Rizzo, Giovanna Vantini, Sanjay Chawla",
        "published": "2019-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8917519"
    },
    {
        "id": 28096,
        "title": "Supervised Reinforcement Learning via Value Function",
        "authors": "Yaozong Pan, Jian Zhang, Chunhui Yuan, Haitao Yang",
        "published": "2019-4-24",
        "citations": 1,
        "abstract": "Using expert samples to improve the performance of reinforcement learning (RL) algorithms has become one of the focuses of research nowadays. However, in different application scenarios, it is hard to guarantee both the quantity and quality of expert samples, which prohibits the practical application and performance of such algorithms. In this paper, a novel RL decision optimization method is proposed. The proposed method is capable of reducing the dependence on expert samples via incorporating the decision-making evaluation mechanism. By introducing supervised learning (SL), our method optimizes the decision making of the RL algorithm by using demonstrations or expert samples. Experiments are conducted in Pendulum and Puckworld scenarios to test the proposed method, and we use representative algorithms such as deep Q-network (DQN) and Double DQN (DDQN) as benchmarks. The results demonstrate that the method adopted in this paper can effectively improve the decision-making performance of agents even when the expert samples are not available.",
        "link": "http://dx.doi.org/10.3390/sym11040590"
    },
    {
        "id": 28097,
        "title": "Segmented Actor-Critic-Advantage Architecture for Reinforcement Learning Tasks",
        "authors": "Martin Kaloev, Georgi Krastev",
        "published": "2022-2-28",
        "citations": 0,
        "abstract": "The article focuses on experiments with a multi module neural networks type of architecture for neuron-like machine used in reinforcing learning. This type of architecture can be used to solve complex robotic or policy optimization tasks and allows segmented storage of trained memory. Such technique speeds up the training process compared to existing actor-critical algorithms.",
        "link": "http://dx.doi.org/10.18421/tem111-27"
    },
    {
        "id": 28098,
        "title": "Time Budget Management in Multifunction Radars Using Reinforcement Learning",
        "authors": "Petteri Pulkkinen, Tuomas Aittomaki, Anders Strom, Visa Koivunen",
        "published": "2021-5-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2147009.2021.9455344"
    },
    {
        "id": 28099,
        "title": "A Reinforcement Learning Approach To Synthesizing Climbing Movements",
        "authors": "Kourosh Naderi, Amin Babadi, Shaghayegh Roohi, Perttu Hamalainen",
        "published": "2019-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8848127"
    },
    {
        "id": 28100,
        "title": "A Review on Reinforcement Learning enabled Cooperative Spectrum Sensing",
        "authors": "Thi Thu Hien Pham, Sungrae Cho",
        "published": "2023-1-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin56518.2023.10048946"
    },
    {
        "id": 28101,
        "title": "Collaborative Multi-Agent Reinforcement Learning of Caching Optimization in Small-Cell Networks",
        "authors": "Xianzhe Xu, Meixia Tao",
        "published": "2018-12",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/glocom.2018.8647341"
    },
    {
        "id": 28102,
        "title": "PSS Control of Multi Machine Power System Using Reinforcement Learning",
        "authors": "Xingwang Xie",
        "published": "2021-6-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icras52289.2021.9476268"
    },
    {
        "id": 28103,
        "title": "Artificial Intelligence Gamers Based on Deep Reinforcement Learning",
        "authors": "Haolin Wang",
        "published": "2024-1-26",
        "citations": 0,
        "abstract": "This study investigates the design and implementation of Artificial Intelligence (AI) game players based on deep reinforcement learning, offering a novel approach to autonomous decision-making and strategy acquisition in intelligent games. Initially, the fundamental principles and algorithms of deep reinforcement learning are introduced, along with the fusion of deep learning and reinforcement learning. Subsequently, existing research is reviewed, and the pros and cons of current methodologies are examined, highlighting the underlying issues and challenges. The utilization of AI players in mainstream games is then introduced, and the influence of AI players on contemporary games is analyzed. Through this analysis of AI players in mainstream games, the strengths and weaknesses of current AI players are identified, and recommendations for optimizing them are provided. This study holds significant implications for guiding the design and development of intelligent game players, while also enriching the application of deep reinforcement learning within the gaming domain.",
        "link": "http://dx.doi.org/10.54097/p9tv1494"
    },
    {
        "id": 28104,
        "title": "Reinforcement Learning for Maritime Communications",
        "authors": "Liang Xiao, Helin Yang, Weihua Zhuang, Minghui Min",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-32138-2"
    },
    {
        "id": 28105,
        "title": "Container Caching Optimization based on Explainable Deep Reinforcement Learning",
        "authors": "Divyashree Jayaram, Saad Jeelani, Genya Ishigaki",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437757"
    },
    {
        "id": 28106,
        "title": "Causal Meta-Reinforcement Learning for Multimodal Remote Sensing Data Classification",
        "authors": "Wei Zhang, Xuesong Wang, Haoyu Wang, Yuhu Cheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "Multimodal remote sensing data classification can enhance the model’s ability to distinguish land features through multimodal data fusion. In this context, how to help models understand the relationship between multimodal data and target tasks is the focus of researchers. Inspired by human feedback learning mechanisms, causal reasoning mechanisms, and knowledge induction mechanisms, this paper integrates causal learning, reinforcement learning, and meta learning into a unified remote sensing data classification framework and proposed the causal meta-reinforcement learning (CMRL). First, based on feedback learning mechanisms, we have overcome the limitations of traditional implicit optimization of fusion features and customized a reinforcement learning environment for multimodal remote sensing data classification tasks. Through feedback interactive learning between agents and the environment, we help them understand the complex relationships between multimodal data and labels, thereby achieving full mining of multimodal complementary information. Second, based on the causal inference mechanism we designed causal distribution prediction action, classification reward, and causal intervention reward, capturing pure causal factors in multimodal data and cutting off false statistical associations between non-causal factors and class labels. Finally, based on the knowledge induction mechanism, we designed a bi-layer optimization mechanism based on meta-learning. By constructing a meta training task and meta validation task simulation model in the generalization scenario of unseen data, we helped the model induce cross-task shared knowledge, thereby improving its generalization ability for unseen multimodal data. The experimental results on multiple sets of multimodal datasets show that the proposed method achieved state-of-the-art performance in multimodal remote sensing data classification tasks.",
        "link": "http://dx.doi.org/10.20944/preprints202402.1296.v1"
    },
    {
        "id": 28107,
        "title": "Biologically inspired reinforcement learning for mobile robot collision avoidance",
        "authors": "Myung Seok Shim, Peng Li",
        "published": "2017-5",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7966242"
    },
    {
        "id": 28108,
        "title": "Deep Reinforcement Learning for Traffic Light Optimization",
        "authors": "Mustafa Coskun, Abdelkader Baggag, Sanjay Chawla",
        "published": "2018-11",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdmw.2018.00088"
    },
    {
        "id": 28109,
        "title": "Deep Residual Reinforcement Learning (Extended Abstract)",
        "authors": "Shangtong Zhang, Wendelin Boehmer, Shimon Whiteson",
        "published": "2021-8",
        "citations": 1,
        "abstract": "We revisit residual algorithms in both model-free and model-based reinforcement learning settings.\n\nWe propose the bidirectional target network technique to stabilize residual algorithms, yielding a residual version of DDPG that significantly outperforms vanilla DDPG in commonly used benchmarks.\n\nMoreover, we find the residual algorithm an effective approach to the distribution mismatch problem in model-based planning.\n\nCompared with the existing TD(k) method, our residual-based method makes weaker assumptions about the model and yields a greater performance boost.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/668"
    },
    {
        "id": 28110,
        "title": "Reinforcement Learning for Efficient Scheduling in Complex Semiconductor Equipment",
        "authors": "Doug Suerich, Terry Young",
        "published": "2020-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asmc49169.2020.9185293"
    },
    {
        "id": 28111,
        "title": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning",
        "authors": "Xiao-Yang Liu",
        "published": "No Date",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4253139"
    },
    {
        "id": 28112,
        "title": "Utterance Censorship of Online Reinforcement Learning Chatbot",
        "authors": "Yixuan Chai, Guohua Liu",
        "published": "2018-11",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai.2018.00063"
    },
    {
        "id": 28113,
        "title": "Deep Hierarchical Reinforcement Learning for Autonomous Driving with Distinct Behaviors",
        "authors": "Jianyu Chen, Zining Wang, Masayoshi Tomizuka",
        "published": "2018-6",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2018.8500368"
    },
    {
        "id": 28114,
        "title": "Disrupted reinforcement learning during post-error slowing in ADHD",
        "authors": "Andre Chevrier, Mehereen Bhaijiwala, Jonathan Lipszyc, Douglas Cheyne, Simon Graham, Russell Schachar",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractADHD is associated with altered dopamine regulated reinforcement learning on prediction errors. Despite evidence of categorically altered error processing in ADHD, neuroimaging advances have largely investigated models of normal reinforcement learning in greater detail. Further, although reinforcement leaning critically relies on ventral striatum exerting error magnitude related thresholding influences on substantia nigra (SN) and dorsal striatum, these thresholding influences have never been identified with neuroimaging. To identify such thresholding influences, we propose that error magnitude related activities must first be separated from opposite activities in overlapping neural regions during error detection. Here we separate error detection from magnitude related adjustment (post-error slowing) during inhibition errors in the stop signal task in typically developing (TD) and ADHD adolescents using fMRI. In TD, we predicted that: 1) deactivation of dorsal striatum on error detection interrupts ongoing processing, and should be proportional to right frontoparietal response phase activity that has been observed in the SST; 2) deactivation of ventral striatum on post-error slowing exerts thresholding influences on, and should be proportional to activity in dorsal striatum. In ADHD, we predicted that ventral striatum would instead correlate with heightened amygdala responses to errors. We found deactivation of dorsal striatum on error detection correlated with response-phase activity in both groups. In TD, post-error slowing deactivation of ventral striatum correlated with activation of dorsal striatum. In ADHD, ventral striatum correlated with heightened amygdala activity. Further, heightened activities in locus coeruleus (norepinephrine), raphe nucleus (serotonin) and medial septal nuclei (acetylcholine), which all compete for control of DA, and are altered in ADHD, exhibited altered correlations with SN. All correlations in TD were replicated in healthy adults. Results in TD are consistent with dopamine regulated reinforcement learning on post-error slowing. In ADHD, results are consistent with heightened activities in the amygdala and non-dopaminergic neurotransmitter nuclei preventing reinforcement learning.",
        "link": "http://dx.doi.org/10.1101/449975"
    },
    {
        "id": 28115,
        "title": "EdgeAlign: platform for deep reinforcement learning based pairwise DNA sequence alignment compatible with embedded edge devices",
        "authors": "Aryan Lall, Siddharth Tallur",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nSequence alignment is an essential component of bioinformatics, for identifying regions of similarity that may indicate functional, structural, or evolutionary relationships between the sequences. Genome-based diagnostics relying on DNA sequencing have benefited hugely from the boom in computing power in recent decades, particularly due to cloud-computing and the rise of graphics processing units (GPUs) and other advanced computing platforms for running advanced algorithms. Translating the success of such breakthroughs in diagnostics to affordable solutions for low-cost healthcare requires development of algorithms that can operate on the edge instead of in the cloud, using low-cost and low-power electronic systems such as microcontrollers and field programmable gate arrays (FPGAs). In this work, we present EdgeAlign, a deep reinforcement learning based framework for performing pairwise DNA sequence alignment on stand-alone edge devices. EdgeAlign uses deep reinforcement learning to train a deep Q-network (DQN) agent for performing sequence alignment on fixed length sub-sequences, using a sliding window that is scanned over the length of the entire sequence. The hardware resource-consumption for implementing this scheme is thus independent of the lengths of the sequences to be aligned, and is further optimized using a novel AutoML based method for neural network model size reduction. Unlike other algorithms for sequence alignment reported in literature, the model demonstrated in this work is highly compact and deployed on two edge devices (NVIDIA® Jetson NanoTM Developer Kit and Digilent Arty A7-100T, containing Xilinx® XC7A35T Artix®-7 FPGA) for demonstration of alignment for sequences from the publicly available Influenza sequences at the National Center for Biotechnology Information (NCBI) Virus Data Hub.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2175890/v1"
    },
    {
        "id": 28116,
        "title": "Robust Lane Change Decision Making for Autonomous Vehicles: An Observation Adversarial Reinforcement Learning Approach",
        "authors": "Xiangkun He, Haohan Yang, Zhongxu Hu, Chen Lv",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning holds the promise of allowing autonomous vehicles to learn complex decision making behaviors through interacting with other traffic participants. However, many real-world driving tasks involve unpredictable perception errors or measurement noises which may mislead an autonomous vehicle into making unsafe decisions, even cause catastrophic failures. In light of these risks, to ensure safety under perception uncertainty, autonomous vehicles are required to be able to cope with the worst case observation perturbations. Therefore, this paper proposes a novel observation adversarial reinforcement learning approach for robust lane change decision making of autonomous vehicles. A constrained observation-robust Markov decision process is presented to model lane change decision making behaviors of autonomous vehicles under policy constraints and observation uncertainties.  Meanwhile, a black-box attack technique based on Bayesian optimization is implemented to approximate the optimal adversarial observation perturbations efficiently. Furthermore, a constrained observation-robust actor-critic algorithm is advanced to optimize autonomous driving lane change policies while keeping the variations of the policies attacked by the optimal adversarial observation perturbations within bounds. Finally, the robust lane change decision making approach is evaluated in three stochastic mixed traffic flows based on different densities. The results demonstrate that the proposed method can not only enhance the performance of an autonomous vehicle but also improve the robustness of lane change policies against adversarial observation perturbations.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21229319"
    },
    {
        "id": 28117,
        "title": "Deep Contextual Bandit and Reinforcement Learning for IRS-assisted MU-MIMO Systems",
        "authors": "Dariel Pereira-Ruisánchez, Óscar Fresnedo, Darian Pérez-Adán, Luis Castedo",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> The combination of multiple-input multiple-output (MIMO) and intelligent reflecting surfaces (IRSs) is foreseen as a key enabler of beyond 5G (B5G) and 6G. In this work, two different approaches are considered for the joint optimization of the IRS phase-shift matrix and MIMO precoders of an IRS-assisted multi-stream (MS) multi-user MIMO (MU-MIMO) system with the aim of maximizing the system sum-rate for every channel realization. The first one is a novel contextual bandit (CB) approach with continuous state and action spaces called deep contextual bandit-oriented deep deterministic policy gradient (DCB-DDPG). The second is an innovative deep reinforcement learning (DRL) formulation where the states, actions and rewards are selected such that the Markov decision process (MDP) property of reinforcement learning (RL) is properly met. Both proposals perform remarkably better than state-of-the-art heuristic methods in high multi-user interference scenarios. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19787551.v1"
    },
    {
        "id": 28118,
        "title": "Optimal Adaptive Prediction Intervals for Electricity Load Forecasting in Distribution Systems via Reinforcement Learning",
        "authors": "Yufan Zhang, Honglin Wen, Qiuwei Wu, Qian Ai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Prediction intervals (PIs) offer an effective tool for quantifying uncertainty of loads in distribution systems. The traditional central PIs cannot adapt well to skewed distributions, and their offline training fashion is vulnerable to the unforeseen change in future load patterns. Therefore, we propose an optimal PI estimation approach, which is online and adaptive to different data distributions by adaptively determining symmetric or asymmetric probability proportion pairs for quantiles of PIs’ bounds. It relies on the online learning ability of reinforcement learning (RL) to integrate the two online tasks, i.e., the adaptive selection of probability proportion pairs and quantile predictions, both of which are modeled by neural networks. As such, the quality of quantiles-formed PI can guide the selection process of optimal probability proportion pairs, which forms a closed loop to improve PIs’ quality. Furthermore, to improve the learning efficiency of quantile forecasts, a prioritized experience replay (PER) strategy is proposed for online quantile regression processes. Case studies on both load and net load demonstrate that the proposed method can better adapt to data distribution compared with online central PIs method. Compared with offline-trained methods, it obtains PIs with better quality and is more robust against concept drift.",
        "link": "http://dx.doi.org/10.36227/techrxiv.17925911.v1"
    },
    {
        "id": 28119,
        "title": "Optimal Adaptive Prediction Intervals for Electricity Load Forecasting in Distribution Systems via Reinforcement Learning",
        "authors": "Yufan Zhang, Honglin Wen, Qiuwei Wu, Qian Ai",
        "published": "No Date",
        "citations": 0,
        "abstract": "Prediction intervals (PIs) offer an effective tool for quantifying uncertainty of loads in distribution systems. The traditional central PIs cannot adapt well to skewed distributions, and their offline training fashion is vulnerable to the unforeseen change in future load patterns. Therefore, we propose an optimal PI estimation approach, which is online and adaptive to different data distributions by adaptively determining symmetric or asymmetric probability proportion pairs for quantiles of PIs’ bounds. It relies on the online learning ability of reinforcement learning (RL) to integrate the two online tasks, i.e., the adaptive selection of probability proportion pairs and quantile predictions, both of which are modeled by neural networks. As such, the quality of quantiles-formed PI can guide the selection process of optimal probability proportion pairs, which forms a closed loop to improve PIs’ quality. Furthermore, to improve the learning efficiency of quantile forecasts, a prioritized experience replay (PER) strategy is proposed for online quantile regression processes. Case studies on both load and net load demonstrate that the proposed method can better adapt to data distribution compared with online central PIs method. Compared with offline-trained methods, it obtains PIs with better quality and is more robust against concept drift.",
        "link": "http://dx.doi.org/10.36227/techrxiv.17925911"
    },
    {
        "id": 28120,
        "title": "Reinforcement Learning for Cyber-Physical Systems",
        "authors": "Xing Liu, Hansong Xu, Weixian Liao, Wei Yu",
        "published": "2019-11",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icii.2019.00063"
    },
    {
        "id": 28121,
        "title": "Online mobile learning resource recommendation method based on deep reinforcement learning",
        "authors": "Pingyang Li, Juan Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijisd.2023.10056670"
    },
    {
        "id": 28122,
        "title": "Utilized System Model Using Channel State Information Network with Gated Recurrent Units (CsiNet-GRUs)",
        "authors": "Hany Helmy, Sherif El Diasty, Hazem Shatila",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "MIMO: multiple-input multiple-output technology uses multiple antennas to use reflected signals to provide channel robustness and throughput gains. It is advantageous in several applications like cellular systems, and users are distributed over a wide coverage area in various applications such as mobile systems, improving channel state information (CSI) processing efficiency in massive MIMO systems. This chapter proposes two channel-based deep learning methods to enhance the performance in a massive MIMO system and compares our proposed technique to the previous methods. The proposed technique is based on the channel state information network combined with the gated recurrent unit’s technique CsiNet-GRUs, which increases recovery efficiency. Besides, a fair balance between compression ratio (CR) and complexity is given using correlation time in training samples. The simulation results show that the proposed CsiNet-GRUs technique fulfills performance improvement compared with the existing literature techniques, namely CS-based methods Conv-LSTM CsiNet, LASSO, Tval3, and CsiNet.",
        "link": "http://dx.doi.org/10.5772/intechopen.111650"
    },
    {
        "id": 28123,
        "title": "Batch Reinforcement Learning on a RoboCup SSL Keepaway Strategy Learning Problem",
        "authors": "Franco Ollino, Miguel A. Solís, Héctor Allende",
        "published": "2019-3-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/5b03f636"
    },
    {
        "id": 28124,
        "title": "Advances in Value-based, Policy-based, and Deep Learning-based Reinforcement Learning",
        "authors": "Haewon Byeon",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140838"
    },
    {
        "id": 28125,
        "title": "Collision-Free UAV Navigation with a Monocular Camera Using Deep Reinforcement Learning",
        "authors": "Yun Chen, Nuria Gonzalez-Prelcic, Robert W. Heath",
        "published": "2020-9",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp49062.2020.9231577"
    },
    {
        "id": 28126,
        "title": "Learning and Generalization of Dynamic Movement Primitives by Hierarchical Deep Reinforcement Learning from Demonstration",
        "authors": "Wonchul Kim, Chungkeun Lee, H. Jin Kim",
        "published": "2018-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2018.8594476"
    },
    {
        "id": 28127,
        "title": "Coupling Effect of Exploration Rate and Learning Rate  for Optimized Scaled Reinforcement Learning",
        "authors": "Smriti Gupta, Sabita Pal, Kundan Kumar, Kuntal Ghosh",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42979-023-02114-3"
    },
    {
        "id": 28128,
        "title": "Session details: Session 3B: Efficient and Secure Deep Learning and Reinforcement Learning in Embedded Systems",
        "authors": "Yanzhi Wang",
        "published": "2020-9-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3422942"
    },
    {
        "id": 28129,
        "title": "Adaptive Online-Learning Volt-Var Control for Smart Inverters Using Deep Reinforcement Learning",
        "authors": "Kirstin Beyer, Robert Beckmann, Stefan Geißendörfer, Karsten von Maydell, Carsten Agert",
        "published": "2021-4-3",
        "citations": 15,
        "abstract": "The increasing penetration of the power grid with renewable distributed generation causes significant voltage fluctuations. Providing reactive power helps balancing the voltage in the grid. This paper proposes a novel adaptive volt-var control algorithm on the basis of deep reinforcement learning. The learning agent is an online-learning deep deterministic policy gradient that is applicable under real-time conditions in smart inverters for reactive power management. The algorithm only uses input data from the grid connection point of the inverter itself; thus, no additional communication devices are needed and it can be applied individually to any inverter in the grid. The proposed volt-var control is successfully simulated at various grid connection points in a 21-bus low-voltage distribution test feeder. The resulting voltage behavior is analyzed and a systematic voltage reduction is observed both in a static grid environment and a dynamic environment. The proposed algorithm enables flexible adaption to changing environments through continuous exploration during the learning process and, thus, contributes to a decentralized, automated voltage control in future power grids.",
        "link": "http://dx.doi.org/10.3390/en14071991"
    },
    {
        "id": 28130,
        "title": "Distributed deep reinforcement learning for simulation control",
        "authors": "Suraj Pawar, Romit Maulik",
        "published": "2021-6-1",
        "citations": 7,
        "abstract": "Abstract\nSeveral applications in the scientific simulation of physical systems can be formulated as control/optimization problems. The computational models for such systems generally contain hyperparameters, which control solution fidelity and computational expense. The tuning of these parameters is non-trivial and the general approach is to manually ‘spot-check’ for good combinations. This is because optimal hyperparameter configuration search becomes intractable when the parameter space is large and when they may vary dynamically. To address this issue, we present a framework based on deep reinforcement learning (RL) to train a deep neural network agent that controls a model solve by varying parameters dynamically. First, we validate our RL framework for the problem of controlling chaos in chaotic systems by dynamically changing the parameters of the system. Subsequently, we illustrate the capabilities of our framework for accelerating the convergence of a steady-state computational fluid dynamics solver by automatically adjusting the relaxation factors of the discretized Navier–Stokes equations during run-time. The results indicate that the run-time control of the relaxation factors by the learned policy leads to a significant reduction in the number of iterations for convergence compared to the random selection of the relaxation factors. Our results point to potential benefits for learning adaptive hyperparameter learning strategies across different geometries and boundary conditions with implications for reduced computational campaign expenses\n4\n\n\n4\nData and codes available at https://github.com/Romit-Maulik/PAR-RL.\n.",
        "link": "http://dx.doi.org/10.1088/2632-2153/abdaf8"
    },
    {
        "id": 28131,
        "title": "Reinforcement Learning Meets Cognitive Situation Management: A Review of Recent Learning Approaches from the Cognitive Situation Management Perspective",
        "authors": "Andrea Salfinger",
        "published": "2020-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cogsima49017.2020.9216026"
    },
    {
        "id": 28132,
        "title": "Key node identification of satellite time-varying network with deep reinforcement learning",
        "authors": "Liyang Wang, Yun Li, Ye Ren, Lina Yu",
        "published": "2022-10-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml57342.2022.10009841"
    },
    {
        "id": 28133,
        "title": "Deep Reinforcement Learning with Copy-oriented Context Awareness and Weighted Rewards for Abstractive Summarization",
        "authors": "Caidong Tan",
        "published": "2023-3-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3590003.3590019"
    },
    {
        "id": 28134,
        "title": "Anomaly Detection in Structural Health Monitoring with Ensemble Learning and Reinforcement Learning",
        "authors": "Nan Huang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2024.0150112"
    },
    {
        "id": 28135,
        "title": "Learning to Drive (L2D) as a Low-Cost Benchmark for Real-World Reinforcement Learning",
        "authors": "Ari Viitala, Rinu Boney, Yi Zhao, Alexander Ilin, Juho Kannala",
        "published": "2021-12-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icar53236.2021.9659342"
    },
    {
        "id": 28136,
        "title": "Automatic berthing using supervised learning and reinforcement learning",
        "authors": "Shoma Shimizu, Kenta Nishihara, Yoshiki Miyauchi, Kouki Wakita, Rin Suyama, Atsuo Maki, Shinichi Shirakawa",
        "published": "2022-12",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2022.112553"
    },
    {
        "id": 28137,
        "title": "Real-Time Pose Recognition for Billiard Players Using Deep Learning",
        "authors": "Zhikang Chen, Wei Qi Yan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "In this book chapter, the authors propose a method for player pose recognition in billiards matches by combining keypoint extraction and an optimized transformer. Given that those human pose analysis methods usually require high labour costs, the authors explore deep learning methods to achieve real-time, high-precision pose recognition. Firstly, they utilize human key point detection technology to extract the key points of players from real-time videos and generate key points. Then, the key point data is input into the transformer model for pose analysis and recognition. In addition, the authors design a human skeletal alignment method for comparison with standard poses. The experimental results show that the method performs well in recognizing players' poses in billiards matches and provides real-time and timely feedback on players' pose information. This research project provides a new and efficient tool for training billiard players and opens up new possibilities for applying deep learning in sports analytics. In addition, one of these contributions is the creation of a dataset for pose recognition.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch010"
    },
    {
        "id": 28138,
        "title": "Design of Nonlinear Iterative Learning Control Based on Deep Reinforcement Learning Algorithm",
        "authors": "Jia Shi, Kechao Wen, Xinghai Xu, Xiongzhe Hu, Dijun Luo",
        "published": "2021-5-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls52934.2021.9455494"
    },
    {
        "id": 28139,
        "title": "Air Learning: a deep reinforcement learning gym for autonomous aerial robot visual navigation",
        "authors": "Srivatsan Krishnan, Behzad Boroujerdian, William Fu, Aleksandra Faust, Vijay Janapa Reddi",
        "published": "2021-9",
        "citations": 15,
        "abstract": "AbstractWe introduce Air Learning, an open-source simulator, and a gym environment for deep reinforcement learning research on resource-constrained aerial robots. Equipped with domain randomization, Air Learning exposes a UAV agent to a diverse set of challenging scenarios. We seed the toolset with point-to-point obstacle avoidance tasks in three different environments and Deep Q Networks (DQN) and Proximal Policy Optimization (PPO) trainers. Air Learning assesses the policies’ performance under various quality-of-flight (QoF) metrics, such as the energy consumed, endurance, and the average trajectory length, on resource-constrained embedded platforms like a Raspberry Pi. We find that the trajectories on an embedded Ras-Pi are vastly different from those predicted on a high-end desktop system, resulting in up to$$40\\%$$40%longer trajectories in one of the environments. To understand the source of such discrepancies, we use Air Learning to artificially degrade high-end desktop performance to mimic what happens on a low-end embedded system. We then propose a mitigation technique that uses the hardware-in-the-loop to determine the latency distribution of running the policy on the target platform (onboard compute on aerial robot). A randomly sampled latency from the latency distribution is then added as an artificial delay within the training loop. Training the policy with artificial delays allows us to minimize the hardware gap (discrepancy in the flight time metric reduced from 37.73% to 0.5%). Thus, Air Learning with hardware-in-the-loop characterizes those differences and exposes how the onboard compute’s choice affects the aerial robot’s performance. We also conduct reliability studies to assess the effect of sensor failures on the learned policies. All put together, Air Learning enables a broad class of deep RL research on UAVs. The source code is available at: https://github.com/harvard-edge/AirLearning.",
        "link": "http://dx.doi.org/10.1007/s10994-021-06006-6"
    },
    {
        "id": 28140,
        "title": "Data-Driven Dynamic Multiobjective Optimal Control: An Aspiration-Satisfying Reinforcement Learning Approach",
        "authors": "Majid Mazouchi, Yongliang Yang, Hamidreza Modares",
        "published": "2022-11",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3072571"
    },
    {
        "id": 28141,
        "title": "Incorporating Observation Uncertainty into Reinforcement Learning-Based Spacecraft Guidance Schemes",
        "authors": "",
        "published": "2022-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2022-1765.vid"
    },
    {
        "id": 28142,
        "title": "Reinforcement learning and stochastic optimisation",
        "authors": "Sebastian Jaimungal",
        "published": "2022-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00780-021-00467-2"
    },
    {
        "id": 28143,
        "title": "Partitioning Distributed Compute Jobs with Reinforcement Learning and Graph Neural Networks",
        "authors": "Christopher  William Falke Parsonson, Zacharaya Shabka, Alessandro Ottino, Georgios Zervas",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4365914"
    },
    {
        "id": 28144,
        "title": "Vehicle Crash Simulation Models for Reinforcement Learning driven crash-detection algorithm calibration",
        "authors": "Shahabaz Afraj, Ondřej Vaculín, Dennis Böhmländer, Luděk Hynčík",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe development of finite element vehicle models for crash simulations is a highly complex task. The main aim of these models is to simulate a variety of crash scenarios and assess all the safety systems for their respective performances. These vehicle models possess a substantial amount of data pertaining to the vehicle's geometry, structure, materials, etc., and are used to estimate a large set of system and component level characteristics using crash simulations. It is understood that even the most well-developed simulation models are prone to deviations in estimation when compared to real-world physical test results. This is generally due to our inability to model the chaos and uncertainties introduced in the real world. Such unavoidable deviations render the use of virtual simulations ineffective for the calibration process of the algorithms that activate the restraint systems in the event of a crash (crash-detection algorithm). In the scope of this research, authors hypothesize the possibility of accounting for such variations introduced in the real world by creating a feedback loop between real-world crash tests and crash simulations. To accomplish this, a Reinforcement Learning (RL) compatible virtual surrogate model is used, which is adapted from crash simulation models. Hence, a conceptual methodology is illustrated in this paper for developing an RL-compatible model that can be trained using the results of crash simulations and crash tests. As the calibration of the crash-detection algorithm is fundamentally dependent upon the crash pulses, the scope of the expected output is limited to advancing the ability to estimate crash pulses. Furthermore, the real-time implementation of the methodology is illustrated using an actual vehicle model.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3004299/v1"
    },
    {
        "id": 28145,
        "title": "3D Generative Design for Non-Experts: Multiview Perceptual Similarity with Agent-Based Reinforcement Learning",
        "authors": "Robert Stuart-Smith, Patrick Danahy",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5151/sigradi2022-sigradi2022_53"
    },
    {
        "id": 28146,
        "title": "Scalability of Multiagent Reinforcement Learning",
        "authors": "Yunkai Zhuang, Yujing Hu, Hao Wang",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789813208742_0001"
    },
    {
        "id": 28147,
        "title": "Deep Reinforcement Learning in Competing Cognitive Tactical Networks",
        "authors": "Yi Chen",
        "published": "2023-2-12",
        "citations": 0,
        "abstract": "These instructions give you guidelines for preparing papers for DRP. Use this document as a template if you are using Microsoft Word 6.0 or later. Otherwise, use this document as an instruction set. The electronic file of your paper will be formatted further at DRP. Paper titles should be written in uppercase and lowercase letters, not all uppercase. Avoid writing long formulas with subscripts in the title; short formulas that identify the elements are fine (e.g., \"Nd-Fe-B\"). Do not write “(Invited)” in the title. Full names of authors are preferred in the author field, but are not required. Put a space between authors’ initials. The abstract must be a concise yet comprehensive reflection of what is in your article. In particular, the abstract must be self-contained, without abbreviations, footnotes, or references. It should be a microcosm of the full article. The abstract must be between 100 - 300 words. Be sure that you adhere to these limits; otherwise, you will need to edit your abstract accordingly. The abstract must be written as one paragraph, and should not contain displayed mathematical equations or tabular material. The abstract should include three or four different keywords or phrases, as this will help readers to find it. It is important to avoid over-repetition of such phrases as this can result in a page being rejected by search engines. Ensure that your abstract reads well and is grammatically correct.",
        "link": "http://dx.doi.org/10.54097/hset.v32i.5186"
    },
    {
        "id": 28148,
        "title": "Intelligent Steam Turbine Start-Up Control Based on Deep Reinforcement Learning",
        "authors": "Guangya Zhu, Ding Guo, Jinxing Li, Yonghui Xie, Di Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4716978"
    },
    {
        "id": 28149,
        "title": "Financial Trading with Feature Preprocessing and Recurrent Reinforcement Learning",
        "authors": "Lin Li",
        "published": "2021-11-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iske54062.2021.9755374"
    },
    {
        "id": 28150,
        "title": "Data-Driven MPC for Linear Systems using Reinforcement Learning",
        "authors": "Zhongqi Sun, Qian Wang, Junan Pan, Yuanqing Xia",
        "published": "2021-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728233"
    },
    {
        "id": 28151,
        "title": "Co-training by Experience Replay for Reinforcement Learning",
        "authors": "Yuyang Huang",
        "published": "2023-4-1",
        "citations": 0,
        "abstract": " In this paper, to improve the efficiency of the reinforcement learning model to explore the environment and get better results, a new method which involves the co-training process in reinforcement learning by sharing the experience pool of each agent in the training process has been developed. In this method, agents can gain a better understanding of the environment since agents use different policies to make action and explore the environment. At the same time, this paper designed an agent called Hard Memory Collector by modifying the value function and combining this agent and a normal agent for co-training. As an experimental result on the ViZDoom platform, the model achieved better results than the original Duel DQN network in terms of score, steps used per game and loss value.",
        "link": "http://dx.doi.org/10.54097/hset.v39i.6585"
    },
    {
        "id": 28152,
        "title": "Sequential Decision Problems",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch1"
    },
    {
        "id": 28153,
        "title": "Direct Lookahead Policies",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch19"
    },
    {
        "id": 28154,
        "title": "Exact Dynamic Programming",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch14"
    },
    {
        "id": 28155,
        "title": "Control of an Insect-Scale Flyer in Complex Flow Using Deep Reinforcement Learning",
        "authors": "Seungpyo Hong, Sejin Kim, Innyoung Kim, Donghyun You",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4432236"
    },
    {
        "id": 28156,
        "title": "Combining heuristics with counterfactual play in reinforcement learning.",
        "authors": "Erik Peterson, Necati Müyesser, Kyle Dunovan, Tim Verstynen",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2018.1151-0"
    },
    {
        "id": 28157,
        "title": "Multi-agent Robust Time Differential Reinforcement Learning Over Communicated Networks",
        "authors": "Jiahong Li, Nan Ma, Xiangmin Han",
        "published": "2018-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2018.8483961"
    },
    {
        "id": 28158,
        "title": "Reinforcement Learning Environment for Tactical Networks",
        "authors": "Thies Mohlenhof, Norman Jansen, Wiam Rachid",
        "published": "2021-5-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmcis52405.2021.9486411"
    },
    {
        "id": 28159,
        "title": "Maze Search Using Reinforcement Learning by a Mobile Robot",
        "authors": "Makoto Katoh",
        "published": "2018-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32474/arme.2018.01.000110"
    },
    {
        "id": 28160,
        "title": "A Full Freedom Pose Measurement Method of Industrial Robot Based on Reinforcement Learning Algorithm",
        "authors": "Xinghua Lu, Yunsheng Chen, Ziyue Yuan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn order to improve the efficiency of robot operation in the field of industrial automation, a full freedom pose measurement method of industrial robot based on reinforcement learning algorithm is proposed. According to the characteristics of two-wheel independent driving industrial robot, the attitude of the robot in three kinds of moving modes in unconstrained space is calculated. The algorithm for measuring the full degree of freedom of industrial robot is given by the multi-agent method of population particle optimization combined with the reinforcement learning algorithm (PSO-QL). The experimental results show that the proposed method has the advantages of low accuracy, high measurement efficiency, high success rate of grabbing and avoiding obstacles and good application effect.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-410169/v1"
    },
    {
        "id": 28161,
        "title": "Modelling global public health strategies in COVID-19 pandemic using deep reinforcement learning",
        "authors": "Kwak Gloria Hyunjung, Lowell Ling, Pan Hui",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nRationale: Unprecedented public health measures have been used during this coronavirus 2019 (COVID-19) pandemic but with a cost to economic and social disruption. It is a challenge to implement timely and appropriate public health interventions.Objectives: This study evaluates the timing and intensity of public health policies in each country and territory in the COVID-19 pandemic, and whether machine learning can help them to find better global health strategies.Methods: Population and COVID-19 epidemiological data between 21st January 2020 to 7th April 2020 from 183 countries and 78 territories were included with the implemented public health interventions. We used deep reinforcement learning, and the model was trained to try to find the optimal public health strategies with maximizing total reward on controlling spread of COVID-19. The results proposed by the model were analyzed against the actual timing and intensity of lockdown and travel restrictions.Measurements and Main Results: Early implementation of the actual lockdown and travel restriction policies were associated with gradually groups of less severe crisis severity, relative to local index case date in each country or territory, not to 31st December 2019. However, our model suggested to initiate at least minimal intensity of lockdown or travel restriction even before index cases in each country and territory. In addition, the model mostly recommended a combination of lockdown and travel restrictions and higher intensity policies than the implemented policies by government, but did not always encourage rapid full lockdown and full border closures.Conclusion: Compared to actual government implementation, our model mostly recommended earlier and higher intensity of lockdown and travel restrictions. Machine learning may be used as a decision support tool for implementation of public health interventions during COVID-19 and future pandemics.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-31226/v1"
    },
    {
        "id": 28162,
        "title": "The Basics of Feedback Control Systems",
        "authors": "Changsheng Hua",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-33034-7_2"
    },
    {
        "id": 28163,
        "title": "Reinforcement Learning Enables Field-Development Policy Optimization",
        "authors": "Chris Carpenter",
        "published": "2021-9-1",
        "citations": 1,
        "abstract": "This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 201254, “Reinforcement Learning for Field-Development Policy Optimization,” by Giorgio De Paola, SPE, and Cristina Ibanez-Llano, Repsol, and Jesus Rios, IBM, et al., prepared for the 2020 SPE Annual Technical Conference and Exhibition, originally scheduled to be held in Denver, Colorado, 5–7 October. The paper has not been peer reviewed.\nA field-development plan consists of a sequence of decisions. Each action taken affects the reservoir and conditions any future decision. The presence of uncertainty associated with this process, however, is undeniable. The novelty of the approach proposed by the authors in the complete paper is the consideration of the sequential nature of the decisions through the framework of dynamic programming (DP) and reinforcement learning (RL). This methodology allows moving the focus from a static field-development plan optimization to a more-dynamic framework that the authors call field-development policy optimization. This synopsis focuses on the methodology, while the complete paper also contains a real-field case of application of the methodology.\n\nMethodology\nDeep RL (DRL). RL is considered an important learning paradigm in artificial intelligence (AI) but differs from supervised or unsupervised learning, the most commonly known types currently studied in the field of machine learning. During the last decade, RL has attracted greater attention because of success obtained in applications related to games and self-driving cars resulting from its combination with deep-learning architectures such as DRL, which has allowed RL to scale on to previously unsolvable problems and, therefore, solve much larger sequential decision problems.\nRL, also referred to as stochastic approximate dynamic programming, is a goal-directed sequential-learning-from-interaction paradigm. The learner or agent is not told what to do but instead has to learn which actions or decisions yield a maximum reward through interaction with an uncertain environment without losing too much reward along the way. This way of learning from interaction to achieve a goal must be achieved in balance with the exploration and exploitation of possible actions. Another key characteristic of this type of problem is its sequential nature, where the actions taken by the agent affect the environment itself and, therefore, the subsequent data it receives and the subsequent actions to be taken.\nMathematically, such problems are formulated in the framework of the Markov decision process (MDP) that primarily arises in the field of optimal control. An RL problem consists of two principal parts: the agent, or decision-making engine, and the environment, the interactive world for an agent (in this case, the reservoir). Sequentially, at each timestep, the agent takes an action (e.g., changing control rates or deciding a well location) that makes the environment (reservoir) transition from one state to another. Next, the agent receives a reward (e.g., a cash flow) and an observation of the state of the environment (partial or total) before taking the next action.\nAll relevant information informing the agent of the state of the system is assumed to be included in the last state observed by the agent (Markov property). If the agent observes the full environment state once it has acted, the MDP is said to be fully observable; otherwise, a partially observable Markov decision process (POMDP) results. The agent’s objective is to learn policy mapping from states (MDPs) or histories (POMDPs) to actions such that the agent’s cumulated (discounted) reward in the long run is maximized.\n",
        "link": "http://dx.doi.org/10.2118/0921-0046-jpt"
    },
    {
        "id": 28164,
        "title": "Constrained Expectation-Maximization Methods for Effective Reinforcement Learning",
        "authors": "Gang Chen, Yiming Peng, Mengjie Zhang",
        "published": "2018-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8488990"
    },
    {
        "id": 28165,
        "title": "Deep Reinforcement Learning for Network Provisioning in Elastic Optical Networks",
        "authors": "Junior Momo Ziazet, Brigitte Jaumard",
        "published": "2022-5-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45855.2022.9839228"
    },
    {
        "id": 28166,
        "title": "Improving Algorithm Conflict Resolution Manoeuvres with Reinforcement Learning",
        "authors": "Marta Ribeiro, Joost Ellerbroek, Jacco Hoekstra",
        "published": "2022-12-19",
        "citations": 2,
        "abstract": "Future high traffic densities with drone operations are expected to exceed the number of aircraft that current air traffic control procedures can control simultaneously. Despite extensive research on geometric CR methods, at higher densities, their performance is hindered by the unpredictable emergent behaviour from surrounding aircraft. In response, research has shifted its attention to creating automated tools capable of generating conflict resolution (CR) actions adapted to the environment and not limited by man-made rules. Several works employing reinforcement learning (RL) methods for conflict resolution have been published recently. Although proving that they have potential, at their current development, the results of the practical implementation of these methods do not reach their expected theoretical performance. Consequently, RL applications cannot yet match the efficacy of geometric CR methods. Nevertheless, these applications can improve the set of rules that geometrical CR methods use to generate a CR manoeuvre. This work employs an RL method responsible for deciding the parameters that a geometric CR method uses to generate the CR manoeuvre for each conflict situation. The results show that this hybrid approach, combining the strengths of geometric CR and RL methods, reduces the total number of losses of minimum separation. Additionally, the large range of different optimal solutions found by the RL method shows that the rules of geometric CR method must be expanded, catering for different conflict geometries.",
        "link": "http://dx.doi.org/10.3390/aerospace9120847"
    },
    {
        "id": 28167,
        "title": "Meta Reinforcement Learning Based Computation Offloading Strategy for Vehicular Networks",
        "authors": "chao Yang, Yangshui Gao, Hongwei Ding, Rencan Nie, Bo Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDeep learning (DL) and reinforcement learning (RL) based methods can efficiently generate offloading strategies for computational offloading problems in mobile edge computing (MEC) environments. However, the rapid movement of vehicles in the vehicular network causes dynamic changes in the network environment, and DL or RL methods require additional training samples and multiple gradient updates before the model converges, which is time-consuming. In this letter, we propose a meta reinforcement learning-based computation task offloading and resource allocation (MRLOA) algorithm for vehicular networks. Specifically, the MRLOA can converge quickly for a new task with a small number of experience and gradient updates based on a pre-trained meta policy. Simulation results show that our proposed algorithm can more quickly adapt to new computational offloading tasks in the vehicular network environment compared to traditional baseline algorithms.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1614949/v1"
    },
    {
        "id": 28168,
        "title": "Object-Oriented State Abstraction in Reinforcement Learning for Video Games",
        "authors": "Yu Chen, Huizhuo Yuan, Yujun Li",
        "published": "2019-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8848099"
    },
    {
        "id": 28169,
        "title": "Calibrating Dead Reckoning with Deep Reinforcement Learning",
        "authors": "Sangmin Lee, Hwangnam Kim",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apcc55198.2022.9943735"
    },
    {
        "id": 28170,
        "title": "Similar Assembly State Discriminator for Reinforcement Learning-Based Robotic Connector Assembly",
        "authors": "Jun-Wan Yun, Minwoo Na, Yuhyeon Hwang, Jae-Bok Song",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4622927"
    },
    {
        "id": 28171,
        "title": "Virtual Autonomous Vehicle Using Deep Reinforcement Learning",
        "authors": "Vanita Jain, Aditya Chaudhry, Manas Batra, Prakhar Gupta",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3607824"
    },
    {
        "id": 28172,
        "title": "Autonomous Surface Vessel Obstacle Avoidance Based on Hierarchical Reinforcement Learning With Potential Field Method",
        "authors": "Chang Zhou, Lei Wang, Huacheng He, Shangyu Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.0005152v"
    },
    {
        "id": 28173,
        "title": "Batch RL, Experience-Replay, DQN, LSPI, Gradient TD",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-13"
    },
    {
        "id": 28174,
        "title": "Application of Reinforcement Learning for Electric Power System",
        "authors": "Yingqi Zhou",
        "published": "2021-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iaecst54258.2021.9695746"
    },
    {
        "id": 28175,
        "title": "A reinforcement learning algorithm shapes maternal care in mice",
        "authors": "Yunyao Xie, Longwen Huang, Alberto Corona, Alexa H. Pagliaro, Stephen D. Shea",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe neural substrates for processing classical rewards such as food or drugs of abuse are well-understood. In contrast, the mechanisms by which organisms perceive social contact as rewarding and subsequently modify their interactions are unclear. Here we tracked the gradual emergence of a repetitive and highly-stereotyped parental behavior and show that trial-by-trial performance correlates with the history of midbrain dopamine (DA) neuron activity. We used a novel behavior paradigm to manipulate the subject’s expectation of imminent pup contact and show that DA signals conform to reward prediction error, a fundamental component of reinforcement learning (RL). Finally, closed-loop optogenetic inactivation of DA neurons at the onset of pup contact dramatically slowed emergence of parental care. We conclude that this prosocial behavior is shaped by an RL mechanism in which social contact itself is the primary reward.One-Sentence SummaryMaternal interactions with offspring are shaped by a dopaminergic reinforcement learning mechanism.",
        "link": "http://dx.doi.org/10.1101/2022.03.21.485130"
    },
    {
        "id": 28176,
        "title": "Critical Discussion and Outlook",
        "authors": "Schirin Bär",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-39179-9_8"
    },
    {
        "id": 28177,
        "title": "Hybrid IRS-Assisted Secure Satellite Downlink Communications: A Fast Deep Reinforcement Learning Approach",
        "authors": "Quynh Ngo, Tran Khoa Phan, Abdun Mahmood, Wei Xiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper studies a secure satellite-terrestrial communication system assisted by a hybrid intelligent reflecting surface (IRS). The hybrid IRS, which composes of active and passive reflecting elements, is deployed to enhance the secure communication from the satellite to multiple users against multiple eavesdroppers. A joint design optimization problem for the satellite beamforming and the hybrid IRS interaction is formulated to maximize the system worst-case secrecy rate under time-varying channel conditions. With high system dynamic and complexity, deep reinforcement learning (DRL) is employed to solve the non-convex optimization problem. We propose a fast DRL algorithm, namely deep PDS-DPG, to obtain the robust secure beamforming design for satellite and hybrid IRS. Numerical results show a better learning efficiency of the proposed algorithm as to the state-of-the-art deep deterministic policy gradient (DDPG) algorithm with comparable system secrecy performance. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20478438"
    },
    {
        "id": 28178,
        "title": "Monopoly Using Reinforcement Learning",
        "authors": "Edupuganti Arun, Harikrishna Rajesh, Debarka Chakrabarti, Harikiran Cherala, Koshy George",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon.2019.8929523"
    },
    {
        "id": 28179,
        "title": "A Reinforcement Learning Approach for Mobile Beamforming",
        "authors": "Anastasios Dimas, Konstantinos Diamantaras, Athina Petropulu",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf44664.2019.9048888"
    },
    {
        "id": 28180,
        "title": "Superstitious learning of abstract order from random reinforcement",
        "authors": "Yuhao Jin, Greg Jensen, Jacqueline Gottlieb, Vincent Ferrera",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2022.1067-0"
    },
    {
        "id": 28181,
        "title": "Automatic Generation Control with Competing GENCOs-A Reinforcement Learning Based Approach",
        "authors": "Devika Jay, K.S Swarup",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npsc.2018.8771789"
    },
    {
        "id": 28182,
        "title": "Model-Free Indirect RL: Monte Carlo",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_3"
    },
    {
        "id": 28183,
        "title": "Reinforcement Learning with Explainability for Traffic Signal Control",
        "authors": "Stefano Giovanni Rizzo, Giovanna Vantini, Sanjay Chawla",
        "published": "2019-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8917519"
    },
    {
        "id": 28184,
        "title": "Supervised Reinforcement Learning via Value Function",
        "authors": "Yaozong Pan, Jian Zhang, Chunhui Yuan, Haitao Yang",
        "published": "2019-4-24",
        "citations": 1,
        "abstract": "Using expert samples to improve the performance of reinforcement learning (RL) algorithms has become one of the focuses of research nowadays. However, in different application scenarios, it is hard to guarantee both the quantity and quality of expert samples, which prohibits the practical application and performance of such algorithms. In this paper, a novel RL decision optimization method is proposed. The proposed method is capable of reducing the dependence on expert samples via incorporating the decision-making evaluation mechanism. By introducing supervised learning (SL), our method optimizes the decision making of the RL algorithm by using demonstrations or expert samples. Experiments are conducted in Pendulum and Puckworld scenarios to test the proposed method, and we use representative algorithms such as deep Q-network (DQN) and Double DQN (DDQN) as benchmarks. The results demonstrate that the method adopted in this paper can effectively improve the decision-making performance of agents even when the expert samples are not available.",
        "link": "http://dx.doi.org/10.3390/sym11040590"
    },
    {
        "id": 28185,
        "title": "Segmented Actor-Critic-Advantage Architecture for Reinforcement Learning Tasks",
        "authors": "Martin Kaloev, Georgi Krastev",
        "published": "2022-2-28",
        "citations": 0,
        "abstract": "The article focuses on experiments with a multi module neural networks type of architecture for neuron-like machine used in reinforcing learning. This type of architecture can be used to solve complex robotic or policy optimization tasks and allows segmented storage of trained memory. Such technique speeds up the training process compared to existing actor-critical algorithms.",
        "link": "http://dx.doi.org/10.18421/tem111-27"
    },
    {
        "id": 28186,
        "title": "Time Budget Management in Multifunction Radars Using Reinforcement Learning",
        "authors": "Petteri Pulkkinen, Tuomas Aittomaki, Anders Strom, Visa Koivunen",
        "published": "2021-5-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2147009.2021.9455344"
    },
    {
        "id": 28187,
        "title": "A Reinforcement Learning Approach To Synthesizing Climbing Movements",
        "authors": "Kourosh Naderi, Amin Babadi, Shaghayegh Roohi, Perttu Hamalainen",
        "published": "2019-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8848127"
    },
    {
        "id": 28188,
        "title": "A Review on Reinforcement Learning enabled Cooperative Spectrum Sensing",
        "authors": "Thi Thu Hien Pham, Sungrae Cho",
        "published": "2023-1-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin56518.2023.10048946"
    },
    {
        "id": 28189,
        "title": "Multi-Copy Relay Node Selection Strategy Based on Reinforcement Learning",
        "authors": "Yang Gao, Fuquan Zhang",
        "published": "2023-7-4",
        "citations": 0,
        "abstract": "Delay tolerant networks (DTNs), are characterized by their difficulty in establishing end-to-end paths and and large message propagation delays. To control network overhead costs, reduce message delays, and improve delivery rates in DTNs, it is essential to not only delete messages that have reached their destination but also to more precisely determine appropriate relay nodes. Based on the above goals, this paper constructs a multi-copy relay node selection router algorithm based on Q-lambda reinforcement learning with reference to the idea of community division (QLCR). In community division, if a node has the highestdegree, it is considered the core node, and nodes with similar interests and structural properties are divided into a community. Node degree refers to the number of nodes associated with the node, indicating its importance in the network. Structural similarity determines the distance between nodes. The selection of relay nodes considers node degree, interests, and structural similarity. The Q-lambda reinforcement learning algorithm enables each node to learn from the entire network, setting corresponding reward values based on encountered nodes meeting the specified conditions. Through iterative processes, the node with the most cumulative reward value is chosen as the final relay node. Experimental results demonstrate that the proposed algorithm achieves a high delivery rate while maintaining low network overhead and delay.",
        "link": "http://dx.doi.org/10.3390/s23136131"
    },
    {
        "id": 28190,
        "title": "A Financial Market Trading Strategy: Improved Deep Reinforcement Learning System Via Multi-Transformer",
        "authors": "ZR Wang, Li Ping, Wu Zhao, GJY Nian",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4516320"
    },
    {
        "id": 28191,
        "title": "Cerebellum: Reinforcement Learning Robotic Arm Control System with Hardware Adaptability",
        "authors": "Shizhuo Zhang, Yue Fang, Sujata Saini, Jia Liu, Xibao Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper proposes a meta-learning and reinforcement-learning based control system ”Cerebellum” for robotic arms to address the issue of adapting to hardware parameter changes. By leveraging the con- cept of meta-learning, the system achieves adaptive control by learn- ing from few samples. Experimental results show our ”Cerebellum” reach 90.09% accuracy in a simulation environment with control- ling newly unseen robotic arm. However, challenges remain in hard- ware experiments due to image distortions and Mechanical collide issues. Further research is needed to overcome these challenges.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3572240/v1"
    },
    {
        "id": 28192,
        "title": "An Actor-Critic Hierarchical Reinforcement Learning Model for Course Recommendation",
        "authors": "Kun Liang, Guoqiang Zhang, Jinhui Guo, Wentao Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract—Online learning platforms provide diverse course resources, but this often result in the issue of information overload. Learners always want to learn courses that are appropriate for their knowledge level and preferences quickly and accurately. Effective course recommendation plays a key role in helping learners select appropriate courses and improving the efficiency of online learning. However, when a user is enrolled in multiple courses, Existing course recommendation methods face the challenge in accurately recommending the target course that is most relevant to the user, because of the noise courses. In this paper, we propose a novel reinforcement learning model named Actor-Critic Hierarchical Reinforcement Learning (ACHRL). The model incorporates the Actor-Critic method to construct the profile reviser. This can remove noise courses and make personalized course recommendation effectively. Furthermore, we propose a policy gradient based on temporal difference error to reduce the variance in the training process, to speed up the convergence of the model, and improves the accuracy of the recommendation. We evaluate the proposed model on two real datasets, and the experimental results show that the proposed model is significantly outperforms the existing recommendation models (improving 3.77% to 13.66% in terms of HR@5).",
        "link": "http://dx.doi.org/10.20944/preprints202310.1367.v1"
    },
    {
        "id": 28193,
        "title": "Hybrid IRS-Assisted Secure Satellite-Terrestrial Communications: A Fast Deep Reinforcement Learning Approach",
        "authors": "Quynh Ngo, Tran Khoa Phan, Abdun Mahmood, Wei Xiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper studies a secure satellite-terrestrial communication system assisted by a hybrid intelligent reflecting surface (IRS). The hybrid IRS, which composes of active and passive reflecting elements, is deployed to enhance the secure communication from the satellite to multiple users against multiple eavesdroppers. A joint design optimization problem for the satellite beamforming and the hybrid IRS interaction is formulated to maximize the system worst-case secrecy rate under time-varying channel conditions. With high system dynamic and complexity, deep reinforcement learning (DRL) is employed to solve the non-convex optimization problem. We propose a fast DRL algorithm, namely deep PDS-DPG, to obtain the robust secure beamforming design for satellite and hybrid IRS. Numerical results show a better learning efficiency of the proposed algorithm as to the state-of-the-art deep deterministic policy gradient (DDPG) algorithm with comparable system secrecy performance. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20478438.v1"
    },
    {
        "id": 28194,
        "title": "Inverse Reinforcement Learning of Interaction Dynamics from Demonstrations",
        "authors": "Mostafa Hussein, Momotaz Begum, Marek Petrik",
        "published": "2019-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8793867"
    },
    {
        "id": 28195,
        "title": "Human-assisted reinforcement learning demonstrated on the Flappy Bird Game",
        "authors": "Jana Ristovska, Domen Šoberl",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26493/scores23.13"
    },
    {
        "id": 28196,
        "title": "Deep Reinforcement Learning in Virtual Environments",
        "authors": "Feng Lin, Hock Soon Seah",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-08234-9_440-1"
    },
    {
        "id": 28197,
        "title": "A Novel Exploration Technique For Multi-Agent Reinforcement Learning",
        "authors": "Devarani Devi Ningombam",
        "published": "2022-10-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcat55367.2022.9971921"
    },
    {
        "id": 28198,
        "title": "Application of Multi-Agent Deep Reinforcement Learning to Optimize Real-World Traffic Signal Controls",
        "authors": "Maxim Friesen, Tian Tan, Jürgen Jasperneite, Jie Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Increasing traffic congestion leads to significant costs associated by additional travel delays, whereby poorly configured signaled intersections are a common bottleneck and root cause. Traditional traffic signal control (TSC) systems employ rule-based or heuristic methods to decide signal timings, while adaptive TSC solutions utilize a traffic-actuated control logic to increase their adaptability to real-time traffic changes. However, such systems are expensive to deploy and are often not flexible enough to adequately adapt to the volatility of today's traffic dynamics. More recently, this problem became a frontier topic in the domain of deep reinforcement learning (DRL) and enabled the development of multi-agent DRL approaches that could operate in environments with several agents present, such as traffic systems with multiple signaled intersections. However, most of these proposed approaches were validated using artificial traffic grids. This paper therefore presents a case study, where real-world traffic data from the town of Lemgo in Germany is used to create a realistic road model within VISSIM. A multi-agent DRL setup, comprising multiple independent deep Q-networks, is applied to the simulated traffic network. Traditional rule-based signal controls, currently employed in the real world at the studied intersections, are integrated in the traffic model with LISA+ and serve as a performance baseline. Our performance evaluation indicates a significant reduction of traffic congestion when using the RL-based signal control policy over the conventional TSC approach in LISA+. Consequently, this paper reinforces the applicability of RL concepts in the domain of TSC engineering by employing a highly realistic traffic model.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16974493.v1"
    },
    {
        "id": 28199,
        "title": "Nonlinear Value Function Approximation",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_7"
    },
    {
        "id": 28200,
        "title": "Policy Gradient",
        "authors": "Ruitong Huang, Tianyang Yu, Zihan Ding, Shanghang Zhang",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_5"
    },
    {
        "id": 28201,
        "title": "Action Masked Deep Reinforcement learning for Controlling Industrial Assembly Lines",
        "authors": "Ali Mohamed Ali, Luca Tirel",
        "published": "2023-6-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiiot58121.2023.10174426"
    },
    {
        "id": 28202,
        "title": "Multi-timescale reinforcement learning in the brain",
        "authors": "Paul Masset, Pablo Tano, HyungGoo R. Kim, Athar N. Malik, Alexandre Pouget, Naoshige Uchida",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractTo thrive in complex environments, animals and artificial agents must learn to act adaptively to maximize fitness and rewards. Such adaptive behavior can be learned through reinforcement learning1, a class of algorithms that has been successful at training artificial agents2–6and at characterizing the firing of dopamine neurons in the midbrain7–9. In classical reinforcement learning, agents discount future rewards exponentially according to a single time scale, controlled by the discount factor. Here, we explore the presence of multiple timescales in biological reinforcement learning. We first show that reinforcement agents learning at a multitude of timescales possess distinct computational benefits. Next, we report that dopamine neurons in mice performing two behavioral tasks encode reward prediction error with a diversity of discount time constants. Our model explains the heterogeneity of temporal discounting in both cue-evoked transient responses and slower timescale fluctuations known as dopamine ramps. Crucially, the measured discount factor of individual neurons is correlated across the two tasks suggesting that it is a cell-specific property. Together, our results provide a new paradigm to understand functional heterogeneity in dopamine neurons, a mechanistic basis for the empirical observation that humans and animals use non-exponential discounts in many situations10–14, and open new avenues for the design of more efficient reinforcement learning algorithms.",
        "link": "http://dx.doi.org/10.1101/2023.11.12.566754"
    },
    {
        "id": 28203,
        "title": "Algorithmic Trading Using Continuous Action Space Deep Reinforcement Learning",
        "authors": "naseh majidi, Mahdi Shamsi, Farokh Marvasti",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4276310"
    },
    {
        "id": 28204,
        "title": "A Reinforcement Learning Based Design of Compressive Sensing Systems for Human Activity Recognition",
        "authors": "Guocheng Liu, Rui Ma, Qi Hao",
        "published": "2018-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsens.2018.8589690"
    },
    {
        "id": 28205,
        "title": "Deep Reinforcement Learning for Wind-Power: An Overview",
        "authors": "Bashar M Ali",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iiceta57613.2023.10351394"
    },
    {
        "id": 28206,
        "title": "Reinforcement learning in MIMO wireless networks with energy harvesting",
        "authors": "Hoda Ayatollahi, Cristiano Tapparello, Wendi Heinzelman",
        "published": "2017-5",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2017.7997229"
    },
    {
        "id": 28207,
        "title": "Deep Reinforcement Learning for Multiparameter Optimization in de novo Drug Design",
        "authors": "Niclas Ståhl, Göran Falkman, Alexander Karlsson, Gunnar Mathiason, Jonas Boström",
        "published": "No Date",
        "citations": 0,
        "abstract": "In medicinal chemistry programs it is key to design and make compounds that are efficacious and safe. This is a long, complex and difficult multi-parameter optimization process, often including several properties with orthogonal trends. New methods for the automated design of compounds against profiles of multiple properties are thus of great value. Here we present a fragment-based reinforcement learning approach based on an actor-critic model, for the generation of novel molecules with optimal properties. The actor and the critic are both modelled with bidirectional long short-term memory (LSTM) networks. The AI method learns how to generate new compounds with desired properties by starting from an initial set of lead molecules and then improve these by replacing some of their fragments. A balanced binary tree based on the similarity of fragments is used in the generative process to bias the output towards structurally similar molecules. The method is demonstrated by a case study showing that 93% of the generated molecules are chemically valid, and a third satisfy the targeted objectives, while there were none in the initial set.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.7990910.v2"
    },
    {
        "id": 28208,
        "title": "RLAuth: A Risk-Based Authentication System Using Reinforcement Learning",
        "authors": "Claudy Picard, Samuel Pierre",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3286376"
    },
    {
        "id": 28209,
        "title": "Secure wireless network system based on deep reinforcement learning network",
        "authors": "Xiaolong Yan, Yingying Feng",
        "published": "2022-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ijleo.2022.170167"
    },
    {
        "id": 28210,
        "title": "Reinforcement Learning-Based Bus Holding for High-Frequency Services",
        "authors": "Francesco Alesiani, Konstantinos Gkiotsalitis",
        "published": "2018-11",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2018.8569473"
    },
    {
        "id": 28211,
        "title": "Risk-sensitive Inverse Reinforcement Learning via Coherent Risk Models",
        "authors": "Anirudha Majumdar, Sumeet Singh, Ajay Mandlekar, Marco Pavone",
        "published": "2017-7-12",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2017.xiii.069"
    },
    {
        "id": 28212,
        "title": "Safe Reinforcement Learning With Model Uncertainty Estimates",
        "authors": "Bjorn Lutjens, Michael Everett, Jonathan P. How",
        "published": "2019-5",
        "citations": 76,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8793611"
    },
    {
        "id": 28213,
        "title": "Application of Multi-Agent Deep Reinforcement Learning to Optimize Real-World Traffic Signal Controls",
        "authors": "Maxim Friesen, Tian Tan, Jürgen Jasperneite, Jie Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Increasing traffic congestion leads to significant costs associated by additional travel delays, whereby poorly configured signaled intersections are a common bottleneck and root cause. Traditional traffic signal control (TSC) systems employ rule-based or heuristic methods to decide signal timings, while adaptive TSC solutions utilize a traffic-actuated control logic to increase their adaptability to real-time traffic changes. However, such systems are expensive to deploy and are often not flexible enough to adequately adapt to the volatility of today's traffic dynamics. More recently, this problem became a frontier topic in the domain of deep reinforcement learning (DRL) and enabled the development of multi-agent DRL approaches that could operate in environments with several agents present, such as traffic systems with multiple signaled intersections. However, most of these proposed approaches were validated using artificial traffic grids. This paper therefore presents a case study, where real-world traffic data from the town of Lemgo in Germany is used to create a realistic road model within VISSIM. A multi-agent DRL setup, comprising multiple independent deep Q-networks, is applied to the simulated traffic network. Traditional rule-based signal controls, currently employed in the real world at the studied intersections, are integrated in the traffic model with LISA+ and serve as a performance baseline. Our performance evaluation indicates a significant reduction of traffic congestion when using the RL-based signal control policy over the conventional TSC approach in LISA+. Consequently, this paper reinforces the applicability of RL concepts in the domain of TSC engineering by employing a highly realistic traffic model.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16974493"
    },
    {
        "id": 28214,
        "title": "Reinforcement Learning based System-of-Systems Approach for UAV Swarms Behavioral Evolution",
        "authors": "Ramakrishnan Raman, Anitha Murugesan",
        "published": "2022-4-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/syscon53536.2022.9773900"
    },
    {
        "id": 28215,
        "title": "Game AI using Reinforcement Learning",
        "authors": "Amogh Sawant, Shahid Shaikh, Dharmesh Sharma",
        "published": "2022-2-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aisp53593.2022.9760576"
    },
    {
        "id": 28216,
        "title": "Online Data-Driven Inverse Reinforcement Learning for Deterministic Systems",
        "authors": "Hamed Jabbari Asl, Eiji Uchibe",
        "published": "2022-12-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci51031.2022.10022226"
    },
    {
        "id": 28217,
        "title": "Online inverse reinforcement learning for nonlinear systems",
        "authors": "Ryan Self, Michael Harlan, Rushikesh Kamalapurkar",
        "published": "2019-8",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccta.2019.8920458"
    },
    {
        "id": 28218,
        "title": "Multi-Agent Deep Reinforcement Learning Based Distributed Resource Allocation",
        "authors": "Odilbek Urmonov, HyungWon Kim",
        "published": "2021-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscas51556.2021.9401656"
    },
    {
        "id": 28219,
        "title": "Constraint-Guided Reinforcement Learning: Augmenting the Agent-Environment-Interaction",
        "authors": "Helge Spieker",
        "published": "2021-7-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533996"
    },
    {
        "id": 28220,
        "title": "Dissociating time and number in reinforcement rate learning",
        "authors": "Joseph M. Austen, Corran Pickering, Rolf Sprengel, David J. Sanderson",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1101/302372"
    },
    {
        "id": 28221,
        "title": "Prioritized Sampling with Intrinsic Motivation in Multi-Task Reinforcement Learning",
        "authors": "Carlo D'Eramo, Georgia Chalvatzaki",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892973"
    },
    {
        "id": 28222,
        "title": "Clustered Autoencoded Variational Inverse Reinforcement Learning",
        "authors": "Yuling Max Chen",
        "published": "2022-1-1",
        "citations": 0,
        "abstract": "Abstract\nVariational Auto-Encoder (VAE) is a handy and computationally friendly Bayesian tool for Inverse Reinforcement Learning (IRL) problems, with the native setting of absent reward functions in a Markov Decision Process (MDP). However, recent works mainly deal with single reward, which turn out to be insufficient for complex dynamic environments with multiple demonstrators of various characteristics (hence multiple reward functions). This paper extends the dimensionality of reward (from ℝ to ℝ\nK\n) by incorporating a latent embedding and clustering step on top of a scalable Bayesian IRL model, which enhances her applicability to multi-reward scenarios. We introduce our method, Clustered Autoencoded Variational Inverse Reinforcement Learning (CAVIRL), which is able to approximate multiple posterior reward functions and learn the corresponding policies for experts of various characteristics and skills. As a by-product, the proposed model also thrives to determine the number of clusters K on her own, as opposed to the competing multi-reward imitation learning models that require K to be prespecified. We trained the proposed model within a grid world with multiple types of players, where we achieved 100% correctness in determining the number of players’ types and 80%-83.9% match between the model-learned policies and the players’ demonstrations from the data.",
        "link": "http://dx.doi.org/10.1515/stat-2022-0109"
    },
    {
        "id": 28223,
        "title": "Demonstration-Efficient Inverse Reinforcement Learning in Procedurally Generated Environments",
        "authors": "Alessandro Sestini, Alexander Kuhnle, Andrew D. Bagdanov",
        "published": "2021-8-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog52621.2021.9619084"
    },
    {
        "id": 28224,
        "title": "Deep Reinforcement Learning Based Mobile Robot Navigation Using Sensor Fusion",
        "authors": "Kejian Yan, Jianqi Gao, Yanjie Li",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240555"
    },
    {
        "id": 28225,
        "title": "Deep reinforcement learning for SPORADIC rewards with HUMAN experience",
        "authors": "Harshit Sinha",
        "published": "2017-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icecct.2017.8117866"
    },
    {
        "id": 28226,
        "title": "Applicability of Reinforcement Learning Algorithms to Usage Parameter Control",
        "authors": "Antonios F. Atlasis, Athanasios V. Vasilakos",
        "published": "2018-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781315220178-8"
    },
    {
        "id": 28227,
        "title": "Grundbegriffe des Bestärkenden Lernens",
        "authors": "Uwe Lorenz",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-662-61651-2_2"
    },
    {
        "id": 28228,
        "title": "Reinforcement Learning in a Large Scale Photonic Network",
        "authors": "Daniel Brunner, Maxime Jacquot, Ingo Fischer, Laurent Larger",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1364/laop.2018.w2a.2"
    },
    {
        "id": 28229,
        "title": "Deep Reinforcement Learning Framework to Optimize Long-Range Transportation Plans",
        "authors": "Jiangbo (Gabe) Yu, Michael Hyland",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4314526"
    },
    {
        "id": 28230,
        "title": "Adjusting Street Plans Using Deep Reinforcement Learning",
        "authors": "Ahmed Alhassan, Muhammed Saeed",
        "published": "2021-2-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccceee49695.2021.9429641"
    },
    {
        "id": 28231,
        "title": "Bayesian Deep Multi-Agent Multimodal Reinforcement Learning for Embedded Systems in Games, Natural Language Processing and Robotics",
        "authors": "Ioannis Kourouklides",
        "published": "No Date",
        "citations": 0,
        "abstract": "Nowadays, Machine Learning is one of the most dynamic fields, as it attracts strong research interest from both industry and academia alike. It is not surprising that a huge amount of funding from government agencies, universities, Tech giants and well-funded startups is currently being allocated exclusively to this field. Reinforcement Learning, one of the three major subfields of Machine Learning, has recently gained a tremendous traction due to the fact that algorithms can run more efficiently. This is mainly due to two reasons, firstly affordable and portable hardware, such as mobile phones, wearables and Internet of Things devices, now has the capacity to run these algorithms and secondly, new methods and models are being proposed that deal with the matter of efficiency from an algorithmic point of view. This proposal is concerned with dealing with open challenges in memory efficiency, while devising and applying such Reinforcement Learning algorithms for embedded systems in the domains of Games, Natural Language Processing and Robotics using Deep Learning models and Bayesian inference, a very powerful framework. Natural Language Processing is a domain of Machine Learning in which the input is given in the form of a text from a natural language that human agents use for everyday communication. It is also a domain that still faces a series of ongoing challenges, as opposed to more saturated domains, such as Computer Vision. One of them is the fact that ground truth is difficult to be decided due to the nature of text in general. Other challenges include the personalized type and tone of the conversation held by the human agents, such as formal, informal, aggressive, polite, etc. Therefore, this proposal deals with all of these matters, mainly in the subdomain of Question Answering systems, also known as chatbots, in a multi-agent setting.",
        "link": "http://dx.doi.org/10.31219/osf.io/sjrkh"
    },
    {
        "id": 28232,
        "title": "Reinforcement Learning of Bimanual Robot Skills",
        "authors": "Adrià Colomé, Carme Torras",
        "published": "2020",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-26326-3"
    },
    {
        "id": 28233,
        "title": "Vision Based Drone Obstacle Avoidance by Deep Reinforcement Learning",
        "authors": "Zhihan Xue, Tad Gonsalves",
        "published": "2021-8-19",
        "citations": 14,
        "abstract": "Research on autonomous obstacle avoidance of drones has recently received widespread attention from researchers. Among them, an increasing number of researchers are using machine learning to train drones. These studies typically adopt supervised learning or reinforcement learning to train the networks. Supervised learning has a disadvantage in that it takes a significant amount of time to build the datasets, because it is difficult to cover the complex and changeable drone flight environment in a single dataset. Reinforcement learning can overcome this problem by using drones to learn data in the environment. However, the current research results based on reinforcement learning are mainly focused on discrete action spaces. In this way, the movement of drones lacks precision and has somewhat unnatural flying behavior. This study aims to use the soft-actor-critic algorithm to train a drone to perform autonomous obstacle avoidance in continuous action space using only the image data. The algorithm is trained and tested in a simulation environment built by Airsim. The results show that our algorithm enables the UAV to avoid obstacles in the training environment only by inputting the depth map. Moreover, it also has a higher obstacle avoidance rate in the reconfigured environment without retraining.",
        "link": "http://dx.doi.org/10.3390/ai2030023"
    },
    {
        "id": 28234,
        "title": "Graph Neural Networks and Reinforcement Learning for Behavior Generation in Semantic Environments",
        "authors": "Patrick Hart, Alois Knoll",
        "published": "2020-10-19",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv47402.2020.9304738"
    },
    {
        "id": 28235,
        "title": "A New Sample-Efficient PAC Reinforcement Learning Algorithm",
        "authors": "Ashkan Zehfroosh, Herbert G. Tanner",
        "published": "2020-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/med48518.2020.9182985"
    },
    {
        "id": 28236,
        "title": "Energy-Efficient Resource Allocation with Dynamic Cache Using Reinforcement Learning",
        "authors": "Zeyu Hu, Zexu Li, Yong Li",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcwkshps45667.2019.9024408"
    },
    {
        "id": 28237,
        "title": "Quadrotor Control using Reinforcement Learning under Wind Disturbance",
        "authors": "Songshuo Lu, Yanjie Li, Zihan Liu",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10326621"
    },
    {
        "id": 28238,
        "title": "How age affects reinforcement learning",
        "authors": "Itamar Lerner, Ravi Sojitra, Mark Gluck",
        "published": "2018-11-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18632/aging.101649"
    },
    {
        "id": 28239,
        "title": "Reinforcement Learning-Based Controller Design for Wind Turbine Yaw Control System",
        "authors": "Rahmat Allah Mirzaei, Iman Larki, Meisam Farajollahi, Seyed Mahdi Shariatzadeh",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4631201"
    },
    {
        "id": 28240,
        "title": "Reinforcement Learning Inspired by Psychology and Neuroscience",
        "authors": "Hongkun Wu",
        "published": "2023-2-7",
        "citations": 0,
        "abstract": "Decision-making is a crucial intelligence shared by animals and humans and it helps them to act in an intricate environment to seek rewards and avoid punishments. Psychologists first became interested in this ability and studied the conditional behavioral problems associated with it, then these studies have further led to the need for unified quantitative explanation models, among which Reinforcement learning is still the most convincing and data-backed model today. The model itself, in turn, facilitates research in neuroscience. In this paper, the researcher first introduces the original framework of reinforcement learning and the potential neural correlates to it. Then the paper reviews new developments in reinforcement learning algorithms that address the limitations of the original model as well as variants further inspired by neuroscience. Finally, the study highlights some new directions for future research. This study focuses on the evolution of reinforcement learning algorithms inspired by neuroscience, shows the relationship of mutual promotion and common development between reinforcement Learning and neuroscience, and clarifies some concerns for future exploration.",
        "link": "http://dx.doi.org/10.54097/ehss.v8i.4673"
    },
    {
        "id": 28241,
        "title": "Deep Reinforcement Learning for Asset Allocation in US Equities",
        "authors": "Miquel Noguer i Alonso, Sonam Srivastava",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3711487"
    },
    {
        "id": 28242,
        "title": "Multi-Task Decomposition Architecture based Deep Reinforcement Learning for Obstacle Avoidance",
        "authors": "Wengang Zhang, Cong He, Teng Wang",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327414"
    },
    {
        "id": 28243,
        "title": "Dynamic Optimal Portfolio with Trading Cost Constraints via Deep Reinforcement Learning",
        "authors": "Chengyu Zhang, Abdallah Aaraba",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4221316"
    },
    {
        "id": 28244,
        "title": "Deep Reinforcement Learning in Virtual Environments",
        "authors": "Feng Lin, Hock Soon Seah",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-23161-2_440"
    },
    {
        "id": 28245,
        "title": "Multi-Condition Multi-Objective Optimization Using Deep Reinforcement Learning",
        "authors": "Sejin Kim, Innyoung Kim, Donghyun You",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4024081"
    },
    {
        "id": 28246,
        "title": "Efficient Practice for Deep Reinforcement Learning",
        "authors": "Venkata Sai Santosh Ravi Teja Kancharla, Minwoo Lee",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci44817.2019.9003129"
    },
    {
        "id": 28247,
        "title": "A Self-Organization Reconstruction Method of Esn Reservoir Structure Based on Reinforcement Learning",
        "authors": "Wei Guo, Huan Yao, Yingqin Zhu, ZhaoZhao Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4531602"
    },
    {
        "id": 28248,
        "title": "Dynamic Traffic Light Control with Reinforcement Learning Based on Gnn Prediction",
        "authors": "Chenguang Zhao, Gang Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4040526"
    },
    {
        "id": 28249,
        "title": "Dynamic Vector Quantization for Reinforcement Learning (DVQRL)",
        "authors": "Evans Miriti, Andrew Mwaura",
        "published": "2018-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscmi.2018.8703233"
    },
    {
        "id": 28250,
        "title": "Temporal Alignment for History Representation in Reinforcement Learning",
        "authors": "Aleksandr Ermolov, Enver Sangineto, Nicu Sebe",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956553"
    },
    {
        "id": 28251,
        "title": "Evolutionary Reinforcement Learning for Adaptively Detecting Database Intrusions",
        "authors": "",
        "published": "2022-5-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1093/jigpal/jzaa030"
    },
    {
        "id": 28252,
        "title": "Modelling Grid Navigation Using Reinforcement Learning Linear Ballistic Accumulators",
        "authors": "Gautham Venugopal, Bapi Surampudi Raju",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10192038"
    },
    {
        "id": 28253,
        "title": "Design of Reinforcement Learning based PI controller for nonlinear Multivariable System",
        "authors": "P Kranthi Kumar, Ketan P Detroja",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178182"
    },
    {
        "id": 28254,
        "title": "A Customizable Reinforcement Learning Environment for Semiconductor Fab Simulation",
        "authors": "Benjamin Kovacs, Pierre Tassel, Martin Gebser, Georg Seidel",
        "published": "2022-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc57314.2022.10015524"
    },
    {
        "id": 28255,
        "title": "Multimodal Surrogate-Enhanced Deep Reinforcement Learning for Policy Optimization of Subsurface Energy Systems",
        "authors": "Zhongzheng Wang, Yuntian Chen, Dongxiao Zhang, Guodong Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4552911"
    },
    {
        "id": 28256,
        "title": "Advanced Policy Gradient Methods",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_11"
    },
    {
        "id": 28257,
        "title": "GNN-based Deep Reinforcement Learning with Adversarial Training for Robust Optimization of Modern Tactical Communication Systems",
        "authors": "Johannes Loevenich, Roberto Rigolin F. Lopes",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper investigates the feasibility of a Graph Neural Network (GNN)-based Deep Reinforcement Learning (DRL) for tackling complex optimization problems in modern communication systems deployed to tactical networks. Our methodology consists of three interacting agents: an environment builder agent responsible for generating complex network graph environments, a DRL agent situated within the control plane that possesses a global view of the current network state and makes decisions based on information gathered from various layers of the multi-layer tactical system, and an adversary designed to perturb the DRL agent, thereby evaluating its performance and robustness against data perturbations. Our numerical results indicate that enabling GNN in conjunction with adversarial training is crucial for the agent to learn the underlying network topology and parameters, ultimately enhancing the robustness of modern tactical communication systems operating in hostile environments.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.23476058.v1"
    },
    {
        "id": 28258,
        "title": "Exploring the behavioural spectrum with efficiency vs. fairness goals in Multi-Agent Reinforcement Learning",
        "authors": "Margarida Silva, Zafeiris Kokkinogenis, Jeremy Pitt, Rosaldo Rossetti",
        "published": "No Date",
        "citations": 0,
        "abstract": "The concept of fairness has been studied in philosophy and economics for thousands of years, so human actors in social systems have had plenty of time to ``learn'' what does, and does not, work. Yet, only recently. However, it is a relatively new question how software agents in a multi-agent system can use Reinforcement Learning models to develop an architecture that promotes equality or equity in the distribution of rewards to the agents within the system. Recent significant contributions have focused on optimising for efficiency based on the assumption that efficiency and fairness are opposites to be traded off against each other, but actually, the result of mixing fair and efficient policies is unknown in multi-agent reinforcement learning settings. In this work, we experiment with fair and efficient behaviours jointly, based on an extension of the state-of-the-art model in fairness SOTO that intertwines efficient and equitable recommendations. We analyse the fair versus efficient behavioural spectrum in the Matthew Effect and Traffic Light Control problems, finding some solutions that outperform the baseline SOTO and others that outperform a selfish baseline with comparable architectural design. We conclude it is possible to optimise for fairness and efficiency  and this is important when computation of the reward distribution has to be paid for from the rewards themselves.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19388147"
    },
    {
        "id": 28259,
        "title": "CaiRL: A High-Performance Reinforcement Learning Environment Toolkit",
        "authors": "Per-Arne Andersen, Morten Goodwin, Ole-Christoffer Granmo",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog51982.2022.9893661"
    },
    {
        "id": 28260,
        "title": "Index",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.index"
    },
    {
        "id": 28261,
        "title": "Enhancing Deep Reinforcement Learning with Executable Specifications",
        "authors": "Raz Yerushalmi",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icse-companion58688.2023.00058"
    },
    {
        "id": 28262,
        "title": "Rule-Based Shields Embedded Safe Reinforcement Learning Approach for Electric Vehicle Charging Control",
        "authors": "Yuxiang Guan, Jin Zhang, Wenhao Ma, Liang Che",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4615066"
    },
    {
        "id": 28263,
        "title": "Behavior-Neutral Smart Charging of Plugin Electric Vehicles: Reinforcement Learning Approach",
        "authors": "Vladimir Dyo",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3183795"
    },
    {
        "id": 28264,
        "title": "Reinforcement Learning with An Abrupt Model Change",
        "authors": "Wuxia Chen, Taposh Banerjee, Jemin George, Carl Busart",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408378"
    },
    {
        "id": 28265,
        "title": "From CNNs to Adaptive Filter Design for Digital Image Denoising Using Reinforcement Q-Learning",
        "authors": "Muhammad Alolaiwy, Murat Tanik, Leon Jololian",
        "published": "2021-3-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/southeastcon45413.2021.9401873"
    },
    {
        "id": 28266,
        "title": "Neuroevolution for deep reinforcement learning problems",
        "authors": "David Ha",
        "published": "2018-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3205651.3207875"
    },
    {
        "id": 28267,
        "title": "Intelligent Traffic Light Switch Based on Deep Reinforcement Learning with Attention Mechanism",
        "authors": "Min Wang, Jianqun Cui, Yanan Chang, Libing Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nHow to design a good traffic signal switch scheme is crucial to alleviate transportation pressure. However, some existing traffic signal control methods based on reinforcement learning apply a convolutional neural network (CNN) to extract features from the state matrix, which is too coarse because state matrices sometimes are sparse. These methods use the discrete traffic state encoding (DTSE) approach to build the position and speed matrices. We overcome this problem in two ways: by reducing the number of zero value in the state matrix and using a spatial attention mechanism in convolution operations. This paper redefines the state at the intersection by dividing each entering road with an unequal length and then concatenating them. Our division method can reduce the state matrix’s sparsity by reducing the size of the input matrix and the number of zero value. We then propose a deep Q-learning network (DQN) with spatial attention in CNN to control the traffic light. The spatial attention mechanism can focus on the specific element in the con-volutional feature vectors and overcome the sparse matrix’s difficult problem to extract features. Besides, we set a discount factor β on the reward definition, which can diminish the received reward and alleviate the overestimation caused by DQN. We conduct experiments on the Simulation of Urban MObility (SUMO) with two datasets, a synthetic intersection and Cologne’s traffic intersection in Germany. Experimental results on synthetic intersection demonstrate that our method can reduce delay, travel time, and queue length than three other methods. Results on Cologne confirm that the proposed method can save 23.7%, 9.1%, 4.5% travel time compared with the fixed-time traffic light control, self-organizing traffic light control, and DQN method, respectively.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3585377/v1"
    },
    {
        "id": 28268,
        "title": "Reinforcement Learning of Code Search Sessions",
        "authors": "Wei Li, Shuhan Yan, Beijun Shen, Yuting Chen",
        "published": "2019-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apsec48747.2019.00068"
    },
    {
        "id": 28269,
        "title": "Deep Reinforcement Learning for Economic Battery Dispatch: A Comprehensive Comparison of Algorithms and Experiment Design Choices",
        "authors": "Manuel Sage, Yaoyao Fiona Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4706893"
    },
    {
        "id": 28270,
        "title": "Supervised and Reinforcement Learning from Observations in Reconnaissance Blind Chess",
        "authors": "Timo Bertram, Johannes Furnkranz, Martin Muller",
        "published": "2022-8-21",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog51982.2022.9893588"
    },
    {
        "id": 28271,
        "title": "Reinforcement Learning Models for Adaptive Low Voltage Power System Operation",
        "authors": "Eleni Stai, Matteo Guscetti, Mathias Duckheim, Gabriela Hug",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/powertech55446.2023.10202750"
    },
    {
        "id": 28272,
        "title": "Exploring the behavioural spectrum with efficiency vs. fairness goals in Multi-Agent Reinforcement Learning",
        "authors": "Margarida Silva, Zafeiris Kokkinogenis, Jeremy Pitt, Rosaldo Rossetti",
        "published": "No Date",
        "citations": 0,
        "abstract": "The concept of fairness has been studied in philosophy and economics for thousands of years, so human actors in social systems have had plenty of time to ``learn'' what does, and does not, work. Yet, only recently. However, it is a relatively new question how software agents in a multi-agent system can use Reinforcement Learning models to develop an architecture that promotes equality or equity in the distribution of rewards to the agents within the system. Recent significant contributions have focused on optimising for efficiency based on the assumption that efficiency and fairness are opposites to be traded off against each other, but actually, the result of mixing fair and efficient policies is unknown in multi-agent reinforcement learning settings. In this work, we experiment with fair and efficient behaviours jointly, based on an extension of the state-of-the-art model in fairness SOTO that intertwines efficient and equitable recommendations. We analyse the fair versus efficient behavioural spectrum in the Matthew Effect and Traffic Light Control problems, finding some solutions that outperform the baseline SOTO and others that outperform a selfish baseline with comparable architectural design. We conclude it is possible to optimise for fairness and efficiency  and this is important when computation of the reward distribution has to be paid for from the rewards themselves.",
        "link": "http://dx.doi.org/10.36227/techrxiv.19388147.v2"
    },
    {
        "id": 28273,
        "title": "Improving Adherence to Medication in an Intelligent Environment Using Reinforcement Learning",
        "authors": "Ahsan Ismail, Muddasar Naeem, Umamah Bint Khalid, Musarat Abbas",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nCorrect and timely medication plays an important role in the treatment and recovery of a patient. An intelligent and efficient patient engagement environment ensures enduring health and positive clinical outcomes. Reinforcement Learning (RL) which brought significant impacts in many areas, has useful applications in healthcare as well. In this paper, we introduce an RL-based intelligent environment that can engage the patient to improve adherence, through proper engagement alerts based on user adherence reports, and suggest him/her to improve the adherence by sending the engagement message to the user. The intelligent system uses the RL agent that decides the optimal decision to send the engagement message to the user based on a patient's behavior. The proposed system could be useful to improve adherence to medication and assist the patient in accurately following his/her medication schedule.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3825821/v1"
    },
    {
        "id": 28274,
        "title": "Reinforcement Learning for Cross-Domain Hyper-Heuristics",
        "authors": "Florian Mischek, Nysret Musliu",
        "published": "2022-7",
        "citations": 3,
        "abstract": "In this paper, we propose a new hyper-heuristic approach that uses reinforcement learning to automatically learn the selection of low-level heuristics across a wide range of problem domains.\n\nWe provide a detailed analysis and evaluation of the algorithm components, including different ways to represent the hyper-heuristic state space and reset strategies to avoid unpromising areas of the solution space.\n\nOur methods have been evaluated using HyFlex, a well-known benchmarking framework for cross-domain hyper-heuristics, and compared with state-of-the-art approaches.\n\nThe experimental evaluation shows that our reinforcement-learning based approach produces results that are competitive with the state-of-the-art, including the top participants of the Cross Domain Hyper-heuristic Search Competition 2011.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/664"
    },
    {
        "id": 28275,
        "title": "Model-Based Indirect RL: Dynamic Programming",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_5"
    },
    {
        "id": 28276,
        "title": "Sentence Simplification with Deep Reinforcement Learning",
        "authors": "Xingxing Zhang, Mirella Lapata",
        "published": "2017",
        "citations": 78,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d17-1062"
    },
    {
        "id": 28277,
        "title": "Event-triggered integral reinforcement learning for nonlinear continuous-time systems",
        "authors": "Qichao Zhang, Dongbin Zhao",
        "published": "2017-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci.2017.8280874"
    },
    {
        "id": 28278,
        "title": "Reinforcement Learning with Imitation for Cavity Filter Tuning",
        "authors": "Simon Lindstah, Xiaoyu Lan",
        "published": "2020-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aim43001.2020.9158839"
    },
    {
        "id": 28279,
        "title": "A Deep Reinforcement Learning Approach for Path Following on a Quadrotor",
        "authors": "Bartomeu Rubi, Bernardo Morcego, Ramon Perez",
        "published": "2020-5",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc51009.2020.9143591"
    },
    {
        "id": 28280,
        "title": "Multi-Agent Deep Reinforcement Learning in Vehicular OCC",
        "authors": "Amirul Islam, Leila Musavian, Nikolaos Thomos",
        "published": "2022-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2022-spring54318.2022.9860869"
    },
    {
        "id": 28281,
        "title": "CMARL: A Multi-agent Deep Reinforcement Learning Model with Emphasis on Communication Content",
        "authors": "Ande Chang, Yuting Ji, Chunguang Wang, Yiming Bie",
        "published": "No Date",
        "citations": 0,
        "abstract": "In recent years, the viability of employing multi-agent reinforcement learning technology for adaptive traffic signal control has been extensively validated. However, owing to restricted communication among agents and the partial observability of the traffic environment, the process of mapping road network states to actions encounters numerous challenges. To address this problem, this paper proposes a multi-agent deep reinforcement learning model with an emphasis on communication content (CMARL). The model decouples the complex relationships between multi-signal agents through centralized training and decentralized execution. Specifically, we first pass the traffic state through an improved deep neural network to achieve the extraction of high-dimensional semantic information and the learning of the communication matrix. Then the agents selectively interact with each other based on the learned communication matrix and generate the final state features. Finally, the features are inputted to the QMIX network to achieve the final action selection. We compare the CMARL model with 6 other baseline algorithms in real traffic networks. The results show that CMARL can significantly reduce vehicle congestion, and run stably in various scenarios.",
        "link": "http://dx.doi.org/10.20944/preprints202401.1565.v1"
    },
    {
        "id": 28282,
        "title": "First-Person Activity Forecasting with Online Inverse Reinforcement Learning",
        "authors": "Nicholas Rhinehart, Kris M. Kitani",
        "published": "2017-10",
        "citations": 65,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2017.399"
    },
    {
        "id": 28283,
        "title": "Reinforcement Learning Based Energy Scheduling for Pelagic Islands",
        "authors": "Chenxu Shi, Lingxiao Yang, Ning Zhang",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icei60179.2023.00014"
    },
    {
        "id": 28284,
        "title": "Multi-Agent Reinforcement Learning Based Energy Efficiency Optimization in NB-IoT Networks",
        "authors": "Yuancheng Guo, Min Xiang",
        "published": "2019-12",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcwkshps45667.2019.9024676"
    },
    {
        "id": 28285,
        "title": "Path Planning Algorithms for USVs via Deep Reinforcement Learning",
        "authors": "Haoran Zhai, Weihong Wang, Wei Zhang, Qingze Li",
        "published": "2021-10-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728038"
    },
    {
        "id": 28286,
        "title": "Deep Reinforcement Learning for Multiparameter Optimization in de novo Drug Design",
        "authors": "Niclas Ståhl, Göran Falkman, Alexander Karlsson, Gunnar Mathiason, Jonas Boström",
        "published": "No Date",
        "citations": 0,
        "abstract": "In medicinal chemistry programs it is key to design and make compounds that are efficacious and safe. This is a long, complex and difficult multi-parameter optimization process, often including several properties with orthogonal trends. New methods for the automated design of compounds against profiles of multiple properties are thus of great value. Here we present a fragment-based reinforcement learning approach based on an actor-critic model, for the generation of novel molecules with optimal properties. The actor and the critic are both modelled with bidirectional long short-term memory (LSTM) networks. The AI method learns how to generate new compounds with desired properties by starting from an initial set of lead molecules and then improve these by replacing some of their fragments. A balanced binary tree based on the similarity of fragments is used in the generative process to bias the output towards structurally similar molecules. The method is demonstrated by a case study showing that 93% of the generated molecules are chemically valid, and a third satisfy the targeted objectives, while there were none in the initial set.",
        "link": "http://dx.doi.org/10.26434/chemrxiv.7990910.v1"
    },
    {
        "id": 28287,
        "title": "Scalable Deep Reinforcement Learning-Based Online Routing for Multi-type Service Requirements",
        "authors": "Chenyi Liu, Pingfei Wu, Mingwei Xu, Yuan Yang, Nan Geng",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This work is under review of IEEE Transactions on Parallel and Distributed Systems.</p>\n<h3><br></h3>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21587436.v1"
    },
    {
        "id": 28288,
        "title": "Backward Curriculum Reinforcement Learning",
        "authors": "KyungMin Ko",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ro-man57019.2023.10309513"
    },
    {
        "id": 28289,
        "title": "Risk Averse Robust Adversarial Reinforcement Learning",
        "authors": "Xinlei Pan, Daniel Seita, Yang Gao, John Canny",
        "published": "2019-5",
        "citations": 33,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8794293"
    },
    {
        "id": 28290,
        "title": "Reinforcement Learning Scheduling Research for Edge Servers",
        "authors": "Yonggui Han, Zhongsheng Wang",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccnea60107.2023.00024"
    },
    {
        "id": 28291,
        "title": "Target-Network Update Linked with Learning Rate Decay Based on Mutual Information and Reward in Deep Reinforcement Learning",
        "authors": "Chayoung Kim",
        "published": "2023-9-28",
        "citations": 0,
        "abstract": "In this study, a target-network update of deep reinforcement learning (DRL) based on mutual information (MI) and rewards is proposed. In DRL, updating the target network from the Q network was used to reduce training diversity and contribute to the stability of learning. If it is not properly updated, the overall update rate is reduced to mitigate this problem. Simply slowing down is not recommended because it reduces the speed of the decaying learning rate. Some studies have been conducted to improve the issues with the t-soft update based on the Student’s-t distribution or a method that does not use the target-network. However, there are certain situations in which using the Student’s-t distribution might fail or force it to use more hyperparameters. A few studies have used MI in deep neural networks to improve the decaying learning rate and directly update the target-network by replaying experiences. Therefore, in this study, the MI and reward provided in the experience replay of DRL are combined to improve both the decaying learning rate and the target-network updating. Utilizing rewards is appropriate for use in environments with intrinsic symmetry. It has been confirmed in various OpenAI gymnasiums that stable learning is possible while maintaining an improvement in the decaying learning rate.",
        "link": "http://dx.doi.org/10.3390/sym15101840"
    },
    {
        "id": 28292,
        "title": "Toward an adaptive deep reinforcement learning agent for maritime platform defense",
        "authors": "Jared Markowitz, Edward W. Staley",
        "published": "2023-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2663831"
    },
    {
        "id": 28293,
        "title": "Evaluating multi-agent reinforcement learning on heterogeneous platforms",
        "authors": "Samuel Wiggins, Yuan Meng, Rajgopal Kannan, Viktor Prasanna",
        "published": "2023-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2668689"
    },
    {
        "id": 28294,
        "title": "Learning to Send Reinforcements: Coordinating Multi-Agent Dynamic Police Patrol Dispatching and Rescheduling via Reinforcement Learning",
        "authors": "Waldy Joe, Hoong Chuin Lau",
        "published": "2023-8",
        "citations": 0,
        "abstract": "We address the problem of coordinating multiple agents in a dynamic police patrol scheduling via a Reinforcement Learning (RL) approach. Our approach utilizes Multi-Agent Value Function Approximation (MAVFA) with a rescheduling heuristic to learn dispatching and rescheduling policies jointly. Often, police operations are divided into multiple sectors for more effective and efficient operations. In a dynamic setting, incidents occur throughout the day across different sectors, disrupting initially-planned patrol schedules. To maximize policing effectiveness, police agents from different sectors cooperate by sending reinforcements to support one another in their incident response and even routine patrol. This poses an interesting research challenge on how to make such complex decision of dispatching and rescheduling involving multiple agents in a coordinated fashion within an operationally reasonable time. Unlike existing Multi-Agent RL (MARL) approaches which solve similar problems by either decomposing the problem or action into multiple components, our approach learns the dispatching and rescheduling policies jointly without any decomposition step. In addition, instead of directly searching over the joint action space, we incorporate an iterative best response procedure as a decentralized optimization heuristic and an explicit coordination mechanism for a scalable and coordinated decision-making. We evaluate our approach against the commonly adopted two-stage approach and conduct a series of ablation studies to ascertain the effectiveness of our proposed learning and coordination mechanisms.",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/18"
    },
    {
        "id": 28295,
        "title": "Vehicle emission control on road with temporal traffic information using deep reinforcement learning",
        "authors": "Zhenyi Xu, Yang Cao, Yu Kang, Zhenyi Zhao",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003190691-11"
    },
    {
        "id": 28296,
        "title": "A Reinforcement Learning Approach for Ensemble Machine Learning Models in Peak Electricity Forecasting",
        "authors": "Warut Pannakkong, Vu Thanh Vinh, Nguyen Ngoc Minh Tuyen, Jirachai Buddhakulsomsiri",
        "published": "2023-7-1",
        "citations": 2,
        "abstract": "Electricity peak load forecasting plays an important role in electricity generation capacity planning to ensure reliable power supplies. To achieve high forecast accuracy, multiple machine learning models have been implemented to forecast the monthly peak load in Thailand over the past few years, yielding promising results. One approach to further improve forecast accuracy is to effectively select the most accurate forecast value for each period from among the forecast values generated by these models. This article presents a novel reinforcement learning approach using the double deep Q-network (Double DQN), which acts as a model selector from a pool of available models. The monthly electricity peak load data of Thailand from 2004 to 2017 are used to demonstrate the effectiveness of the proposed method. A hyperparameter tuning methodology using a fractional factorial design is implemented to significantly reduce the number of required experimental runs. The results indicate that the proposed selection model using Double DQN outperforms all tested individual machine learning models in terms of mean square error.",
        "link": "http://dx.doi.org/10.3390/en16135099"
    },
    {
        "id": 28297,
        "title": "Rapidly Learning Bayesian Networks for Complex System Diagnosis: A Reinforcement Learning Directed Greedy Search Approach",
        "authors": "Wenfeng Zhang, Wenquan Feng, Hongbo Zhao, Qi Zhao",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2019.2952143"
    },
    {
        "id": 28298,
        "title": "Reinforcement Learning Based Online Active Learning for Human Activity Recognition",
        "authors": "Yulai Cui, Shruthi Kashinath Hiremath, Thomas Ploetz",
        "published": "2022-9-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3544794.3558457"
    },
    {
        "id": 28299,
        "title": "Structured Online Learning‐Based Control of Continuous‐Time Nonlinear Systems",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch5"
    },
    {
        "id": 28300,
        "title": "Deep reinforcement learning to study spatial navigation, learning and memory in artificial and biological agents",
        "authors": "Edgar Bermudez-Contreras",
        "published": "2021-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00422-021-00862-0"
    },
    {
        "id": 28301,
        "title": "Deep reinforcement learning to study spatial navigation, learning and memory in artificial and biological agents",
        "authors": "Edgar Bermudez-Contreras",
        "published": "2021-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00422-021-00862-0"
    },
    {
        "id": 28302,
        "title": "Towards learning behavior modeling of military logistics agent utilizing profit sharing reinforcement learning algorithm",
        "authors": "Xiong Li, Wei Pu, Xiaodong Zhao",
        "published": "2021-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.asoc.2021.107784"
    },
    {
        "id": 28303,
        "title": "Constraints Penalized Q-learning for Safe Offline Reinforcement Learning",
        "authors": "Haoran Xu, Xianyuan Zhan, Xiangyu Zhu",
        "published": "2022-6-28",
        "citations": 3,
        "abstract": "We study the problem of safe offline reinforcement learning (RL), the goal is to learn a policy that maximizes long-term reward while satisfying safety constraints given only offline data, without further interaction with the environment. This problem is more appealing for real world RL applications, in which data collection is costly or dangerous. Enforcing constraint satisfaction is non-trivial, especially in offline settings, as there is a potential large discrepancy between the policy distribution and the data distribution, causing errors in estimating the value of safety constraints. We show that naïve approaches that combine techniques from safe RL and offline RL can only learn sub-optimal solutions. We thus develop a simple yet effective algorithm, Constraints Penalized Q-Learning (CPQ), to solve the problem. Our method admits the use of data generated by mixed behavior policies. We present a theoretical analysis and demonstrate empirically that our approach can learn robustly across a variety of benchmark control tasks, outperforming several baselines.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i8.20855"
    },
    {
        "id": 28304,
        "title": "Goal-Conditioned Reinforcement Learning with Latent Representations using Contrastive Learning",
        "authors": "Takaya YAMADA, Koich OGAWARA",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1299/jsmermd.2021.1p1-i15"
    },
    {
        "id": 28305,
        "title": "Distributed transmission control for wireless networks using multi-agent reinforcement learning",
        "authors": "Collin Farquhar, Prem Kumar, Anu Jagannath, Jithin Jagannath",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2621086"
    },
    {
        "id": 28306,
        "title": "A learning search algorithm with propagational reinforcement learning",
        "authors": "Wei Zhang",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-020-02117-0"
    },
    {
        "id": 28307,
        "title": "Enhancing Control in Manufacturing and Microgrid Systems: Deep Reinforcement Learning with Double Q-Learning",
        "authors": "Mohamed Ballouch, Omar Souissi, Mohammed Raiss El-Fenni",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sita60746.2023.10373737"
    },
    {
        "id": 28308,
        "title": "Scan Chain Clustering and Optimization with Constrained Clustering and Reinforcement Learning",
        "authors": "Naiju Karim Abdul, George Antony, Rahul M. Rao, Suriya T. Skariah",
        "published": "2022-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlcad55463.2022.9900094"
    },
    {
        "id": 28309,
        "title": "A Review of Three Methods of Artificial Intelligence in Smart Grid Cyber Security: Machine Learning, Reinforcement Learning, Ensemble Methods",
        "authors": "Luyao Xu",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "Renewable energy is gradually replacing traditional fossil fuels. The change of power generation energy structure brings new challenges to the traditional power grid. Through the efficient bidirectional movement of electricity and information, smart grids might include renewable energy. For the complex informational and financial operations required by smart grid, communication systems are crucial, but they also make smart grid vulnerable to numerous cyber attacks. Smart grid cyber security has been widely concerned. The purpose of this paper is to explore the use of artificial intelligence technology in smart grid cyber security. Three methods in the field of artificial intelligence are highlighted: Machine Learning, Reinforcement Learning, and Ensemble Methods. This paper summarizes the benefits and drawbacks of their use of smart grid cyber security, and further makes a qualitative comparison of the three methods from multiple performance indicators.",
        "link": "http://dx.doi.org/10.54097/79xf0y91"
    },
    {
        "id": 28310,
        "title": "Learning Virtual Grasp with Failed Demonstrations via Bayesian Inverse Reinforcement Learning",
        "authors": "Xu Xie, Changyang Li, Chi Zhang, Yixin Zhu, Song-Chun Zhu",
        "published": "2019-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros40897.2019.8968063"
    },
    {
        "id": 28311,
        "title": "An improved deep reinforcement learning for robot navigation",
        "authors": "Qifeng Zheng, Xiaogang Huang, Chen Dong, Yuting Liu, Dong Chen",
        "published": "2023-5-25",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2675144"
    },
    {
        "id": 28312,
        "title": "Multi-Robot Real-time Game Strategy Learning based on Deep Reinforcement Learning",
        "authors": "Ki Deng, Yanjie Li, Songshuo Lu, Yongjin Mu, Xizheng Pang, Qi Liu",
        "published": "2022-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio55434.2022.10011827"
    },
    {
        "id": 28313,
        "title": "Bounded Rationality in Learning, Perception, Decision-Making, and Stochastic Games",
        "authors": "Panagiotis Tsiotras",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_17"
    },
    {
        "id": 28314,
        "title": "Deep Q-Learning Based Reinforcement Learning Approach for Network Intrusion Detection",
        "authors": "Hooman Alavizadeh, Hootan Alavizadeh, Julian Jang-Jaccard",
        "published": "2022-3-11",
        "citations": 52,
        "abstract": "The rise of the new generation of cyber threats demands more sophisticated and intelligent cyber defense solutions equipped with autonomous agents capable of learning to make decisions without the knowledge of human experts. Several reinforcement learning methods (e.g., Markov) for automated network intrusion tasks have been proposed in recent years. In this paper, we introduce a new generation of the network intrusion detection method, which combines a Q-learning based reinforcement learning with a deep feed forward neural network method for network intrusion detection. Our proposed Deep Q-Learning (DQL) model provides an ongoing auto-learning capability for a network environment that can detect different types of network intrusions using an automated trial-error approach and continuously enhance its detection capabilities. We provide the details of fine-tuning different hyperparameters involved in the DQL model for more effective self-learning. According to our extensive experimental results based on the NSL-KDD dataset, we confirm that the lower discount factor, which is set as 0.001 under 250 episodes of training, yields the best performance results. Our experimental results also show that our proposed DQL is highly effective in detecting different intrusion classes and outperforms other similar machine learning approaches.",
        "link": "http://dx.doi.org/10.3390/computers11030041"
    },
    {
        "id": 28315,
        "title": "Reinforcement Learning for Hyperparameter Tuning in Deep Learning-based Side-channel Analysis",
        "authors": "Jorai Rijsdijk, Lichao Wu, Guilherme Perin, Stjepan Picek",
        "published": "2021-7-9",
        "citations": 55,
        "abstract": "Deep learning represents a powerful set of techniques for profiling sidechannel analysis. The results in the last few years show that neural network architectures like multilayer perceptron and convolutional neural networks give strong attack performance where it is possible to break targets protected with various countermeasures. Considering that deep learning techniques commonly have a plethora of hyperparameters to tune, it is clear that such top attack results can come with a high price in preparing the attack. This is especially problematic as the side-channel community commonly uses random search or grid search techniques to look for the best hyperparameters.In this paper, we propose to use reinforcement learning to tune the convolutional neural network hyperparameters. In our framework, we investigate the Q-Learning paradigm and develop two reward functions that use side-channel metrics. We mount an investigation on three commonly used datasets and two leakage models where the results show that reinforcement learning can find convolutional neural networks exhibiting top performance while having small numbers of trainable parameters. We note that our approach is automated and can be easily adapted to different datasets. Several of our newly developed architectures outperform the current state-of-the-art results. Finally, we make our source code publicly available.\r\nhttps://github.com/AISyLab/Reinforcement-Learning-for-SCA",
        "link": "http://dx.doi.org/10.46586/tches.v2021.i3.677-707"
    },
    {
        "id": 28316,
        "title": "Real-Time Pose Recognition for Billiard Players Using Deep Learning",
        "authors": "Zhikang Chen, Wei Qi Yan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "In this book chapter, the authors propose a method for player pose recognition in billiards matches by combining keypoint extraction and an optimized transformer. Given that those human pose analysis methods usually require high labour costs, the authors explore deep learning methods to achieve real-time, high-precision pose recognition. Firstly, they utilize human key point detection technology to extract the key points of players from real-time videos and generate key points. Then, the key point data is input into the transformer model for pose analysis and recognition. In addition, the authors design a human skeletal alignment method for comparison with standard poses. The experimental results show that the method performs well in recognizing players' poses in billiards matches and provides real-time and timely feedback on players' pose information. This research project provides a new and efficient tool for training billiard players and opens up new possibilities for applying deep learning in sports analytics. In addition, one of these contributions is the creation of a dataset for pose recognition.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch010"
    },
    {
        "id": 28317,
        "title": "Monopoly Using Reinforcement Learning",
        "authors": "Edupuganti Arun, Harikrishna Rajesh, Debarka Chakrabarti, Harikiran Cherala, Koshy George",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tencon.2019.8929523"
    },
    {
        "id": 28318,
        "title": "Partitioning Distributed Compute Jobs with Reinforcement Learning and Graph Neural Networks",
        "authors": "Christopher  William Falke Parsonson, Zacharaya Shabka, Alessandro Ottino, Georgios Zervas",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4365914"
    },
    {
        "id": 28319,
        "title": "Vehicle Crash Simulation Models for Reinforcement Learning driven crash-detection algorithm calibration",
        "authors": "Shahabaz Afraj, Ondřej Vaculín, Dennis Böhmländer, Luděk Hynčík",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe development of finite element vehicle models for crash simulations is a highly complex task. The main aim of these models is to simulate a variety of crash scenarios and assess all the safety systems for their respective performances. These vehicle models possess a substantial amount of data pertaining to the vehicle's geometry, structure, materials, etc., and are used to estimate a large set of system and component level characteristics using crash simulations. It is understood that even the most well-developed simulation models are prone to deviations in estimation when compared to real-world physical test results. This is generally due to our inability to model the chaos and uncertainties introduced in the real world. Such unavoidable deviations render the use of virtual simulations ineffective for the calibration process of the algorithms that activate the restraint systems in the event of a crash (crash-detection algorithm). In the scope of this research, authors hypothesize the possibility of accounting for such variations introduced in the real world by creating a feedback loop between real-world crash tests and crash simulations. To accomplish this, a Reinforcement Learning (RL) compatible virtual surrogate model is used, which is adapted from crash simulation models. Hence, a conceptual methodology is illustrated in this paper for developing an RL-compatible model that can be trained using the results of crash simulations and crash tests. As the calibration of the crash-detection algorithm is fundamentally dependent upon the crash pulses, the scope of the expected output is limited to advancing the ability to estimate crash pulses. Furthermore, the real-time implementation of the methodology is illustrated using an actual vehicle model.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3004299/v1"
    },
    {
        "id": 28320,
        "title": "3D Generative Design for Non-Experts: Multiview Perceptual Similarity with Agent-Based Reinforcement Learning",
        "authors": "Robert Stuart-Smith, Patrick Danahy",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5151/sigradi2022-sigradi2022_53"
    },
    {
        "id": 28321,
        "title": "Scalability of Multiagent Reinforcement Learning",
        "authors": "Yunkai Zhuang, Yujing Hu, Hao Wang",
        "published": "2018-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789813208742_0001"
    },
    {
        "id": 28322,
        "title": "Deep Reinforcement Learning in Competing Cognitive Tactical Networks",
        "authors": "Yi Chen",
        "published": "2023-2-12",
        "citations": 0,
        "abstract": "These instructions give you guidelines for preparing papers for DRP. Use this document as a template if you are using Microsoft Word 6.0 or later. Otherwise, use this document as an instruction set. The electronic file of your paper will be formatted further at DRP. Paper titles should be written in uppercase and lowercase letters, not all uppercase. Avoid writing long formulas with subscripts in the title; short formulas that identify the elements are fine (e.g., \"Nd-Fe-B\"). Do not write “(Invited)” in the title. Full names of authors are preferred in the author field, but are not required. Put a space between authors’ initials. The abstract must be a concise yet comprehensive reflection of what is in your article. In particular, the abstract must be self-contained, without abbreviations, footnotes, or references. It should be a microcosm of the full article. The abstract must be between 100 - 300 words. Be sure that you adhere to these limits; otherwise, you will need to edit your abstract accordingly. The abstract must be written as one paragraph, and should not contain displayed mathematical equations or tabular material. The abstract should include three or four different keywords or phrases, as this will help readers to find it. It is important to avoid over-repetition of such phrases as this can result in a page being rejected by search engines. Ensure that your abstract reads well and is grammatically correct.",
        "link": "http://dx.doi.org/10.54097/hset.v32i.5186"
    },
    {
        "id": 28323,
        "title": "Constrained Expectation-Maximization Methods for Effective Reinforcement Learning",
        "authors": "Gang Chen, Yiming Peng, Mengjie Zhang",
        "published": "2018-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8488990"
    },
    {
        "id": 28324,
        "title": "Deep Reinforcement Learning in Virtual Environments",
        "authors": "Feng Lin, Hock Soon Seah",
        "published": "2022",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-08234-9_440-1"
    },
    {
        "id": 28325,
        "title": "Financial Trading with Feature Preprocessing and Recurrent Reinforcement Learning",
        "authors": "Lin Li",
        "published": "2021-11-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iske54062.2021.9755374"
    },
    {
        "id": 28326,
        "title": "Data-Driven MPC for Linear Systems using Reinforcement Learning",
        "authors": "Zhongqi Sun, Qian Wang, Junan Pan, Yuanqing Xia",
        "published": "2021-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728233"
    },
    {
        "id": 28327,
        "title": "Scalable Deep Reinforcement Learning-Based Online Routing for Multi-type Service Requirements",
        "authors": "Chenyi Liu, Pingfei Wu, Mingwei Xu, Yuan Yang, Nan Geng",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This work is under review of IEEE Transactions on Parallel and Distributed Systems.</p>\n<h3><br></h3>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21587436"
    },
    {
        "id": 28328,
        "title": "Deep Contextual Bandit and Reinforcement Learning for IRS-assisted MU-MIMO Systems",
        "authors": "Dariel Pereira-Ruisánchez, Óscar Fresnedo, Darian Pérez-Adán, Luis Castedo",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p> The combination of multiple-input multiple-output (MIMO) and intelligent reflecting surfaces (IRSs) is foreseen as a key enabler of beyond 5G (B5G) and 6G. In this work, two different approaches are considered for the joint optimization of the IRS phase-shift matrix and MIMO precoders of an IRS-assisted multi-stream (MS) multi-user MIMO (MU-MIMO) system with the aim of maximizing the system sum-rate for every channel realization. The first one is a novel contextual bandit (CB) approach with continuous state and action spaces called deep contextual bandit-oriented deep deterministic policy gradient (DCB-DDPG). The second is an innovative deep reinforcement learning (DRL) formulation where the states, actions and rewards are selected such that the Markov decision process (MDP) property of reinforcement learning (RL) is properly met. Both proposals perform remarkably better than state-of-the-art heuristic methods in high multi-user interference scenarios. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19787551"
    },
    {
        "id": 28329,
        "title": "Co-training by Experience Replay for Reinforcement Learning",
        "authors": "Yuyang Huang",
        "published": "2023-4-1",
        "citations": 0,
        "abstract": " In this paper, to improve the efficiency of the reinforcement learning model to explore the environment and get better results, a new method which involves the co-training process in reinforcement learning by sharing the experience pool of each agent in the training process has been developed. In this method, agents can gain a better understanding of the environment since agents use different policies to make action and explore the environment. At the same time, this paper designed an agent called Hard Memory Collector by modifying the value function and combining this agent and a normal agent for co-training. As an experimental result on the ViZDoom platform, the model achieved better results than the original Duel DQN network in terms of score, steps used per game and loss value.",
        "link": "http://dx.doi.org/10.54097/hset.v39i.6585"
    },
    {
        "id": 28330,
        "title": "Improving Algorithm Conflict Resolution Manoeuvres with Reinforcement Learning",
        "authors": "Marta Ribeiro, Joost Ellerbroek, Jacco Hoekstra",
        "published": "2022-12-19",
        "citations": 2,
        "abstract": "Future high traffic densities with drone operations are expected to exceed the number of aircraft that current air traffic control procedures can control simultaneously. Despite extensive research on geometric CR methods, at higher densities, their performance is hindered by the unpredictable emergent behaviour from surrounding aircraft. In response, research has shifted its attention to creating automated tools capable of generating conflict resolution (CR) actions adapted to the environment and not limited by man-made rules. Several works employing reinforcement learning (RL) methods for conflict resolution have been published recently. Although proving that they have potential, at their current development, the results of the practical implementation of these methods do not reach their expected theoretical performance. Consequently, RL applications cannot yet match the efficacy of geometric CR methods. Nevertheless, these applications can improve the set of rules that geometrical CR methods use to generate a CR manoeuvre. This work employs an RL method responsible for deciding the parameters that a geometric CR method uses to generate the CR manoeuvre for each conflict situation. The results show that this hybrid approach, combining the strengths of geometric CR and RL methods, reduces the total number of losses of minimum separation. Additionally, the large range of different optimal solutions found by the RL method shows that the rules of geometric CR method must be expanded, catering for different conflict geometries.",
        "link": "http://dx.doi.org/10.3390/aerospace9120847"
    },
    {
        "id": 28331,
        "title": "Meta Reinforcement Learning Based Computation Offloading Strategy for Vehicular Networks",
        "authors": "chao Yang, Yangshui Gao, Hongwei Ding, Rencan Nie, Bo Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDeep learning (DL) and reinforcement learning (RL) based methods can efficiently generate offloading strategies for computational offloading problems in mobile edge computing (MEC) environments. However, the rapid movement of vehicles in the vehicular network causes dynamic changes in the network environment, and DL or RL methods require additional training samples and multiple gradient updates before the model converges, which is time-consuming. In this letter, we propose a meta reinforcement learning-based computation task offloading and resource allocation (MRLOA) algorithm for vehicular networks. Specifically, the MRLOA can converge quickly for a new task with a small number of experience and gradient updates based on a pre-trained meta policy. Simulation results show that our proposed algorithm can more quickly adapt to new computational offloading tasks in the vehicular network environment compared to traditional baseline algorithms.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1614949/v1"
    },
    {
        "id": 28332,
        "title": "Object-Oriented State Abstraction in Reinforcement Learning for Video Games",
        "authors": "Yu Chen, Huizhuo Yuan, Yujun Li",
        "published": "2019-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8848099"
    },
    {
        "id": 28333,
        "title": "Calibrating Dead Reckoning with Deep Reinforcement Learning",
        "authors": "Sangmin Lee, Hwangnam Kim",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apcc55198.2022.9943735"
    },
    {
        "id": 28334,
        "title": "Combining heuristics with counterfactual play in reinforcement learning.",
        "authors": "Erik Peterson, Necati Müyesser, Kyle Dunovan, Tim Verstynen",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2018.1151-0"
    },
    {
        "id": 28335,
        "title": "Model-Free Indirect RL: Monte Carlo",
        "authors": "Shengbo Eben Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-7784-8_3"
    },
    {
        "id": 28336,
        "title": "Reinforcement Learning with Explainability for Traffic Signal Control",
        "authors": "Stefano Giovanni Rizzo, Giovanna Vantini, Sanjay Chawla",
        "published": "2019-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8917519"
    },
    {
        "id": 28337,
        "title": "Multi-agent Robust Time Differential Reinforcement Learning Over Communicated Networks",
        "authors": "Jiahong Li, Nan Ma, Xiangmin Han",
        "published": "2018-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2018.8483961"
    },
    {
        "id": 28338,
        "title": "Human-assisted reinforcement learning demonstrated on the Flappy Bird Game",
        "authors": "Jana Ristovska, Domen Šoberl",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26493/scores23.13"
    },
    {
        "id": 28339,
        "title": "Maze Search Using Reinforcement Learning by a Mobile Robot",
        "authors": "Makoto Katoh",
        "published": "2018-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32474/arme.2018.01.000110"
    },
    {
        "id": 28340,
        "title": "A Full Freedom Pose Measurement Method of Industrial Robot Based on Reinforcement Learning Algorithm",
        "authors": "Xinghua Lu, Yunsheng Chen, Ziyue Yuan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn order to improve the efficiency of robot operation in the field of industrial automation, a full freedom pose measurement method of industrial robot based on reinforcement learning algorithm is proposed. According to the characteristics of two-wheel independent driving industrial robot, the attitude of the robot in three kinds of moving modes in unconstrained space is calculated. The algorithm for measuring the full degree of freedom of industrial robot is given by the multi-agent method of population particle optimization combined with the reinforcement learning algorithm (PSO-QL). The experimental results show that the proposed method has the advantages of low accuracy, high measurement efficiency, high success rate of grabbing and avoiding obstacles and good application effect.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-410169/v1"
    },
    {
        "id": 28341,
        "title": "Modelling global public health strategies in COVID-19 pandemic using deep reinforcement learning",
        "authors": "Kwak Gloria Hyunjung, Lowell Ling, Pan Hui",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nRationale: Unprecedented public health measures have been used during this coronavirus 2019 (COVID-19) pandemic but with a cost to economic and social disruption. It is a challenge to implement timely and appropriate public health interventions.Objectives: This study evaluates the timing and intensity of public health policies in each country and territory in the COVID-19 pandemic, and whether machine learning can help them to find better global health strategies.Methods: Population and COVID-19 epidemiological data between 21st January 2020 to 7th April 2020 from 183 countries and 78 territories were included with the implemented public health interventions. We used deep reinforcement learning, and the model was trained to try to find the optimal public health strategies with maximizing total reward on controlling spread of COVID-19. The results proposed by the model were analyzed against the actual timing and intensity of lockdown and travel restrictions.Measurements and Main Results: Early implementation of the actual lockdown and travel restriction policies were associated with gradually groups of less severe crisis severity, relative to local index case date in each country or territory, not to 31st December 2019. However, our model suggested to initiate at least minimal intensity of lockdown or travel restriction even before index cases in each country and territory. In addition, the model mostly recommended a combination of lockdown and travel restrictions and higher intensity policies than the implemented policies by government, but did not always encourage rapid full lockdown and full border closures.Conclusion: Compared to actual government implementation, our model mostly recommended earlier and higher intensity of lockdown and travel restrictions. Machine learning may be used as a decision support tool for implementation of public health interventions during COVID-19 and future pandemics.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-31226/v1"
    },
    {
        "id": 28342,
        "title": "The Basics of Feedback Control Systems",
        "authors": "Changsheng Hua",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-33034-7_2"
    },
    {
        "id": 28343,
        "title": "Deep Reinforcement Learning for Network Provisioning in Elastic Optical Networks",
        "authors": "Junior Momo Ziazet, Brigitte Jaumard",
        "published": "2022-5-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45855.2022.9839228"
    },
    {
        "id": 28344,
        "title": "Virtual Autonomous Vehicle Using Deep Reinforcement Learning",
        "authors": "Vanita Jain, Aditya Chaudhry, Manas Batra, Prakhar Gupta",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3607824"
    },
    {
        "id": 28345,
        "title": "Autonomous Surface Vessel Obstacle Avoidance Based on Hierarchical Reinforcement Learning With Potential Field Method",
        "authors": "Chang Zhou, Lei Wang, Huacheng He, Shangyu Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.0005152v"
    },
    {
        "id": 28346,
        "title": "Batch RL, Experience-Replay, DQN, LSPI, Gradient TD",
        "authors": "Ashwin Rao, Tikhon Jelvis",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003229193-13"
    },
    {
        "id": 28347,
        "title": "Hybrid IRS-Assisted Secure Satellite Downlink Communications: A Fast Deep Reinforcement Learning Approach",
        "authors": "Quynh Ngo, Tran Khoa Phan, Abdun Mahmood, Wei Xiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper studies a secure satellite-terrestrial communication system assisted by a hybrid intelligent reflecting surface (IRS). The hybrid IRS, which composes of active and passive reflecting elements, is deployed to enhance the secure communication from the satellite to multiple users against multiple eavesdroppers. A joint design optimization problem for the satellite beamforming and the hybrid IRS interaction is formulated to maximize the system worst-case secrecy rate under time-varying channel conditions. With high system dynamic and complexity, deep reinforcement learning (DRL) is employed to solve the non-convex optimization problem. We propose a fast DRL algorithm, namely deep PDS-DPG, to obtain the robust secure beamforming design for satellite and hybrid IRS. Numerical results show a better learning efficiency of the proposed algorithm as to the state-of-the-art deep deterministic policy gradient (DDPG) algorithm with comparable system secrecy performance. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20478438"
    },
    {
        "id": 28348,
        "title": "A Reinforcement Learning Approach for Mobile Beamforming",
        "authors": "Anastasios Dimas, Konstantinos Diamantaras, Athina Petropulu",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf44664.2019.9048888"
    },
    {
        "id": 28349,
        "title": "Superstitious learning of abstract order from random reinforcement",
        "authors": "Yuhao Jin, Greg Jensen, Jacqueline Gottlieb, Vincent Ferrera",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2022.1067-0"
    },
    {
        "id": 28350,
        "title": "Control of an Insect-Scale Flyer in Complex Flow Using Deep Reinforcement Learning",
        "authors": "Seungpyo Hong, Sejin Kim, Innyoung Kim, Donghyun You",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4432236"
    },
    {
        "id": 28351,
        "title": "Hierarchical Defect Detection Based On Reinforcement Learning",
        "authors": "Fen Fang, Qianli Xu, Joo-Hwee Lim",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897947"
    },
    {
        "id": 28352,
        "title": "Automatic Generation Control with Competing GENCOs-A Reinforcement Learning Based Approach",
        "authors": "Devika Jay, K.S Swarup",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/npsc.2018.8771789"
    },
    {
        "id": 28353,
        "title": "A Review on Reinforcement Learning enabled Cooperative Spectrum Sensing",
        "authors": "Thi Thu Hien Pham, Sungrae Cho",
        "published": "2023-1-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin56518.2023.10048946"
    },
    {
        "id": 28354,
        "title": "Segmented Actor-Critic-Advantage Architecture for Reinforcement Learning Tasks",
        "authors": "Martin Kaloev, Georgi Krastev",
        "published": "2022-2-28",
        "citations": 0,
        "abstract": "The article focuses on experiments with a multi module neural networks type of architecture for neuron-like machine used in reinforcing learning. This type of architecture can be used to solve complex robotic or policy optimization tasks and allows segmented storage of trained memory. Such technique speeds up the training process compared to existing actor-critical algorithms.",
        "link": "http://dx.doi.org/10.18421/tem111-27"
    },
    {
        "id": 28355,
        "title": "Time Budget Management in Multifunction Radars Using Reinforcement Learning",
        "authors": "Petteri Pulkkinen, Tuomas Aittomaki, Anders Strom, Visa Koivunen",
        "published": "2021-5-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/radarconf2147009.2021.9455344"
    },
    {
        "id": 28356,
        "title": "Application of Reinforcement Learning for Electric Power System",
        "authors": "Yingqi Zhou",
        "published": "2021-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iaecst54258.2021.9695746"
    },
    {
        "id": 28357,
        "title": "A reinforcement learning algorithm shapes maternal care in mice",
        "authors": "Yunyao Xie, Longwen Huang, Alberto Corona, Alexa H. Pagliaro, Stephen D. Shea",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe neural substrates for processing classical rewards such as food or drugs of abuse are well-understood. In contrast, the mechanisms by which organisms perceive social contact as rewarding and subsequently modify their interactions are unclear. Here we tracked the gradual emergence of a repetitive and highly-stereotyped parental behavior and show that trial-by-trial performance correlates with the history of midbrain dopamine (DA) neuron activity. We used a novel behavior paradigm to manipulate the subject’s expectation of imminent pup contact and show that DA signals conform to reward prediction error, a fundamental component of reinforcement learning (RL). Finally, closed-loop optogenetic inactivation of DA neurons at the onset of pup contact dramatically slowed emergence of parental care. We conclude that this prosocial behavior is shaped by an RL mechanism in which social contact itself is the primary reward.One-Sentence SummaryMaternal interactions with offspring are shaped by a dopaminergic reinforcement learning mechanism.",
        "link": "http://dx.doi.org/10.1101/2022.03.21.485130"
    },
    {
        "id": 28358,
        "title": "Critical Discussion and Outlook",
        "authors": "Schirin Bär",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-39179-9_8"
    },
    {
        "id": 28359,
        "title": "An Actor-Critic Hierarchical Reinforcement Learning Model for Course Recommendation",
        "authors": "Kun Liang, Guoqiang Zhang, Jinhui Guo, Wentao Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract—Online learning platforms provide diverse course resources, but this often result in the issue of information overload. Learners always want to learn courses that are appropriate for their knowledge level and preferences quickly and accurately. Effective course recommendation plays a key role in helping learners select appropriate courses and improving the efficiency of online learning. However, when a user is enrolled in multiple courses, Existing course recommendation methods face the challenge in accurately recommending the target course that is most relevant to the user, because of the noise courses. In this paper, we propose a novel reinforcement learning model named Actor-Critic Hierarchical Reinforcement Learning (ACHRL). The model incorporates the Actor-Critic method to construct the profile reviser. This can remove noise courses and make personalized course recommendation effectively. Furthermore, we propose a policy gradient based on temporal difference error to reduce the variance in the training process, to speed up the convergence of the model, and improves the accuracy of the recommendation. We evaluate the proposed model on two real datasets, and the experimental results show that the proposed model is significantly outperforms the existing recommendation models (improving 3.77% to 13.66% in terms of HR@5).",
        "link": "http://dx.doi.org/10.20944/preprints202310.1367.v1"
    },
    {
        "id": 28360,
        "title": "A Novel Exploration Technique For Multi-Agent Reinforcement Learning",
        "authors": "Devarani Devi Ningombam",
        "published": "2022-10-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcat55367.2022.9971921"
    },
    {
        "id": 28361,
        "title": "Application of Multi-Agent Deep Reinforcement Learning to Optimize Real-World Traffic Signal Controls",
        "authors": "Maxim Friesen, Tian Tan, Jürgen Jasperneite, Jie Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Increasing traffic congestion leads to significant costs associated by additional travel delays, whereby poorly configured signaled intersections are a common bottleneck and root cause. Traditional traffic signal control (TSC) systems employ rule-based or heuristic methods to decide signal timings, while adaptive TSC solutions utilize a traffic-actuated control logic to increase their adaptability to real-time traffic changes. However, such systems are expensive to deploy and are often not flexible enough to adequately adapt to the volatility of today's traffic dynamics. More recently, this problem became a frontier topic in the domain of deep reinforcement learning (DRL) and enabled the development of multi-agent DRL approaches that could operate in environments with several agents present, such as traffic systems with multiple signaled intersections. However, most of these proposed approaches were validated using artificial traffic grids. This paper therefore presents a case study, where real-world traffic data from the town of Lemgo in Germany is used to create a realistic road model within VISSIM. A multi-agent DRL setup, comprising multiple independent deep Q-networks, is applied to the simulated traffic network. Traditional rule-based signal controls, currently employed in the real world at the studied intersections, are integrated in the traffic model with LISA+ and serve as a performance baseline. Our performance evaluation indicates a significant reduction of traffic congestion when using the RL-based signal control policy over the conventional TSC approach in LISA+. Consequently, this paper reinforces the applicability of RL concepts in the domain of TSC engineering by employing a highly realistic traffic model.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16974493.v1"
    },
    {
        "id": 28362,
        "title": "Nonlinear Value Function Approximation",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_7"
    },
    {
        "id": 28363,
        "title": "Hybrid IRS-Assisted Secure Satellite-Terrestrial Communications: A Fast Deep Reinforcement Learning Approach",
        "authors": "Quynh Ngo, Tran Khoa Phan, Abdun Mahmood, Wei Xiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This paper studies a secure satellite-terrestrial communication system assisted by a hybrid intelligent reflecting surface (IRS). The hybrid IRS, which composes of active and passive reflecting elements, is deployed to enhance the secure communication from the satellite to multiple users against multiple eavesdroppers. A joint design optimization problem for the satellite beamforming and the hybrid IRS interaction is formulated to maximize the system worst-case secrecy rate under time-varying channel conditions. With high system dynamic and complexity, deep reinforcement learning (DRL) is employed to solve the non-convex optimization problem. We propose a fast DRL algorithm, namely deep PDS-DPG, to obtain the robust secure beamforming design for satellite and hybrid IRS. Numerical results show a better learning efficiency of the proposed algorithm as to the state-of-the-art deep deterministic policy gradient (DDPG) algorithm with comparable system secrecy performance. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20478438.v1"
    },
    {
        "id": 28364,
        "title": "Reinforcement learning and stochastic optimisation",
        "authors": "Sebastian Jaimungal",
        "published": "2022-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00780-021-00467-2"
    },
    {
        "id": 28365,
        "title": "A Reinforcement Learning Approach To Synthesizing Climbing Movements",
        "authors": "Kourosh Naderi, Amin Babadi, Shaghayegh Roohi, Perttu Hamalainen",
        "published": "2019-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8848127"
    },
    {
        "id": 28366,
        "title": "Direct Lookahead Policies",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch19"
    },
    {
        "id": 28367,
        "title": "Exact Dynamic Programming",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch14"
    },
    {
        "id": 28368,
        "title": "Sequential Decision Problems",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch1"
    },
    {
        "id": 28369,
        "title": "The Societal Implications of Deep Reinforcement Learning",
        "authors": "Jess Whittlestone, Kai Arulkumaran, Matthew Crosby",
        "published": "2021-3-8",
        "citations": 13,
        "abstract": "Deep Reinforcement Learning (DRL) is an avenue of research in Artificial Intelligence (AI) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. DRL is one of the most promising routes towards developing more autonomous AI systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct ‘answer’. This could have substantial implications for people’s lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. In this paper, we review recent progress in DRL, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand DRL’s societal implications.\r\n\r\n\r\n\r\nThis article appears in the special track on AI and Society.\r\n\r\n\r\n",
        "link": "http://dx.doi.org/10.1613/jair.1.12360"
    },
    {
        "id": 28370,
        "title": "Deep Reinforcement Learning with Adjustments",
        "authors": "Hamed Khorasgani, Haiyan Wang, Chetan Gupta, Susumu Serita",
        "published": "2021-7-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indin45523.2021.9557543"
    },
    {
        "id": 28371,
        "title": "Coordinated Random Access for Industrial IoT With Correlated Traffic By Reinforcement-Learning",
        "authors": "Alberto Rech, Stefano Tomasin",
        "published": "2021-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcwkshps52748.2021.9682141"
    },
    {
        "id": 28372,
        "title": "Multi-Copy Relay Node Selection Strategy Based on Reinforcement Learning",
        "authors": "Yang Gao, Fuquan Zhang",
        "published": "2023-7-4",
        "citations": 0,
        "abstract": "Delay tolerant networks (DTNs), are characterized by their difficulty in establishing end-to-end paths and and large message propagation delays. To control network overhead costs, reduce message delays, and improve delivery rates in DTNs, it is essential to not only delete messages that have reached their destination but also to more precisely determine appropriate relay nodes. Based on the above goals, this paper constructs a multi-copy relay node selection router algorithm based on Q-lambda reinforcement learning with reference to the idea of community division (QLCR). In community division, if a node has the highestdegree, it is considered the core node, and nodes with similar interests and structural properties are divided into a community. Node degree refers to the number of nodes associated with the node, indicating its importance in the network. Structural similarity determines the distance between nodes. The selection of relay nodes considers node degree, interests, and structural similarity. The Q-lambda reinforcement learning algorithm enables each node to learn from the entire network, setting corresponding reward values based on encountered nodes meeting the specified conditions. Through iterative processes, the node with the most cumulative reward value is chosen as the final relay node. Experimental results demonstrate that the proposed algorithm achieves a high delivery rate while maintaining low network overhead and delay.",
        "link": "http://dx.doi.org/10.3390/s23136131"
    },
    {
        "id": 28373,
        "title": "A Financial Market Trading Strategy: Improved Deep Reinforcement Learning System Via Multi-Transformer",
        "authors": "ZR Wang, Li Ping, Wu Zhao, GJY Nian",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4516320"
    },
    {
        "id": 28374,
        "title": "Cerebellum: Reinforcement Learning Robotic Arm Control System with Hardware Adaptability",
        "authors": "Shizhuo Zhang, Yue Fang, Sujata Saini, Jia Liu, Xibao Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper proposes a meta-learning and reinforcement-learning based control system ”Cerebellum” for robotic arms to address the issue of adapting to hardware parameter changes. By leveraging the con- cept of meta-learning, the system achieves adaptive control by learn- ing from few samples. Experimental results show our ”Cerebellum” reach 90.09% accuracy in a simulation environment with control- ling newly unseen robotic arm. However, challenges remain in hard- ware experiments due to image distortions and Mechanical collide issues. Further research is needed to overcome these challenges.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3572240/v1"
    },
    {
        "id": 28375,
        "title": "Inverse Reinforcement Learning: A Control Lyapunov Approach",
        "authors": "Samuel Tesfazgi, Armin Lederer, Sandra Hirche",
        "published": "2021-12-14",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683494"
    },
    {
        "id": 28376,
        "title": "Reinforcement Learning Enables Field-Development Policy Optimization",
        "authors": "Chris Carpenter",
        "published": "2021-9-1",
        "citations": 1,
        "abstract": "This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 201254, “Reinforcement Learning for Field-Development Policy Optimization,” by Giorgio De Paola, SPE, and Cristina Ibanez-Llano, Repsol, and Jesus Rios, IBM, et al., prepared for the 2020 SPE Annual Technical Conference and Exhibition, originally scheduled to be held in Denver, Colorado, 5–7 October. The paper has not been peer reviewed.\nA field-development plan consists of a sequence of decisions. Each action taken affects the reservoir and conditions any future decision. The presence of uncertainty associated with this process, however, is undeniable. The novelty of the approach proposed by the authors in the complete paper is the consideration of the sequential nature of the decisions through the framework of dynamic programming (DP) and reinforcement learning (RL). This methodology allows moving the focus from a static field-development plan optimization to a more-dynamic framework that the authors call field-development policy optimization. This synopsis focuses on the methodology, while the complete paper also contains a real-field case of application of the methodology.\n\nMethodology\nDeep RL (DRL). RL is considered an important learning paradigm in artificial intelligence (AI) but differs from supervised or unsupervised learning, the most commonly known types currently studied in the field of machine learning. During the last decade, RL has attracted greater attention because of success obtained in applications related to games and self-driving cars resulting from its combination with deep-learning architectures such as DRL, which has allowed RL to scale on to previously unsolvable problems and, therefore, solve much larger sequential decision problems.\nRL, also referred to as stochastic approximate dynamic programming, is a goal-directed sequential-learning-from-interaction paradigm. The learner or agent is not told what to do but instead has to learn which actions or decisions yield a maximum reward through interaction with an uncertain environment without losing too much reward along the way. This way of learning from interaction to achieve a goal must be achieved in balance with the exploration and exploitation of possible actions. Another key characteristic of this type of problem is its sequential nature, where the actions taken by the agent affect the environment itself and, therefore, the subsequent data it receives and the subsequent actions to be taken.\nMathematically, such problems are formulated in the framework of the Markov decision process (MDP) that primarily arises in the field of optimal control. An RL problem consists of two principal parts: the agent, or decision-making engine, and the environment, the interactive world for an agent (in this case, the reservoir). Sequentially, at each timestep, the agent takes an action (e.g., changing control rates or deciding a well location) that makes the environment (reservoir) transition from one state to another. Next, the agent receives a reward (e.g., a cash flow) and an observation of the state of the environment (partial or total) before taking the next action.\nAll relevant information informing the agent of the state of the system is assumed to be included in the last state observed by the agent (Markov property). If the agent observes the full environment state once it has acted, the MDP is said to be fully observable; otherwise, a partially observable Markov decision process (POMDP) results. The agent’s objective is to learn policy mapping from states (MDPs) or histories (POMDPs) to actions such that the agent’s cumulated (discounted) reward in the long run is maximized.\n",
        "link": "http://dx.doi.org/10.2118/0921-0046-jpt"
    },
    {
        "id": 28377,
        "title": "Inverse Reinforcement Learning of Interaction Dynamics from Demonstrations",
        "authors": "Mostafa Hussein, Momotaz Begum, Marek Petrik",
        "published": "2019-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8793867"
    },
    {
        "id": 28378,
        "title": "Policy Gradient",
        "authors": "Ruitong Huang, Tianyang Yu, Zihan Ding, Shanghang Zhang",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_5"
    },
    {
        "id": 28379,
        "title": "Reliable Reinforcement Learning Based NOMA Schemes for URLLC",
        "authors": "Waleed Ahsan, Wenqiang Yi, Yuanwei Liu, Arumugam Nallanathan",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685621"
    },
    {
        "id": 28380,
        "title": "Regret Bounds for Reinforcement Learning via Markov Chain Concentration",
        "authors": "Ronald Ortner",
        "published": "2020-1-23",
        "citations": 11,
        "abstract": "We give a simple optimistic algorithm for which it is easy to derive regret bounds of O(sqrt{t-mix SAT}) steps in uniformly ergodic Markov decision processes with S states, A actions, and mixing time parameter t-mix. These bounds are the first regret bounds in the general, non-episodic setting with an optimal dependence on all given parameters. They could only be improved by using an alternative mixing time parameter.",
        "link": "http://dx.doi.org/10.1613/jair.1.11316"
    },
    {
        "id": 28381,
        "title": "Evolutionary Reinforcement Learning: A Survey",
        "authors": "Hui Bai, Ran Cheng, Yaochu Jin",
        "published": "2023-1",
        "citations": 4,
        "abstract": "Reinforcement learning (RL) is a machine learning approach that trains agents to maximize cumulative rewards through interactions with environments. The integration of RL with deep learning has recently resulted in impressive achievements in a wide range of challenging tasks, including board games, arcade games, and robot control. Despite these successes, several critical challenges remain, such as brittle convergence properties caused by sensitive hyperparameters, difficulties in temporal credit assignment with long time horizons and sparse rewards, a lack of diverse exploration, particularly in continuous search space scenarios, challenges in credit assignment in multi-agent RL, and conflicting objectives for rewards. Evolutionary computation (EC), which maintains a population of learning agents, has demonstrated promising performance in addressing these limitations. This article presents a comprehensive survey of state-of-the-art methods for integrating EC into RL, referred to as evolutionary reinforcement learning (EvoRL). We categorize EvoRL methods according to key research areas in RL, including hyperparameter optimization, policy search, exploration, reward shaping, meta-RL, and multi-objective RL. We then discuss future research directions in terms of efficient methods, benchmarks, and scalable platforms. This survey serves as a resource for researchers and practitioners interested in the field of EvoRL, highlighting the important challenges and opportunities for future research. With the help of this survey, researchers and practitioners can develop more efficient methods and tailored benchmarks for EvoRL, further advancing this promising cross-disciplinary research field.",
        "link": "http://dx.doi.org/10.34133/icomputing.0025"
    },
    {
        "id": 28382,
        "title": "Similar Assembly State Discriminator for Reinforcement Learning-Based Robotic Connector Assembly",
        "authors": "Jun-Wan Yun, Minwoo Na, Yuhyeon Hwang, Jae-Bok Song",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4622927"
    },
    {
        "id": 28383,
        "title": "Supervised Reinforcement Learning via Value Function",
        "authors": "Yaozong Pan, Jian Zhang, Chunhui Yuan, Haitao Yang",
        "published": "2019-4-24",
        "citations": 1,
        "abstract": "Using expert samples to improve the performance of reinforcement learning (RL) algorithms has become one of the focuses of research nowadays. However, in different application scenarios, it is hard to guarantee both the quantity and quality of expert samples, which prohibits the practical application and performance of such algorithms. In this paper, a novel RL decision optimization method is proposed. The proposed method is capable of reducing the dependence on expert samples via incorporating the decision-making evaluation mechanism. By introducing supervised learning (SL), our method optimizes the decision making of the RL algorithm by using demonstrations or expert samples. Experiments are conducted in Pendulum and Puckworld scenarios to test the proposed method, and we use representative algorithms such as deep Q-network (DQN) and Double DQN (DDQN) as benchmarks. The results demonstrate that the method adopted in this paper can effectively improve the decision-making performance of agents even when the expert samples are not available.",
        "link": "http://dx.doi.org/10.3390/sym11040590"
    },
    {
        "id": 28384,
        "title": "Deterministic Sequencing of Exploration and Exploitation for Reinforcement Learning",
        "authors": "Piyush Gupta, Vaibhav Srivastava",
        "published": "2022-12-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc51059.2022.9992857"
    },
    {
        "id": 28385,
        "title": "Centralized Coordination of DER Smart Inverters using Deep Reinforcement Learning",
        "authors": "Daniel Glover, Anamika Dubey",
        "published": "2023-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ias54024.2023.10406599"
    },
    {
        "id": 28386,
        "title": "Reinforcement Learning Environment for Tactical Networks",
        "authors": "Thies Mohlenhof, Norman Jansen, Wiam Rachid",
        "published": "2021-5-4",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmcis52405.2021.9486411"
    },
    {
        "id": 28387,
        "title": "A reinforcement learning model to inform optimal decision paths for HIV elimination<sup>1</sup>",
        "authors": "Seyedeh N. Khatami, Chaitra Gopalappa",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractThe ‘Ending the HIV Epidemic (EHE)’ national plan aims to reduce annual HIV incidence in the United States from 38,000 in 2015 to 9,300 by 2025 and 3,300 by 2030. Diagnosis and treatment are two most effective interventions, and thus, identifying corresponding optimal combinations of testing and retention-in-care rates would help inform implementation of relevant programs. Considering the dynamic and stochastic complexity of the disease and the time dynamics of decision-making, solving for optimal combinations using commonly used methods of parametric optimization or exhaustive evaluation of pre-selected options are infeasible. Reinforcement learning (RL), an artificial intelligence method, is ideal; however, training RL algorithms and ensuring convergence to optimality are computationally challenging for large-scale stochastic problems. We evaluate its feasibility in the context of the EHE goal.We trained an RL algorithm to identify a ‘sequence’ of combinations of HIV-testing and retention-in-care rates at 5-year intervals over 2015-2070, which optimally leads towards HIV elimination. We defined optimality as a sequence that maximizes quality-adjusted-life-years lived and minimizes HIV-testing and care-and-treatment costs. We show that solving for testing and retention-in-care rates through appropriate reformulation using proxy decision-metrics overcomes the computational challenges of RL. We used a stochastic agent-based simulation to train the RL algorithm. As there is variability in support-programs needed to address barriers to care-access, we evaluated the sensitivity of optimal decisions to three cost-functions.The model suggests to scale-up retention-in-care programs to achieve and maintain high annual retention-rates while initiating with a high testing-frequency but relaxing it over a 10-year period as incidence decreases. Results were mainly robust to the uncertainty in costs. However, testing and retention-in-care alone did not achieve the 2030 EHE targets, suggesting the need for additional interventions. The results from the model demonstrated convergence. RL is suitable for evaluating phased public health decisions for infectious disease control.",
        "link": "http://dx.doi.org/10.1101/2021.07.11.21260328"
    },
    {
        "id": 28388,
        "title": "Automatic Hyperparameter Tuning in Deep Convolutional Neural Networks Using Asynchronous Reinforcement Learning",
        "authors": "Patrick Neary",
        "published": "2018-7",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc.2018.00017"
    },
    {
        "id": 28389,
        "title": "ACB Scheme based on Reinforcement Learning in M2M Communication",
        "authors": "Dong Zhang, Jianlong Liu, Wenan Zhou",
        "published": "2020-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom42002.2020.9322144"
    },
    {
        "id": 28390,
        "title": "Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning",
        "authors": "Yoshinari Motokawa, Toshiharu Sugawara",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191825"
    },
    {
        "id": 28391,
        "title": "Intelligent Steam Turbine Start-Up Control Based on Deep Reinforcement Learning",
        "authors": "Guangya Zhu, Ding Guo, Jinxing Li, Yonghui Xie, Di Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4716978"
    },
    {
        "id": 28392,
        "title": "Deep reinforcement learning in loop fusion problem",
        "authors": "Mahsa Ziraksima, Shahriar Lotfi, Jafar Razmara",
        "published": "2022-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2022.01.032"
    },
    {
        "id": 28393,
        "title": "Reinforcement Learning and Portfolio Allocation: Challenging Traditional Allocation Methods",
        "authors": "Matus Lavko, Tony Klein, Thomas Walther",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4346043"
    },
    {
        "id": 28394,
        "title": "Microbial Interactions from a New Perspective: Reinforcement Learning Reveals New Insights into Microbiome Evolution",
        "authors": "Parsa Ghadermazi, Siu Hung Joshua Chan",
        "published": "No Date",
        "citations": 5,
        "abstract": "AbstractMicrobes are essential part of all ecosystems, influencing material flow and shaping their surroundings. Metabolic modeling has been a useful tool and provided tremendous insights into microbial community metabolism. However, current methods based on flux balance analysis (FBA) usually fail to predict metabolic and regulatory strategies that lead to long-term survival and stability especially in heterogenous communities. Here we introduce a novel reinforcement learning algorithm, Self-Playing Microbes in Dynamic FBA, that treats microbial metabolism as a decision-making process, allowing individual microbial agents to evolve by learning and adapting metabolic strategies for enhanced long-term fitness. This algorithm predicts what microbial flux regulation policies will stabilize in the dynamic ecosystem of interest in presence of other microbes with minimal reliance on predefined strategies. Throughout this article, we present several scenarios wherein our algorithm outperforms existing methods in reproducing outcomes, and we explore the biological significance of these predictions.",
        "link": "http://dx.doi.org/10.1101/2023.05.07.539711"
    },
    {
        "id": 28395,
        "title": "Deep Reinforcement Learning Algorithm for Wellbore Cleaning Across Drilling Operation",
        "authors": "S. Keshavarz, A. Elmgerbi, G. Thonhauser",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202439018"
    },
    {
        "id": 28396,
        "title": "Dynamic Cloth Manipulation with Deep Reinforcement Learning",
        "authors": "Rishabh Jangir, Guillem Alenya, Carme Torras",
        "published": "2020-5",
        "citations": 52,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra40945.2020.9196659"
    },
    {
        "id": 28397,
        "title": "A Survey of Multi-Task Deep Reinforcement Learning",
        "authors": "Nelson Vithayathil Varghese, Qusay H. Mahmoud",
        "published": "2020-8-22",
        "citations": 62,
        "abstract": "Driven by the recent technological advancements within the field of artificial intelligence research, deep learning has emerged as a promising representation learning technique across all of the machine learning classes, especially within the reinforcement learning arena. This new direction has given rise to the evolution of a new technological domain named deep reinforcement learning, which combines the representational learning power of deep learning with existing reinforcement learning methods. Undoubtedly, the inception of deep reinforcement learning has played a vital role in optimizing the performance of reinforcement learning-based intelligent agents with model-free based approaches. Although these methods could improve the performance of agents to a greater extent, they were mainly limited to systems that adopted reinforcement learning algorithms focused on learning a single task. At the same moment, the aforementioned approach was found to be relatively data-inefficient, particularly when reinforcement learning agents needed to interact with more complex and rich data environments. This is primarily due to the limited applicability of deep reinforcement learning algorithms to many scenarios across related tasks from the same environment. The objective of this paper is to survey the research challenges associated with multi-tasking within the deep reinforcement arena and present the state-of-the-art approaches by comparing and contrasting recent solutions, namely DISTRAL (DIStill & TRAnsfer Learning), IMPALA(Importance Weighted Actor-Learner Architecture) and PopArt that aim to address core challenges such as scalability, distraction dilemma, partial observability, catastrophic forgetting and negative knowledge transfer.",
        "link": "http://dx.doi.org/10.3390/electronics9091363"
    },
    {
        "id": 28398,
        "title": "Load Transfer Optimization Method of Active Distribution Network Based on Graph Structure Deep Reinforcement Learning",
        "authors": "Pingping Yang, Tie Chen, Hongxin Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4712530"
    },
    {
        "id": 28399,
        "title": "An Effective Method for Satellite Mission Scheduling Based on Reinforcement Learning",
        "authors": "Xiaoli Bao, Shumei Zhang, Xiuyun Zhang",
        "published": "2020-11-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327581"
    },
    {
        "id": 28400,
        "title": "Inverse Reinforcement Learning via Neural Network in Driver Behavior Modeling",
        "authors": "QiJie Zou, Haoyu Li, Rubo Zhang",
        "published": "2018-6",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2018.8500666"
    },
    {
        "id": 28401,
        "title": "Deep Q-learning based Reinforcement Learning Approach for Radio Frequency Fingerprinting in Internet of Things",
        "authors": "Jiao Ye, Dongyang Xu, Tiantian Zhang, Keping Yu",
        "published": "2024-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icce59016.2024.10444146"
    },
    {
        "id": 28402,
        "title": "A composite learning method for multi-ship collision avoidance based on reinforcement learning and inverse control",
        "authors": "Shuo Xie, Xiumin Chu, Mao Zheng, Chenguang Liu",
        "published": "2020-10",
        "citations": 35,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2020.05.089"
    },
    {
        "id": 28403,
        "title": "Gait Self-learning for Damaged Robots Combining Bionic Inspiration and Deep Reinforcement Learning",
        "authors": "Ming Zeng, Yu Ma, Zhijing Wang, Qi Li",
        "published": "2021-7-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9549968"
    },
    {
        "id": 28404,
        "title": "Learning to Walk on a Human Musculoskeletal Model Wearing a Knee Orthosis via Deep Reinforcement Learning",
        "authors": "Omer Kayan, Hulya Yalcin",
        "published": "2023-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hora58378.2023.10156789"
    },
    {
        "id": 28405,
        "title": "Dialogue Generation: From Imitation Learning to Inverse Reinforcement Learning",
        "authors": "Ziming Li, Julia Kiseleva, Maarten De Rijke",
        "published": "2019-7-17",
        "citations": 13,
        "abstract": "The performance of adversarial dialogue generation models relies on the quality of the reward signal produced by the discriminator. The reward signal from a poor discriminator can be very sparse and unstable, which may lead the generator to fall into a local optimum or to produce nonsense replies. To alleviate the first problem, we first extend a recently proposed adversarial dialogue generation method to an adversarial imitation learning solution. Then, in the framework of adversarial inverse reinforcement learning, we propose a new reward model for dialogue generation that can provide a more accurate and precise reward signal for generator training. We evaluate the performance of the resulting model with automatic metrics and human evaluations in two annotation settings. Our experimental results demonstrate that our model can generate more high-quality responses and achieve higher overall performance than the state-of-the-art.",
        "link": "http://dx.doi.org/10.1609/aaai.v33i01.33016722"
    },
    {
        "id": 28406,
        "title": "A teaching-learning-based optimization algorithm with reinforcement learning to address wind farm layout optimization problem",
        "authors": "Xiaobing Yu, Wen Zhang",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.asoc.2023.111135"
    },
    {
        "id": 28407,
        "title": "Goal exploration augmentation via pre-trained skills for sparse-reward long-horizon goal-conditioned reinforcement learning",
        "authors": "Lisheng Wu, Ke Chen",
        "published": "2024-2-5",
        "citations": 0,
        "abstract": "AbstractReinforcement learning often struggles to accomplish a sparse-reward long-horizon task in a complex environment. Goal-conditioned reinforcement learning (GCRL) has been employed to tackle this difficult problem via a curriculum of easy-to-reach sub-goals. In GCRL, exploring novel sub-goals is essential for the agent to ultimately find the pathway to the desired goal. How to explore novel sub-goals efficiently is one of the most challenging issues in GCRL. Several goal exploration methods have been proposed to address this issue but still struggle to find the desired goals efficiently. In this paper, we propose a novel learning objective by optimizing the entropy of both achieved and new goals to be explored for more efficient goal exploration in sub-goal selection based GCRL. To optimize this objective, we first explore and exploit the frequently occurring goal-transition patterns mined in the environments similar to the current task to compose skills via skill learning. Then, the pre-trained skills are applied in goal exploration with theoretical justification. Evaluation on a variety of spare-reward long-horizon benchmark tasks suggests that incorporating our method into several state-of-the-art GCRL baselines significantly boosts their exploration efficiency while improving or maintaining their performance.",
        "link": "http://dx.doi.org/10.1007/s10994-023-06503-w"
    },
    {
        "id": 28408,
        "title": "Goal-conditioned offline reinforcement learning through state space partitioning",
        "authors": "Mianchu Wang, Yue Jin, Giovanni Montana",
        "published": "2024-2-5",
        "citations": 0,
        "abstract": "AbstractOffline reinforcement learning (RL) aims to create policies for sequential decision-making using exclusively offline datasets. This presents a significant challenge, especially when attempting to accomplish multiple distinct goals or outcomes within a given scenario while receiving sparse rewards. Prior methods using advantage weighting for offline goal-conditioned learning improve policies monotonically. However, they still face challenges from distribution shift and multi-modality that arise due to conflicting ways to reach a goal. This issue is especially challenging in long-horizon tasks, where the presence of multiple, often conflicting, solutions makes it hard to identify a single optimal policy for transitioning from a state to a desired goal. To address these challenges, we introduce a complementary advantage-based weighting scheme that incorporates an additional source of inductive bias. Given a value-based partitioning of the state space, the contribution of actions expected to lead to target regions that are easier to reach, compared to the final goal, is further increased. Our proposed approach, Dual-Advantage Weighted Offline Goal-conditioned RL, outperforms several competing offline algorithms in widely used benchmarks. Furthermore, we provide a theoretical guarantee that the learned policy will not be inferior to the underlying behavior policy.",
        "link": "http://dx.doi.org/10.1007/s10994-023-06500-z"
    },
    {
        "id": 28409,
        "title": "Learning basketball dribbling skills using trajectory optimization and deep reinforcement learning",
        "authors": "Libin Liu, Jessica Hodgins",
        "published": "2018-8-31",
        "citations": 59,
        "abstract": "Basketball is one of the world's most popular sports because of the agility and speed demonstrated by the players. This agility and speed makes designing controllers to realize robust control of basketball skills a challenge for physics-based character animation. The highly dynamic behaviors and precise manipulation of the ball that occur in the game are difficult to reproduce for simulated players. In this paper, we present an approach for learning robust basketball dribbling controllers from motion capture data. Our system decouples a basketball controller into locomotion control and arm control components and learns each component separately. To achieve robust control of the ball, we develop an efficient pipeline based on trajectory optimization and deep reinforcement learning and learn non-linear arm control policies. We also present a technique for learning skills and the transition between skills simultaneously. Our system is capable of learning robust controllers for various basketball dribbling skills, such as dribbling between the legs and crossover moves. The resulting control graphs enable a simulated player to perform transitions between these skills and respond to user interaction.",
        "link": "http://dx.doi.org/10.1145/3197517.3201315"
    },
    {
        "id": 28410,
        "title": "Learning Kinematic Feasibility for Mobile Manipulation Through Deep Reinforcement Learning",
        "authors": "Daniel Honerkamp, Tim Welschehold, Abhinav Valada",
        "published": "2021-10",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lra.2021.3092685"
    },
    {
        "id": 28411,
        "title": "Hierarchical Reinforcement Learning: A Survey and Open Research Challenges",
        "authors": "Matthias Hutsebaut-Buysse, Kevin Mets, Steven Latré",
        "published": "2022-2-17",
        "citations": 23,
        "abstract": "Reinforcement learning (RL) allows an agent to solve sequential decision-making problems by interacting with an environment in a trial-and-error fashion. When these environments are very complex, pure random exploration of possible solutions often fails, or is very sample inefficient, requiring an unreasonable amount of interaction with the environment. Hierarchical reinforcement learning (HRL) utilizes forms of temporal- and state-abstractions in order to tackle these challenges, while simultaneously paving the road for behavior reuse and increased interpretability of RL systems. In this survey paper we first introduce a selection of problem-specific approaches, which provided insight in how to utilize often handcrafted abstractions in specific task settings. We then introduce the Options framework, which provides a more generic approach, allowing abstractions to be discovered and learned semi-automatically. Afterwards we introduce the goal-conditional approach, which allows sub-behaviors to be embedded in a continuous space. In order to further advance the development of HRL agents, capable of simultaneously learning abstractions and how to use them, solely from interaction with complex high dimensional environments, we also identify a set of promising research directions.",
        "link": "http://dx.doi.org/10.3390/make4010009"
    },
    {
        "id": 28412,
        "title": "Reinforcement learning in robotic motion planning by combined experience-based planning and self-imitation learning",
        "authors": "Sha Luo, Lambert Schomaker",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.robot.2023.104545"
    },
    {
        "id": 28413,
        "title": "Reinforcement Learning in Energy Trading Game Among Smart Microgrids",
        "authors": "Huiwei Wang, Huaqing Li, Bo Zhou",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-4528-7_7"
    },
    {
        "id": 28414,
        "title": "Learning an Efficient Gait Cycle of a Biped Robot Based on Reinforcement Learning and Artificial Neural Networks",
        "authors": "Cristyan Gil, Hiram Calvo, Humberto Sossa",
        "published": "2019-2-1",
        "citations": 32,
        "abstract": "Programming robots for performing different activities requires calculating sequences of values of their joints by taking into account many factors, such as stability and efficiency, at the same time. Particularly for walking, state of the art techniques to approximate these sequences are based on reinforcement learning (RL). In this work we propose a multi-level system, where the same RL method is used first to learn the configuration of robot joints (poses) that allow it to stand with stability, and then in the second level, we find the sequence of poses that let it reach the furthest distance in the shortest time, while avoiding falling down and keeping a straight path. In order to evaluate this, we focus on measuring the time it takes for the robot to travel a certain distance. To our knowledge, this is the first work focusing both on speed and precision of the trajectory at the same time. We implement our model in a simulated environment using q-learning. We compare with the built-in walking modes of an NAO robot by improving normal-speed and enhancing robustness in fast-speed. The proposed model can be extended to other tasks and is independent of a particular robot model.",
        "link": "http://dx.doi.org/10.3390/app9030502"
    },
    {
        "id": 28415,
        "title": "Sample-efficient Reinforcement Learning Representation Learning with Curiosity Contrastive Forward Dynamics Model",
        "authors": "Thanh Nguyen, Tung M. Luu, Thang Vu, Chang D. Yoo",
        "published": "2021-9-27",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros51168.2021.9636536"
    },
    {
        "id": 28416,
        "title": "Evolutionary Deep Reinforcement Learning Environment: Transfer Learning-Based Genetic Algorithm",
        "authors": "Badr Hirchoua, Imadeddine Mountasser, Brahim Ouhbi, Bouchra Frikh",
        "published": "2021-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3487664.3487698"
    },
    {
        "id": 28417,
        "title": "Generalization Ability of Deep Reinforcement Learning-based Navigation Based on Curriculum Learning",
        "authors": "Yuchen He, Simone Baldi",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "Abstract\nIn this paper, curriculum learning is studied as an approach to improve generalization ability in navigation tasks, that is, improve the agent’s ability of navigating in scenarios different from those used for training. The agent is trained based on the TD3 algorithm, and curriculum learning selects different stages (i.e. different curricula) of Empty/Sparse/Normal worlds for training. Via extensive numerical comparisons with agents trained under such curricula, it is shown that properly used curriculum learning improves the agent’s ability of generalization. Furthermore, an automatic curriculum learning (Auto-CL) approach is proposed. Auto-CL is shown to have even better generalization than the standard curriculum learning, since it makes the agent able to navigate in new environments with more than 6% shorter paths in more than 21% shorter time.",
        "link": "http://dx.doi.org/10.1088/1742-6596/2593/1/012003"
    },
    {
        "id": 28418,
        "title": "Continuous Value Iteration (CVI) Reinforcement Learning and Imaginary Experience Replay (IER) For Learning Multi-Goal, Continuous Action and State Space Controllers",
        "authors": "Andreas Gerken, Michael Spranger",
        "published": "2019-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8794347"
    },
    {
        "id": 28419,
        "title": "A Deep Reinforcement Learning Approach for Federated Learning Optimization with UAV Trajectory Planning",
        "authors": "Chunyu Zhang, Yiming Liu, Zhi Zhang",
        "published": "2023-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pimrc56721.2023.10293884"
    },
    {
        "id": 28420,
        "title": "An Improved Reinforcement Learning Method Based on Unsupervised Learning",
        "authors": "Xin Chang, Yanbin Li, Guanjie Zhang, Donghui Liu, Changjun Fu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2024.3351696"
    },
    {
        "id": 28421,
        "title": "Poster Abstract: Deep Learning Workloads Scheduling with Reinforcement Learning on GPU Clusters",
        "authors": "Zhaoyun Chen, Lei Luo, Wei Quan, Mei Wen, Chunyuan Zhang",
        "published": "2019-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infcomw.2019.8845276"
    },
    {
        "id": 28422,
        "title": "Agent-advising approaches in an interactive reinforcement learning scenario",
        "authors": "Francisco Cruz, Peter Wuppen, Sven Magg, Alvin Fazrie, Stefan Wermter",
        "published": "2017-9",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/devlrn.2017.8329809"
    },
    {
        "id": 28423,
        "title": "Trajectory-Based Active Inverse Reinforcement Learning for Learning from Demonstration",
        "authors": "Sehee Kweon, Himchan Hwang, Frank C. Park",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iccas59377.2023.10316798"
    },
    {
        "id": 28424,
        "title": "Approximate Inverse Reinforcement Learning from Vision-based Imitation Learning",
        "authors": "Keuntaek Lee, Bogdan Vlahov, Jason Gibson, James M. Rehg, Evangelos A. Theodorou",
        "published": "2021-5-30",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9560916"
    },
    {
        "id": 28425,
        "title": "Maintaining flexibility in smart grid consumption through deep learning and deep reinforcement learning",
        "authors": "Fernando Gallego, Cristian Martín, Manuel Díaz, Daniel Garrido",
        "published": "2023-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.egyai.2023.100241"
    },
    {
        "id": 28426,
        "title": "Deep Reinforcement Learning and Model Predictive Control in Hybrid Deep Learning for Rubber Yield Forecast",
        "authors": "Lince Rachel Varghese, K. Vanitha",
        "published": "2021-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18280/ria.350502"
    },
    {
        "id": 28427,
        "title": "Learning about Other Persons’ Character Traits Relies on Combining Reinforcement Learning with Representations of Trait Similarities",
        "authors": "Koen Frolichs, Benjamin Kuper-Smith, Jan Gläscher, Gabriela Rosenblau, Christoph Korn",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2019.1236-0"
    },
    {
        "id": 28428,
        "title": "Correction: Linking Individual Learning Styles to Approach-Avoidance Motivational Traits and Computational Aspects of Reinforcement Learning",
        "authors": "Kristoffer Carl Aberg, Kimberly C. Doell, Sophie Schwartz",
        "published": "2017-2-13",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1371/journal.pone.0172379"
    },
    {
        "id": 28429,
        "title": "Study on recommendation of personalised learning resources based on deep reinforcement learning",
        "authors": "Zilong Li, Hongdong Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijict.2023.134832"
    },
    {
        "id": 28430,
        "title": "Self-Learning Robot Autonomous Navigation with Deep Reinforcement Learning Techniques",
        "authors": "Borja Pintos Gómez de las Heras, Rafael Martínez-Tomás, José Manuel Cuadra Troncoso",
        "published": "2023-12-30",
        "citations": 0,
        "abstract": "Complex and high-computational-cost algorithms are usually the state-of-the-art solution for autonomous driving cases in which non-holonomic robots must be controlled in scenarios with spatial restrictions and interaction with dynamic obstacles while fulfilling at all times safety, comfort, and legal requirements. These highly complex software solutions must cover the high variability of use cases that might appear in traffic conditions, especially when involving scenarios with dynamic obstacles. Reinforcement learning algorithms are seen as a powerful tool in autonomous driving scenarios since the complexity of the algorithm is automatically learned by trial and error with the help of simple reward functions. This paper proposes a methodology to properly define simple reward functions and come up automatically with a complex and successful autonomous driving policy. The proposed methodology has no motion planning module so that the computational power can be limited like in the reactive robotic paradigm. Reactions are learned based on the maximization of the cumulative reward obtained during the learning process. Since the motion is based on the cumulative reward, the proposed algorithm is not bound to any embedded model of the robot and is not being affected by uncertainties of these models or estimators, making it possible to generate trajectories with the consideration of non-holonomic constrains. This paper explains the proposed methodology and discusses the setup of experiments and the results for the validation of the methodology in scenarios with dynamic obstacles. A comparison between the reinforcement learning algorithm and state-of-the-art approaches is also carried out to highlight how the methodology proposed outperforms state-of-the-art algorithms.",
        "link": "http://dx.doi.org/10.3390/app14010366"
    },
    {
        "id": 28431,
        "title": "Multi-Agent Reinforcement Learning-Based Controller Load Balancing in SD-WANs",
        "authors": "Zehua Guo",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-4874-9_3"
    },
    {
        "id": 28432,
        "title": "Deep Reinforcement Learning by Parallelizing Reward and Punishment using the MaxPain Architecture",
        "authors": "Jiexin Wang, Stefan Elfwing, Eiji Uchibe",
        "published": "2018-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/devlrn.2018.8761044"
    },
    {
        "id": 28433,
        "title": "Learning-Based Intelligent Reflecting Surface-Aided Secure Maritime Communications",
        "authors": "Liang Xiao, Helin Yang, Weihua Zhuang, Minghui Min",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-32138-2_2"
    },
    {
        "id": 28434,
        "title": "A Deep Reinforcement Learning Control Method for a Four-Link Brachiation Robot",
        "authors": "Xuanyu Zhang, Zishang Ji, Haodong Zhang, Rong Xiong",
        "published": "2023-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlccim60412.2023.00085"
    },
    {
        "id": 28435,
        "title": "Learning Heterogeneous Strategies via Graph-based Multi-agent Reinforcement Learning",
        "authors": "Yang Li, Xiangfeng Luo, Shaorong Xie",
        "published": "2021-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai52525.2021.00112"
    },
    {
        "id": 28436,
        "title": "A Hybrid Reinforcement Learning and Genetic Algorithm for VLSI Floorplanning",
        "authors": "Ke Liu, Jian Gu, Hao Gu, Ziran Zhu",
        "published": "2023-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3587716.3587785"
    },
    {
        "id": 28437,
        "title": "Inverse Reinforcement Learning with Hybrid-weight Trust-region Optimization and Curriculum Learning for Autonomous Maneuvering",
        "authors": "Yu Shen, Weizi Li, Ming C. Lin",
        "published": "2022-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981103"
    },
    {
        "id": 28438,
        "title": "Bounded Rationality in Learning, Perception, Decision-Making, and Stochastic Games",
        "authors": "Panagiotis Tsiotras",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_17"
    },
    {
        "id": 28439,
        "title": "Goal-Conditioned Reinforcement Learning with Latent Representations using Contrastive Learning",
        "authors": "Takaya YAMADA, Koich OGAWARA",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1299/jsmermd.2021.1p1-i15"
    },
    {
        "id": 28440,
        "title": "Distributed transmission control for wireless networks using multi-agent reinforcement learning",
        "authors": "Collin Farquhar, Prem Kumar, Anu Jagannath, Jithin Jagannath",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2621086"
    },
    {
        "id": 28441,
        "title": "A learning search algorithm with propagational reinforcement learning",
        "authors": "Wei Zhang",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-020-02117-0"
    },
    {
        "id": 28442,
        "title": "Scan Chain Clustering and Optimization with Constrained Clustering and Reinforcement Learning",
        "authors": "Naiju Karim Abdul, George Antony, Rahul M. Rao, Suriya T. Skariah",
        "published": "2022-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlcad55463.2022.9900094"
    },
    {
        "id": 28443,
        "title": "An improved deep reinforcement learning for robot navigation",
        "authors": "Qifeng Zheng, Xiaogang Huang, Chen Dong, Yuting Liu, Dong Chen",
        "published": "2023-5-25",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2675144"
    },
    {
        "id": 28444,
        "title": "Enhancing Control in Manufacturing and Microgrid Systems: Deep Reinforcement Learning with Double Q-Learning",
        "authors": "Mohamed Ballouch, Omar Souissi, Mohammed Raiss El-Fenni",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sita60746.2023.10373737"
    },
    {
        "id": 28445,
        "title": "Learning Virtual Grasp with Failed Demonstrations via Bayesian Inverse Reinforcement Learning",
        "authors": "Xu Xie, Changyang Li, Chi Zhang, Yixin Zhu, Song-Chun Zhu",
        "published": "2019-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros40897.2019.8968063"
    },
    {
        "id": 28446,
        "title": "Multi-Robot Real-time Game Strategy Learning based on Deep Reinforcement Learning",
        "authors": "Ki Deng, Yanjie Li, Songshuo Lu, Yongjin Mu, Xizheng Pang, Qi Liu",
        "published": "2022-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio55434.2022.10011827"
    },
    {
        "id": 28447,
        "title": "A Review of Three Methods of Artificial Intelligence in Smart Grid Cyber Security: Machine Learning, Reinforcement Learning, Ensemble Methods",
        "authors": "Luyao Xu",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "Renewable energy is gradually replacing traditional fossil fuels. The change of power generation energy structure brings new challenges to the traditional power grid. Through the efficient bidirectional movement of electricity and information, smart grids might include renewable energy. For the complex informational and financial operations required by smart grid, communication systems are crucial, but they also make smart grid vulnerable to numerous cyber attacks. Smart grid cyber security has been widely concerned. The purpose of this paper is to explore the use of artificial intelligence technology in smart grid cyber security. Three methods in the field of artificial intelligence are highlighted: Machine Learning, Reinforcement Learning, and Ensemble Methods. This paper summarizes the benefits and drawbacks of their use of smart grid cyber security, and further makes a qualitative comparison of the three methods from multiple performance indicators.",
        "link": "http://dx.doi.org/10.54097/79xf0y91"
    },
    {
        "id": 28448,
        "title": "Reinforcement Learning for Hyperparameter Tuning in Deep Learning-based Side-channel Analysis",
        "authors": "Jorai Rijsdijk, Lichao Wu, Guilherme Perin, Stjepan Picek",
        "published": "2021-7-9",
        "citations": 55,
        "abstract": "Deep learning represents a powerful set of techniques for profiling sidechannel analysis. The results in the last few years show that neural network architectures like multilayer perceptron and convolutional neural networks give strong attack performance where it is possible to break targets protected with various countermeasures. Considering that deep learning techniques commonly have a plethora of hyperparameters to tune, it is clear that such top attack results can come with a high price in preparing the attack. This is especially problematic as the side-channel community commonly uses random search or grid search techniques to look for the best hyperparameters.In this paper, we propose to use reinforcement learning to tune the convolutional neural network hyperparameters. In our framework, we investigate the Q-Learning paradigm and develop two reward functions that use side-channel metrics. We mount an investigation on three commonly used datasets and two leakage models where the results show that reinforcement learning can find convolutional neural networks exhibiting top performance while having small numbers of trainable parameters. We note that our approach is automated and can be easily adapted to different datasets. Several of our newly developed architectures outperform the current state-of-the-art results. Finally, we make our source code publicly available.\r\nhttps://github.com/AISyLab/Reinforcement-Learning-for-SCA",
        "link": "http://dx.doi.org/10.46586/tches.v2021.i3.677-707"
    },
    {
        "id": 28449,
        "title": "Deep Q-Learning Based Reinforcement Learning Approach for Network Intrusion Detection",
        "authors": "Hooman Alavizadeh, Hootan Alavizadeh, Julian Jang-Jaccard",
        "published": "2022-3-11",
        "citations": 52,
        "abstract": "The rise of the new generation of cyber threats demands more sophisticated and intelligent cyber defense solutions equipped with autonomous agents capable of learning to make decisions without the knowledge of human experts. Several reinforcement learning methods (e.g., Markov) for automated network intrusion tasks have been proposed in recent years. In this paper, we introduce a new generation of the network intrusion detection method, which combines a Q-learning based reinforcement learning with a deep feed forward neural network method for network intrusion detection. Our proposed Deep Q-Learning (DQL) model provides an ongoing auto-learning capability for a network environment that can detect different types of network intrusions using an automated trial-error approach and continuously enhance its detection capabilities. We provide the details of fine-tuning different hyperparameters involved in the DQL model for more effective self-learning. According to our extensive experimental results based on the NSL-KDD dataset, we confirm that the lower discount factor, which is set as 0.001 under 250 episodes of training, yields the best performance results. Our experimental results also show that our proposed DQL is highly effective in detecting different intrusion classes and outperforms other similar machine learning approaches.",
        "link": "http://dx.doi.org/10.3390/computers11030041"
    },
    {
        "id": 28450,
        "title": "Energy-efficient and damage-recovery slithering gait design for a snake-like robot based on reinforcement learning and inverse reinforcement learning",
        "authors": "Zhenshan Bing, Christian Lemke, Long Cheng, Kai Huang, Alois Knoll",
        "published": "2020-9",
        "citations": 29,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2020.05.029"
    },
    {
        "id": 28451,
        "title": "Survival-Oriented Reinforcement Learning Model: An Effcient and Robust Deep Reinforcement Learning Algorithm for Autonomous Driving Problem",
        "authors": "Changkun Ye, Huimin Ma, Xiaoqin Zhang, Kai Zhang, Shaodi You",
        "published": "2017",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-71589-6_36"
    },
    {
        "id": 28452,
        "title": "Study on Aggregator’s Strategy of Controlling Electric Vehicles to Compensate Imbalances in Power Systems Using Reinforcement Learning",
        "authors": "Ryuhei Furuta,  , Ryuji Matsuhashi",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijoee.7.2.67-71"
    },
    {
        "id": 28453,
        "title": "Towards modeling the learning process of aviators using deep reinforcement learning",
        "authors": "Joost van Oijen, Gerald Poppinga, Olaf Brouwer, Andi Aliko, Jan Joris Roessingh",
        "published": "2017-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc.2017.8123162"
    },
    {
        "id": 28454,
        "title": "Improving Deep Reinforcement Learning for Financial Trading Using Deep Adaptive Group-Based Normalization",
        "authors": "Angelos Nalmpantis, Nikolaos Passalis, Avraam Tsantekidis, Anastasios Tefas",
        "published": "2021-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp52302.2021.9596155"
    },
    {
        "id": 28455,
        "title": "Large-scale Passenger Behavior Learning and Prediction in Airport Terminals based on Multi-Agent Reinforcement Learning",
        "authors": "Yue Li, Guokang Gao",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "For the problem of predicting passenger flow in airport terminals, multi-agent reinforcement learning is applied to airport terminals simulation. Multi-Agent Reinforcement Learning based on Group Shared Policy with Mean-field and Intrinsic Rewards (GQ-MFI) is proposed to predict passenger behavior in order to simulate the distribution of flow in different areas of the terminal at different time periods. Independent learning of multi-agent may lead to environmental instability and long convergence time. To improve the adaptability of agents in non-stationary environments and accelerate learning time, a multi-agent grouping learning strategy is proposed. Clustering is used to group multi-agent, and a shared Q-table is set within each group to improve the learning efficiency of multi-agent. Meanwhile, in order to simplify the interaction information among the agent after grouping, the idea of average field is used to transmit partial global information among the agent within the group. Intrinsic rewards are added to make the agent closer to human cognition and behavioral patterns. By conducting the airport terminal simulations using Anylogic, the experimental results show that the training speed of this algorithm is 17% higher than that of Q-learning algorithm, and it achieves good prediction accuracy in predicting the number of security check passengers with a time scale of 10 minutes.",
        "link": "http://dx.doi.org/10.54097/fcis.v5i1.12008"
    },
    {
        "id": 28456,
        "title": "Modeling Agent Behaviors for Policy Analysis via Reinforcement Learning",
        "authors": "Osonde A. Osoba, Raffaele Vardavas, Justin Grana, Rushil Zutshi, Amber Jaycocks",
        "published": "2020-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla51294.2020.00043"
    },
    {
        "id": 28457,
        "title": "FLASH-RL: Federated Learning Addressing System and Static Heterogeneity using Reinforcement Learning",
        "authors": "Sofiane Bouaziz, Hadjer Benmeziane, Youcef Imine, Leila Hamdad, Smail Niar, Hamza Ouarnoughi",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccd58817.2023.00074"
    },
    {
        "id": 28458,
        "title": "The Guiding Role of Reward Based on Phased Goal in Reinforcement Learning",
        "authors": "Yiming Liu, Zheng Hu",
        "published": "2020-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3383972.3384039"
    },
    {
        "id": 28459,
        "title": "Curriculum Learning Based on Reward Sparseness for Deep Reinforcement Learning of Task Completion Dialogue Management",
        "authors": "Atsushi Saito",
        "published": "2018",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-5707"
    },
    {
        "id": 28460,
        "title": "On Learning Intrinsic Rewards for Faster Multi-Agent Reinforcement Learning based MAC Protocol Design in 6G Wireless Networks",
        "authors": "Luciano Miuccio, Salvatore Riolo, Mehdi Bennis, Daniela Panno",
        "published": "2023-5-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279625"
    },
    {
        "id": 28461,
        "title": "Learning Multi-Task Transferable Rewards via Variational Inverse Reinforcement Learning",
        "authors": "Se-Wook Yoo, Seung-Woo Seo",
        "published": "2022-5-23",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9811697"
    },
    {
        "id": 28462,
        "title": "An Emotional Virtual Character: A Deep Learning Approach with Reinforcement Learning",
        "authors": "Gilzamir Gomes, Creto A. Vidal, Joaquim B. Cavalcante Neto, Yuri L. B. Nogueira",
        "published": "2019-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/svr.2019.00047"
    },
    {
        "id": 28463,
        "title": "A Multi-Stage Deep Reinforcement Learning with Search-Based Optimization for Air–Ground Unmanned System Navigation",
        "authors": "Xiaohui Chen, Yuhua Qi, Yizhen Yin, Yidong Chen, Li Liu, Hongbo Chen",
        "published": "2023-2-9",
        "citations": 4,
        "abstract": "An important challenge for air–ground unmanned systems achieving autonomy is navigation, which is essential for them to accomplish various tasks in unknown environments. This paper proposes an end-to-end framework for solving air–ground unmanned system navigation using deep reinforcement learning (DRL) while optimizing by using a priori information from search-based path planning methods, which we call search-based optimizing DRL (SO-DRL) for the air–ground unmanned system. SO-DRL enables agents, i.e., an unmanned aerial vehicle (UAV) or an unmanned ground vehicle (UGV) to move to a given target in a completely unknown environment using only Lidar, without additional mapping or global planning. Our framework is equipped with Deep Deterministic Policy Gradient (DDPG), an actor–critic-based reinforcement learning algorithm, to input the agents’ state and laser scan measurements into the network and map them to continuous motion control. SO-DRL draws on current excellent search-based algorithms to demonstrate path planning and calculate rewards for its behavior. The demonstrated strategies are replayed in an experienced pool along with the autonomously trained strategies according to their priority. We use a multi-stage training approach based on course learning to train SO-DRL on the 3D simulator Gazebo and verify the robustness and success of the algorithm using new test environments for path planning in unknown environments. The experimental results show that SO-DRL can achieve faster algorithm convergence and a higher success rate. We piggybacked SO-DRL directly onto a real air–ground unmanned system, and SO-DRL can guide a UAV or UGV for navigation without adjusting any networks.",
        "link": "http://dx.doi.org/10.3390/app13042244"
    },
    {
        "id": 28464,
        "title": "Developing Robust Digital Twins and Reinforcement Learning for Accelerator Control Systems at the Fermilab Booster",
        "authors": "Diana Kafkes, C. Herwig, William Pellico, Gabriel Perdue, Andres Quintero Parra, B. Schupbach, Kiyomi Seiya, Jason St. John, N. Tran, Malachi Schram, Y. Huang, Javier Duarte, R. Keller",
        "published": "2021-1-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1825276"
    },
    {
        "id": 28465,
        "title": "Deep Reinforcement Learning for 5G Radio Access Network Slicing with Spectrum Coexistence",
        "authors": "Yi Shi, Parisa Rahimzadeh, Maice Costa, Tugba Erpek, Yalin E. Sagduyu",
        "published": "No Date",
        "citations": 0,
        "abstract": "The paper presents a reinforcement learning solution to dynamic admission control and resource allocation for 5G radio access network (RAN) slicing requests, when the spectrum is potentially shared between 5G and an incumbent user such as in the Citizens Broadband Radio Service scenarios. Available communication resources (frequency-time resource blocks and transmit powers) and computational resources (processor power) not used by the incumbent user can be allocated to stochastic arrivals of network slicing requests. Each request arrives with priority (weight), throughput, computational resource, and latency (deadline) requirements. As online algorithms, the greedy and myopic solutions that do not consider heterogeneity of future requests and their arrival process become ineffective for network slicing. Therefore, reinforcement learning solutions (Q-learning and Deep Q-learning) are presented to maximize the network utility in terms of the total weight of granted network slicing requests over a time horizon, subject to communication and computational constraints. Results show that reinforcement learning provides improvements in the 5G network utility relative to myopic, greedy, random, and first come first served solutions. In particular, deep Q-learning reduces the complexity and allows practical implementation as the state-action space grows, and effectively admits/rejects requests when 5G needs to share the spectrum with incumbent users that may dynamically occupy some of the frequency-time blocks. Furthermore, the robustness of deep reinforcement learning is demonstrated in the presence of the misdetection/false alarm errors in detecting the incumbent user's activity.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16632526"
    },
    {
        "id": 28466,
        "title": "HyLEAR: Hybrid Deep Reinforcement Learning and Planning for Safe and Comfortable Automated Driving",
        "authors": "Dikshant Gupta, Matthias Klusch",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv55152.2023.10186781"
    },
    {
        "id": 28467,
        "title": "Research on Stock Trading Strategy Based on Deep Reinforcement Learning",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ajcis.2022.051010"
    },
    {
        "id": 28468,
        "title": "Derivative‐Free Stochastic Search",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch7"
    },
    {
        "id": 28469,
        "title": "Optimal Compensation Scheme Considering Overwork and Effort Cost: a Reinforcement Learning Approach",
        "authors": "Toshihiko Nanba, Takahiro Inada",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4558316"
    },
    {
        "id": 28470,
        "title": "A Markov Game Approach Based on Multiagent Reinforcement Learning Solution for Cyber-Physical Attacks in Smart Grid",
        "authors": "Kübra Bitirgen, Ümmühan Başaran Filik",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4626686"
    },
    {
        "id": 28471,
        "title": "Design of Control Systems Using Active Uncertainty Reduction-Based Reinforcement Learning",
        "authors": "Zequn Wang, Narendra Patwardhan",
        "published": "2020-8-17",
        "citations": 0,
        "abstract": "Abstract\nModel-free reinforcement learning based methods such as Proximal Policy Optimization, or Q-learning typically require thousands of interactions with the environment to approximate the optimal controller which may not always be feasible in robotics due to safety and time consumption. Model-based methods such as PILCO or BlackDrops, while data-efficient, provide solutions with limited robustness and complexity. To address this tradeoff, we introduce active uncertainty reduction-based virtual environments, which are formed through limited trials conducted in the original environment. We provide an efficient method for uncertainty management, which is used as a metric for self-improvement by identification of the points with maximum expected improvement through adaptive sampling. Capturing the uncertainty also allows for better mimicking of the reward responses of the original system. Our approach enables the use of complex policy structures and reward functions through a unique combination of model-based and model-free methods, while still retaining the data efficiency. We demonstrate the validity of our method on several classic reinforcement learning problems in OpenAI gym. We prove that our approach offers a better modeling capacity for complex system dynamics as compared to established methods.",
        "link": "http://dx.doi.org/10.1115/detc2020-22014"
    },
    {
        "id": 28472,
        "title": "Dimmer: Self-Adaptive Network-Wide Flooding with Reinforcement Learning",
        "authors": "Valentin Poirot, Olaf Landsiedel",
        "published": "2021-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdcs51616.2021.00036"
    },
    {
        "id": 28473,
        "title": "Deep Reinforcement Learning Based Evasion Generative Adversarial Network for Botnet Detection",
        "authors": "Rizwan Hamid Randhawa, Nauman Aslam, Mohammad Alauthman, Muhammad Khalid, Husnain Rafiq",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4333338"
    },
    {
        "id": 28474,
        "title": "NAC Aided Performance Optimization of Stochastic Systems",
        "authors": "Changsheng Hua",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-33034-7_5"
    },
    {
        "id": 28475,
        "title": "Modeling the influence of working memory, reinforcement, and action uncertainty on reaction time and choice during instrumental learning",
        "authors": "Samuel McDougle, Anne Collins",
        "published": "No Date",
        "citations": 2,
        "abstract": "What determines the speed of our decisions? Various models of decision-making have focused on perceptual evidence, past experience, and task complexity as important factors determining the degree of deliberation needed for a decision. Here, we build on a sequential sampling decision-making framework to develop a new model that captures a range of reaction time (RT) effects by accounting for both working memory and instrumental learning processes. The model captures choices and RTs at various stages of learning, and in learning environments with varying complexity. Moreover, the model generalizes from tasks with deterministic reward contingencies to probabilistic ones. The model succeeds in part by incorporating prior uncertainty over actions when modeling RT. This straightforward process model provides a parsimonious account of decision dynamics during instrumental learning and makes unique predictions about internal representations of action values.",
        "link": "http://dx.doi.org/10.31234/osf.io/gcwxn"
    },
    {
        "id": 28476,
        "title": "Home Energy Management with V2X Capability using Reinforcement Learning",
        "authors": "Zachary Tchir, Marek Z. Reformat, Petr Musilek",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00046"
    },
    {
        "id": 28477,
        "title": "Object Detection Using Policy-Based Reinforcement Learning",
        "authors": "Keong-Hun Choi, Jong-Eun Ha",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iccas59377.2023.10316786"
    },
    {
        "id": 28478,
        "title": "Packet Routing with Graph Attention Multi-Agent Reinforcement Learning",
        "authors": "Xuan Mai, Quanzhi Fu, Yi Chen",
        "published": "2021-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685941"
    },
    {
        "id": 28479,
        "title": "Tourism Route Recommendation Using Reinforcement Learning",
        "authors": "Muhammad Ilham Mubarak, Z. K. A. Baizal",
        "published": "2023-4-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i2ct57861.2023.10126347"
    },
    {
        "id": 28480,
        "title": "Deep Reinforcement Learning based Intelligent Traffic Control",
        "authors": "Arpan Nookala, Eeshaan Asodekar, Aryan Solanki, Narendra Bhagat, Deepak Karia",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tensymp55890.2023.10223639"
    },
    {
        "id": 28481,
        "title": "Experience Selection in Multi-agent Deep Reinforcement Learning",
        "authors": "Yishen Wang, Zongzhang Zhang",
        "published": "2019-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai.2019.00123"
    },
    {
        "id": 28482,
        "title": "Deep-Reinforcement Learning Multiple Access for Heterogeneous Wireless Networks",
        "authors": "Yiding Yu, Taotao Wang, Soung Chang Liew",
        "published": "2018-5",
        "citations": 38,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2018.8422168"
    },
    {
        "id": 28483,
        "title": "Manufacturing Scheduling Using Colored Petri Nets and Reinforcement Learning",
        "authors": "Maria Drakaki, Panagiotis Tzionas",
        "published": "2017-2-3",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/app7020136"
    },
    {
        "id": 28484,
        "title": "Transfer of Temporal Logic Formulas in Reinforcement Learning",
        "authors": "Zhe Xu, Ufuk Topcu",
        "published": "2019-8",
        "citations": 23,
        "abstract": "Transferring high-level knowledge from a source task to a target task is an effective way to expedite reinforcement learning (RL). For example, propositional logic and first-order logic have been used as representations of such knowledge. We study the transfer of knowledge between tasks in which the timing of the events matters. We call such tasks temporal tasks. We concretize similarity between temporal tasks through a notion of logical transferability, and develop a transfer learning approach between different yet similar temporal tasks. We first propose an inference technique to extract metric interval temporal logic (MITL) formulas in sequential disjunctive normal form from labeled trajectories collected in RL of the two tasks. If logical transferability is identified through this inference, we construct a timed automaton for each sequential conjunctive subformula of the inferred MITL formulas from both tasks. We perform RL on the extended state which includes the locations and clock valuations of the timed automata for the source task. We then establish mappings between the corresponding components (clocks, locations, etc.) of the timed automata from the two tasks, and transfer the extended Q-functions based on the established mappings. Finally, we perform RL on the extended state for the target task, starting with the transferred extended Q-functions. Our implementation results show, depending on how similar the source task and the target task are, that the sampling efficiency for the target task can be improved by up to one order of magnitude by performing RL in the extended state space, and further improved by up to another order of magnitude using the transferred extended Q-functions.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/557"
    },
    {
        "id": 28485,
        "title": "MERGE: Meta Reinforcement Learning for Tunable RL Agents at the Edge",
        "authors": "Sharda Tripathi, Carla Fabiana Chiasserini",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437278"
    },
    {
        "id": 28486,
        "title": "Teaching on a Budget in Multi-Agent Deep Reinforcement Learning",
        "authors": "Ercument Ilhan, Jeremy Gow, Diego Perez-Liebana",
        "published": "2019-8",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8847988"
    },
    {
        "id": 28487,
        "title": "A Deep Reinforcement Learning Agent for Traffic Intersection Control Optimization",
        "authors": "Deepeka Garg, Maria Chli, George Vogiatzis",
        "published": "2019-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8917361"
    },
    {
        "id": 28488,
        "title": "Subgoal-Based Reward Shaping to Improve Efficiency in Reinforcement Learning",
        "authors": "Takato Okudo, Seiji Yamada",
        "published": "2021",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3090364"
    },
    {
        "id": 28489,
        "title": "The Formation Control of Mobile Autonomous Multi-Agent Systems Using Deep Reinforcement Learning",
        "authors": "Qishuai Liu, Qing Hui",
        "published": "2019-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/syscon.2019.8836902"
    },
    {
        "id": 28490,
        "title": "Neuroevolution for deep reinforcement learning problems",
        "authors": "David Ha",
        "published": "2020-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3377929.3389859"
    },
    {
        "id": 28491,
        "title": "Reinforcement Learning Applications in Cyber Security: A Review",
        "authors": "Emine CENGİZ, Murat GÖK",
        "published": "2023-4-30",
        "citations": 2,
        "abstract": "In the modern age we live in, the internet has become an essential part of our daily life. A significant portion of our personal data is stored online and organizations run their business online. In addition, with the development of the internet, many devices such as autonomous systems, investment portfolio tools and entertainment tools in our homes and workplaces have become or are becoming intelligent. In parallel with this development, cyberattacks aimed at damaging smart systems are increasing day by day. As cyberattack methods become more sophisticated, the damage done by attackers is increasing exponentially. Traditional computer algorithms may be insufficient against these attacks in the virtual world. Therefore, artificial intelligence-based methods are needed. Reinforcement Learning (RL), a machine learning method, is used in the field of cyber security. Although RL for cyber security is a new topic in the literature, studies are carried out to predict, prevent and stop attacks. In this study; we reviewed the literature on RL's penetration testing, intrusion detection systems (IDS) and cyberattacks in cyber security.",
        "link": "http://dx.doi.org/10.16984/saufenbilder.1237742"
    },
    {
        "id": 28492,
        "title": "Efficient Neural Network Pruning Using Model-Based Reinforcement Learning",
        "authors": "Blanka Bencsik, Marton Szemenyei",
        "published": "2022-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ismcr56534.2022.9950598"
    },
    {
        "id": 28493,
        "title": "Reinforcement Learning in Social Media Marketing",
        "authors": "Patrik Eklund",
        "published": "2022-7-8",
        "citations": 0,
        "abstract": "In this chapter, the authors describe an architecture for reinforcement learning in social media marketing. The rule bases used for action selection within the architecture build upon many-valued (fuzzy) logic. Action evaluation and internal learning is based on neural network like structures. In using variables measuring the effect of advertising, we must understand direction of influence between advertiser, owning the content of the advertisement, and advertisee, as the target of an advertisement, and as facilitated by social media marketing. Examples are drawn from Facebook marketing.",
        "link": "http://dx.doi.org/10.4018/978-1-6684-7123-4.ch045"
    },
    {
        "id": 28494,
        "title": "Flight AI using Reinforcement Learning",
        "authors": " Poonam Sharma,  Ashima Narang",
        "published": "2022-7-28",
        "citations": 0,
        "abstract": "This paper will be featuring an aircraft with fully featured physics simulated in unity\nengine. The agent or the AI will be created for possessing and controlling the aircraft so as to\nnavigate it in the 3D world space environment provided. The agent will have to consider the\ndifferent physical dynamics applied on a real-world aircraft and based on these parameters it will\nhave to create an effective piloting for the aircraft. Various other aspects such as engine dynamics\nand the fuel parameters will also be considered for an effective training environment.",
        "link": "http://dx.doi.org/10.46647/ijetms.2022.v06i04.0010"
    },
    {
        "id": 28495,
        "title": "Quantization aware training for efficient reinforcement learning network",
        "authors": "Qiaolin Li",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "In recent years, reinforcement learning has been applied to many areas, especially deep reinforcement learning. Deep learning-based methods use a large computing method known as Quantization Aware Training (QAT). For this project, QAT will be used to investigate the performance of Deep Q-Networks. This method of QAT can reduce memory usage of the network, but may affect the training performance of the network with some games.",
        "link": "http://dx.doi.org/10.54254/2755-2721/4/20230493"
    },
    {
        "id": 28496,
        "title": "Rule-based Reinforcement Learning for Lane Change Decision-making: A Risk Assessment Approach",
        "authors": "Lu Xiong, Danyang Zhong, Puhang Xu, Chen Tang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTo solve problems of poor security guarantee and insufficient training efficiency in the conventional reinforcement learning methods for decision-making, this study proposes a hybrid framework to combine deep reinforcement learning with rule-based decision-making methods. A risk assessment model for lane-change maneuver considering uncertain prediction of surrounding vehicles is established as a safety filter to improve the learning efficiency while corrects dangerous actions for safety enhancement. On this basis, a Risk-fused DDQN is constructed utilizing the model-based risk assessment and supervision mechanism. The proposed reinforcement learning algorithm sets up a separate experience buffer for dangerous trials and punishes such actions, which is shown to improve the sampling efficiency and training outcomes. Compared with conventional DDQN methods, the proposed algorithm improves the convergence value of cumulated reward by 7.6\\% and 2.2\\% in the two constructed scenarios in the simulation study, and reduces the number of training episodes by 52.2\\% and 66.8\\% respectively. The success rate of lane change is improved by 57.3\\% while the time headway is increased at least by 16.5\\% in real vehicle tests, which confirms the higher training efficiency, scenario adaptability and security of the proposed Risk-fused DDQN.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2536475/v1"
    },
    {
        "id": 28497,
        "title": "Reinforcement Learning for Placement Optimization",
        "authors": "Anna Goldie, Azalia Mirhoseini",
        "published": "2021-3-22",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3439706.3446883"
    },
    {
        "id": 28498,
        "title": "Reinforcement Learning Based Autonomous Air Combat with Energy Budgets",
        "authors": "Hasan Isci, Emre Koyuncu",
        "published": "2022-1-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2022-0786"
    },
    {
        "id": 28499,
        "title": "Multi-Agent Deep Reinforcement Learning with Human Strategies",
        "authors": "Thanh Nguyen, Ngoc Duy Nguyen, Saeid Nahavandi",
        "published": "2019-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit.2019.8755032"
    },
    {
        "id": 28500,
        "title": "REINFORCEMENT LEARNING IN CONTROL SYSTEMS OF OBJECTS WITH A TRANSPORT DELAY",
        "authors": "V.S. Borovik, S.V. Shidlovskiy",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15372/aut20210306"
    },
    {
        "id": 28501,
        "title": "Model-Based Reinforcement Learning For Robot Control",
        "authors": "Xiang Li, Weiwei Shang, Shuang Cong",
        "published": "2020-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icarm49381.2020.9195341"
    },
    {
        "id": 28502,
        "title": "Exploration of Reinforcement Learning to Play Snake Game",
        "authors": "Ali Jaber Almalki, Pawel Wocjan",
        "published": "2019-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/csci49370.2019.00073"
    },
    {
        "id": 28503,
        "title": "Block-Decentralized Model-Free Reinforcement Learning Control of Two Time-Scale Networks",
        "authors": "Sayak Mukherjee, Aranya Chakrabortty, He Bai",
        "published": "2019-7",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2019.8815077"
    },
    {
        "id": 28504,
        "title": "A Reinforcement-Learning Meta-Control Architecture Based on the Dual-Process Theory of Moral Decision-Making",
        "authors": "Maximilian Maier, Vanessa Cheung, Falk Lieder",
        "published": "No Date",
        "citations": 0,
        "abstract": "Deep neural networks are increasingly tasked with making complex, real-world decisions that can have morally significant consequences. But it is difficult to predict when a deep neural network will go wrong, and wrong decisions can cause significantly negative outcomes. In contrast, human moral decision-making is often remarkably robust. This is partly achieved by relying on both moral rules and cost-benefit reasoning. In this paper, we reverse-engineer people's capacity for robust moral decision-making as a cognitively inspired reinforcement-learning (RL) architecture that learns how much weight to give to following rules vs. cost-benefit reasoning. We confirm the predictions of our model in a large online experiment on human moral learning. We find that our RL architecture can capture how people learn to make moral decisions, suggesting that it could be applied to make AI decision-making safer and more robustly beneficial to society.",
        "link": "http://dx.doi.org/10.31234/osf.io/j6fhk"
    },
    {
        "id": 28505,
        "title": "Optimizing Forecasted Activity Notifications with Reinforcement Learning",
        "authors": "Muhammad Fikry, Sozo Inoue",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "In this paper, we propose the notification optimization method by providing multiple alternative times as a reminder for a forecasted activity with and without probabilistic considerations for the activity that needs to be completed and needs notification. It is important to consider various factors when sending notifications to people after obtaining the results of the forecasted activity. We should not send notifications only when we have forecasted results because future daily activities are unpredictable. Therefore, it is important to strike a balance between providing useful reminders and avoiding excessive interruptions, especially for low probabilities of forecasted activity. Our study investigates the impact of the low probability of forecasted activity and optimizes the notification time with reinforcement learning. We also show the gaps between forecasted activities that are useful for self-improvement by people for the balance of important tasks, such as tasks completed as planned and additional tasks to be completed. For evaluation, we utilize two datasets: the existing dataset and data we collected in the field with the technology we have developed. In the data collection, we have 23 activities from six participants. To evaluate the effectiveness of these approaches, we assess the percentage of positive responses, user response rate, and response duration as performance criteria. Our proposed method provides a more effective way to optimize notifications. By incorporating the probability level of activity that needs to be done and needs notification into the state, we achieve a better response rate than the baseline, with the advantage of reaching 27.15%, as well as than the other criteria, which are also improved by using probability.",
        "link": "http://dx.doi.org/10.3390/s23146510"
    },
    {
        "id": 28506,
        "title": "Feature selection method using multi-agent reinforcement learning based on guide agents",
        "authors": "Minwoo Kim, Jinhee Bae, Bohyun Wang, Joon S. Lim",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nIn this study, we propose a method to automatically find features from a dataset that are effective for classification or prediction, using a new method called multi-agent reinforcement learning and a guide agent. Each feature of the dataset has one each of main and guide agents, and these agents decide whether to select a feature. Main agents select the optimal features, and guide agents present the criteria for judging the main agents’ actions. After obtaining the main and guide rewards for the features selected by the agents, the main agent that behaves differently from the guide agent, by calculating the learning reward delivered to the main agents, updates their Q-values. The behavior comparison between them helps the main agent decide whether its own behavior is correct, without using other algorithms. After performing this process for each episode, the features are finally selected. The feature selection method proposed in this study uses multiple agents, which reduces the number of actions that each agent can perform and finds optimal features effectively and quickly. Finally, comparative experimental results on multiple datasets show that the proposed method can select effective features for classification and increase the classification accuracy.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1732607/v1"
    },
    {
        "id": 28507,
        "title": "A Reinforcement Learning Model for Quantum  Network Data Aggregation and Analysis",
        "authors": "",
        "published": "2022-2-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33168/jsms.2022.0120"
    },
    {
        "id": 28508,
        "title": "Deep Reinforcement Learning for 5G Radio Access Network Slicing with Spectrum Coexistence",
        "authors": "Yi Shi, Parisa Rahimzadeh, Maice Costa, Tugba Erpek, Yalin E. Sagduyu",
        "published": "No Date",
        "citations": 0,
        "abstract": "The paper presents a reinforcement learning solution to dynamic admission control and resource allocation for 5G radio access network (RAN) slicing requests, when the spectrum is potentially shared between 5G and an incumbent user such as in the Citizens Broadband Radio Service scenarios. Available communication resources (frequency-time resource blocks and transmit powers) and computational resources (processor power) not used by the incumbent user can be allocated to stochastic arrivals of network slicing requests. Each request arrives with priority (weight), throughput, computational resource, and latency (deadline) requirements. As online algorithms, the greedy and myopic solutions that do not consider heterogeneity of future requests and their arrival process become ineffective for network slicing. Therefore, reinforcement learning solutions (Q-learning and Deep Q-learning) are presented to maximize the network utility in terms of the total weight of granted network slicing requests over a time horizon, subject to communication and computational constraints. Results show that reinforcement learning provides improvements in the 5G network utility relative to myopic, greedy, random, and first come first served solutions. In particular, deep Q-learning reduces the complexity and allows practical implementation as the state-action space grows, and effectively admits/rejects requests when 5G needs to share the spectrum with incumbent users that may dynamically occupy some of the frequency-time blocks. Furthermore, the robustness of deep reinforcement learning is demonstrated in the presence of the misdetection/false alarm errors in detecting the incumbent user's activity.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16632526.v1"
    },
    {
        "id": 28509,
        "title": "A Reinforcement Learning Approach to Web API Recommendation for Mashup Development",
        "authors": "Richard Anarfi, Kenneth K. Fletcher",
        "published": "2019-7",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/services.2019.00109"
    },
    {
        "id": 28510,
        "title": "Spontaneous eye blink rate predicts individual differences in exploration and exploitation during reinforcement learning",
        "authors": "Joanne C. Van Slooten, Sara Jahfari, Jan Theeuwesu",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractSpontaneous eye blink rate (sEBR) has been linked to striatal dopamine function and to how individuals make value-based choices after a period of reinforcement learning (RL). While sEBR is thought to reflect how individuals learn from the negative outcomes of their choices, this idea has not been tested explicitly. This study assessed how individual differences in sEBR relate to learning by focusing on the cognitive processes that drive RL. Using Bayesian latent mixture modelling to quantify the mapping between RL behaviour and its underlying cognitive processes, we were able to differentiate low and high sEBR individuals at the level of these cognitive processes. Further inspection of these cognitive processes indicated that sEBR uniquely indexed explore-exploit tendencies during RL: lower sEBR predicted exploitative choices for high valued options, whereas higher sEBR predicted exploration of lower value options. This relationship was additionally supported by a network analysis where, notably, no link was observed between sEBR and how individuals learned from negative outcomes. Our findings challenge the notion that sEBR predicts learning from negative outcomes during RL, and suggest that sEBR predicts individual explore-exploit tendencies. These then influence value sensitivity during choices to support successful performance when facing uncertain reward.",
        "link": "http://dx.doi.org/10.1101/661553"
    },
    {
        "id": 28511,
        "title": "A Reinforcement-Learning Approach to Control Robotic Manipulator Based on Improved DDPG",
        "authors": "Saikat Majumder, Soumya Ranjan Sahoo",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10442503"
    },
    {
        "id": 28512,
        "title": "Dynamic selective auditory attention detection using RNN and reinforcement learning",
        "authors": "Masoud Geravanchizadeh, Hossein Roushan",
        "published": "2021-7-29",
        "citations": 14,
        "abstract": "AbstractThe cocktail party phenomenon describes the ability of the human brain to focus auditory attention on a particular stimulus while ignoring other acoustic events. Selective auditory attention detection (SAAD) is an important issue in the development of brain-computer interface systems and cocktail party processors. This paper proposes a new dynamic attention detection system to process the temporal evolution of the input signal. The proposed dynamic SAAD is modeled as a sequential decision-making problem, which is solved by recurrent neural network (RNN) and reinforcement learning methods of Q-learning and deep Q-learning. Among different dynamic learning approaches, the evaluation results show that the deep Q-learning approach with RNN as agent provides the highest classification accuracy (94.2%) with the least detection delay. The proposed SAAD system is advantageous, in the sense that the detection of attention is performed dynamically for the sequential inputs. Also, the system has the potential to be used in scenarios, where the attention of the listener might be switched in time in the presence of various acoustic events.",
        "link": "http://dx.doi.org/10.1038/s41598-021-94876-0"
    },
    {
        "id": 28513,
        "title": "Designing Efficient Local Flexibility Markets in the Presence of Reinforcement-Learning Agents",
        "authors": "Haoyang Zhang, Georgios Tsaousoglou, Sen Zhan, Koen Kok, Nikolaos Paterakis",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170775541.17616673/v1"
    },
    {
        "id": 28514,
        "title": "Tactical Reward Shaping: Bypassing Reinforcement Learning with Strategy-Based Goals",
        "authors": "Yizheng Zhang, Andre Rosendo",
        "published": "2019-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio49542.2019.8961549"
    },
    {
        "id": 28515,
        "title": "Model-Based or Model-Free, a Review of Approaches in Reinforcement Learning",
        "authors": "Qingyan Huang",
        "published": "2020-8",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cds49703.2020.00051"
    },
    {
        "id": 28516,
        "title": "Flow Control in Wings and Discovery of Novel Approaches via Deep Reinforcement Learning",
        "authors": "Ricardo Vinuesa, Oriol Lehmkuhl, Adrian Lozano-Duran, Jean Rabault",
        "published": "No Date",
        "citations": 2,
        "abstract": "In this review we summarize existing trends of flow control used to improve the aerodynamic efficiency of wings. We first discuss active methods to control turbulence, starting with flat-plate geometries and building towards the more complicated flow around wings. Then, we discuss active approaches to control separation, a crucial aspect towards achieving high aerodynamic efficiency. Furthermore, we highlight methods relying on turbulence simulation, and discuss various levels of modelling. Finally, we thoroughly revise data-driven methods, their application to flow control, and focus on deep reinforcement learning (DRL). We conclude that this methodology has the potential to discover novel control strategies in complex turbulent flows of aerodynamic relevance.",
        "link": "http://dx.doi.org/10.20944/preprints202201.0050.v1"
    },
    {
        "id": 28517,
        "title": "Intelligent and Resizable Control Plane for Software Defined Vehicular Network : A Deep Reinforcement Learning Approach",
        "authors": "karima Smida, Hajer Tounsi, Mounir Frikha",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nSoftware-Defined Networking (SDN) has become one of the most promising paradigms to manage large scale networks. Distributing the SDN Control proved its performance in terms of resiliency and scalability. However, the choice of the number of controllers to use remains problematic. A large number of controllers may be oversized inducing an overhead in the investment cost and the synchronization cost in terms of delay and traffic load. However, a small number of controllers may be insufficient to achieve the objective of the distributed approach. So, the number of used controllers should be tuned in function of the traffic charge and application requirements. In this paper, we present an Intelligent and Resizable Control Plane for Software Defined Vehicular Network architecture (IRCP-SDVN), where SDN capabilities coupled with Deep Reinforcement Learning (DRL) allow achieving better QoS for Vehicular Applications. Interacting with SDVN, DRL agent decides the optimal number of distributed controllers to deploy according to the network environment (number of vehicles, load, speed etc.). To the best of our knowledge, this is the first work that adjusts the number of controllers by learning from the vehicular environment dynamicity. Experimental results proved that our proposed system outperforms static distributed SDVN architecture in terms of end-to-end delay and packet loss.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-276280/v1"
    },
    {
        "id": 28518,
        "title": "Hierarchical Reinforcement Learning for Quadruped Locomotion",
        "authors": "Deepali Jain, Atil Iscen, Ken Caluwaerts",
        "published": "2019-11",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros40897.2019.8967913"
    },
    {
        "id": 28519,
        "title": "Deep Reinforcement Learning Based Handover management for Vehicular Platoon",
        "authors": "Amaira Arwa, Trabelsi Alaa, Zarai Faouzi",
        "published": "2023-6-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwcmc58020.2023.10183146"
    },
    {
        "id": 28520,
        "title": "Discrete Event Modeling and Simulation for Reinforcement Learning System Design",
        "authors": "Laurent Capocchi, Jean-François Santucci",
        "published": "2022-2-28",
        "citations": 7,
        "abstract": "Discrete event modeling and simulation and reinforcement learning are two frameworks suited for cyberphysical system design, which, when combined, can give powerful tools for system optimization or decision making process for example. This paper describes how discrete event modeling and simulation could be integrated into reinforcement learning concepts and tools in order to assist in the realization of reinforcement learning systems, more specially considering the temporal, hierarchical, and multi-agent aspects. An overview of these different improvements are given based on the implementation of the Q-Learning reinforcement learning algorithm in the framework of the Discrete Event system Specification (DEVS) and System Entity Structure (SES) formalisms.",
        "link": "http://dx.doi.org/10.3390/info13030121"
    },
    {
        "id": 28521,
        "title": "Multiobjective Reinforcement Learning for Reconfigurable Adaptive Optimal Control of Manufacturing Processes",
        "authors": "Johannes Dornheim, Norbert Link",
        "published": "2018-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isetc.2018.8583854"
    },
    {
        "id": 28522,
        "title": "Deep reinforcement learning based lane detection and localization",
        "authors": "Zhiyuan Zhao, Qi Wang, Xuelong Li",
        "published": "2020-11",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2020.06.094"
    },
    {
        "id": 28523,
        "title": "Deep reinforcement learning for recommender systems",
        "authors": "Isshu Munemasa, Yuta Tomomatsu, Kunioki Hayashi, Tomohiro Takagi",
        "published": "2018-3",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoiact.2018.8350761"
    },
    {
        "id": 28524,
        "title": "Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Yumeya Yamamori, Oliver J Robinson, Jonathan P Roiser",
        "published": "No Date",
        "citations": 1,
        "abstract": "Although avoidance is a prevalent feature of anxiety-related psychopathology, differences in existing measures of avoidance between humans and non-human animals impede progress in its theoretical understanding and treatment. To address this, we developed a novel translational measure of anxiety-related avoidance in the form of an approach-avoidance reinforcement learning task, by adapting a paradigm from the non-human animal literature to study the same cognitive processes in human participants. We used computational modelling to probe the putative cognitive mechanisms underlying approach-avoidance behaviour in this task and investigated how they relate to subjective task-induced anxiety. In a large online study, participants (n = 372) who experienced greater task- induced anxiety avoided choices associated with punishment, even when this resulted in lower overall reward. Computational modelling revealed that this effect was explained by greater individual sensitivities to punishment relative to rewards. We replicated these findings in an independent sample (n = 627) and we also found fair-to-excellent reliability of measures of task performance in a sub-sample retested one week later (n = 57). Our findings demonstrate the potential of approach-avoidance reinforcement learning tasks as translational and computational models of anxiety-related avoidance. Future studies should assess the predictive validity of this approach in clinical samples and experimental manipulations of anxiety.",
        "link": "http://dx.doi.org/10.7554/elife.87720.1"
    },
    {
        "id": 28525,
        "title": "Author response: Valence biases in reinforcement learning shift across adolescence and modulate subsequent memory",
        "authors": "Gail M Rosenbaum, Hannah L Grassie, Catherine A Hartley",
        "published": "2021-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.64620.sa2"
    },
    {
        "id": 28526,
        "title": "Federated Deep Reinforcement Learning for Task Participation in Mobile Crowdsensing",
        "authors": "Sumedh Dongare, Andrea Ortiz, Anja Klein",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436786"
    },
    {
        "id": 28527,
        "title": "Reinforcement Learning Data-Acquiring for Causal Inference of Regulatory Networks",
        "authors": "Mohammad Alali, Mahdi Imani",
        "published": "2023-5-31",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10155867"
    },
    {
        "id": 28528,
        "title": "Safe Reinforcement Learning with Policy-Guided Planning for Autonomous Driving",
        "authors": "Jikun Rong, Nan Luan",
        "published": "2020-10-13",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icma49215.2020.9233522"
    },
    {
        "id": 28529,
        "title": "Adversarial Reinforcement Learning for Procedural Content Generation",
        "authors": "Linus Gisslen, Andy Eakins, Camilo Gordillo, Joakim Bergdahl, Konrad Tollmar",
        "published": "2021-8-17",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog52621.2021.9619053"
    },
    {
        "id": 28530,
        "title": "Modeling of Soft Robotic Grippers for Reinforcement Learning-based Grasp Planning in Simulation",
        "authors": "Nijil George, Vighnesh Vatsal",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10442683"
    },
    {
        "id": 28531,
        "title": "Deep Reinforcement Learning Based Antenna Selection for Cell Outage Compensation",
        "authors": "Masayoshi Iwamoto, Akito Suzuki, Masahiro Kobayashi",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279017"
    },
    {
        "id": 28532,
        "title": "Cooperative multi-agent game based on reinforcement learning",
        "authors": "Hongbo Liu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.hcc.2024.100205"
    },
    {
        "id": 28533,
        "title": "Online Reinforcement Learning in Periodic MDP",
        "authors": "Ayush Aniket, Arpan Chattopadhyay",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tai.2024.3375258"
    },
    {
        "id": 28534,
        "title": "Survival-Oriented Reinforcement Learning Model: An Effcient and Robust Deep Reinforcement Learning Algorithm for Autonomous Driving Problem",
        "authors": "Changkun Ye, Huimin Ma, Xiaoqin Zhang, Kai Zhang, Shaodi You",
        "published": "2017",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-71589-6_36"
    },
    {
        "id": 28535,
        "title": "Energy-efficient and damage-recovery slithering gait design for a snake-like robot based on reinforcement learning and inverse reinforcement learning",
        "authors": "Zhenshan Bing, Christian Lemke, Long Cheng, Kai Huang, Alois Knoll",
        "published": "2020-9",
        "citations": 29,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2020.05.029"
    },
    {
        "id": 28536,
        "title": "Study on Aggregator’s Strategy of Controlling Electric Vehicles to Compensate Imbalances in Power Systems Using Reinforcement Learning",
        "authors": "Ryuhei Furuta,  , Ryuji Matsuhashi",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijoee.7.2.67-71"
    },
    {
        "id": 28537,
        "title": "Bounded Rationality in Learning, Perception, Decision-Making, and Stochastic Games",
        "authors": "Panagiotis Tsiotras",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_17"
    },
    {
        "id": 28538,
        "title": "Goal-Conditioned Reinforcement Learning with Latent Representations using Contrastive Learning",
        "authors": "Takaya YAMADA, Koich OGAWARA",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1299/jsmermd.2021.1p1-i15"
    },
    {
        "id": 28539,
        "title": "Distributed transmission control for wireless networks using multi-agent reinforcement learning",
        "authors": "Collin Farquhar, Prem Kumar, Anu Jagannath, Jithin Jagannath",
        "published": "2022-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2621086"
    },
    {
        "id": 28540,
        "title": "A learning search algorithm with propagational reinforcement learning",
        "authors": "Wei Zhang",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-020-02117-0"
    },
    {
        "id": 28541,
        "title": "Scan Chain Clustering and Optimization with Constrained Clustering and Reinforcement Learning",
        "authors": "Naiju Karim Abdul, George Antony, Rahul M. Rao, Suriya T. Skariah",
        "published": "2022-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlcad55463.2022.9900094"
    },
    {
        "id": 28542,
        "title": "An improved deep reinforcement learning for robot navigation",
        "authors": "Qifeng Zheng, Xiaogang Huang, Chen Dong, Yuting Liu, Dong Chen",
        "published": "2023-5-25",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2675144"
    },
    {
        "id": 28543,
        "title": "Enhancing Control in Manufacturing and Microgrid Systems: Deep Reinforcement Learning with Double Q-Learning",
        "authors": "Mohamed Ballouch, Omar Souissi, Mohammed Raiss El-Fenni",
        "published": "2023-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sita60746.2023.10373737"
    },
    {
        "id": 28544,
        "title": "Learning Virtual Grasp with Failed Demonstrations via Bayesian Inverse Reinforcement Learning",
        "authors": "Xu Xie, Changyang Li, Chi Zhang, Yixin Zhu, Song-Chun Zhu",
        "published": "2019-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros40897.2019.8968063"
    },
    {
        "id": 28545,
        "title": "Multi-Robot Real-time Game Strategy Learning based on Deep Reinforcement Learning",
        "authors": "Ki Deng, Yanjie Li, Songshuo Lu, Yongjin Mu, Xizheng Pang, Qi Liu",
        "published": "2022-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio55434.2022.10011827"
    },
    {
        "id": 28546,
        "title": "A Review of Three Methods of Artificial Intelligence in Smart Grid Cyber Security: Machine Learning, Reinforcement Learning, Ensemble Methods",
        "authors": "Luyao Xu",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "Renewable energy is gradually replacing traditional fossil fuels. The change of power generation energy structure brings new challenges to the traditional power grid. Through the efficient bidirectional movement of electricity and information, smart grids might include renewable energy. For the complex informational and financial operations required by smart grid, communication systems are crucial, but they also make smart grid vulnerable to numerous cyber attacks. Smart grid cyber security has been widely concerned. The purpose of this paper is to explore the use of artificial intelligence technology in smart grid cyber security. Three methods in the field of artificial intelligence are highlighted: Machine Learning, Reinforcement Learning, and Ensemble Methods. This paper summarizes the benefits and drawbacks of their use of smart grid cyber security, and further makes a qualitative comparison of the three methods from multiple performance indicators.",
        "link": "http://dx.doi.org/10.54097/79xf0y91"
    },
    {
        "id": 28547,
        "title": "Reinforcement Learning for Hyperparameter Tuning in Deep Learning-based Side-channel Analysis",
        "authors": "Jorai Rijsdijk, Lichao Wu, Guilherme Perin, Stjepan Picek",
        "published": "2021-7-9",
        "citations": 55,
        "abstract": "Deep learning represents a powerful set of techniques for profiling sidechannel analysis. The results in the last few years show that neural network architectures like multilayer perceptron and convolutional neural networks give strong attack performance where it is possible to break targets protected with various countermeasures. Considering that deep learning techniques commonly have a plethora of hyperparameters to tune, it is clear that such top attack results can come with a high price in preparing the attack. This is especially problematic as the side-channel community commonly uses random search or grid search techniques to look for the best hyperparameters.In this paper, we propose to use reinforcement learning to tune the convolutional neural network hyperparameters. In our framework, we investigate the Q-Learning paradigm and develop two reward functions that use side-channel metrics. We mount an investigation on three commonly used datasets and two leakage models where the results show that reinforcement learning can find convolutional neural networks exhibiting top performance while having small numbers of trainable parameters. We note that our approach is automated and can be easily adapted to different datasets. Several of our newly developed architectures outperform the current state-of-the-art results. Finally, we make our source code publicly available.\r\nhttps://github.com/AISyLab/Reinforcement-Learning-for-SCA",
        "link": "http://dx.doi.org/10.46586/tches.v2021.i3.677-707"
    },
    {
        "id": 28548,
        "title": "Deep Q-Learning Based Reinforcement Learning Approach for Network Intrusion Detection",
        "authors": "Hooman Alavizadeh, Hootan Alavizadeh, Julian Jang-Jaccard",
        "published": "2022-3-11",
        "citations": 52,
        "abstract": "The rise of the new generation of cyber threats demands more sophisticated and intelligent cyber defense solutions equipped with autonomous agents capable of learning to make decisions without the knowledge of human experts. Several reinforcement learning methods (e.g., Markov) for automated network intrusion tasks have been proposed in recent years. In this paper, we introduce a new generation of the network intrusion detection method, which combines a Q-learning based reinforcement learning with a deep feed forward neural network method for network intrusion detection. Our proposed Deep Q-Learning (DQL) model provides an ongoing auto-learning capability for a network environment that can detect different types of network intrusions using an automated trial-error approach and continuously enhance its detection capabilities. We provide the details of fine-tuning different hyperparameters involved in the DQL model for more effective self-learning. According to our extensive experimental results based on the NSL-KDD dataset, we confirm that the lower discount factor, which is set as 0.001 under 250 episodes of training, yields the best performance results. Our experimental results also show that our proposed DQL is highly effective in detecting different intrusion classes and outperforms other similar machine learning approaches.",
        "link": "http://dx.doi.org/10.3390/computers11030041"
    },
    {
        "id": 28549,
        "title": "Deep Reinforcement Learning for 5G Radio Access Network Slicing with Spectrum Coexistence",
        "authors": "Yi Shi, Parisa Rahimzadeh, Maice Costa, Tugba Erpek, Yalin E. Sagduyu",
        "published": "No Date",
        "citations": 0,
        "abstract": "The paper presents a reinforcement learning solution to dynamic admission control and resource allocation for 5G radio access network (RAN) slicing requests, when the spectrum is potentially shared between 5G and an incumbent user such as in the Citizens Broadband Radio Service scenarios. Available communication resources (frequency-time resource blocks and transmit powers) and computational resources (processor power) not used by the incumbent user can be allocated to stochastic arrivals of network slicing requests. Each request arrives with priority (weight), throughput, computational resource, and latency (deadline) requirements. As online algorithms, the greedy and myopic solutions that do not consider heterogeneity of future requests and their arrival process become ineffective for network slicing. Therefore, reinforcement learning solutions (Q-learning and Deep Q-learning) are presented to maximize the network utility in terms of the total weight of granted network slicing requests over a time horizon, subject to communication and computational constraints. Results show that reinforcement learning provides improvements in the 5G network utility relative to myopic, greedy, random, and first come first served solutions. In particular, deep Q-learning reduces the complexity and allows practical implementation as the state-action space grows, and effectively admits/rejects requests when 5G needs to share the spectrum with incumbent users that may dynamically occupy some of the frequency-time blocks. Furthermore, the robustness of deep reinforcement learning is demonstrated in the presence of the misdetection/false alarm errors in detecting the incumbent user's activity.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16632526"
    },
    {
        "id": 28550,
        "title": "Runtime Safety Assurance Using Reinforcement Learning",
        "authors": "Christopher Lazarus, James G. Lopez, Mykel J. Kochenderfer",
        "published": "2020-10-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dasc50938.2020.9256446"
    },
    {
        "id": 28551,
        "title": "Research on Stock Trading Strategy Based on Deep Reinforcement Learning",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ajcis.2022.051010"
    },
    {
        "id": 28552,
        "title": "Design of Control Systems Using Active Uncertainty Reduction-Based Reinforcement Learning",
        "authors": "Zequn Wang, Narendra Patwardhan",
        "published": "2020-8-17",
        "citations": 0,
        "abstract": "Abstract\nModel-free reinforcement learning based methods such as Proximal Policy Optimization, or Q-learning typically require thousands of interactions with the environment to approximate the optimal controller which may not always be feasible in robotics due to safety and time consumption. Model-based methods such as PILCO or BlackDrops, while data-efficient, provide solutions with limited robustness and complexity. To address this tradeoff, we introduce active uncertainty reduction-based virtual environments, which are formed through limited trials conducted in the original environment. We provide an efficient method for uncertainty management, which is used as a metric for self-improvement by identification of the points with maximum expected improvement through adaptive sampling. Capturing the uncertainty also allows for better mimicking of the reward responses of the original system. Our approach enables the use of complex policy structures and reward functions through a unique combination of model-based and model-free methods, while still retaining the data efficiency. We demonstrate the validity of our method on several classic reinforcement learning problems in OpenAI gym. We prove that our approach offers a better modeling capacity for complex system dynamics as compared to established methods.",
        "link": "http://dx.doi.org/10.1115/detc2020-22014"
    },
    {
        "id": 28553,
        "title": "Dimmer: Self-Adaptive Network-Wide Flooding with Reinforcement Learning",
        "authors": "Valentin Poirot, Olaf Landsiedel",
        "published": "2021-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdcs51616.2021.00036"
    },
    {
        "id": 28554,
        "title": "Deep Reinforcement Learning Based Evasion Generative Adversarial Network for Botnet Detection",
        "authors": "Rizwan Hamid Randhawa, Nauman Aslam, Mohammad Alauthman, Muhammad Khalid, Husnain Rafiq",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4333338"
    },
    {
        "id": 28555,
        "title": "NAC Aided Performance Optimization of Stochastic Systems",
        "authors": "Changsheng Hua",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-33034-7_5"
    },
    {
        "id": 28556,
        "title": "Home Energy Management with V2X Capability using Reinforcement Learning",
        "authors": "Zachary Tchir, Marek Z. Reformat, Petr Musilek",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00046"
    },
    {
        "id": 28557,
        "title": "Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning",
        "authors": "Florian Shkurti, Nikhil Kakodkar, Gregory Dudek",
        "published": "2018-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2018.8463196"
    },
    {
        "id": 28558,
        "title": "Reinforcement learning in a prisoner's dilemma",
        "authors": "Arthur Dolgopolov",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.geb.2024.01.004"
    },
    {
        "id": 28559,
        "title": "Neuroevolution for deep reinforcement learning problems",
        "authors": "David Ha",
        "published": "2020-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3377929.3389859"
    },
    {
        "id": 28560,
        "title": "Provably Efficient Reinforcement Learning for Episodic Stochastic Inventory Control Models",
        "authors": "Xiao-Yue Gong, David Simchi-Levi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3637705"
    },
    {
        "id": 28561,
        "title": "Reinforcement Learning-based Multiple Camera Collaboration Control Scheme",
        "authors": "Dongchil Kim, Chang Mo Yang",
        "published": "2022-7-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icufn55119.2022.9829605"
    },
    {
        "id": 28562,
        "title": "Intelligent Systems Context-Aware Adaptive Reinforcement Learning Approach for Autonomous Vehicles",
        "authors": "Chandrasekaran Sakthivel, V. Chandrasekar",
        "published": "2023-2-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icctech57499.2023.00012"
    },
    {
        "id": 28563,
        "title": "Modeling the influence of working memory, reinforcement, and action uncertainty on reaction time and choice during instrumental learning",
        "authors": "Samuel McDougle, Anne Collins",
        "published": "No Date",
        "citations": 2,
        "abstract": "What determines the speed of our decisions? Various models of decision-making have focused on perceptual evidence, past experience, and task complexity as important factors determining the degree of deliberation needed for a decision. Here, we build on a sequential sampling decision-making framework to develop a new model that captures a range of reaction time (RT) effects by accounting for both working memory and instrumental learning processes. The model captures choices and RTs at various stages of learning, and in learning environments with varying complexity. Moreover, the model generalizes from tasks with deterministic reward contingencies to probabilistic ones. The model succeeds in part by incorporating prior uncertainty over actions when modeling RT. This straightforward process model provides a parsimonious account of decision dynamics during instrumental learning and makes unique predictions about internal representations of action values.",
        "link": "http://dx.doi.org/10.31234/osf.io/gcwxn"
    },
    {
        "id": 28564,
        "title": "Efficient Neural Network Pruning Using Model-Based Reinforcement Learning",
        "authors": "Blanka Bencsik, Marton Szemenyei",
        "published": "2022-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ismcr56534.2022.9950598"
    },
    {
        "id": 28565,
        "title": "Reinforcement Learning in Social Media Marketing",
        "authors": "Patrik Eklund",
        "published": "2022-7-8",
        "citations": 0,
        "abstract": "In this chapter, the authors describe an architecture for reinforcement learning in social media marketing. The rule bases used for action selection within the architecture build upon many-valued (fuzzy) logic. Action evaluation and internal learning is based on neural network like structures. In using variables measuring the effect of advertising, we must understand direction of influence between advertiser, owning the content of the advertisement, and advertisee, as the target of an advertisement, and as facilitated by social media marketing. Examples are drawn from Facebook marketing.",
        "link": "http://dx.doi.org/10.4018/978-1-6684-7123-4.ch045"
    },
    {
        "id": 28566,
        "title": "Flight AI using Reinforcement Learning",
        "authors": " Poonam Sharma,  Ashima Narang",
        "published": "2022-7-28",
        "citations": 0,
        "abstract": "This paper will be featuring an aircraft with fully featured physics simulated in unity\nengine. The agent or the AI will be created for possessing and controlling the aircraft so as to\nnavigate it in the 3D world space environment provided. The agent will have to consider the\ndifferent physical dynamics applied on a real-world aircraft and based on these parameters it will\nhave to create an effective piloting for the aircraft. Various other aspects such as engine dynamics\nand the fuel parameters will also be considered for an effective training environment.",
        "link": "http://dx.doi.org/10.46647/ijetms.2022.v06i04.0010"
    },
    {
        "id": 28567,
        "title": "Quantization aware training for efficient reinforcement learning network",
        "authors": "Qiaolin Li",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "In recent years, reinforcement learning has been applied to many areas, especially deep reinforcement learning. Deep learning-based methods use a large computing method known as Quantization Aware Training (QAT). For this project, QAT will be used to investigate the performance of Deep Q-Networks. This method of QAT can reduce memory usage of the network, but may affect the training performance of the network with some games.",
        "link": "http://dx.doi.org/10.54254/2755-2721/4/20230493"
    },
    {
        "id": 28568,
        "title": "Rule-based Reinforcement Learning for Lane Change Decision-making: A Risk Assessment Approach",
        "authors": "Lu Xiong, Danyang Zhong, Puhang Xu, Chen Tang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTo solve problems of poor security guarantee and insufficient training efficiency in the conventional reinforcement learning methods for decision-making, this study proposes a hybrid framework to combine deep reinforcement learning with rule-based decision-making methods. A risk assessment model for lane-change maneuver considering uncertain prediction of surrounding vehicles is established as a safety filter to improve the learning efficiency while corrects dangerous actions for safety enhancement. On this basis, a Risk-fused DDQN is constructed utilizing the model-based risk assessment and supervision mechanism. The proposed reinforcement learning algorithm sets up a separate experience buffer for dangerous trials and punishes such actions, which is shown to improve the sampling efficiency and training outcomes. Compared with conventional DDQN methods, the proposed algorithm improves the convergence value of cumulated reward by 7.6\\% and 2.2\\% in the two constructed scenarios in the simulation study, and reduces the number of training episodes by 52.2\\% and 66.8\\% respectively. The success rate of lane change is improved by 57.3\\% while the time headway is increased at least by 16.5\\% in real vehicle tests, which confirms the higher training efficiency, scenario adaptability and security of the proposed Risk-fused DDQN.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2536475/v1"
    },
    {
        "id": 28569,
        "title": "Optimizing Automated Trading Systems with Deep Reinforcement Learning",
        "authors": "Minh Tran, Duc Pham-Hi, Marc Bui",
        "published": "2023-1-1",
        "citations": 6,
        "abstract": "In this paper, we propose a novel approach to optimize parameters for strategies in automated trading systems. Based on the framework of Reinforcement learning, our work includes the development of a learning environment, state representation, reward function, and learning algorithm for the cryptocurrency market. Considering two simple objective functions, cumulative return and Sharpe ratio, the results showed that Deep Reinforcement Learning approach with Double Deep Q-Network setting and the Bayesian Optimization approach can provide positive average returns. Among the settings being studied, Double Deep Q-Network setting with Sharpe ratio as reward function is the best Q-learning trading system. With a daily trading goal, the system shows outperformed results in terms of cumulative return, volatility and execution time when compared with the Bayesian Optimization approach. This helps traders to make quick and efficient decisions with the latest information from the market. In long-term trading, Bayesian Optimization is a method of parameter optimization that brings higher profits. Deep Reinforcement Learning provides solutions to the high-dimensional problem of Bayesian Optimization in upcoming studies such as optimizing portfolios with multiple assets and diverse trading strategies.",
        "link": "http://dx.doi.org/10.3390/a16010023"
    },
    {
        "id": 28570,
        "title": "Multi-Agent Reinforcement Learning for Autonomic SDN-enabled LiFi Attocellular Networks Slicing",
        "authors": "Hamada Alshaer, Harald Haas",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278643"
    },
    {
        "id": 28571,
        "title": "Accelerating the Deep Reinforcement Learning with Neural Network Compression",
        "authors": "Hongjie Zhang, Zhuocheng He, Jing Li",
        "published": "2019-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2019.8852451"
    },
    {
        "id": 28572,
        "title": "Designing a double deep reinforcement learning selection tool for resilient demand prediction",
        "authors": "Bilel Abderrahmane Benziane, Benoit Lardeux, Maher Jr, Ayoub Mcharek",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nSupply chain optimization has attracted many scientific studies for decades. This topic continues to evolve due to the increasing worldwide commercial interactions and the recent progress of artificial intelligence. Artificial intelligence models have emerged as competitive forecasting methods for several years and consumer demands of goods remain a hot topic in supply chain optimisation. However, the process of selecting an appropriate forecasting solution becomes a daunting task, particularly for non-experts because they exhibit a strong dependency on databases. Even specialists face potential errors, considering that each model aligns better with a specific set of problems. This complexity arises due to the distinctive features inherent in each dataset, such as high seasonality, substantial bias, noise, volatility, intermittence, and more. This research endeavors to construct a double deep reinforcement learning model, enabling the automatic selection of a forecasting model from the forecasting committee at the time of prediction. The set of forecasting methods available to be selected in this research holds various neural network based forecasting approaches which have been proven to perform well each one in different dataset configurations; such as feed forward neural networks, recurrent neural networks, convolutional neural networks, boosting ensemble neural networks and stacking ensemble neural networks. To assess the model's performance, an empirical study is conducted using two distinct datasets selected for their diversity in order to ensure robustness to variations of the overall solution approach. The results of experiments demonstrate the resilience of the proposed approach when compared to the state-of-the-art methods.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3847826/v1"
    },
    {
        "id": 28573,
        "title": "Membership Inference Attacks Against Temporally Correlated Data in Deep Reinforcement Learning",
        "authors": "Maziar Gomrokchi, Susan Amin, Hossein Aboutalebi, Alexander Wong, Doina Precup",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nWhile significant research advances have been made in the field of deep reinforcement learning, there have been no concreteadversarial attack strategies in literature tailored for studying the vulnerability of deep reinforcement learning algorithms tomembership inference attacks. In such attacking systems, the adversary targets the set of collected input data on which thedeep reinforcement learning algorithm has been trained. To address this gap, we propose an adversarial attack frameworkdesigned for testing the vulnerability of a state-of-the-art deep reinforcement learning algorithm to a membership inferenceattack. In particular, we design a series of experiments to investigate the impact of temporal correlation, which naturally existsin reinforcement learning training data, on the probability of information leakage. Moreover, we compare the performance ofcollective and individual membership attacks against the deep reinforcement learning algorithm. Experimental results showthat the proposed adversarial attack framework is surprisingly effective at inferring data with an accuracy exceeding 84% inindividual and 97% in collective modes in three different continuous control Mujoco tasks, which raises serious privacy concernsin this regard. Finally, we show that the learning state of the reinforcement learning algorithm influences the level of privacybreaches significantly.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2261000/v1"
    },
    {
        "id": 28574,
        "title": "Reinforcement Learning for Placement Optimization",
        "authors": "Anna Goldie, Azalia Mirhoseini",
        "published": "2021-3-22",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3439706.3446883"
    },
    {
        "id": 28575,
        "title": "Reinforcement Learning Based Autonomous Air Combat with Energy Budgets",
        "authors": "Hasan Isci, Emre Koyuncu",
        "published": "2022-1-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2022-0786"
    },
    {
        "id": 28576,
        "title": "Multi-Agent Deep Reinforcement Learning with Human Strategies",
        "authors": "Thanh Nguyen, Ngoc Duy Nguyen, Saeid Nahavandi",
        "published": "2019-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit.2019.8755032"
    },
    {
        "id": 28577,
        "title": "REINFORCEMENT LEARNING IN CONTROL SYSTEMS OF OBJECTS WITH A TRANSPORT DELAY",
        "authors": "V.S. Borovik, S.V. Shidlovskiy",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15372/aut20210306"
    },
    {
        "id": 28578,
        "title": "Towards Optimal Attacks on Reinforcement Learning Policies",
        "authors": "Alessio Russo, Alexandre Proutiere",
        "published": "2021-5-25",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9483025"
    },
    {
        "id": 28579,
        "title": "Access Point Selection Using Reinforcement Learning in Dense Mobile Networks",
        "authors": "Yared Zerihun Bekele, Young June-Choi",
        "published": "2020-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin48656.2020.9016560"
    },
    {
        "id": 28580,
        "title": "Reinforcement Learning for Optimal Allocation of Superconducting Fault Current Limiters",
        "authors": "Xiejin Ling, Yinhong Li",
        "published": "2020-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm41954.2020.9281474"
    },
    {
        "id": 28581,
        "title": "Deep Reinforcement Learning for Optimization",
        "authors": "Md Mahmudul Hasan, Md Shahinur Rahman, Adrian Bell",
        "published": "2020-11-27",
        "citations": 0,
        "abstract": "Deep reinforcement learning (DRL) has transformed the field of artificial intelligence (AI) especially after the success of Google DeepMind. This branch of machine learning epitomizes a step toward building autonomous systems by understanding of the visual world. Deep reinforcement learning (RL) is currently applied to different sorts of problems that were previously obstinate. In this chapter, at first, the authors started with an introduction of the general field of RL and Markov decision process (MDP). Then, they clarified the common DRL framework and the necessary components RL settings. Moreover, they analyzed the stochastic gradient descent (SGD)-based optimizers such as ADAM and a non-specific multi-policy selection mechanism in a multi-objective Markov decision process. In this chapter, the authors also included the comparison for different Deep Q networks. In conclusion, they describe several challenges and trends in research within the deep reinforcement learning field. ",
        "link": "http://dx.doi.org/10.4018/978-1-7998-7705-9.ch070"
    },
    {
        "id": 28582,
        "title": "Multi-Agent Reinforcement Learning for Autonomous On Demand Vehicles",
        "authors": "Ali Boyali, Naohisa Hashimoto, Vijay John, Tankut Acarman",
        "published": "2019-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2019.8813876"
    },
    {
        "id": 28583,
        "title": "Reinforcement Learning for Sequential Low-Thrust Orbit Raising Problem",
        "authors": "Lakshay Arora, Atri Dutta",
        "published": "2020-1-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2020-2186"
    },
    {
        "id": 28584,
        "title": "Deep Reinforcement Learning Pairs Trading with a Double Deep Q-Network",
        "authors": "Andrew Brim",
        "published": "2020-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccwc47524.2020.9031159"
    },
    {
        "id": 28585,
        "title": "The driver and the engineer: Reinforcement learning and robust control",
        "authors": "Natalie Bernat, Jiexin Chen, Nikolai Matni, John Doyle",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147347"
    },
    {
        "id": 28586,
        "title": "Dynamic Optimization for Secure MIMO Beamforming using Large-scale Reinforcement Learning",
        "authors": "Xinran Zhang, Songlin Sun",
        "published": "2019-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc.2019.8885667"
    },
    {
        "id": 28587,
        "title": "Distributed Input Design with Combination of System Identification and Reinforcement Learning",
        "authors": "Xiangyu Mao, Jianping He",
        "published": "2022-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus55513.2022.9987073"
    },
    {
        "id": 28588,
        "title": "Review of reinforcement learning research",
        "authors": "Jingkai Jia, Wenlin Wang",
        "published": "2020-10-16",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/yac51587.2020.9337653"
    },
    {
        "id": 28589,
        "title": "The Formation Control of Mobile Autonomous Multi-Agent Systems Using Deep Reinforcement Learning",
        "authors": "Qishuai Liu, Qing Hui",
        "published": "2019-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/syscon.2019.8836902"
    },
    {
        "id": 28590,
        "title": "Model Based Human‐Robot Interaction Control",
        "authors": "",
        "published": "2021-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.ch3"
    },
    {
        "id": 28591,
        "title": "Morl4pdes: Data-Driven Discovery of Pdes Based on Multi-Objective Optimization and Reinforcement Learning",
        "authors": "xiaoxia zhang, Junsheng Guan, Yanjun Liu, Guoyin Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4555059"
    },
    {
        "id": 28592,
        "title": "Lateral flow control of connected vehicles through deep reinforcement learning",
        "authors": "Abdul Rahman Kreidieh, Yashar Farid, Kentaro Oguchi",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv55152.2023.10186790"
    },
    {
        "id": 28593,
        "title": "Multi-Objective Traffic Signal Control Using Network-Wide Agent Coordinated Reinforcement Learning",
        "authors": "jie fang, Ya You, Mengyun Xu, Juanmeizi Wang, Sibin Cai",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4374888"
    },
    {
        "id": 28594,
        "title": "Loads Alleviation on an Airfoil via Reinforcement Learning",
        "authors": "Esteban A. Hufstedler, Philippe Chatelain",
        "published": "2019-1-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2019-0404"
    },
    {
        "id": 28595,
        "title": "Efficient Detailed Routing for FPGA Back-End Flow Using Reinforcement Learning",
        "authors": "Imran Baig, Umer Farooq",
        "published": "2022-7-18",
        "citations": 3,
        "abstract": "Over the past few years, the computation capability of field-programmable gate arrays (FPGAs) has increased tremendously. This has led to the increase in the complexity of the designs implemented on FPGAs and to the time taken by the FPGA back-end flow. The FPGA back-end flow comprises of many steps, and routing is one of the most critical steps among them. Routing normally constitutes more than 50% of the total time taken by the back-end flow and an optimization at this step can lead to overall optimization of the back-end flow. In this work, we propose enhancements to the routing step by incorporating a reinforcement learning (RL)-based framework. In the proposed RL-based framework, we use the ϵ-greedy approach and customized reward functions to speed up the routing step while maintaining similar or better quality of results (QoR) as compared to the conventional negotiation-based congestion-driven routing solution. For experimentation, we use two sets of widely deployed, large heterogeneous benchmarks. Our results show that, for the RL-based framework, the ϵ-greedy greedy approach combined with a modified reward function gives better results as compared to purely greedy or exploratory approaches. Moreover, the incorporation of the proposed reward function in the RL-based framework and its comparison with a conventional routing algorithm shows that the proposed enhancement requires less routing time while giving similar or better QoR. On average, a speedup of 35% is recorded for the proposed routing enhancement as compared to negotiation-based congestion-driven routing solutions. Finally, the speedup of the routing step leads to an overall reduction in the execution time of the back-end flow of 25%.",
        "link": "http://dx.doi.org/10.3390/electronics11142240"
    },
    {
        "id": 28596,
        "title": "Active Flow Control on Airfoils by Reinforcement Learning",
        "authors": "Koldo Portal-Porras, Unai Fernandez-Gamiz, Ekaitz Zulueta, Roberto Garcia-Fernandez, Saioa Etxebarria Berrizbeitia",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4474588"
    },
    {
        "id": 28597,
        "title": "Intelligent Decision Making in Autonomous Vehicles using Cognition Aided Reinforcement Learning",
        "authors": "Heena Rathore, Vikram Bhadauria",
        "published": "2022-4-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc51071.2022.9771728"
    },
    {
        "id": 28598,
        "title": "Research on Underwater Gliders Path Planning Method Based on Deep Reinforcement Learning Enhanced Rrt",
        "authors": "Nan Jiang, Qinghai Zhao, Chong Xu, Xin Dong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4453330"
    },
    {
        "id": 28599,
        "title": "Aircraft Engine Maintenance Based on Reinforcement Learning",
        "authors": "Xuedong Xu, Xiao Wang",
        "published": "2020-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icitbs49701.2020.00213"
    },
    {
        "id": 28600,
        "title": "Real-Time Scheduling Using Reinforcement Learning Technique for the Connected Vehicles",
        "authors": "Seongjin Park, Younghwan Yoo",
        "published": "2018-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtcspring.2018.8417770"
    },
    {
        "id": 28601,
        "title": "Reinforcement Learning for Long-term Reward Optimization in Recommender Systems",
        "authors": "Anton Dorozhko",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sibircon48586.2019.8958202"
    },
    {
        "id": 28602,
        "title": "Autonomous Car Parking System using Deep Reinforcement Learning",
        "authors": "Rikuya Takehara, Tad Gonsalves",
        "published": "2021-9-23",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icitech50181.2021.9590169"
    },
    {
        "id": 28603,
        "title": "Dynamic Channel Access and Power Control via Deep Reinforcement Learning",
        "authors": "Ziyang Lu, M. Cenk Gursoy",
        "published": "2019-9",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtcfall.2019.8891391"
    },
    {
        "id": 28604,
        "title": "A Layered Reference Model for Penetration Testing with Reinforcement Learning and Attack Graphs",
        "authors": "Tyler Cody",
        "published": "2022-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/stc55697.2022.00015"
    },
    {
        "id": 28605,
        "title": "Reinforcement learning strategies for vessel navigation",
        "authors": "Andrius Daranda, Gintautas Dzemyda",
        "published": "2022-11-24",
        "citations": 3,
        "abstract": "Safe navigation at sea is more important than ever. Cargo is usually transported by vessel because it makes economic sense. However, marine accidents can cause huge losses of people, cargo, and the vessel itself, as well as irreversible ecological disasters. These are the reasons to strive for safe vessel navigation. The navigator shall ensure safe vessel navigation. He must plan every maneuver and act safely. At the same time, he must evaluate and predict the actions of other vessels in dense maritime traffic. This is a complicated process and requires constant human concentration. It is a very tiring and long-lasting duty. Therefore, human error is the main reason of collisions between vessels. In this paper, different reinforcement learning strategies have been explored in order to find the most appropriate one for the real-life problem of ensuring safe maneuvring in maritime traffic. An experiment using different algorithms was conducted to discover a suitable method for autonomous vessel navigation. The experiments indicate that the most effective algorithm (Deep SARSA) allows reaching 92.08% accuracy. The efficiency of the proposed model is demonstrated through a real-life collision between two vessels and how it could have been avoided.",
        "link": "http://dx.doi.org/10.3233/ica-220688"
    },
    {
        "id": 28606,
        "title": "Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Yumeya Yamamori, Oliver J Robinson, Jonathan P Roiser",
        "published": "No Date",
        "citations": 0,
        "abstract": "Although avoidance is a prevalent feature of anxiety-related psychopathology, differences in existing measures of avoidance between humans and non-human animals impede progress in its theoretical understanding and treatment. To address this, we developed a novel translational measure of anxiety-related avoidance in the form of an approach-avoidance reinforcement learning task, by adapting a paradigm from the non-human animal literature to study the same cognitive processes in human participants. We used computational modelling to probe the putative cognitive mechanisms underlying approach-avoidance behaviour in this task and investigated how they relate to subjective task-induced anxiety. In a large online study, participants (n = 372) who experienced greater task-induced anxiety avoided choices associated with punishment, even when this resulted in lower overall reward. Computational modelling revealed that this effect was explained by greater individual sensitivities to punishment relative to rewards. We replicated these findings in an independent sample (n = 627) and we also found fair-to-excellent reliability of measures of task performance in a sub-sample retested one week later (n = 57). Our findings demonstrate the potential of approach-avoidance reinforcement learning tasks as translational and computational models of anxiety-related avoidance. Future studies should assess the predictive validity of this approach in clinical samples and experimental manipulations of anxiety.",
        "link": "http://dx.doi.org/10.7554/elife.87720.2"
    },
    {
        "id": 28607,
        "title": "5G Network Slice Admission Control Using Optimization and Reinforcement Learning",
        "authors": "Md Ariful Haque, Vassilka Kirova",
        "published": "2022-4-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc51071.2022.9771643"
    },
    {
        "id": 28608,
        "title": "On-Policy Deep Reinforcement Learning Approach to Multi Agent Problems",
        "authors": "Ziya Tan, Mehmet Karakose",
        "published": "2021-9-14",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003202240-58"
    },
    {
        "id": 28609,
        "title": "A Policy Search Method For Temporal Logic Specified Reinforcement Learning Tasks",
        "authors": "Xiao Li, Yao Ma, Calin Belta",
        "published": "2018-6",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2018.8431181"
    },
    {
        "id": 28610,
        "title": "The segregation of vocal circuits solves a credit assignment problem associated with multi-objective reinforcement learning",
        "authors": "Don Murdoch, Ruidong Chen, Jesse Goldberg",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMotor circuits vary in topographic organization, ranging from a coarse relationship between neuron location and function to highly localized regions controlling specific behaviors. For unclear reasons, vocal learning circuits lie at this second extreme: they repeatedly evolved to be spatially segregated from other parts of the motor system. Here we show that spatially segregated motor circuits can solve a specific problem that arises when an animal tries to learn two things at once. We trained songbirds in vocal and place learning paradigms with brief strobe light flashes and noise bursts. Strobe light negatively reinforced place learning but did not affect song syllable learning. Noise bursts positively reinforced place preference but negatively reinforced syllable learning. These double dissociations indicate that vocalization-related reinforcement signals specifically target the vocal motor system, while place-related reinforcement signals specifically target the navigation system. Non-global, target-specific reinforcement signals have established utility in machine implementation of multi-objective learning. In vocal learners, such signals could enable an animal to practice vocalizing as it does other things such as forage for food or learn to walk.",
        "link": "http://dx.doi.org/10.1101/236273"
    },
    {
        "id": 28611,
        "title": "Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Yumeya Yamamori, Oliver J Robinson, Jonathan P Roiser",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractAlthough avoidance is a prevalent feature of anxiety-related psychopathology, differences in the measurement of avoidance between humans and non-human animals hinder our progress in its theoretical understanding and treatment. To address this, we developed a novel translational measure of anxiety-related avoidance in the form of an approach-avoidance reinforcement learning task, by adapting a paradigm from the non-human animal literature to study the same cognitive processes in human participants. We used computational modelling to probe the putative cognitive mechanisms underlying approach-avoidance behaviour in this task and investigated how they relate to subjective task-induced anxiety. In a large online study (n = 372), participants who experienced greater task-induced anxiety avoided choices associated with punishment, even when this resulted in lower overall reward. Computational modelling revealed that this effect was explained by greater individual sensitivities to punishment relative to rewards. We replicated these findings in an independent sample (n = 627) and we also found fair-to-excellent reliability of measures of task performance in a sub-sample retested one week later (n = 57). Our findings demonstrate the potential of approach-avoidance reinforcement learning tasks as translational and computational models of anxiety-related avoidance. Future studies should assess the predictive validity of this approach in clinical samples and experimental manipulations of anxiety.",
        "link": "http://dx.doi.org/10.1101/2023.04.04.535526"
    },
    {
        "id": 28612,
        "title": "Computational characteristics of the striatal dopamine system described by reinforcement learning with fast generalization",
        "authors": "Yoshihisa Fujita, Sho Yagishita, Haruo Kasai, Shin Ishii",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractGeneralization enables applying past experience to similar but nonidentical situations. Therefore, it may be essential for adaptive behaviors. Recent neurobiological observation indicates that the striatal dopamine system achieves generalization and subsequent discrimination by updating corticostriatal synaptic connections in differential response to reward and punishment. To analyze how the computational characteristics in this system affect behaviors, we proposed a novel reinforcement learning model with multilayer neural networks in which the synaptic weights of only the last layer are updated according to the prediction error. We set fixed connections between the input and hidden layers so as to maintain the similarity of inputs in the hidden-layer representation. This network enabled fast generalization, and thereby facilitated safe and efficient exploration in reinforcement learning tasks, compared to algorithms which do not show generalization. However, disturbance in the network induced aberrant valuation. In conclusion, the unique computation suggested by corticostriatal plasticity has the advantage of providing safe and quick adaptations to unknown environments, but on the other hand has the potential defect which can induce maladaptive behaviors like delusional symptoms of psychiatric disorders.Author summaryThe brain has an ability to generalize knowledge obtained from reward- and punishment-related learning. Animals that have been trained to associate a stimulus with subsequent reward or punishment respond not only to the same stimulus but also to resembling stimuli. How does generalization affect behaviors in situations where individuals are required to adapt to unknown environments? It may enable efficient learning and promote adaptive behaviors, but inappropriate generalization may disrupt behaviors by associating reward or punishment with irrelevant stimuli. The effect of generalization here should depend on computational characteristics of underlying biological basis in the brain, namely, the striatal dopamine system. In this research, we made a novel computational model based on the characteristics of the striatal dopamine system. Our model enabled fast generalization and showed its advantage of providing safe and quick adaptation to unknown environments. By contrast, disturbance of our model induced abnormal behaviors. The results suggested the advantage and the shortcoming of generalization by the striatal dopamine system.",
        "link": "http://dx.doi.org/10.1101/2019.12.12.873950"
    },
    {
        "id": 28613,
        "title": "Cooperative reinforcement learning for multiple units combat in starCraft",
        "authors": "Kun Shao, Yuanheng Zhu, Dongbin Zhao",
        "published": "2017-11",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci.2017.8280949"
    },
    {
        "id": 28614,
        "title": "Improve PID controller through reinforcement learning",
        "authors": "Yunxiao Qin, Weiguo Zhang, Jingping Shi, Jinglong Liu",
        "published": "2018-8",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gncc42960.2018.9019095"
    },
    {
        "id": 28615,
        "title": "Task-Oriented Query Reformulation with Reinforcement Learning",
        "authors": "Rodrigo Nogueira, Kyunghyun Cho",
        "published": "2017",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d17-1061"
    },
    {
        "id": 28616,
        "title": "Curved Path Following with Deep Reinforcement Learning: Results from Three Vessel Models",
        "authors": "Andreas B. Martinsen, Anastasios M. Lekkas",
        "published": "2018-10",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/oceans.2018.8604829"
    },
    {
        "id": 28617,
        "title": "Autonomous Intersection Management by Using Reinforcement Learning",
        "authors": "P. Karthikeyan, Wei-Lun Chen, Pao-Ann Hsiung",
        "published": "2022-9-13",
        "citations": 4,
        "abstract": "Developing a safer and more effective intersection-control system is essential given the trends of rising populations and vehicle numbers. Additionally, as vehicle communication and self-driving technologies evolve, we may create a more intelligent control system to reduce traffic accidents. We recommend deep reinforcement learning-inspired autonomous intersection management (DRLAIM) to improve traffic environment efficiency and safety. The three primary models used in this methodology are the priority assignment model, the intersection-control model learning, and safe brake control. The brake-safe control module is utilized to make sure that each vehicle travels safely, and we train the system to acquire an effective model by using reinforcement learning. We have simulated our proposed method by using a simulation of urban mobility tools. Experimental results show that our approach outperforms the traditional method.",
        "link": "http://dx.doi.org/10.3390/a15090326"
    },
    {
        "id": 28618,
        "title": "Power network fault identification based on deep reinforcement learning",
        "authors": "",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2020.04520"
    },
    {
        "id": 28619,
        "title": "Power Control in Internet of Drones by Deep Reinforcement Learning",
        "authors": "Jingjing Yao, Nirwan Ansari",
        "published": "2020-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc40277.2020.9148749"
    },
    {
        "id": 28620,
        "title": "Mobile Cellular-Connected UAVs: Reinforcement Learning for Sky Limits",
        "authors": "M. Mahdi Azari, Atefeh Hajijamali Arani, Fernando Rosas",
        "published": "2020-12",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcwkshps50303.2020.9367580"
    },
    {
        "id": 28621,
        "title": "Data Driven Equation Discovery Reveals Non-linear Reinforcement Learning in Humans",
        "authors": "Kyle J. LaFollette, Janni Yuval, Roey Schurr, David Melnikoff, Amit Goldenberg",
        "published": "No Date",
        "citations": 0,
        "abstract": "Computational models of reinforcement learning (RL), have significantly contributed to our understanding of human behavior and decision-making. Traditional RL models, however, often adopt a linear approach to updating reward expectations, potentially oversimplifying the nuanced relationship between human behavior and rewards. To address these challenges and explore new models of reinforcement learning, we utilized a novel method of model discovery using equation discovery algorithms. This method, currently used mainly in physics and biology, attempts to capture data by proposing a differential equation from an array of suggested linear and nonlinear functions. Using this novel method, we were able to identify a new model of RL which we termed, the Quadratic Q-Weighted model. The model suggests that reward prediction errors obey nonlinear dynamics and exhibit negativity biases, resulting in an underweighting of reward when expectations are low, and an overweighting of the absence of reward when expectations are high. We tested the generalizability of our model by comparing it to classical models used in 9 published studies. Our model surpassed traditional models in predictive accuracy across eight out of these nine published datasets, demonstrating not only its generalizability but also its potential to offer new insights into the complexities of human learning. This work showcases the integration of a novel behavioral task with advanced computational methodologies as a potent strategy for uncovering the intricate patterns of human cognition, marking a significant step forward in the development of computational models that are both interpretable and broadly applicable.",
        "link": "http://dx.doi.org/10.31234/osf.io/65jqh"
    },
    {
        "id": 28622,
        "title": "Reinforcement Learning for a Continuous DC Motor Controller",
        "authors": "Bucur Cosmin, Tasu Sorin",
        "published": "2023-6-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecai58194.2023.10193912"
    },
    {
        "id": 28623,
        "title": "Explainability of Deep Reinforcement Learning Method with Drones",
        "authors": "Ender Çetin, Cristina Barrado, Enric Pastor",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dasc58513.2023.10311156"
    },
    {
        "id": 28624,
        "title": "Cooperative Search Method for Multiple UAVs Based on Deep Reinforcement Learning",
        "authors": "Mingsheng Gao, Xiaoxuan Zhang",
        "published": "2022-9-6",
        "citations": 2,
        "abstract": "In this paper, a cooperative search method for multiple UAVs is proposed to solve the problem of low efficiency of multi-UAV task execution by using a cooperative game with incomplete information. To improve search efficiency, CBBA (Consensus-Based Bundle Algorithm) is applied to designate the tasks area for each UAV. Then, Independent Deep Reinforcement Learning (IDRL) is used to solve Nash equilibrium to improve UAVs’ collaborations. The proposed reward function is smartly developed to guide UAVs to fly along the path with higher reward value while avoiding the collisions between UAVs during flights. Finally, extensive experiments are carried out to compare our proposed method with other algorithms. Simulation results show that the proposed method can obtain more rewards in the same period of time as other algorithms.",
        "link": "http://dx.doi.org/10.3390/s22186737"
    },
    {
        "id": 28625,
        "title": "FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance",
        "authors": "Xiao-Yang Liu",
        "published": "No Date",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3737257"
    },
    {
        "id": 28626,
        "title": "Door opening by joining reinforcement learning and intelligent control",
        "authors": "Bojan Nemec, Leon Zlajpah, Ales Ude",
        "published": "2017-7",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icar.2017.8023522"
    },
    {
        "id": 28627,
        "title": "Efficient hyperparameter optimization through model-based reinforcement learning",
        "authors": "Jia Wu, SenPeng Chen, XiYuan Liu",
        "published": "2020-10",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2020.06.064"
    },
    {
        "id": 28628,
        "title": "Energy-efficient and damage-recovery slithering gait design for a snake-like robot based on reinforcement learning and inverse reinforcement learning",
        "authors": "Zhenshan Bing, Christian Lemke, Long Cheng, Kai Huang, Alois Knoll",
        "published": "2020-9",
        "citations": 29,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2020.05.029"
    },
    {
        "id": 28629,
        "title": "Survival-Oriented Reinforcement Learning Model: An Effcient and Robust Deep Reinforcement Learning Algorithm for Autonomous Driving Problem",
        "authors": "Changkun Ye, Huimin Ma, Xiaoqin Zhang, Kai Zhang, Shaodi You",
        "published": "2017",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-71589-6_36"
    },
    {
        "id": 28630,
        "title": "Study on Aggregator’s Strategy of Controlling Electric Vehicles to Compensate Imbalances in Power Systems Using Reinforcement Learning",
        "authors": "Ryuhei Furuta,  , Ryuji Matsuhashi",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijoee.7.2.67-71"
    },
    {
        "id": 28631,
        "title": "Towards modeling the learning process of aviators using deep reinforcement learning",
        "authors": "Joost van Oijen, Gerald Poppinga, Olaf Brouwer, Andi Aliko, Jan Joris Roessingh",
        "published": "2017-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc.2017.8123162"
    },
    {
        "id": 28632,
        "title": "Improving Deep Reinforcement Learning for Financial Trading Using Deep Adaptive Group-Based Normalization",
        "authors": "Angelos Nalmpantis, Nikolaos Passalis, Avraam Tsantekidis, Anastasios Tefas",
        "published": "2021-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp52302.2021.9596155"
    },
    {
        "id": 28633,
        "title": "Large-scale Passenger Behavior Learning and Prediction in Airport Terminals based on Multi-Agent Reinforcement Learning",
        "authors": "Yue Li, Guokang Gao",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "For the problem of predicting passenger flow in airport terminals, multi-agent reinforcement learning is applied to airport terminals simulation. Multi-Agent Reinforcement Learning based on Group Shared Policy with Mean-field and Intrinsic Rewards (GQ-MFI) is proposed to predict passenger behavior in order to simulate the distribution of flow in different areas of the terminal at different time periods. Independent learning of multi-agent may lead to environmental instability and long convergence time. To improve the adaptability of agents in non-stationary environments and accelerate learning time, a multi-agent grouping learning strategy is proposed. Clustering is used to group multi-agent, and a shared Q-table is set within each group to improve the learning efficiency of multi-agent. Meanwhile, in order to simplify the interaction information among the agent after grouping, the idea of average field is used to transmit partial global information among the agent within the group. Intrinsic rewards are added to make the agent closer to human cognition and behavioral patterns. By conducting the airport terminal simulations using Anylogic, the experimental results show that the training speed of this algorithm is 17% higher than that of Q-learning algorithm, and it achieves good prediction accuracy in predicting the number of security check passengers with a time scale of 10 minutes.",
        "link": "http://dx.doi.org/10.54097/fcis.v5i1.12008"
    },
    {
        "id": 28634,
        "title": "Modeling Agent Behaviors for Policy Analysis via Reinforcement Learning",
        "authors": "Osonde A. Osoba, Raffaele Vardavas, Justin Grana, Rushil Zutshi, Amber Jaycocks",
        "published": "2020-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla51294.2020.00043"
    },
    {
        "id": 28635,
        "title": "FLASH-RL: Federated Learning Addressing System and Static Heterogeneity using Reinforcement Learning",
        "authors": "Sofiane Bouaziz, Hadjer Benmeziane, Youcef Imine, Leila Hamdad, Smail Niar, Hamza Ouarnoughi",
        "published": "2023-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccd58817.2023.00074"
    },
    {
        "id": 28636,
        "title": "The Guiding Role of Reward Based on Phased Goal in Reinforcement Learning",
        "authors": "Yiming Liu, Zheng Hu",
        "published": "2020-2-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3383972.3384039"
    },
    {
        "id": 28637,
        "title": "Curriculum Learning Based on Reward Sparseness for Deep Reinforcement Learning of Task Completion Dialogue Management",
        "authors": "Atsushi Saito",
        "published": "2018",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-5707"
    },
    {
        "id": 28638,
        "title": "On Learning Intrinsic Rewards for Faster Multi-Agent Reinforcement Learning based MAC Protocol Design in 6G Wireless Networks",
        "authors": "Luciano Miuccio, Salvatore Riolo, Mehdi Bennis, Daniela Panno",
        "published": "2023-5-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279625"
    },
    {
        "id": 28639,
        "title": "Learning Multi-Task Transferable Rewards via Variational Inverse Reinforcement Learning",
        "authors": "Se-Wook Yoo, Seung-Woo Seo",
        "published": "2022-5-23",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9811697"
    },
    {
        "id": 28640,
        "title": "An Emotional Virtual Character: A Deep Learning Approach with Reinforcement Learning",
        "authors": "Gilzamir Gomes, Creto A. Vidal, Joaquim B. Cavalcante Neto, Yuri L. B. Nogueira",
        "published": "2019-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/svr.2019.00047"
    },
    {
        "id": 28641,
        "title": "A Multi-Stage Deep Reinforcement Learning with Search-Based Optimization for Air–Ground Unmanned System Navigation",
        "authors": "Xiaohui Chen, Yuhua Qi, Yizhen Yin, Yidong Chen, Li Liu, Hongbo Chen",
        "published": "2023-2-9",
        "citations": 4,
        "abstract": "An important challenge for air–ground unmanned systems achieving autonomy is navigation, which is essential for them to accomplish various tasks in unknown environments. This paper proposes an end-to-end framework for solving air–ground unmanned system navigation using deep reinforcement learning (DRL) while optimizing by using a priori information from search-based path planning methods, which we call search-based optimizing DRL (SO-DRL) for the air–ground unmanned system. SO-DRL enables agents, i.e., an unmanned aerial vehicle (UAV) or an unmanned ground vehicle (UGV) to move to a given target in a completely unknown environment using only Lidar, without additional mapping or global planning. Our framework is equipped with Deep Deterministic Policy Gradient (DDPG), an actor–critic-based reinforcement learning algorithm, to input the agents’ state and laser scan measurements into the network and map them to continuous motion control. SO-DRL draws on current excellent search-based algorithms to demonstrate path planning and calculate rewards for its behavior. The demonstrated strategies are replayed in an experienced pool along with the autonomously trained strategies according to their priority. We use a multi-stage training approach based on course learning to train SO-DRL on the 3D simulator Gazebo and verify the robustness and success of the algorithm using new test environments for path planning in unknown environments. The experimental results show that SO-DRL can achieve faster algorithm convergence and a higher success rate. We piggybacked SO-DRL directly onto a real air–ground unmanned system, and SO-DRL can guide a UAV or UGV for navigation without adjusting any networks.",
        "link": "http://dx.doi.org/10.3390/app13042244"
    },
    {
        "id": 28642,
        "title": "Applications of Cellular Learning Automata and Reinforcement Learning in Global Optimization",
        "authors": "Reza Vafashoar, Hossein Morshedlou, Alireza Rezvanian, Mohammad Reza Meybodi",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-53141-6_4"
    },
    {
        "id": 28643,
        "title": "The Wisdom of the Crowd: Reliable Deep Reinforcement Learning Through Ensembles of Q-Functions",
        "authors": "Daniel L. Elliott, Charles Anderson",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3089425"
    },
    {
        "id": 28644,
        "title": "Deep Learning and Reinforcement Learning for Autonomous Unmanned Aerial Systems: Roadmap for Theory to Deployment",
        "authors": "Jithin Jagannath, Anu Jagannath, Sean Furman, Tyler Gwin",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-77939-9_2"
    },
    {
        "id": 28645,
        "title": "Deep Reinforcement Learning for 5G Radio Access Network Slicing with Spectrum Coexistence",
        "authors": "Yi Shi, Parisa Rahimzadeh, Maice Costa, Tugba Erpek, Yalin E. Sagduyu",
        "published": "No Date",
        "citations": 0,
        "abstract": "The paper presents a reinforcement learning solution to dynamic admission control and resource allocation for 5G radio access network (RAN) slicing requests, when the spectrum is potentially shared between 5G and an incumbent user such as in the Citizens Broadband Radio Service scenarios. Available communication resources (frequency-time resource blocks and transmit powers) and computational resources (processor power) not used by the incumbent user can be allocated to stochastic arrivals of network slicing requests. Each request arrives with priority (weight), throughput, computational resource, and latency (deadline) requirements. As online algorithms, the greedy and myopic solutions that do not consider heterogeneity of future requests and their arrival process become ineffective for network slicing. Therefore, reinforcement learning solutions (Q-learning and Deep Q-learning) are presented to maximize the network utility in terms of the total weight of granted network slicing requests over a time horizon, subject to communication and computational constraints. Results show that reinforcement learning provides improvements in the 5G network utility relative to myopic, greedy, random, and first come first served solutions. In particular, deep Q-learning reduces the complexity and allows practical implementation as the state-action space grows, and effectively admits/rejects requests when 5G needs to share the spectrum with incumbent users that may dynamically occupy some of the frequency-time blocks. Furthermore, the robustness of deep reinforcement learning is demonstrated in the presence of the misdetection/false alarm errors in detecting the incumbent user's activity.",
        "link": "http://dx.doi.org/10.36227/techrxiv.16632526"
    },
    {
        "id": 28646,
        "title": "HyLEAR: Hybrid Deep Reinforcement Learning and Planning for Safe and Comfortable Automated Driving",
        "authors": "Dikshant Gupta, Matthias Klusch",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv55152.2023.10186781"
    },
    {
        "id": 28647,
        "title": "Research on Stock Trading Strategy Based on Deep Reinforcement Learning",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ajcis.2022.051010"
    },
    {
        "id": 28648,
        "title": "Derivative‐Free Stochastic Search",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch7"
    },
    {
        "id": 28649,
        "title": "Optimal Compensation Scheme Considering Overwork and Effort Cost: a Reinforcement Learning Approach",
        "authors": "Toshihiko Nanba, Takahiro Inada",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4558316"
    },
    {
        "id": 28650,
        "title": "A Markov Game Approach Based on Multiagent Reinforcement Learning Solution for Cyber-Physical Attacks in Smart Grid",
        "authors": "Kübra Bitirgen, Ümmühan Başaran Filik",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4626686"
    },
    {
        "id": 28651,
        "title": "Design of Control Systems Using Active Uncertainty Reduction-Based Reinforcement Learning",
        "authors": "Zequn Wang, Narendra Patwardhan",
        "published": "2020-8-17",
        "citations": 0,
        "abstract": "Abstract\nModel-free reinforcement learning based methods such as Proximal Policy Optimization, or Q-learning typically require thousands of interactions with the environment to approximate the optimal controller which may not always be feasible in robotics due to safety and time consumption. Model-based methods such as PILCO or BlackDrops, while data-efficient, provide solutions with limited robustness and complexity. To address this tradeoff, we introduce active uncertainty reduction-based virtual environments, which are formed through limited trials conducted in the original environment. We provide an efficient method for uncertainty management, which is used as a metric for self-improvement by identification of the points with maximum expected improvement through adaptive sampling. Capturing the uncertainty also allows for better mimicking of the reward responses of the original system. Our approach enables the use of complex policy structures and reward functions through a unique combination of model-based and model-free methods, while still retaining the data efficiency. We demonstrate the validity of our method on several classic reinforcement learning problems in OpenAI gym. We prove that our approach offers a better modeling capacity for complex system dynamics as compared to established methods.",
        "link": "http://dx.doi.org/10.1115/detc2020-22014"
    },
    {
        "id": 28652,
        "title": "Dimmer: Self-Adaptive Network-Wide Flooding with Reinforcement Learning",
        "authors": "Valentin Poirot, Olaf Landsiedel",
        "published": "2021-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdcs51616.2021.00036"
    },
    {
        "id": 28653,
        "title": "Deep Reinforcement Learning Based Evasion Generative Adversarial Network for Botnet Detection",
        "authors": "Rizwan Hamid Randhawa, Nauman Aslam, Mohammad Alauthman, Muhammad Khalid, Husnain Rafiq",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4333338"
    },
    {
        "id": 28654,
        "title": "NAC Aided Performance Optimization of Stochastic Systems",
        "authors": "Changsheng Hua",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-33034-7_5"
    },
    {
        "id": 28655,
        "title": "Modeling the influence of working memory, reinforcement, and action uncertainty on reaction time and choice during instrumental learning",
        "authors": "Samuel McDougle, Anne Collins",
        "published": "No Date",
        "citations": 2,
        "abstract": "What determines the speed of our decisions? Various models of decision-making have focused on perceptual evidence, past experience, and task complexity as important factors determining the degree of deliberation needed for a decision. Here, we build on a sequential sampling decision-making framework to develop a new model that captures a range of reaction time (RT) effects by accounting for both working memory and instrumental learning processes. The model captures choices and RTs at various stages of learning, and in learning environments with varying complexity. Moreover, the model generalizes from tasks with deterministic reward contingencies to probabilistic ones. The model succeeds in part by incorporating prior uncertainty over actions when modeling RT. This straightforward process model provides a parsimonious account of decision dynamics during instrumental learning and makes unique predictions about internal representations of action values.",
        "link": "http://dx.doi.org/10.31234/osf.io/gcwxn"
    },
    {
        "id": 28656,
        "title": "Home Energy Management with V2X Capability using Reinforcement Learning",
        "authors": "Zachary Tchir, Marek Z. Reformat, Petr Musilek",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00046"
    },
    {
        "id": 28657,
        "title": "Object Detection Using Policy-Based Reinforcement Learning",
        "authors": "Keong-Hun Choi, Jong-Eun Ha",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iccas59377.2023.10316786"
    },
    {
        "id": 28658,
        "title": "Packet Routing with Graph Attention Multi-Agent Reinforcement Learning",
        "authors": "Xuan Mai, Quanzhi Fu, Yi Chen",
        "published": "2021-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685941"
    },
    {
        "id": 28659,
        "title": "Tourism Route Recommendation Using Reinforcement Learning",
        "authors": "Muhammad Ilham Mubarak, Z. K. A. Baizal",
        "published": "2023-4-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i2ct57861.2023.10126347"
    },
    {
        "id": 28660,
        "title": "Deep Reinforcement Learning based Intelligent Traffic Control",
        "authors": "Arpan Nookala, Eeshaan Asodekar, Aryan Solanki, Narendra Bhagat, Deepak Karia",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tensymp55890.2023.10223639"
    },
    {
        "id": 28661,
        "title": "Experience Selection in Multi-agent Deep Reinforcement Learning",
        "authors": "Yishen Wang, Zongzhang Zhang",
        "published": "2019-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai.2019.00123"
    },
    {
        "id": 28662,
        "title": "Deep-Reinforcement Learning Multiple Access for Heterogeneous Wireless Networks",
        "authors": "Yiding Yu, Taotao Wang, Soung Chang Liew",
        "published": "2018-5",
        "citations": 38,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2018.8422168"
    },
    {
        "id": 28663,
        "title": "Manufacturing Scheduling Using Colored Petri Nets and Reinforcement Learning",
        "authors": "Maria Drakaki, Panagiotis Tzionas",
        "published": "2017-2-3",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/app7020136"
    },
    {
        "id": 28664,
        "title": "Transfer of Temporal Logic Formulas in Reinforcement Learning",
        "authors": "Zhe Xu, Ufuk Topcu",
        "published": "2019-8",
        "citations": 23,
        "abstract": "Transferring high-level knowledge from a source task to a target task is an effective way to expedite reinforcement learning (RL). For example, propositional logic and first-order logic have been used as representations of such knowledge. We study the transfer of knowledge between tasks in which the timing of the events matters. We call such tasks temporal tasks. We concretize similarity between temporal tasks through a notion of logical transferability, and develop a transfer learning approach between different yet similar temporal tasks. We first propose an inference technique to extract metric interval temporal logic (MITL) formulas in sequential disjunctive normal form from labeled trajectories collected in RL of the two tasks. If logical transferability is identified through this inference, we construct a timed automaton for each sequential conjunctive subformula of the inferred MITL formulas from both tasks. We perform RL on the extended state which includes the locations and clock valuations of the timed automata for the source task. We then establish mappings between the corresponding components (clocks, locations, etc.) of the timed automata from the two tasks, and transfer the extended Q-functions based on the established mappings. Finally, we perform RL on the extended state for the target task, starting with the transferred extended Q-functions. Our implementation results show, depending on how similar the source task and the target task are, that the sampling efficiency for the target task can be improved by up to one order of magnitude by performing RL in the extended state space, and further improved by up to another order of magnitude using the transferred extended Q-functions.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/557"
    },
    {
        "id": 28665,
        "title": "MERGE: Meta Reinforcement Learning for Tunable RL Agents at the Edge",
        "authors": "Sharda Tripathi, Carla Fabiana Chiasserini",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437278"
    },
    {
        "id": 28666,
        "title": "Teaching on a Budget in Multi-Agent Deep Reinforcement Learning",
        "authors": "Ercument Ilhan, Jeremy Gow, Diego Perez-Liebana",
        "published": "2019-8",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8847988"
    },
    {
        "id": 28667,
        "title": "A Deep Reinforcement Learning Agent for Traffic Intersection Control Optimization",
        "authors": "Deepeka Garg, Maria Chli, George Vogiatzis",
        "published": "2019-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8917361"
    },
    {
        "id": 28668,
        "title": "Subgoal-Based Reward Shaping to Improve Efficiency in Reinforcement Learning",
        "authors": "Takato Okudo, Seiji Yamada",
        "published": "2021",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3090364"
    },
    {
        "id": 28669,
        "title": "The Formation Control of Mobile Autonomous Multi-Agent Systems Using Deep Reinforcement Learning",
        "authors": "Qishuai Liu, Qing Hui",
        "published": "2019-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/syscon.2019.8836902"
    },
    {
        "id": 28670,
        "title": "Neuroevolution for deep reinforcement learning problems",
        "authors": "David Ha",
        "published": "2020-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3377929.3389859"
    },
    {
        "id": 28671,
        "title": "Reinforcement Learning Applications in Cyber Security: A Review",
        "authors": "Emine CENGİZ, Murat GÖK",
        "published": "2023-4-30",
        "citations": 2,
        "abstract": "In the modern age we live in, the internet has become an essential part of our daily life. A significant portion of our personal data is stored online and organizations run their business online. In addition, with the development of the internet, many devices such as autonomous systems, investment portfolio tools and entertainment tools in our homes and workplaces have become or are becoming intelligent. In parallel with this development, cyberattacks aimed at damaging smart systems are increasing day by day. As cyberattack methods become more sophisticated, the damage done by attackers is increasing exponentially. Traditional computer algorithms may be insufficient against these attacks in the virtual world. Therefore, artificial intelligence-based methods are needed. Reinforcement Learning (RL), a machine learning method, is used in the field of cyber security. Although RL for cyber security is a new topic in the literature, studies are carried out to predict, prevent and stop attacks. In this study; we reviewed the literature on RL's penetration testing, intrusion detection systems (IDS) and cyberattacks in cyber security.",
        "link": "http://dx.doi.org/10.16984/saufenbilder.1237742"
    },
    {
        "id": 28672,
        "title": "Efficient Neural Network Pruning Using Model-Based Reinforcement Learning",
        "authors": "Blanka Bencsik, Marton Szemenyei",
        "published": "2022-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ismcr56534.2022.9950598"
    },
    {
        "id": 28673,
        "title": "Reinforcement Learning in Social Media Marketing",
        "authors": "Patrik Eklund",
        "published": "2022-7-8",
        "citations": 0,
        "abstract": "In this chapter, the authors describe an architecture for reinforcement learning in social media marketing. The rule bases used for action selection within the architecture build upon many-valued (fuzzy) logic. Action evaluation and internal learning is based on neural network like structures. In using variables measuring the effect of advertising, we must understand direction of influence between advertiser, owning the content of the advertisement, and advertisee, as the target of an advertisement, and as facilitated by social media marketing. Examples are drawn from Facebook marketing.",
        "link": "http://dx.doi.org/10.4018/978-1-6684-7123-4.ch045"
    },
    {
        "id": 28674,
        "title": "Flight AI using Reinforcement Learning",
        "authors": " Poonam Sharma,  Ashima Narang",
        "published": "2022-7-28",
        "citations": 0,
        "abstract": "This paper will be featuring an aircraft with fully featured physics simulated in unity\nengine. The agent or the AI will be created for possessing and controlling the aircraft so as to\nnavigate it in the 3D world space environment provided. The agent will have to consider the\ndifferent physical dynamics applied on a real-world aircraft and based on these parameters it will\nhave to create an effective piloting for the aircraft. Various other aspects such as engine dynamics\nand the fuel parameters will also be considered for an effective training environment.",
        "link": "http://dx.doi.org/10.46647/ijetms.2022.v06i04.0010"
    },
    {
        "id": 28675,
        "title": "Quantization aware training for efficient reinforcement learning network",
        "authors": "Qiaolin Li",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "In recent years, reinforcement learning has been applied to many areas, especially deep reinforcement learning. Deep learning-based methods use a large computing method known as Quantization Aware Training (QAT). For this project, QAT will be used to investigate the performance of Deep Q-Networks. This method of QAT can reduce memory usage of the network, but may affect the training performance of the network with some games.",
        "link": "http://dx.doi.org/10.54254/2755-2721/4/20230493"
    },
    {
        "id": 28676,
        "title": "Rule-based Reinforcement Learning for Lane Change Decision-making: A Risk Assessment Approach",
        "authors": "Lu Xiong, Danyang Zhong, Puhang Xu, Chen Tang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTo solve problems of poor security guarantee and insufficient training efficiency in the conventional reinforcement learning methods for decision-making, this study proposes a hybrid framework to combine deep reinforcement learning with rule-based decision-making methods. A risk assessment model for lane-change maneuver considering uncertain prediction of surrounding vehicles is established as a safety filter to improve the learning efficiency while corrects dangerous actions for safety enhancement. On this basis, a Risk-fused DDQN is constructed utilizing the model-based risk assessment and supervision mechanism. The proposed reinforcement learning algorithm sets up a separate experience buffer for dangerous trials and punishes such actions, which is shown to improve the sampling efficiency and training outcomes. Compared with conventional DDQN methods, the proposed algorithm improves the convergence value of cumulated reward by 7.6\\% and 2.2\\% in the two constructed scenarios in the simulation study, and reduces the number of training episodes by 52.2\\% and 66.8\\% respectively. The success rate of lane change is improved by 57.3\\% while the time headway is increased at least by 16.5\\% in real vehicle tests, which confirms the higher training efficiency, scenario adaptability and security of the proposed Risk-fused DDQN.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2536475/v1"
    },
    {
        "id": 28677,
        "title": "Reinforcement Learning for Placement Optimization",
        "authors": "Anna Goldie, Azalia Mirhoseini",
        "published": "2021-3-22",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3439706.3446883"
    },
    {
        "id": 28678,
        "title": "Reinforcement Learning Based Autonomous Air Combat with Energy Budgets",
        "authors": "Hasan Isci, Emre Koyuncu",
        "published": "2022-1-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2022-0786"
    },
    {
        "id": 28679,
        "title": "Multi-Agent Deep Reinforcement Learning with Human Strategies",
        "authors": "Thanh Nguyen, Ngoc Duy Nguyen, Saeid Nahavandi",
        "published": "2019-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit.2019.8755032"
    },
    {
        "id": 28680,
        "title": "REINFORCEMENT LEARNING IN CONTROL SYSTEMS OF OBJECTS WITH A TRANSPORT DELAY",
        "authors": "V.S. Borovik, S.V. Shidlovskiy",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15372/aut20210306"
    },
    {
        "id": 28681,
        "title": "Provably Efficient Reinforcement Learning for Episodic Stochastic Inventory Control Models",
        "authors": "Xiao-Yue Gong, David Simchi-Levi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3637705"
    },
    {
        "id": 28682,
        "title": "Towards Optimal Attacks on Reinforcement Learning Policies",
        "authors": "Alessio Russo, Alexandre Proutiere",
        "published": "2021-5-25",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9483025"
    },
    {
        "id": 28683,
        "title": "Access Point Selection Using Reinforcement Learning in Dense Mobile Networks",
        "authors": "Yared Zerihun Bekele, Young June-Choi",
        "published": "2020-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin48656.2020.9016560"
    },
    {
        "id": 28684,
        "title": "Reinforcement Learning for Optimal Allocation of Superconducting Fault Current Limiters",
        "authors": "Xiejin Ling, Yinhong Li",
        "published": "2020-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm41954.2020.9281474"
    },
    {
        "id": 28685,
        "title": "Deep Reinforcement Learning for Optimization",
        "authors": "Md Mahmudul Hasan, Md Shahinur Rahman, Adrian Bell",
        "published": "2020-11-27",
        "citations": 0,
        "abstract": "Deep reinforcement learning (DRL) has transformed the field of artificial intelligence (AI) especially after the success of Google DeepMind. This branch of machine learning epitomizes a step toward building autonomous systems by understanding of the visual world. Deep reinforcement learning (RL) is currently applied to different sorts of problems that were previously obstinate. In this chapter, at first, the authors started with an introduction of the general field of RL and Markov decision process (MDP). Then, they clarified the common DRL framework and the necessary components RL settings. Moreover, they analyzed the stochastic gradient descent (SGD)-based optimizers such as ADAM and a non-specific multi-policy selection mechanism in a multi-objective Markov decision process. In this chapter, the authors also included the comparison for different Deep Q networks. In conclusion, they describe several challenges and trends in research within the deep reinforcement learning field. ",
        "link": "http://dx.doi.org/10.4018/978-1-7998-7705-9.ch070"
    },
    {
        "id": 28686,
        "title": "Multi-Agent Reinforcement Learning for Autonomous On Demand Vehicles",
        "authors": "Ali Boyali, Naohisa Hashimoto, Vijay John, Tankut Acarman",
        "published": "2019-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2019.8813876"
    },
    {
        "id": 28687,
        "title": "Monocular Vision based Autonomous Landing of Quadrotor through Deep Reinforcement Learning",
        "authors": "Yinbo Xu, Zhihong Liu, Xiangke Wang",
        "published": "2018-7",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2018.8482830"
    },
    {
        "id": 28688,
        "title": "Model Based Human‐Robot Interaction Control",
        "authors": "",
        "published": "2021-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119782773.ch3"
    },
    {
        "id": 28689,
        "title": "Morl4pdes: Data-Driven Discovery of Pdes Based on Multi-Objective Optimization and Reinforcement Learning",
        "authors": "xiaoxia zhang, Junsheng Guan, Yanjun Liu, Guoyin Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4555059"
    },
    {
        "id": 28690,
        "title": "Lateral flow control of connected vehicles through deep reinforcement learning",
        "authors": "Abdul Rahman Kreidieh, Yashar Farid, Kentaro Oguchi",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv55152.2023.10186790"
    },
    {
        "id": 28691,
        "title": "Multi-Objective Traffic Signal Control Using Network-Wide Agent Coordinated Reinforcement Learning",
        "authors": "jie fang, Ya You, Mengyun Xu, Juanmeizi Wang, Sibin Cai",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4374888"
    },
    {
        "id": 28692,
        "title": "Loads Alleviation on an Airfoil via Reinforcement Learning",
        "authors": "Esteban A. Hufstedler, Philippe Chatelain",
        "published": "2019-1-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2019-0404"
    },
    {
        "id": 28693,
        "title": "Efficient Detailed Routing for FPGA Back-End Flow Using Reinforcement Learning",
        "authors": "Imran Baig, Umer Farooq",
        "published": "2022-7-18",
        "citations": 3,
        "abstract": "Over the past few years, the computation capability of field-programmable gate arrays (FPGAs) has increased tremendously. This has led to the increase in the complexity of the designs implemented on FPGAs and to the time taken by the FPGA back-end flow. The FPGA back-end flow comprises of many steps, and routing is one of the most critical steps among them. Routing normally constitutes more than 50% of the total time taken by the back-end flow and an optimization at this step can lead to overall optimization of the back-end flow. In this work, we propose enhancements to the routing step by incorporating a reinforcement learning (RL)-based framework. In the proposed RL-based framework, we use the ϵ-greedy approach and customized reward functions to speed up the routing step while maintaining similar or better quality of results (QoR) as compared to the conventional negotiation-based congestion-driven routing solution. For experimentation, we use two sets of widely deployed, large heterogeneous benchmarks. Our results show that, for the RL-based framework, the ϵ-greedy greedy approach combined with a modified reward function gives better results as compared to purely greedy or exploratory approaches. Moreover, the incorporation of the proposed reward function in the RL-based framework and its comparison with a conventional routing algorithm shows that the proposed enhancement requires less routing time while giving similar or better QoR. On average, a speedup of 35% is recorded for the proposed routing enhancement as compared to negotiation-based congestion-driven routing solutions. Finally, the speedup of the routing step leads to an overall reduction in the execution time of the back-end flow of 25%.",
        "link": "http://dx.doi.org/10.3390/electronics11142240"
    },
    {
        "id": 28694,
        "title": "Active Flow Control on Airfoils by Reinforcement Learning",
        "authors": "Koldo Portal-Porras, Unai Fernandez-Gamiz, Ekaitz Zulueta, Roberto Garcia-Fernandez, Saioa Etxebarria Berrizbeitia",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4474588"
    },
    {
        "id": 28695,
        "title": "Intelligent Decision Making in Autonomous Vehicles using Cognition Aided Reinforcement Learning",
        "authors": "Heena Rathore, Vikram Bhadauria",
        "published": "2022-4-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc51071.2022.9771728"
    },
    {
        "id": 28696,
        "title": "Dynamic Optimization for Secure MIMO Beamforming using Large-scale Reinforcement Learning",
        "authors": "Xinran Zhang, Songlin Sun",
        "published": "2019-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc.2019.8885667"
    },
    {
        "id": 28697,
        "title": "Runtime Safety Assurance Using Reinforcement Learning",
        "authors": "Christopher Lazarus, James G. Lopez, Mykel J. Kochenderfer",
        "published": "2020-10-11",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dasc50938.2020.9256446"
    },
    {
        "id": 28698,
        "title": "Research on Underwater Gliders Path Planning Method Based on Deep Reinforcement Learning Enhanced Rrt",
        "authors": "Nan Jiang, Qinghai Zhao, Chong Xu, Xin Dong",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4453330"
    },
    {
        "id": 28699,
        "title": "Aircraft Engine Maintenance Based on Reinforcement Learning",
        "authors": "Xuedong Xu, Xiao Wang",
        "published": "2020-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icitbs49701.2020.00213"
    },
    {
        "id": 28700,
        "title": "Real-Time Scheduling Using Reinforcement Learning Technique for the Connected Vehicles",
        "authors": "Seongjin Park, Younghwan Yoo",
        "published": "2018-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtcspring.2018.8417770"
    },
    {
        "id": 28701,
        "title": "DanZero: Mastering GuanDan Game with Reinforcement Learning",
        "authors": "Yudong Lu, Jian zhao, Youpeng Zhao, Wengang Zhou, Houqiang Li",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog57401.2023.10333179"
    },
    {
        "id": 28702,
        "title": "Secure Offloading Optimization Based on Deep Reinforcement Learning in Internet of Vehicles",
        "authors": "Jianbin Xue, Jia Yao, Jiahao Wang, Xiaofeng Xu, Tingjuan Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4554029"
    },
    {
        "id": 28703,
        "title": "A Deep Reinforcement Learning Traffic Control Model for Pedestrian and Vehicle Evacuation in the Parking Lot",
        "authors": "zhao zhang, Yuhan Fei, Daocheng Fu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4584750"
    },
    {
        "id": 28704,
        "title": "VPE: Variational Policy Embedding for Transfer Reinforcement Learning",
        "authors": "Isac Atnekvist, Danica Kragic, Johannes A. Stork",
        "published": "2019-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8793556"
    },
    {
        "id": 28705,
        "title": "Option-Like Portfolio Insurance Over a Rolling Window: Introduction and Derivation by Reinforcement Learning",
        "authors": "Alexey Medvedev",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4141312"
    },
    {
        "id": 28706,
        "title": "HyLEAR: Hybrid Deep Reinforcement Learning and Planning for Safe and Comfortable Automated Driving",
        "authors": "Dikshant Gupta, Matthias Klusch",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv55152.2023.10186781"
    },
    {
        "id": 28707,
        "title": "Reinforcement Learning Applications in Cyber Security: A Review",
        "authors": "Emine CENGİZ, Murat GÖK",
        "published": "2023-4-30",
        "citations": 2,
        "abstract": "In the modern age we live in, the internet has become an essential part of our daily life. A significant portion of our personal data is stored online and organizations run their business online. In addition, with the development of the internet, many devices such as autonomous systems, investment portfolio tools and entertainment tools in our homes and workplaces have become or are becoming intelligent. In parallel with this development, cyberattacks aimed at damaging smart systems are increasing day by day. As cyberattack methods become more sophisticated, the damage done by attackers is increasing exponentially. Traditional computer algorithms may be insufficient against these attacks in the virtual world. Therefore, artificial intelligence-based methods are needed. Reinforcement Learning (RL), a machine learning method, is used in the field of cyber security. Although RL for cyber security is a new topic in the literature, studies are carried out to predict, prevent and stop attacks. In this study; we reviewed the literature on RL's penetration testing, intrusion detection systems (IDS) and cyberattacks in cyber security.",
        "link": "http://dx.doi.org/10.16984/saufenbilder.1237742"
    },
    {
        "id": 28708,
        "title": "Derivative‐Free Stochastic Search",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch7"
    },
    {
        "id": 28709,
        "title": "Optimal Compensation Scheme Considering Overwork and Effort Cost: a Reinforcement Learning Approach",
        "authors": "Toshihiko Nanba, Takahiro Inada",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4558316"
    },
    {
        "id": 28710,
        "title": "A Markov Game Approach Based on Multiagent Reinforcement Learning Solution for Cyber-Physical Attacks in Smart Grid",
        "authors": "Kübra Bitirgen, Ümmühan Başaran Filik",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4626686"
    },
    {
        "id": 28711,
        "title": "Reinforcement Learning for Autonomous Vehicle Movements in Wireless Sensor Networks",
        "authors": "Haitham Afifi, Arunselvan Ramaswamy, Holger Karl",
        "published": "2021-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc42927.2021.9500318"
    },
    {
        "id": 28712,
        "title": "Network Intrusion Detection System Using Reinforcement Learning Techniques",
        "authors": "Malika Malik, Kamaljit Singh Saini",
        "published": "2023-8-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccpct58313.2023.10245608"
    },
    {
        "id": 28713,
        "title": "Explainable Sentiment-Based Tail Risk Connectedness Portfolio Optimization Using Deep Reinforcement Learning",
        "authors": "Mohammad Abdullah, Mohammad Ashraful Ferdous Chowdhury, Zunaidah Sulong, RUMI MASIH",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4627988"
    },
    {
        "id": 28714,
        "title": "An ocular biomechanics environment for reinforcement learning",
        "authors": "Julie Iskander, Mohammed Hossny",
        "published": "2022-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jbiomech.2022.110943"
    },
    {
        "id": 28715,
        "title": "Transfer of Temporal Logic Formulas in Reinforcement Learning",
        "authors": "Zhe Xu, Ufuk Topcu",
        "published": "2019-8",
        "citations": 23,
        "abstract": "Transferring high-level knowledge from a source task to a target task is an effective way to expedite reinforcement learning (RL). For example, propositional logic and first-order logic have been used as representations of such knowledge. We study the transfer of knowledge between tasks in which the timing of the events matters. We call such tasks temporal tasks. We concretize similarity between temporal tasks through a notion of logical transferability, and develop a transfer learning approach between different yet similar temporal tasks. We first propose an inference technique to extract metric interval temporal logic (MITL) formulas in sequential disjunctive normal form from labeled trajectories collected in RL of the two tasks. If logical transferability is identified through this inference, we construct a timed automaton for each sequential conjunctive subformula of the inferred MITL formulas from both tasks. We perform RL on the extended state which includes the locations and clock valuations of the timed automata for the source task. We then establish mappings between the corresponding components (clocks, locations, etc.) of the timed automata from the two tasks, and transfer the extended Q-functions based on the established mappings. Finally, we perform RL on the extended state for the target task, starting with the transferred extended Q-functions. Our implementation results show, depending on how similar the source task and the target task are, that the sampling efficiency for the target task can be improved by up to one order of magnitude by performing RL in the extended state space, and further improved by up to another order of magnitude using the transferred extended Q-functions.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/557"
    },
    {
        "id": 28716,
        "title": "A Deep Reinforcement Learning Agent for Traffic Intersection Control Optimization",
        "authors": "Deepeka Garg, Maria Chli, George Vogiatzis",
        "published": "2019-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8917361"
    },
    {
        "id": 28717,
        "title": "Subgoal-Based Reward Shaping to Improve Efficiency in Reinforcement Learning",
        "authors": "Takato Okudo, Seiji Yamada",
        "published": "2021",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3090364"
    },
    {
        "id": 28718,
        "title": "Multi-Agent Deep Reinforcement Learning for Multi-Robot Applications: A Survey",
        "authors": "James Orr, Ayan Dutta",
        "published": "2023-3-30",
        "citations": 23,
        "abstract": "Deep reinforcement learning has produced many success stories in recent years. Some example fields in which these successes have taken place include mathematics, games, health care, and robotics. In this paper, we are especially interested in multi-agent deep reinforcement learning, where multiple agents present in the environment not only learn from their own experiences but also from each other and its applications in multi-robot systems. In many real-world scenarios, one robot might not be enough to complete the given task on its own, and, therefore, we might need to deploy multiple robots who work together towards a common global objective of finishing the task. Although multi-agent deep reinforcement learning and its applications in multi-robot systems are of tremendous significance from theoretical and applied standpoints, the latest survey in this domain dates to 2004 albeit for traditional learning applications as deep reinforcement learning was not invented. We classify the reviewed papers in our survey primarily based on their multi-robot applications. Our survey also discusses a few challenges that the current research in this domain faces and provides a potential list of future applications involving multi-robot systems that can benefit from advances in multi-agent deep reinforcement learning.",
        "link": "http://dx.doi.org/10.3390/s23073625"
    },
    {
        "id": 28719,
        "title": "Single Image Dehazing via Reinforcement Learning",
        "authors": "Yu Zhang, Yunlong Dong",
        "published": "2020-11-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciba50161.2020.9277382"
    },
    {
        "id": 28720,
        "title": "Experience Selection in Multi-agent Deep Reinforcement Learning",
        "authors": "Yishen Wang, Zongzhang Zhang",
        "published": "2019-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai.2019.00123"
    },
    {
        "id": 28721,
        "title": "Packet Routing with Graph Attention Multi-Agent Reinforcement Learning",
        "authors": "Xuan Mai, Quanzhi Fu, Yi Chen",
        "published": "2021-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685941"
    },
    {
        "id": 28722,
        "title": "Tourism Route Recommendation Using Reinforcement Learning",
        "authors": "Muhammad Ilham Mubarak, Z. K. A. Baizal",
        "published": "2023-4-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i2ct57861.2023.10126347"
    },
    {
        "id": 28723,
        "title": "Deep Reinforcement Learning based Intelligent Traffic Control",
        "authors": "Arpan Nookala, Eeshaan Asodekar, Aryan Solanki, Narendra Bhagat, Deepak Karia",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tensymp55890.2023.10223639"
    },
    {
        "id": 28724,
        "title": "Deep-Reinforcement Learning Multiple Access for Heterogeneous Wireless Networks",
        "authors": "Yiding Yu, Taotao Wang, Soung Chang Liew",
        "published": "2018-5",
        "citations": 38,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2018.8422168"
    },
    {
        "id": 28725,
        "title": "Manufacturing Scheduling Using Colored Petri Nets and Reinforcement Learning",
        "authors": "Maria Drakaki, Panagiotis Tzionas",
        "published": "2017-2-3",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3390/app7020136"
    },
    {
        "id": 28726,
        "title": "Object Detection Using Policy-Based Reinforcement Learning",
        "authors": "Keong-Hun Choi, Jong-Eun Ha",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iccas59377.2023.10316786"
    },
    {
        "id": 28727,
        "title": "Solving the Pallet Loading Problem with Deep Reinforcement Learning",
        "authors": "Safa Bhar Layeb, Oussema Omri",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-8851-6_17-1"
    },
    {
        "id": 28728,
        "title": "Editor's evaluation: Dynamic organization of cerebellar climbing fiber response and synchrony in multiple functional components reduces dimensions for reinforcement learning",
        "authors": "Jennifer L Raymond",
        "published": "2023-3-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7554/elife.86340.sa0"
    },
    {
        "id": 28729,
        "title": "Advances in quantum reinforcement learning",
        "authors": "Vedran Dunjko, Jacob M. Taylor, Hans J. Briegel",
        "published": "2017-10",
        "citations": 41,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc.2017.8122616"
    },
    {
        "id": 28730,
        "title": "Fuzzy Lyapunov Reinforcement Learning for Non Linear Systems",
        "authors": "Abhishek Kumar, Rajneesh Sharma",
        "published": "2017-3",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.isatra.2017.01.026"
    },
    {
        "id": 28731,
        "title": "A hidden anti-jamming method based on deep reinforcement learning",
        "authors": "",
        "published": "2021-9-30",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2021.09.019"
    },
    {
        "id": 28732,
        "title": "Parameterized Adaptive Controller Design using Reinforcement Learning and Deep Neural Networks",
        "authors": "Kranthi Kumar P, Ketan P Detroja",
        "published": "2022-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc56513.2022.10093404"
    },
    {
        "id": 28733,
        "title": "Monocular Vision based Autonomous Landing of Quadrotor through Deep Reinforcement Learning",
        "authors": "Yinbo Xu, Zhihong Liu, Xiangke Wang",
        "published": "2018-7",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2018.8482830"
    },
    {
        "id": 28734,
        "title": "Optimizing stock market execution costs using reinforcement learning",
        "authors": "Abdulrahman A. Ahmed, Ayman Ghoneim, Mohamed Saleh",
        "published": "2020-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci47803.2020.9308153"
    },
    {
        "id": 28735,
        "title": "Reliability of Decision-Making and Reinforcement Learning Computational Parameters",
        "authors": "Anahit Mkrtchian, Vincent Valton, Jonathan P. Roiser",
        "published": "2023-2-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5334/cpsy.86"
    },
    {
        "id": 28736,
        "title": "Hierarchical graph multi-agent reinforcement learning for traffic signal control",
        "authors": "Shantian Yang",
        "published": "2023-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.03.087"
    },
    {
        "id": 28737,
        "title": "A Deep Reinforcement Learning Approach for Fair Traffic Signal Control",
        "authors": "Majid Raeis, Alberto Leon-Garcia",
        "published": "2021-9-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9564847"
    },
    {
        "id": 28738,
        "title": "Implementing Game Strategies Based on Reinforcement Learning",
        "authors": "Botong Liu",
        "published": "2020-11-20",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3449301.3449311"
    },
    {
        "id": 28739,
        "title": "Collaborative Partially-Observable Reinforcement Learning Using Wireless Communications",
        "authors": "Eisaku Ko, Kwang-Cheng Chen, Shao-Yu Lien",
        "published": "2021-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc42927.2021.9500922"
    },
    {
        "id": 28740,
        "title": "Reinforcement Learning for Attack Mitigation in SDN-enabled Networks",
        "authors": "Mikhail Zolotukhin, Sanjay Kumar, Timo Hamalainen",
        "published": "2020-6",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/netsoft48620.2020.9165383"
    },
    {
        "id": 28741,
        "title": "End-to-End Video Captioning With Multitask Reinforcement Learning",
        "authors": "Lijun Li, Boqing Gong",
        "published": "2019-1",
        "citations": 38,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv.2019.00042"
    },
    {
        "id": 28742,
        "title": "Traffic Signal Control with Deep Reinforcement Learning",
        "authors": "Tongyu Zhao, Peng Wang, Songjinag Li",
        "published": "2019-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicas48597.2019.00164"
    },
    {
        "id": 28743,
        "title": "Deep Reinforcement Learning based Data Placement optimization in Data Center Networks",
        "authors": "Ravi Kaler, Durga Toshniwal",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386549"
    },
    {
        "id": 28744,
        "title": "Utilizing benign files to obfuscate malware via deep reinforcement learning",
        "authors": "Jiyao Gao, Zhiyang Fang",
        "published": "2022-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iip57348.2022.00067"
    },
    {
        "id": 28745,
        "title": "Robust Fault Diagnosis for Gas Turbine Rotor via Transfer Reinforcement Learning",
        "authors": "Yufei Zhang",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394000"
    },
    {
        "id": 28746,
        "title": "Dynamic Handoff Policy for RAN&amp;nbsp;Slicing by Exploiting Deep Reinforcement Learning",
        "authors": "Yuansheng Wu, Guanqun Zhao, Dadong Ni, Junyi Du",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nIt has been widely acknowledged that network slicing is a key architectural technology to accommodate diversified services for the next generation network (5G). By partitioning the underlying network into multiple dedicated logical networks, 5G can support a variety of extreme business service needs. As network slicing is implemented in radio access networks (RAN), user handoff becomes much more complicated than that in traditional mobile networks. As both physical resource constraints of base stations (BSs) and logical connection constraints of network slices should be considered in handoff decision, an intelligent handoff policy becomes imperative. In this paper, we model the handoff in RAN slicing as a Markov decision process (MDP) and resort to deep reinforcement learning to pursue long-term performance improvement in terms of user quality of Service (QoS) and network throughput. The effectiveness of our proposed handoff policy is validated via simulation experiments.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-137200/v1"
    },
    {
        "id": 28747,
        "title": "State-of-the-Art Reinforcement Learning Algorithms",
        "authors": " Deepanshu Mehta,  ",
        "published": "2020-1-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17577/ijertv8is120332"
    },
    {
        "id": 28748,
        "title": "Reinforcement Learning-Based Joint Power and Resource Allocation for URLLC in 5G",
        "authors": "Medhat Elsayed, Melike Erol-Kantarci",
        "published": "2019-12",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9014032"
    },
    {
        "id": 28749,
        "title": "Reinforcement Learning-Enabled Reliable Wireless Sensor Networks in Dynamic Underground Environments",
        "authors": "Hongzhi Guo, Bincy Ben",
        "published": "2019-11",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/milcom47813.2019.9021051"
    },
    {
        "id": 28750,
        "title": "Deep Reinforcement Learning-based Online Resource Management for UAV-Assisted Edge Computing with Dual Connectivity",
        "authors": "Linh Hoang, Chuyen T. Nguyen, Anh T Pham",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>Mobile Edge Computing (MEC) is a key technology towards delay-sensitive and computation-intensive applications in future cellular networks. In this paper, we consider a multi-user, multi-server system where the cellular base station is assisted by a UAV, both of which provide additional MEC services to the terrestrial users. Via dual connectivity (DC), each user can simultaneously offload tasks to the macro base station and the UAV-mounted MEC server for parallel computing, while also processing some tasks locally. We aim to propose an online resource management framework that minimizes the average power consumption of the whole system, considering long-term constraints on queue stability and computational delay of the queueing system. Due to the coexistence of two servers, the problem is highly complex and formulated as a multi-stage mixed integer non-linear programming (MINLP) problem. To solve the MINLP with reduced computational complexity, we first adopt Lyapunov optimization to transform the original multi-stage problem into deterministic problems that are manageable in each time slot. Afterward, the transformed problem is solved using an integrated learning-optimization approach, where model-free Deep Reinforcement Learning (DRL) is combined with modelbased optimization. Via extensive simulation and theoretical analyses, we show that the proposed framework is guaranteed to converge and can produce nearly the same performance as the optimal solution obtained via an exhaustive search.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22340134.v2"
    },
    {
        "id": 28751,
        "title": "Automating post-exploitation with deep reinforcement learning",
        "authors": "Ryusei Maeda, Mamoru Mimura",
        "published": "2021-1",
        "citations": 36,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cose.2020.102108"
    },
    {
        "id": 28752,
        "title": "A distributed deep reinforcement learning method for traffic light control",
        "authors": "Bo Liu, Zhengtao Ding",
        "published": "2022-6",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2021.11.106"
    },
    {
        "id": 28753,
        "title": "Trajectory Optimization for Urban Rail Transit Considering Regenerative Energy Utilization: A Reinforcement Learning Approach",
        "authors": "Wenbo Wei, Xiaomin Wang",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055605"
    },
    {
        "id": 28754,
        "title": "MERGE: Meta Reinforcement Learning for Tunable RL Agents at the Edge",
        "authors": "Sharda Tripathi, Carla Fabiana Chiasserini",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437278"
    },
    {
        "id": 28755,
        "title": "Teaching on a Budget in Multi-Agent Deep Reinforcement Learning",
        "authors": "Ercument Ilhan, Jeremy Gow, Diego Perez-Liebana",
        "published": "2019-8",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8847988"
    },
    {
        "id": 28756,
        "title": "Pattern recognition of human arm movement using deep reinforcement learning",
        "authors": "W. Seok, Y. Kim, C. Park",
        "published": "2018-1",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin.2018.8343257"
    },
    {
        "id": 28757,
        "title": "A Multi-Stage Deep Reinforcement Learning with Search-Based Optimization for Air–Ground Unmanned System Navigation",
        "authors": "Xiaohui Chen, Yuhua Qi, Yizhen Yin, Yidong Chen, Li Liu, Hongbo Chen",
        "published": "2023-2-9",
        "citations": 4,
        "abstract": "An important challenge for air–ground unmanned systems achieving autonomy is navigation, which is essential for them to accomplish various tasks in unknown environments. This paper proposes an end-to-end framework for solving air–ground unmanned system navigation using deep reinforcement learning (DRL) while optimizing by using a priori information from search-based path planning methods, which we call search-based optimizing DRL (SO-DRL) for the air–ground unmanned system. SO-DRL enables agents, i.e., an unmanned aerial vehicle (UAV) or an unmanned ground vehicle (UGV) to move to a given target in a completely unknown environment using only Lidar, without additional mapping or global planning. Our framework is equipped with Deep Deterministic Policy Gradient (DDPG), an actor–critic-based reinforcement learning algorithm, to input the agents’ state and laser scan measurements into the network and map them to continuous motion control. SO-DRL draws on current excellent search-based algorithms to demonstrate path planning and calculate rewards for its behavior. The demonstrated strategies are replayed in an experienced pool along with the autonomously trained strategies according to their priority. We use a multi-stage training approach based on course learning to train SO-DRL on the 3D simulator Gazebo and verify the robustness and success of the algorithm using new test environments for path planning in unknown environments. The experimental results show that SO-DRL can achieve faster algorithm convergence and a higher success rate. We piggybacked SO-DRL directly onto a real air–ground unmanned system, and SO-DRL can guide a UAV or UGV for navigation without adjusting any networks.",
        "link": "http://dx.doi.org/10.3390/app13042244"
    },
    {
        "id": 28758,
        "title": "Human-Centered AI using Ethical Causality and Learning Representation for Multi-Agent Deep Reinforcement Learning",
        "authors": "Joshua Ho, Chien-Min Wang",
        "published": "2021-9-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichms53169.2021.9582667"
    },
    {
        "id": 28759,
        "title": "Deep reinforcement learning collision avoidance using policy gradient optimisation and Q-learning",
        "authors": "Bishoy H. Mikhail, Shady A. Maged",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijcvr.2020.10029037"
    },
    {
        "id": 28760,
        "title": "An improvement of the learning speed through Influence Map on Reinforcement Learning",
        "authors": "Yong-Woo Shin",
        "published": "2017-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7583/jkgs.2017.17.4.109"
    },
    {
        "id": 28761,
        "title": "Unmanned Aerial Vehicles (UAV) relay in Vehicular Ad Hoc Network (VANETs) against smart jamming with reinforcement learning/deep learning",
        "authors": "",
        "published": "2022-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbtr036e_ch5"
    },
    {
        "id": 28762,
        "title": "TeachMe: Three-phase learning framework for robotic motion imitation based on interactive teaching and reinforcement learning",
        "authors": "Taewoo Kim, Joo-Haeng Lee",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ro-man46459.2019.8956326"
    },
    {
        "id": 28763,
        "title": "An online scalarization multi-objective reinforcement learning algorithm: TOPSIS Q-learning",
        "authors": "Mohammad Mirzanejad, Morteza Ebrahimi, Peter Vamplew, Hadi Veisi",
        "published": "2022",
        "citations": 3,
        "abstract": "Abstract\nConventional reinforcement learning focuses on problems with single objective. However, many problems have multiple objectives or criteria that may be independent, related, or contradictory. In such cases, multi-objective reinforcement learning is used to propose a compromise among the solutions to balance the objectives. TOPSIS is a multi-criteria decision method that selects the alternative with minimum distance from the positive ideal solution and the maximum distance from the negative ideal solution, so it can be used effectively in the decision-making process to select the next action. In this research a single-policy algorithm called TOPSIS Q-Learning is provided with focus on its performance in online mode. Unlike all single-policy methods, in the first version of the algorithm, there is no need for the user to specify the weights of the objectives. The user’s preferences may not be completely definite, so all weight preferences are combined together as decision criteria and a solution is generated by considering all these preferences at once and user can model the uncertainty and weight changes of objectives around their specified preferences of objectives. If the user only wants to apply the algorithm for a specific set of weights the second version of the algorithm efficiently accomplishes that.",
        "link": "http://dx.doi.org/10.1017/s0269888921000163"
    },
    {
        "id": 28764,
        "title": "Uncertainty-Aware Autonomous Mobile Robot Navigation with Deep Reinforcement Learning",
        "authors": "Lynnette González-Rodríguez, Armando Plasencia-Salgueiro",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-77939-9_7"
    },
    {
        "id": 28765,
        "title": "Fairness in Learning-Based Sequential Decision Algorithms: A Survey",
        "authors": "Xueru Zhang, Mingyan Liu",
        "published": "2021",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_18"
    },
    {
        "id": 28766,
        "title": "Nondominated Policy-Guided Learning in Multi-Objective Reinforcement Learning",
        "authors": "Man-Je Kim, Hyunsoo Park, Chang Wook Ahn",
        "published": "2022-3-28",
        "citations": 1,
        "abstract": "Control intelligence is a typical field where there is a trade-off between target objectives, and researchers in this field have longed for artificial intelligence that achieves the target objectives. Multi-objective deep reinforcement learning was sufficient to satisfy this need. In particular, multi-objective deep reinforcement learning methods based on policy optimization are leading the optimization of control intelligence. However, multi-objective reinforcement learning has difficulties when finding various Pareto optimals of multi-objectives due to the greedy nature of reinforcement learning. We propose a method of policy assimilation to solve this problem. This method was applied to MO-V-MPO, one of preference-based multi-objective reinforcement learning, to increase diversity. The performance of this method has been verified through experiments in a continuous control environment.",
        "link": "http://dx.doi.org/10.3390/electronics11071069"
    },
    {
        "id": 28767,
        "title": "A reinforcement learning approach for thermostat setpoint preference learning",
        "authors": "Hussein Elehwany, Mohamed Ouf, Burak Gunay, Nunzio Cotrufo, Jean-Simon Venne",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s12273-023-1056-7"
    },
    {
        "id": 28768,
        "title": "BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?",
        "authors": "Xingmeng Zhao, Tongnian Wang, Sheri Osborn, Anthony Rios",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-babylm.16"
    },
    {
        "id": 28769,
        "title": "Train timetabling with the general learning environment and multi-agent deep reinforcement learning",
        "authors": "Wenqing Li, Shaoquan Ni",
        "published": "2022-3",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.trb.2022.02.006"
    },
    {
        "id": 28770,
        "title": "Learning to Design Games: Strategic Environments in Reinforcement Learning",
        "authors": "Haifeng Zhang, Jun Wang, Zhiming Zhou, Weinan Zhang, Yin Wen, Yong Yu, Wenxin Li",
        "published": "2018-7",
        "citations": 6,
        "abstract": "In typical reinforcement learning (RL), the environment is assumed given and the goal of the learning is to identify an optimal policy for the agent taking actions through its interactions with the environment.  In this paper, we extend this setting by considering the environment is not given, but controllable and learnable through its interaction with the agent at the same time. This extension is motivated by environment design scenarios in the real-world, including game design, shopping space design and traffic signal design. Theoretically, we find a dual Markov decision process (MDP) w.r.t. the environment to that w.r.t. the agent, and derive a policy gradient solution to optimizing the parametrized environment. Furthermore, discontinuous environments are addressed by a proposed general generative framework. Our experiments on a Maze game design task show the effectiveness of the proposed algorithms in generating diverse and challenging Mazes against various agent settings.",
        "link": "http://dx.doi.org/10.24963/ijcai.2018/426"
    },
    {
        "id": 28771,
        "title": "Parallel reward and punishment control in humans and robots: Safe reinforcement learning using the MaxPain algorithm",
        "authors": "Stefan Elfwing, Ben Seymour",
        "published": "2017-9",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/devlrn.2017.8329799"
    },
    {
        "id": 28772,
        "title": "Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning",
        "authors": "Linhai Xie, Sen Wang, Stefano Rosa, Andrew Markham, Niki Trigoni",
        "published": "2018-5",
        "citations": 47,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2018.8461203"
    },
    {
        "id": 28773,
        "title": "K-Qbot: Language Learning Chatbot Based on Reinforcement Learning",
        "authors": "Nurziya Oralbayeva, Aidar Shakerimov, Shamil Sarmonov, Kanagat Kantoreyeva, Fatima Dadebayeva, Nuray Serkali, Anara Sandygulova",
        "published": "2022-3-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hri53351.2022.9889428"
    },
    {
        "id": 28774,
        "title": "Deep Reinforcement Learning for Quadrotor Path Following and Obstacle Avoidance",
        "authors": "Bartomeu Rubí, Bernardo Morcego, Ramon Pérez",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-77939-9_17"
    },
    {
        "id": 28775,
        "title": "Applications of Cellular Learning Automata and Reinforcement Learning in Global Optimization",
        "authors": "Reza Vafashoar, Hossein Morshedlou, Alireza Rezvanian, Mohammad Reza Meybodi",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-53141-6_4"
    },
    {
        "id": 28776,
        "title": "The Wisdom of the Crowd: Reliable Deep Reinforcement Learning Through Ensembles of Q-Functions",
        "authors": "Daniel L. Elliott, Charles Anderson",
        "published": "2023-1",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3089425"
    },
    {
        "id": 28777,
        "title": "Deep Learning and Reinforcement Learning for Autonomous Unmanned Aerial Systems: Roadmap for Theory to Deployment",
        "authors": "Jithin Jagannath, Anu Jagannath, Sean Furman, Tyler Gwin",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-77939-9_2"
    },
    {
        "id": 28778,
        "title": "Reinforcement Learning in the Load Balancing Problem for the iFDAQ of the COMPASS Experiment at CERN",
        "authors": "Ondřej Šubrt, Martin Bodlák, Matouš Jandek, Vladimír Jarý, Antonín Květoň, Josef Nový, Miroslav Virius, Martin Zemko",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009035107340741"
    },
    {
        "id": 28779,
        "title": "Real-time Allocation of Shared Parking Spaces Based on Deep Reinforcement Learning",
        "authors": "Minghai Yuan Minghai Yuan, Chenxi Zhang Minghai Yuan, Kaiwen Zhou Chenxi Zhang, Fengque Pei Kaiwen Zhou",
        "published": "2023-1",
        "citations": 0,
        "abstract": "\n                        <p>Aiming at the parking space heterogeneity problem in shared parking space management, a multi-objective optimization model for parking space allocation is constructed with the optimization objectives of reducing the average walking distance of users and improving the utilization rate of parking spaces, a real-time allocation method for shared parking spaces based on deep reinforcement learning is proposed, which includes a state space for heterogeneous regions, an action space based on policy selection and a reward function with variable coefficients. To accurately evaluate the model performance, dynamic programming is used to derive the theoretical optimal values. Simulation results show that the improved algorithm not only improves the training success rate, but also increases the Agent performance by at least 12.63% and maintains the advantage for different sizes of parking demand, reducing the user walking distance by 53.58% and improving the parking utilization by 6.67% on average, and keeping the response time less than 0.2 seconds.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/160792642023012401004"
    },
    {
        "id": 28780,
        "title": "DERLight: A Deep Reinforcement Learning Traffic Light Control Algorithm with Dual Experience Replay",
        "authors": "Zhichao Yang Zhichao Yang, Yan Kong Zhichao Yang, Chih-Hsien Hsia Yan Kong",
        "published": "2024-1",
        "citations": 0,
        "abstract": "\n                        <p>In recent years, with the increasingly severe traffic environment, most cities are facing various traffic congestion problems, and the demand for intelligent regulation of traffic signals is also increasing. In this study, we propose a new intelligent traffic light control algorithm, dual experience replay light (DERLight), which innovatively and efficiently designs a dual experience replay training mechanism based on the classic deep Q network (DQN) framework and considers the dynamic epoch function. As results show that compared with some state-of-the-art algorithms, DERLight can shorten the average travel time of vehicles, increase the throughput at intersections, and also speed up the convergence of the network. In addition, the design of this algorithm framework is not only limited to the field of intelligent transportation, but also has transferability for some other fields.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/160792642024012501007"
    },
    {
        "id": 28781,
        "title": "A computational view on motor exploration during reinforcement learning",
        "authors": "Richard Hahnloser, Anja Zai",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ibror.2019.07.155"
    },
    {
        "id": 28782,
        "title": "Applying Real Options with Reinforcement Learning to Assess Commercial Ccu Deployment",
        "authors": "Jeehwan  Steve Lee, Woopill Chun, Kosan Roh, Seongmin Heo, Jay Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4535371"
    },
    {
        "id": 28783,
        "title": "Correction: A Deep Reinforcement Learning Approach to Collision Avoidance",
        "authors": "Barton J. Bacon",
        "published": "2022-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2022-0623.c1"
    },
    {
        "id": 28784,
        "title": "Effects of Spectral Normalization in Multi-Agent Reinforcement Learning",
        "authors": "Kinal Mehta, Anuj Mahajan, Pawan Kumar",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191226"
    },
    {
        "id": 28785,
        "title": "Inverse Reinforcement Learning for Identification of Linear-Quadratic Zero-Sum Differential Games",
        "authors": "Emin Martirosyan, Ming Cao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4103314"
    },
    {
        "id": 28786,
        "title": "Backward Approximate Dynamic Programming",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch15"
    },
    {
        "id": 28787,
        "title": "Deep Reinforcement Learning-Based Secure Standalone Intelligent Reflecting Surface Operation",
        "authors": "Yasaman Omid, Yansha Deng, Arumugam Nallanathan",
        "published": "2022-12-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001596"
    },
    {
        "id": 28788,
        "title": "Co-Adaptive Reinforcement Learning in Microscopic Traffic Systems",
        "authors": "Liza L. Lemos, Ana L.C. Bazzan, Marcia Pasin",
        "published": "2018-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec.2018.8477713"
    },
    {
        "id": 28789,
        "title": "Programmatic Marketing via Reinforcement Learning",
        "authors": "Mengmeng Chen,  ",
        "published": "2017-11-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15520/jbme.2017.vol5.iss11.269.pp01-04"
    },
    {
        "id": 28790,
        "title": "Negative Reinforcement in Social Learning Theory",
        "authors": "Maxine Notice, Jinsook Song, Janet Robertson",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-15877-8_47-1"
    },
    {
        "id": 28791,
        "title": "Deep Hedging of Derivatives Using Reinforcement Learning",
        "authors": "Jay Cao, Jacky Chen, John C. Hull, Zissis Poulos",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3514586"
    },
    {
        "id": 28792,
        "title": "Hierarchical Deep Reinforcement Learning based Dynamic RAN Slicing for 5G V2X",
        "authors": "Umuralp Kaytaz, Fikret Sivrikaya, Sahin Albayrak",
        "published": "2021-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685588"
    },
    {
        "id": 28793,
        "title": "REAL-TIME BATCHING IN JOB SHOPS BASED ON SIMULATION AND REINFORCEMENT LEARNING",
        "authors": "Tao Zhang, Shufang Xie, Oliver Rose",
        "published": "2018-12",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc.2018.8632524"
    },
    {
        "id": 28794,
        "title": "Policy Gradient using Weak Derivatives for Reinforcement Learning",
        "authors": "Sujay Bhatt, Alec Koppel, Vikram Krishnamurthy",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc40024.2019.9029403"
    },
    {
        "id": 28795,
        "title": "Cognitive radar mode control: a comparison of different reinforcement learning algorithms",
        "authors": "S. A. Ford, M. Ritchie",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.2300"
    },
    {
        "id": 28796,
        "title": "Survivability Based Optimal Air Combat Mission Planning with Reinforcement Learning",
        "authors": "Bans Baspinar, Emre Koyuncu",
        "published": "2018-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccta.2018.8511604"
    },
    {
        "id": 28797,
        "title": "Deep Reinforcement Learning Approach for Building of Autonomous Robots Formations",
        "authors": "Vanya Dimitrova Markova, Ventseslav Kirilov Shopov",
        "published": "2020-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infotech49733.2020.9211059"
    },
    {
        "id": 28798,
        "title": "Automated Multi-Step Web Application Attack Analysis Using Reinforcement Learning and Vulnerability Assessment Tools",
        "authors": "Mohammad Sedigh Hamidi, Mohammadali Doostari, Shahriar Bijani",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWeb applications remain persistently vulnerable to malicious attacks, necessitating a thorough exploration of adversarial strategies in multi-step web application attacks to proactively enhance preventive measures. Nonetheless, the complexity of such audits demands specialized expertise, and notably, the automation of this complicated process lacks practical methodologies. This paper presents an innovative approach that automates multi-step web application attacks by integrating reinforcement learning techniques with established tools like Metasploit, SQLmap, and Weevely. Within this framework, reinforcement learning agents exploit SQL injection and known vulnerabilities outlined in the OWASP Top Ten. A comparative analysis of two reinforcement learning models, Q-Learning and Deep Q-Network, reveals the superior reward accumulation prowess of the Q-Learning model during the training phase. Furthermore, the effectiveness of these trained agents is assessed using the vulnerable DVWA web application, demonstrating the Q-Learning-trained agent could gain persistent access to the web server and successfully extracting sensitive data.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3283231/v1"
    },
    {
        "id": 28799,
        "title": "Inter-Task Similarity for Lifelong Reinforcement Learning in Heterogeneous Tasks",
        "authors": "Sergio A. Serrano",
        "published": "2021-8",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) is a learning paradigm in which an agent interacts with the environment it inhabits to learn in a trial-and-error way. By letting the agent acquire knowledge from its own experience, RL has been successfully applied to complex domains such as robotics. However, for non-trivial problems, training an RL agent can take very long periods of time. Lifelong machine learning (LML) is a learning setting in which the agent learns to solve tasks sequentially, by leveraging knowledge accumulated from previously solved tasks to learn better/faster in a new one. Most LML works heavily rely on the assumption that tasks are similar to each other. However, this may not be true for some domains with a high degree of task-diversity that could benefit from adopting a lifelong learning approach, e.g., service robotics. Therefore, in this research we will address the problem of learning to solve a sequence of RL heterogeneous tasks (i.e., tasks that differ in their state-action space).",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/689"
    },
    {
        "id": 28800,
        "title": "Dynamic Data Publishing with Differential Privacy via Reinforcement Learning",
        "authors": "Ruichao Gao, Xuebin Ma",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/compsac.2019.00111"
    },
    {
        "id": 28801,
        "title": "Research on Autonomous Decision-Making of UCAV Based on Deep Reinforcement Learning",
        "authors": "Linxiang Wang, Hongtao Wei",
        "published": "2022-5-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictc55111.2022.9778652"
    },
    {
        "id": 28802,
        "title": "Improving generalization ability in a puzzle game using reinforcement learning",
        "authors": "Hiroya Oonishi, Hitoshi Iima",
        "published": "2017-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2017.8080441"
    },
    {
        "id": 28803,
        "title": "Developing Monte Carlo Simulator of Reinforcement Learning Type",
        "authors": "Georgi Tsochev",
        "published": "2020-11-2",
        "citations": 0,
        "abstract": "Monte Carlo methods are a way to solve the reinforcement learning problem based on average test results. To ensure that well-defined results are available, Monte Carlo methods are used only for episodic tasks. The Monte Carlo term is often used more widely in any valuation method whose operation involves significant participation on a random basis. Here it is specifically used for methods based on the average of full results (as opposed to methods that are learned from incomplete results). The paper describes a simulator for estimating raindrops in a specific area using the package matlib. Keywords: Monte Carlo, reinforcement learning, simulation, matlib, python",
        "link": "http://dx.doi.org/10.7546/pecr.73.20.04"
    },
    {
        "id": 28804,
        "title": "A Reinforcement-learning Account of Tourette Syndrome",
        "authors": "T. Maia",
        "published": "2017-4",
        "citations": 0,
        "abstract": "BackgroundTourette syndrome (TS) has long been thought to involve dopaminergic disturbances, given the effectiveness of antipsychotics in diminishing tics. Molecular-imaging studies have, by and large, confirmed that there are specific alterations in the dopaminergic system in TS. In parallel, multiple lines of evidence have implicated the motor cortico-basal ganglia-thalamo-cortical (CBGTC) loop in TS. Finally, several studies demonstrate that patients with TS exhibit exaggerated habit learning. This talk will present a computational theory of TS that ties together these multiple findings.MethodsThe computational theory builds on computational reinforcement-learning models, and more specifically on a recent model of the role of the direct and indirect basal-ganglia pathways in learning from positive and negative outcomes, respectively.ResultsA model defined by a small set of equations that characterize the role of dopamine in modulating learning and excitability in the direct and indirect pathways explains, in an integrated way: (1) the role of dopamine in the development of tics; (2) the relation between dopaminergic disturbances, involvement of the motor CBGTC loop, and excessive habit learning in TS; (3) the mechanism of action of antipsychotics in TS; and (4) the psychological and neural mechanisms of action of habit-reversal training, the main behavioral therapy for TS.ConclusionsA simple computational model, thoroughly grounded on computational theory and basic-science findings concerning dopamine and the basal ganglia, provides an integrated, rigorous mathematical explanation for a broad range of empirical findings in TS.Disclosure of interestThe author has not supplied his declaration of competing interest.",
        "link": "http://dx.doi.org/10.1016/j.eurpsy.2017.01.083"
    },
    {
        "id": 28805,
        "title": "Vibration Suppression for Large-Scale Flexible Structures Using Deep Reinforcement Learning Based on Cable-Driven Parallel Robots",
        "authors": "Haining Sun, Xiaoqiang Tang, Jinhao Wei",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.0003804v"
    },
    {
        "id": 28806,
        "title": "Reinforcement Learning with Temporal Logic Constraints",
        "authors": "Bengt Lennartson, Qing-Shan Jia",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2021.04.044"
    },
    {
        "id": 28807,
        "title": "A Layered Reference Model for Penetration Testing with Reinforcement Learning and Attack Graphs",
        "authors": "Tyler Cody",
        "published": "2022-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/stc55697.2022.00015"
    },
    {
        "id": 28808,
        "title": "Towards Multi-agent Reinforcement Learning for Wireless Network Protocol Synthesis",
        "authors": "Hrishikesh Dutta, Subir Biswas",
        "published": "2021-1-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/comsnets51098.2021.9352858"
    },
    {
        "id": 28809,
        "title": "Global Routing Under a Congestion-Aware Reinforcement Learning Model",
        "authors": "Lin Li, Yici Cai, Qiang Zhou",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iseda59274.2023.10218371"
    },
    {
        "id": 28810,
        "title": "Reinforcement learning applied to games",
        "authors": "João Crespo, Andreas Wichert",
        "published": "2020-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42452-020-2560-3"
    },
    {
        "id": 28811,
        "title": "Simulation and Comparison of Reinforcement Learning Algorithms",
        "authors": "Konstantinos Spyropoulos, Dionysios N. Sotiropoulos",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisa59645.2023.10345966"
    },
    {
        "id": 28812,
        "title": "Control of Shared Energy Storage Assets Within Building Clusters Using Reinforcement Learning",
        "authors": "Philip Odonkor, Kemper Lewis",
        "published": "2018-8-26",
        "citations": 5,
        "abstract": "This work leverages the current state of the art in reinforcement learning for continuous control, the Deep Deterministic Policy Gradient (DDPG) algorithm, towards the optimal 24-hour dispatch of shared energy assets within building clusters. The modeled DDPG agent interacts with a battery environment, designed to emulate a shared battery system. The aim here is to not only learn an efficient charged/discharged policy, but to also address the continuous domain question of how much energy should be charged or discharged. Experimentally, we examine the impact of the learned dispatch strategy towards minimizing demand peaks within the building cluster. Our results show that across the variety of building cluster combinations studied, the algorithm is able to learn and exploit energy arbitrage, tailoring it into battery dispatch strategies for peak demand shifting.",
        "link": "http://dx.doi.org/10.1115/detc2018-86094"
    },
    {
        "id": 28813,
        "title": "Modeling variation in empathic sensitivity using go/no-go social reinforcement learning",
        "authors": "Katherine O'Connell, Marissa Walsh, Brandon Padgett, Sarah Connell, Abigail Marsh",
        "published": "No Date",
        "citations": 1,
        "abstract": "Empathic experiences shape social behaviors and display considerable individual variation. Recent advances in computational behavioral modeling can help rigorously quantify individual differences, but remain understudied in the context of empathy and antisocial behavior. We adapted a go/no-go reinforcement learning task across social and non-social contexts such that monetary gains and losses explicitly impacted the subject, a study partner, or no one. Empathy was operationalized as sensitivity to others’ rewards, sensitivity to others’ losses, and as the Pavlovian influence of empathic outcomes on approach and avoidance behavior. Results showed that 61 subjects learned for a partner in a way that was computationally similar to how they learned for themselves. Results supported the psychometric value of individualized model parameters such as sensitivity to others’ loss, which was inversely associated with antisociality. Modeled empathic sensitivity also mapped onto motivation ratings, but was not associated with self-reported trait empathy. This work is the first to apply a social reinforcement learning task that spans affect and action requirement (go/no-go) to measure multiple facets of empathic sensitivity.",
        "link": "http://dx.doi.org/10.31234/osf.io/drcq6"
    },
    {
        "id": 28814,
        "title": "Towards High-Level Intrinsic Exploration in Reinforcement Learning",
        "authors": "Nicolas Bougie, Ryutaro Ichise",
        "published": "2020-7",
        "citations": 2,
        "abstract": "Deep reinforcement learning (DRL) methods traditionally struggle with tasks where environment rewards are sparse or delayed, which entails that exploration remains one of the key challenges of DRL. Instead of solely relying on extrinsic rewards, many state-of-the-art methods use intrinsic curiosity as exploration signal. While they hold promise of better local exploration, discovering global exploration strategies is beyond the reach of current methods. We propose a novel end-to-end intrinsic reward formulation that introduces high-level exploration in reinforcement learning. Our curiosity signal is driven by a fast reward that deals with local exploration and a slow reward that incentivizes long-time horizon exploration strategies. We formulate curiosity as the error in an agent’s ability to reconstruct the observations given their contexts. Experimental results show that this high-level exploration enables our agents to outperform prior work in several Atari games.",
        "link": "http://dx.doi.org/10.24963/ijcai.2020/733"
    },
    {
        "id": 28815,
        "title": "Rule-Based System Against Reinforcement Learning*",
        "authors": "Bozhan Orozov, Daniela Orozova",
        "published": "2021-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3472410.3472437"
    },
    {
        "id": 28816,
        "title": "Model-Free Reinforcement Learning-Based Control for Continuous-Time Systems",
        "authors": "Kyriakos G. Vamvoudakis",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4471-5102-9_100065-1"
    },
    {
        "id": 28817,
        "title": "Further Exploration and Next Steps",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_10"
    },
    {
        "id": 28818,
        "title": "Reinforcement Learning Based Multi-Access Control with Energy Harvesting",
        "authors": "Man Chu, Hang Li, Xuewen Liao, Shuguang Cui",
        "published": "2018-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/glocom.2018.8647438"
    },
    {
        "id": 28819,
        "title": "Dynamic Channel Access and Power Control via Deep Reinforcement Learning",
        "authors": "Ziyang Lu, M. Cenk Gursoy",
        "published": "2019-9",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtcfall.2019.8891391"
    },
    {
        "id": 28820,
        "title": "Reinforcement Learning for Multi-Well SAGD Optimization: A Policy Gradient Approach",
        "authors": "J. L. Guevara, J. Trivedi",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "Abstract\nFinding an optimal steam injection strategy for a SAGD process is considered a major challenge due to the complex dynamics of the physical phenomena. Recently, reinforcement learning (RL) has been presented as alternative to conventional methods (e.g., adjoint-optimization, model predictive control) as an effective way to address the cited challenge. In general, RL represents a model-free strategy where an agent is trained to find the optimal policy - the action at every time step that will maximize cumulative long-term performance of a given process- only by continuous interactions with the environment (e.g., SAGD process). This environment is modeled as a Markov-Decision-Process (MDP) and a state must be defined to characterize it. During the interaction process, at each time step, the agent executes an action, receives a scalar reward (e.g., net present value) due to the action taken and observes the new state (e.g., pressure distribution of the reservoir) of the environment. This process continuous for a number of simulations or episodes until convergence is achieved. One approach to solve the RL problem is to parametrize the policy using well-known methods, e.g., linear functions, SVR, neural networks, etc. This approach is based on maximizing the performance of the process with respect to the parameters of the policy. Using the Monte Carlo algorithm, after every episode a long-term performance of the process is obtained and the parameters of the policy are updated using gradient-ascent methods. In this work policy gradient is used to find the steam injection policy that maximizes cumulative net present value of a SAGD process. The environment is represented by a reservoir simulation model inspired by northern Alberta reservoir and the policy is parametrized using a deep neural network. Results show that optimal steam injection can be characterized in two regions: 1) an increase or slight increase of steam injection rates, and 2) a sharp decrease until reaching the minimum value. Furthermore, the first region's objective appears to be more of pressure maintenance using high steam injection rates. In the second region, the objective is to collect more reward or achieve high values of daily net present value due to the reduction of steam injection while keeping high oil production values.",
        "link": "http://dx.doi.org/10.2118/213104-ms"
    },
    {
        "id": 28821,
        "title": "Prioritization Techniques for Android Test Suites Generated by a Reinforcement Learning Algorithm",
        "authors": "Md Khorrom Khan, Ryan Michaels, Farhan Rahman Arnob, Renée Bryce",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4450321"
    },
    {
        "id": 28822,
        "title": "Pseudo Random Number Generation through Reinforcement Learning and Recurrent Neural Networks",
        "authors": "Luca Pasqualini, Maurizio Parton",
        "published": "2020-11-23",
        "citations": 2,
        "abstract": "A Pseudo-Random Number Generator (PRNG) is any algorithm generating a sequence of numbers approximating properties of random numbers. These numbers are widely employed in mid-level cryptography and in software applications. Test suites are used to evaluate the quality of PRNGs by checking statistical properties of the generated sequences. These sequences are commonly represented bit by bit. This paper proposes a Reinforcement Learning (RL) approach to the task of generating PRNGs from scratch by learning a policy to solve a partially observable Markov Decision Process (MDP), where the full state is the period of the generated sequence, and the observation at each time-step is the last sequence of bits appended to such states. We use Long-Short Term Memory (LSTM) architecture to model the temporal relationship between observations at different time-steps by tasking the LSTM memory with the extraction of significant features of the hidden portion of the MDP’s states. We show that modeling a PRNG with a partially observable MDP and an LSTM architecture largely improves the results of the fully observable feedforward RL approach introduced in previous work.",
        "link": "http://dx.doi.org/10.3390/a13110307"
    },
    {
        "id": 28823,
        "title": "PokerBot: Hand Strength Reinforcement Learning",
        "authors": "Angela Ramirez, Solomon Reinman, Narges Norouzi",
        "published": "2019-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/inista.2019.8778267"
    },
    {
        "id": 28824,
        "title": "Reinforcement Learning for Smart Charging of Electric Buses in Smart Grid",
        "authors": "Wenzhuo Chen, Peng Zhuang, Hao Liang",
        "published": "2019-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9014160"
    },
    {
        "id": 28825,
        "title": "Reinforcement Learning Method with Internal World Model Training",
        "authors": "Kenji Hirata, Hiroyuki Iizuka, Masahito Yamamoto",
        "published": "2020-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sii46433.2020.9026225"
    },
    {
        "id": 28826,
        "title": "Planning with a Model: AlphaZero",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_14"
    },
    {
        "id": 28827,
        "title": "Deep Reinforcement Learning Based Collision Avoidance System for Autonomous Ships",
        "authors": "Yong Wang, Haixiang Xu, Hui Feng, Jianhua He, Haojie Yang, Lian Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4566668"
    },
    {
        "id": 28828,
        "title": "Deep Reinforcement Learning based Resource Allocation in NOMA",
        "authors": "N. Iswarya, R. Venkateswari",
        "published": "2022-9-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciiet55458.2022.9967604"
    },
    {
        "id": 28829,
        "title": "Reinforcement Learning Based Autonomous Vehicles Lateral Control",
        "authors": "Eslam Mahmoud, Dalil Ichalal, Saïd Mammar",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnsc58704.2023.10318986"
    },
    {
        "id": 28830,
        "title": "Sentence Selective Neural Extractive Summarization with Reinforcement Learning",
        "authors": "Laifu Chen, Minh Le Nguyen",
        "published": "2019-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/kse.2019.8919490"
    },
    {
        "id": 28831,
        "title": "A Data-Driven Choice of Misfit Function for FWI Using Reinforcement Learning",
        "authors": "B. Sun, T. Alkhalifah",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202010203"
    },
    {
        "id": 28832,
        "title": "Building load management clusters using reinforcement learning",
        "authors": "Shamwilu Ahmed, Francois Bouffard",
        "published": "2017-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iemcon.2017.8117147"
    },
    {
        "id": 28833,
        "title": "A Practical Deep Reinforcement Learning Approach to Semiconductor Equipment Scheduling",
        "authors": "Changhee Lee, Sunghee Lee",
        "published": "2021-3-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit46573.2021.9453533"
    },
    {
        "id": 28834,
        "title": "State Complexity Reduction in Reinforcement Learning based Adaptive Traffic Signal Control",
        "authors": "Mladen Miletic, Kresimir Kusic, Martin Greguric, Edouard Ivanjko",
        "published": "2020-9",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elmar49956.2020.9219024"
    },
    {
        "id": 28835,
        "title": "Channel Pruning via Lookahead Search Guided Reinforcement Learning",
        "authors": "Zi Wang, Chengcheng Li",
        "published": "2022-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv51458.2022.00357"
    },
    {
        "id": 28836,
        "title": "Multi-Agent Deep Reinforcement Learning-Based Maintenance Optimization for Multi-Dependent Component Systems",
        "authors": "Van Thai Nguyen, Phuc Do, Alexandre Voisin, benoit IUNG",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4496792"
    },
    {
        "id": 28837,
        "title": "Low-Latency Virtual Network Function Scheduling Algorithm Based on Deep Reinforcement Learning",
        "authors": "Zhiwei Liu, Zhaogang Shu, Shuwu Chen, Yiwen Zhong, Jiaxiang Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4563908"
    },
    {
        "id": 28838,
        "title": "A Lunar Robot Obstacle Avoidance Planning Method Using Deep Reinforcement Learning for Data Fusion",
        "authors": "Ruijun Hu, Zhaokui Wang",
        "published": "2019-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac48633.2019.8997266"
    },
    {
        "id": 28839,
        "title": "Multi-Agent Reinforcement Learning for Dynamic Spectrum Access",
        "authors": "Huijuan Jiang, Tianyu Wang, Shaowei Wang",
        "published": "2019-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2019.8761786"
    },
    {
        "id": 28840,
        "title": "Multi-Constraints Guidance and Maneuvering Penetration Strategy via Meta Deep Reinforcement Learning",
        "authors": "Sibo Zhao, JianWen Zhu, Weimin Bao, Xiaoping Li, Haifeng Sun",
        "published": "No Date",
        "citations": 1,
        "abstract": "In response to the issue of vehicle escape guidance, this manuscript proposes a unified intelligent control strategy synthesizing optimal guidance and Meta Deep Reinforcement Learning (DRL). Optimal control with minor energy consumption is introduced to meet terminal latitude, longitude and altitude. Maneuvering escape is realized by adding longitudinal and lateral direction maneuver overloads. Maneuver command decision model is calculated based on Soft-Actor-Critic (SAC) networks. Meta learning is introduced to enhance autonomous escape capability, which improves generalization performance to time-varying scenarios not encountered in the training process. In order to obtain training samples at a faster speed, this manuscript uses the prediction method to solve reward values, which avoiding a large number of numerical integration. The simulation results manifest that the proposed intelligent strategy can achieve high precise guidance and effective escape.",
        "link": "http://dx.doi.org/10.20944/preprints202308.1512.v1"
    },
    {
        "id": 28841,
        "title": "Harnessing Deep Reinforcement Learning to Construct Time-Dependent Optimal Fields for Quantum Control Dynamics",
        "authors": "Yuanqi Gao, Xian Wang, Nanpeng Yu, Bryan Wong",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present an efficient deep reinforcement learning (DRL) approach to automatically construct time-dependent optimal control fields that enable desired transitions in reduced-dimensional chemical systems. Our DRL approach gives impressive performance in autonomously and efficiently constructing optimal control fields, even for cases that are difficult to converge with existing gradient-based approaches. We provide a detailed description of the algorithms and hyperparameters as well as performance metrics for our DRL-based approach. Our results demonstrate that DRL can be employed as an effective artificial intelligence approach to efficiently and autonomously design control fields in continuous quantum dynamical chemical systems.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2022-kh0jk"
    },
    {
        "id": 28842,
        "title": "Flexibility in valenced reinforcement learning computations across development",
        "authors": "Kate Nussenbaum, Juan A. Velez, Bradli T. Washington, Hannah E. Hamling, Catherine A. Hartley",
        "published": "No Date",
        "citations": 1,
        "abstract": "Optimal integration of positive and negative outcomes during learning varies depending on an environment’s reward statistics. The present study investigated the extent to which children, adolescents, and adults (N = 142 8 - 25 year-olds, 55% female, 42% White, 31% Asian, 17% mixed race, and 8% Black) adapt their weighting of better-than-expected and worse-than-expected outcomes when learning from reinforcement. Participants made a series of choices across two contexts: one in which weighting positive outcomes more heavily than negative outcomes led to better performance, and one in which the reverse was true. Reinforcement learning modeling revealed that across age, participants shifted their valence biases in accordance with the structure of the environment. Exploratory analyses revealed increases in context-dependent flexibility with age.",
        "link": "http://dx.doi.org/10.31234/osf.io/5f9uc"
    },
    {
        "id": 28843,
        "title": "Permutation Invariant Agent-Specific Centralized Critic in Multi-Agent Reinforcement Learning",
        "authors": "Patsornchai Noppakun, Khajonpong Akkarajitsakul",
        "published": "2022-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incit56086.2022.10067429"
    },
    {
        "id": 28844,
        "title": "Reinforcement Learning Based Resource Allocation for Network Slicing in 5G C-RAN",
        "authors": "Xiaofei Wang, Tiankui Zhang",
        "published": "2019-10",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/comcomap46287.2019.9018774"
    },
    {
        "id": 28845,
        "title": "Canonical Problems and Applications",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch2"
    },
    {
        "id": 28846,
        "title": "Distributed Intelligence for Dynamic Task Migration in the 6G User Plane using Deep Reinforcement Learning",
        "authors": "Sayantini Majumdar, Susanna Schwarzmann, Riccardo Trivisonno, Georg Carle",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In-Network Computing (INC) is a currently emerging paradigm. Realizing INC in 6G networks could mean that user plane entities (UPEs) carry out computations on packets while transmitting them. These computations may have specific requirements in terms of their completion time. In case of high compute pressure at one UPE, migrating computations to another UPE may be beneficial, in order to avoid exceeding the completion time requirement. Centralized migration approaches suffer from increased signaling and are prone to react too slow. Therefore, this paper investigates the applicability of distributed intelligence to tackle the problem of compute task migration in the 6G user plane. Each UPE is equipped with an intelligent agent, enabling autonomous decisions on whether computations should be migrated to another UPE. To enable the intelligent agents to learn and apply an optimal task migration policy, we investigate and compare of two state-of-the-art Deep Reinforcement Learning (DRL) approaches: Advantage Actor-Critic (A2C) and Double Deep Q-Network (DDQN). We show, via simulations, that the performance of both solutions, in terms of the percentage of tasks exceeding their completion time requirement, is near-optimal and training A2C is at least 60% faster. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24459487.v1"
    },
    {
        "id": 28847,
        "title": "Requirements Business Value and Test Case Attributes based Test Case Prioritization Using Deep Reinforcement Learning.",
        "authors": "Tamim Ahmed Khan, Zobia Jabeen Akhtar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTesting is a cost-intensive process whereas regression testing is used for the purpose of finding out if there is a regression in a post-evolution or post-maintenance scenario. It is expensive to run all test cases for regression testing and therefore we require a test case selection strategy so that we may select a subset of test cases from a previously available set of all test cases. Test case prioritization is a mechanism that can be used as a test subset selection strategy for regression testing. We investigate how requirements priority and importance as well as test case parameters such as fault severity, fault count, and their inter-dependency can be used for proposing a reward function to improve results. We use a deep reinforcement learning approach to optimize or prioritize the test cases using our reward function. We make use of an off-policy, model-free, and value-based test case prioritization approach considering a deep Q-Network (DQN) agent and a reward system based on a business value of test cases that prioritize the test cases against those business values. We use performance metrics to measure the performance and effectiveness of our research. We have compared our results with the published research and conclude that the results achieved from our approach outperform the previously published results of state-of-the-art studies.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3497933/v1"
    },
    {
        "id": 28848,
        "title": "RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning",
        "authors": "Dongsheng Zuo, Yikang Ouyang, Yuzhe Ma",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dac56929.2023.10247941"
    },
    {
        "id": 28849,
        "title": "Is Reinforcement Learning a Panacea for Solving All Contingencies in UAVs?",
        "authors": "Chiman Kwan",
        "published": "2019-12-6",
        "citations": 0,
        "abstract": "We would like to point out that reinforcement learning (RL) needs to be used with caution in contingency planning for UAVs. A more practical approach is proposed in this opinion.",
        "link": "http://dx.doi.org/10.19080/raej.2019.05.555655"
    },
    {
        "id": 28850,
        "title": "Autonomous Car Parking System using Deep Reinforcement Learning",
        "authors": "Rikuya Takehara, Tad Gonsalves",
        "published": "2021-9-23",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icitech50181.2021.9590169"
    },
    {
        "id": 28851,
        "title": "An inductive bias for slowly changing features in human reinforcement learning",
        "authors": "Noa L. Hedrich, Eric Schulz, Sam Hall-McMaster, Nicolas W. Schuck",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractIdentifying goal-relevant features in novel environments is a central challenge for efficient behaviour. We asked whether humans address this challenge by relying on prior knowledge about common properties of reward-predicting features. One such property is the rate of change of features, given that behaviourally relevant processes tend to change on a slower timescale than noise. Hence, we asked whether humans are biased to learn more when task-relevant features are slow rather than fast. To test this idea, 100 human participants were asked to learn the rewards of two-dimensional bandits when either a slowly or quickly changing feature of the bandit predicted reward. Participants accrued more reward and achieved better generalisation to unseen feature values when a bandit’s relevant feature changed slowly, and its irrelevant feature quickly, as compared to the opposite. Participants were also more likely to incorrectly base their choices on the irrelevant feature when it changed slowly versus quickly. These effects were stronger when participants experienced the feature speed before learning about rewards. Modelling this behaviour with a set of four function approximation Kalman filter models that embodied alternative hypotheses about how feature speed could affect learning revealed that participants had a higher learning rate for the slow feature, and adjusted their learning to both the relevance and the speed of feature changes. The larger the improvement in participants’ performance for slow compared to fast bandits, the more strongly they adjusted their learning rates. These results provide evidence that human reinforcement learning favours slower features, suggesting a bias in how humans approach reward learning.Author SummaryLearning experiments in the laboratory are often assumed to exist in a vacuum, where participants solve a given task independently of how they learn in more natural circumstances. But humans and other animals are in fact well known to “meta learn”, i.e. to leverage generalisable assumptions abouthow to learnfrom other experiences. Taking inspiration from a well-known machine learning technique known as slow feature analysis, we investigated one specific instance of such an assumption in learning: the possibility that humans tend to focus on slowly rather than quickly changing features when learning about rewards. To test this, we developed a task where participants had to learn the value of stimuli composed of two features. Participants indeed learned better from a slowly rather than quickly changing feature that predicted reward and were more distracted by the reward-irrelevant feature when it changed slowly. Computational modelling of participant behaviour indicated that participants had a higher learning rate for slowly changing features from the outset. Hence, our results support the idea that human reinforcement learning reflects a priori assumptions about the reward structure in natural environments.",
        "link": "http://dx.doi.org/10.1101/2024.01.24.576910"
    },
    {
        "id": 28852,
        "title": "Robust Control of An Inverted Pendulum System Based on Policy Iteration in Reinforcement Learning",
        "authors": "Xu Dengguo, Ma Yan, Huang Jiashun, Li Yahui",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper is primarily focused on the robust control of an inverted pendulum system based \r\non the policy iteration in reinforcement learning. First, a mathematical model of the single inverted \r\npendulum system is established through a force analysis of the pendulum and trolley. Second,  \r\nbased on the theory of robust optimal control, the robust control of the uncertain linear inverted  \r\npendulum system is transformed into an optimal control problem with an appropriate performance  \r\nindex. Moreover, for the uncertain linear and nonlinear systems, two reinforcement-learning control  \r\nalgorithms are proposed using the policy iteration method. Finally, two numerical examples are  \r\nprovided to validate the reinforcement learning algorithms for the robust control of the inverted  \r\npendulum systems.",
        "link": "http://dx.doi.org/10.20944/preprints202310.1100.v1"
    },
    {
        "id": 28853,
        "title": "Deep Reinforcement Learning for Intelligent Penetration Testing Path Design",
        "authors": "Junkai Yi, Xiaoyan Liu",
        "published": "2023-8-21",
        "citations": 1,
        "abstract": "Penetration testing is an important method to evaluate the security degree of a network system. The importance of penetration testing attack path planning lies in its ability to simulate attacker behavior, identify vulnerabilities, reduce potential losses, and continuously improve security strategies. By systematically simulating various attack scenarios, it enables proactive risk assessment and the development of robust security measures. To address the problems of inaccurate path prediction and difficult convergence in the training process of attack path planning, an algorithm which combines attack graph tools (i.e., MulVAL, multi-stage vulnerability analysis language) and the double deep Q network is proposed. This algorithm first constructs an attack tree, searches paths in the attack graph, and then builds a transfer matrix based on depth-first search to obtain all reachable paths in the target system. Finally, the optimal path for target system attack path planning is obtained by using the deep double Q network (DDQN) algorithm. The MulVAL double deep Q network(MDDQN) algorithm is tested in different scale penetration testing environments. The experimental results show that, compared with the traditional deep Q network (DQN) algorithm, the MDDQN algorithm is able to reach convergence faster and more stably and improve the efficiency of attack path planning.",
        "link": "http://dx.doi.org/10.3390/app13169467"
    },
    {
        "id": 28854,
        "title": "On Computational Models of Theory of Mind and the Imitative Reinforcement Learning in Spiking Neural Networks",
        "authors": "Mohammad Ganjtabesh, Ashena G. Mohammadi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTheory of Mind is referred to the ability of inferring other's mental states, and it plays a crucial role in social cognition and learning. Biological evidences indicate that complex circuits are involved in this ability, including the mirror neuron system. The mirror neuron system influences imitation abilities and action understanding, leading to learn through observing others. To simulate this imitative learning behavior, a Theory-of-Mind-based Imitative Reinforcement Learning (ToM-based ImRL) framework is proposed. Employing the bio-inspired spiking neural networks and the mechanisms of the mirror neuron system, ToM-based ImRL is a bio-inspired computational model which enables an agent to effectively learn how to act in an interactive environment through observing an expert, inferring its goals, and imitating its behaviors. The aim of this paper is to review some computational attempts in modeling ToM and to explain the proposed ToM-based ImRL framework which is tested in the environment of River Raid game from Atari 2600 series.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3341817/v1"
    },
    {
        "id": 28855,
        "title": "5G Network Slice Admission Control Using Optimization and Reinforcement Learning",
        "authors": "Md Ariful Haque, Vassilka Kirova",
        "published": "2022-4-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc51071.2022.9771643"
    },
    {
        "id": 28856,
        "title": "On-Policy Deep Reinforcement Learning Approach to Multi Agent Problems",
        "authors": "Ziya Tan, Mehmet Karakose",
        "published": "2021-9-14",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003202240-58"
    },
    {
        "id": 28857,
        "title": "Transfer Reinforcement Learning under Unobserved Contextual Information",
        "authors": "Yan Zhang, Michael M. Zavlanos",
        "published": "2020-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccps48487.2020.00015"
    },
    {
        "id": 28858,
        "title": "Reinforcement Learning for Mobile Robot Navigation: An overview",
        "authors": "Nessrine Khlif, Nahla Khraief, Safya Belghith",
        "published": "2022-7-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsis56166.2022.10118362"
    },
    {
        "id": 28859,
        "title": "Inclined Quadrotor Landing using Deep Reinforcement Learning",
        "authors": "Jacob E. Kooi, Robert Babuska",
        "published": "2021-9-27",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros51168.2021.9636096"
    },
    {
        "id": 28860,
        "title": "A Policy Search Method For Temporal Logic Specified Reinforcement Learning Tasks",
        "authors": "Xiao Li, Yao Ma, Calin Belta",
        "published": "2018-6",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2018.8431181"
    },
    {
        "id": 28861,
        "title": "Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Yumeya Yamamori, Oliver J Robinson, Jonathan P Roiser",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractAlthough avoidance is a prevalent feature of anxiety-related psychopathology, differences in the measurement of avoidance between humans and non-human animals hinder our progress in its theoretical understanding and treatment. To address this, we developed a novel translational measure of anxiety-related avoidance in the form of an approach-avoidance reinforcement learning task, by adapting a paradigm from the non-human animal literature to study the same cognitive processes in human participants. We used computational modelling to probe the putative cognitive mechanisms underlying approach-avoidance behaviour in this task and investigated how they relate to subjective task-induced anxiety. In a large online study (n = 372), participants who experienced greater task-induced anxiety avoided choices associated with punishment, even when this resulted in lower overall reward. Computational modelling revealed that this effect was explained by greater individual sensitivities to punishment relative to rewards. We replicated these findings in an independent sample (n = 627) and we also found fair-to-excellent reliability of measures of task performance in a sub-sample retested one week later (n = 57). Our findings demonstrate the potential of approach-avoidance reinforcement learning tasks as translational and computational models of anxiety-related avoidance. Future studies should assess the predictive validity of this approach in clinical samples and experimental manipulations of anxiety.",
        "link": "http://dx.doi.org/10.1101/2023.04.04.535526"
    },
    {
        "id": 28862,
        "title": "The segregation of vocal circuits solves a credit assignment problem associated with multi-objective reinforcement learning",
        "authors": "Don Murdoch, Ruidong Chen, Jesse Goldberg",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMotor circuits vary in topographic organization, ranging from a coarse relationship between neuron location and function to highly localized regions controlling specific behaviors. For unclear reasons, vocal learning circuits lie at this second extreme: they repeatedly evolved to be spatially segregated from other parts of the motor system. Here we show that spatially segregated motor circuits can solve a specific problem that arises when an animal tries to learn two things at once. We trained songbirds in vocal and place learning paradigms with brief strobe light flashes and noise bursts. Strobe light negatively reinforced place learning but did not affect song syllable learning. Noise bursts positively reinforced place preference but negatively reinforced syllable learning. These double dissociations indicate that vocalization-related reinforcement signals specifically target the vocal motor system, while place-related reinforcement signals specifically target the navigation system. Non-global, target-specific reinforcement signals have established utility in machine implementation of multi-objective learning. In vocal learners, such signals could enable an animal to practice vocalizing as it does other things such as forage for food or learn to walk.",
        "link": "http://dx.doi.org/10.1101/236273"
    },
    {
        "id": 28863,
        "title": "Computational characteristics of the striatal dopamine system described by reinforcement learning with fast generalization",
        "authors": "Yoshihisa Fujita, Sho Yagishita, Haruo Kasai, Shin Ishii",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractGeneralization enables applying past experience to similar but nonidentical situations. Therefore, it may be essential for adaptive behaviors. Recent neurobiological observation indicates that the striatal dopamine system achieves generalization and subsequent discrimination by updating corticostriatal synaptic connections in differential response to reward and punishment. To analyze how the computational characteristics in this system affect behaviors, we proposed a novel reinforcement learning model with multilayer neural networks in which the synaptic weights of only the last layer are updated according to the prediction error. We set fixed connections between the input and hidden layers so as to maintain the similarity of inputs in the hidden-layer representation. This network enabled fast generalization, and thereby facilitated safe and efficient exploration in reinforcement learning tasks, compared to algorithms which do not show generalization. However, disturbance in the network induced aberrant valuation. In conclusion, the unique computation suggested by corticostriatal plasticity has the advantage of providing safe and quick adaptations to unknown environments, but on the other hand has the potential defect which can induce maladaptive behaviors like delusional symptoms of psychiatric disorders.Author summaryThe brain has an ability to generalize knowledge obtained from reward- and punishment-related learning. Animals that have been trained to associate a stimulus with subsequent reward or punishment respond not only to the same stimulus but also to resembling stimuli. How does generalization affect behaviors in situations where individuals are required to adapt to unknown environments? It may enable efficient learning and promote adaptive behaviors, but inappropriate generalization may disrupt behaviors by associating reward or punishment with irrelevant stimuli. The effect of generalization here should depend on computational characteristics of underlying biological basis in the brain, namely, the striatal dopamine system. In this research, we made a novel computational model based on the characteristics of the striatal dopamine system. Our model enabled fast generalization and showed its advantage of providing safe and quick adaptation to unknown environments. By contrast, disturbance of our model induced abnormal behaviors. The results suggested the advantage and the shortcoming of generalization by the striatal dopamine system.",
        "link": "http://dx.doi.org/10.1101/2019.12.12.873950"
    },
    {
        "id": 28864,
        "title": "Cooperative reinforcement learning for multiple units combat in starCraft",
        "authors": "Kun Shao, Yuanheng Zhu, Dongbin Zhao",
        "published": "2017-11",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci.2017.8280949"
    },
    {
        "id": 28865,
        "title": "Reinforcement learning strategies for vessel navigation",
        "authors": "Andrius Daranda, Gintautas Dzemyda",
        "published": "2022-11-24",
        "citations": 3,
        "abstract": "Safe navigation at sea is more important than ever. Cargo is usually transported by vessel because it makes economic sense. However, marine accidents can cause huge losses of people, cargo, and the vessel itself, as well as irreversible ecological disasters. These are the reasons to strive for safe vessel navigation. The navigator shall ensure safe vessel navigation. He must plan every maneuver and act safely. At the same time, he must evaluate and predict the actions of other vessels in dense maritime traffic. This is a complicated process and requires constant human concentration. It is a very tiring and long-lasting duty. Therefore, human error is the main reason of collisions between vessels. In this paper, different reinforcement learning strategies have been explored in order to find the most appropriate one for the real-life problem of ensuring safe maneuvring in maritime traffic. An experiment using different algorithms was conducted to discover a suitable method for autonomous vessel navigation. The experiments indicate that the most effective algorithm (Deep SARSA) allows reaching 92.08% accuracy. The efficiency of the proposed model is demonstrated through a real-life collision between two vessels and how it could have been avoided.",
        "link": "http://dx.doi.org/10.3233/ica-220688"
    },
    {
        "id": 28866,
        "title": "Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Yumeya Yamamori, Oliver J Robinson, Jonathan P Roiser",
        "published": "No Date",
        "citations": 0,
        "abstract": "Although avoidance is a prevalent feature of anxiety-related psychopathology, differences in existing measures of avoidance between humans and non-human animals impede progress in its theoretical understanding and treatment. To address this, we developed a novel translational measure of anxiety-related avoidance in the form of an approach-avoidance reinforcement learning task, by adapting a paradigm from the non-human animal literature to study the same cognitive processes in human participants. We used computational modelling to probe the putative cognitive mechanisms underlying approach-avoidance behaviour in this task and investigated how they relate to subjective task-induced anxiety. In a large online study, participants (n = 372) who experienced greater task-induced anxiety avoided choices associated with punishment, even when this resulted in lower overall reward. Computational modelling revealed that this effect was explained by greater individual sensitivities to punishment relative to rewards. We replicated these findings in an independent sample (n = 627) and we also found fair-to-excellent reliability of measures of task performance in a sub-sample retested one week later (n = 57). Our findings demonstrate the potential of approach-avoidance reinforcement learning tasks as translational and computational models of anxiety-related avoidance. Future studies should assess the predictive validity of this approach in clinical samples and experimental manipulations of anxiety.",
        "link": "http://dx.doi.org/10.7554/elife.87720.2"
    },
    {
        "id": 28867,
        "title": "Optimizing Reinforcement Learning-Based Visual Navigation for Resource-Constrained Devices",
        "authors": "U. Vijetha, V. Geetha",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3323801"
    },
    {
        "id": 28868,
        "title": "Application of Deep Reinforcement Learning in Werewolf Game Agents",
        "authors": "Tianhe Wang, Tomoyuki Kaneko",
        "published": "2018-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/taai.2018.00016"
    },
    {
        "id": 28869,
        "title": "Reinforcement Learning for Long-term Reward Optimization in Recommender Systems",
        "authors": "Anton Dorozhko",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sibircon48586.2019.8958202"
    },
    {
        "id": 28870,
        "title": "FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance",
        "authors": "Xiao-Yang Liu",
        "published": "No Date",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3737257"
    },
    {
        "id": 28871,
        "title": "Task-Oriented Query Reformulation with Reinforcement Learning",
        "authors": "Rodrigo Nogueira, Kyunghyun Cho",
        "published": "2017",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d17-1061"
    },
    {
        "id": 28872,
        "title": "Efficient hyperparameter optimization through model-based reinforcement learning",
        "authors": "Jia Wu, SenPeng Chen, XiYuan Liu",
        "published": "2020-10",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2020.06.064"
    },
    {
        "id": 28873,
        "title": "Application of Reinforcement Learning in SINS/GNSS/DVL Integrated Navigation",
        "authors": "Xiangyuan Li, Xuesong Tang, Xingshu Wang, Wenfeng Tan, Jiaxing Zheng, Yingwei Zhao",
        "published": "2022-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlccim55934.2022.00066"
    },
    {
        "id": 28874,
        "title": "Deep reinforcement learning collision avoidance using policy gradient optimisation and Q-learning",
        "authors": "Shady A. Maged, Bishoy H. Mikhail",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijcvr.2020.107253"
    },
    {
        "id": 28875,
        "title": "Multiagent Coordination Systems Based on Neuro-Fuzzy Models with Reinforcement Learning",
        "authors": "Leonardo Alfredo Mendoza, Evelyn Batista, Harold Dias De Mello, Marco Aurelio Pacheco",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00151"
    },
    {
        "id": 28876,
        "title": "A Study on Application of Curriculum Learning in Deep Reinforcement Learning : Action Acquisition in Shooting Game AI as Example",
        "authors": "Ikumi Kodaka, Fumiaki Saitoh",
        "published": "2021-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwcia52852.2021.9626020"
    },
    {
        "id": 28877,
        "title": "Implementation of Reinforcement Learning Simulated Madel on Physical UGV Using Robot Operating System for Continual Learning",
        "authors": "Edgar M. Perez, Abhijit Majumdar, Patrick Benavidez, Mo Jamshidi",
        "published": "2019-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sysose.2019.8753801"
    },
    {
        "id": 28878,
        "title": "Deep Reinforcement Learning for Optimization",
        "authors": "Md Mahmudul Hasan, Md Shahinur Rahman, Adrian Bell",
        "published": "2019",
        "citations": 1,
        "abstract": "Deep reinforcement learning (DRL) has transformed the field of artificial intelligence (AI) especially after the success of Google DeepMind. This branch of machine learning epitomizes a step toward building autonomous systems by understanding of the visual world. Deep reinforcement learning (RL) is currently applied to different sorts of problems that were previously obstinate. In this chapter, at first, the authors started with an introduction of the general field of RL and Markov decision process (MDP). Then, they clarified the common DRL framework and the necessary components RL settings. Moreover, they analyzed the stochastic gradient descent (SGD)-based optimizers such as ADAM and a non-specific multi-policy selection mechanism in a multi-objective Markov decision process. In this chapter, the authors also included the comparison for different Deep Q networks. In conclusion, they describe several challenges and trends in research within the deep reinforcement learning field. ",
        "link": "http://dx.doi.org/10.4018/978-1-5225-7862-8.ch011"
    },
    {
        "id": 28879,
        "title": "Coordinated Voltage Regulation of Microgrid Clusters Based on Deep Reinforcement Learning Approach",
        "authors": "Xiaozhe Xue, Hui Ge",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10166110"
    },
    {
        "id": 28880,
        "title": "When Does Communication Learning Need Hierarchical Multi-Agent Deep Reinforcement Learning",
        "authors": "Marie Ossenkopf, Mackenzie Jorgensen, Kurt Geihs",
        "published": "2019-11-17",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/01969722.2019.1677335"
    },
    {
        "id": 28881,
        "title": "Correction: Optimum trajectory learning in musculoskeletal systems with model predictive control and deep reinforcement learning",
        "authors": "Berat Denizdurduran, Henry Markram, Marc-Oliver Gewaltig",
        "published": "2022-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00422-022-00949-2"
    },
    {
        "id": 28882,
        "title": "Developing multi-agent adversarial environment using reinforcement learning and imitation learning",
        "authors": "Ziyao Han, Yupeng Liang, Kazuhiro Ohkura",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10015-023-00912-9"
    },
    {
        "id": 28883,
        "title": "Learning to Grasp on the Moon from 3D Octree Observations with Deep Reinforcement Learning",
        "authors": "Andrej Orsula, Simon Bøgh, Miguel Olivares-Mendez, Carol Martinez",
        "published": "2022-10-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981661"
    },
    {
        "id": 28884,
        "title": "Reinforcement Learning-Based Network Slice Resource Allocation for Federated Learning Applications",
        "authors": "Zhouxiang Wu, Genya Ishigaki, Riti Gour, Congzhou Li, Jason P. Jue",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001715"
    },
    {
        "id": 28885,
        "title": "A Curriculum Learning Based Multi-agent Reinforcement Learning Method for Realtime Strategy Game",
        "authors": "Dayu Zhang, Weidong Bao, Wenqian Liang, Guanlin Wu, Jiang Cao",
        "published": "2022-8-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdia56350.2022.9874056"
    },
    {
        "id": 28886,
        "title": "Cooperative zone-based rebalancing of idle overhead hoist transportations using multi-agent reinforcement learning with graph representation learning",
        "authors": "Kyuree Ahn, Jinkyoo Park",
        "published": "2021-2-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/24725854.2020.1851823"
    },
    {
        "id": 28887,
        "title": "Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-Resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning",
        "authors": "Md Tamjid Hossain, John W. T. Lee",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmlc58545.2023.10327979"
    },
    {
        "id": 28888,
        "title": "Learning to navigate a crystallization model with Deep Reinforcement Learning",
        "authors": "Vidhyadhar Manee, Roberto Baratti, Jose A. Romagnoli",
        "published": "2022-2",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cherd.2021.12.005"
    },
    {
        "id": 28889,
        "title": "Learning Human-Aware Robot Navigation from Physical Interaction via Inverse Reinforcement Learning",
        "authors": "Marina Kollmitz, Torsten Koller, Joschka Boedecker, Wolfram Burgard",
        "published": "2020-10-24",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros45743.2020.9340865"
    },
    {
        "id": 28890,
        "title": "Learning from Unreliable Human Action Advice in Interactive Reinforcement Learning",
        "authors": "Lisa Scherf, Cigdem Turan, Dorothea Koert",
        "published": "2022-11-28",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/humanoids53995.2022.10000078"
    },
    {
        "id": 28891,
        "title": "MABSearch: The Bandit Way of Learning the Learning Rate—A Harmony Between Reinforcement Learning and Gradient Descent",
        "authors": "A. S. Syed Shahul Hameed, Narendran Rajagopalan",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s40009-023-01292-1"
    },
    {
        "id": 28892,
        "title": "Learning State Representations for Query Optimization with Deep Reinforcement Learning",
        "authors": "Jennifer Ortiz, Magdalena Balazinska, Johannes Gehrke, S. Sathiya Keerthi",
        "published": "2018-6-15",
        "citations": 63,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3209889.3209890"
    },
    {
        "id": 28893,
        "title": "MASK-RL: Multiagent Video Object Segmentation Framework Through Reinforcement Learning",
        "authors": "Giuseppe Vecchio, Simone Palazzo, Daniela Giordano, Francesco Rundo, Concetto Spampinato",
        "published": "2020-12",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2019.2963282"
    },
    {
        "id": 28894,
        "title": "Toward a Reinforcement Learning Environment Toolbox for Intelligent Electric Motor Control",
        "authors": "Arne Traue, Gerrit Book, Wilhelm Kirchgassner, Oliver Wallscheid",
        "published": "2022-3",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2020.3029573"
    },
    {
        "id": 28895,
        "title": "Deep Reinforcement Learning for Cybersecurity Applications",
        "authors": "Alex Mathew",
        "published": "2021-12-30",
        "citations": 1,
        "abstract": "There has been a rapid growth of the devices connected to the internet in the last decade for the various internet (IoT) of things applications. The increase of these smart devices has posed a great security concern in the internet of things ecosystem. The internet of things ecosystem must be protected from these threats. Reinforcement learning has been proposed by the cybersecurity professionals to provide the needed security tools for securing the IoT system since it is able to interact with the environment and learn how to detect the threats. This paper presents a comprehensive research on cybersecurity threats to the IoT system applications. The RL algorithms are also presented to understand the attacks on the IoT. Reinforcement learning is widely employed in cybersecurity because it can learn on its own experience by investigating and capitalizing on the unknown ecosystem, this enables it solve many complex problems. The RL capabilities on dealing with cybercrime challenges are also exploited in this paper.",
        "link": "http://dx.doi.org/10.47760/ijcsmc.2021.v10i12.005"
    },
    {
        "id": 28896,
        "title": "Reinforcement Learning Based Refresh Optimized Volatile STT-RAM Cache",
        "authors": "Shashank Suman, Hemangee K. Kapoor",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isvlsi49217.2020.00048"
    },
    {
        "id": 28897,
        "title": "Distributed Reinforcement Learning Based Optimal Controller For Mobile Robot Formation",
        "authors": "Chinmay Shinde, Kaushik Das, Swagat Kumar, Laxmidhar Behera",
        "published": "2018-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc.2018.8550590"
    },
    {
        "id": 28898,
        "title": "Reinforcement learning in Connect 4 Game",
        "authors": "",
        "published": "2021-4-5",
        "citations": 0,
        "abstract": "Reinforcement learning allows a machine or software agent to learn its behaviour based on the response of the environment. It permits machine and software package agents to mechanically confirm the perfect behaviour in a very explicit context to maximise its performance. The distinction in reinforcement learning for supervised learning is that only partial feedback is given regarding the learner’s predictions. Beside, predictions can have long-term effects by affecting the future state of the controlled system. Thus, time plays a special role. The goal of reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithm’s qualifications and limitations. It is very interesting to learn reinforcement learning from a large number of useful practical applications from artificial intelligence problems to control engineering. In this project, we focus on those algorithms of reinforcement learning. Scaling the project looks at challenges for reinforcement learning in Connect4 game, together with a review of proposed solution methods. While this list has a game-centric approach and some items are specific to the game, a large part of this overview may also provide an understanding of other types of applications.",
        "link": "http://dx.doi.org/10.30534/ijatcse/2021/181022021"
    },
    {
        "id": 28899,
        "title": "Entropy-Aware Model Initialization for Effective Exploration In Deep Reinforcement Learning",
        "authors": "Sooyoung Jang, Hyung-Il Kim",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4047895"
    },
    {
        "id": 28900,
        "title": "Dynamic Scheduling for Multi-Objective Flexible Job Shop Via Deep Reinforcement Learning",
        "authors": "Erdong Yuan, Liejun Wang, Shiji Song, Shuli Cheng, Wei Fan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4696880"
    },
    {
        "id": 28901,
        "title": "Power Control in Internet of Drones by Deep Reinforcement Learning",
        "authors": "Jingjing Yao, Nirwan Ansari",
        "published": "2020-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc40277.2020.9148749"
    },
    {
        "id": 28902,
        "title": "Explainability of Deep Reinforcement Learning Method with Drones",
        "authors": "Ender Çetin, Cristina Barrado, Enric Pastor",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dasc58513.2023.10311156"
    },
    {
        "id": 28903,
        "title": "Efficient PAC Reinforcement Learning in Regular Decision Processes",
        "authors": "Alessandro Ronca, Giuseppe De Giacomo",
        "published": "2021-8",
        "citations": 0,
        "abstract": "Recently regular decision processes have been proposed as a well-behaved form of non-Markov decision process. Regular decision processes are characterised by a transition function and a reward function that depend on the whole history, though regularly (as in regular languages). In practice both the transition and the reward functions can be seen as finite transducers. We study reinforcement learning in regular decision processes. Our main contribution is to show that a near-optimal policy can be PAC-learned in polynomial time in a set of parameters that describe the underlying decision process. We argue that the identified set of parameters is minimal and it reasonably captures the difficulty of a regular decision process.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/279"
    },
    {
        "id": 28904,
        "title": "Research on Autonomous Decision-Making of UCAV Based on Deep Reinforcement Learning",
        "authors": "Linxiang Wang, Hongtao Wei",
        "published": "2022-5-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictc55111.2022.9778652"
    },
    {
        "id": 28905,
        "title": "Improving generalization ability in a puzzle game using reinforcement learning",
        "authors": "Hiroya Oonishi, Hitoshi Iima",
        "published": "2017-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2017.8080441"
    },
    {
        "id": 28906,
        "title": "Developing Monte Carlo Simulator of Reinforcement Learning Type",
        "authors": "Georgi Tsochev",
        "published": "2020-11-2",
        "citations": 0,
        "abstract": "Monte Carlo methods are a way to solve the reinforcement learning problem based on average test results. To ensure that well-defined results are available, Monte Carlo methods are used only for episodic tasks. The Monte Carlo term is often used more widely in any valuation method whose operation involves significant participation on a random basis. Here it is specifically used for methods based on the average of full results (as opposed to methods that are learned from incomplete results). The paper describes a simulator for estimating raindrops in a specific area using the package matlib. Keywords: Monte Carlo, reinforcement learning, simulation, matlib, python",
        "link": "http://dx.doi.org/10.7546/pecr.73.20.04"
    },
    {
        "id": 28907,
        "title": "A Reinforcement-learning Account of Tourette Syndrome",
        "authors": "T. Maia",
        "published": "2017-4",
        "citations": 0,
        "abstract": "BackgroundTourette syndrome (TS) has long been thought to involve dopaminergic disturbances, given the effectiveness of antipsychotics in diminishing tics. Molecular-imaging studies have, by and large, confirmed that there are specific alterations in the dopaminergic system in TS. In parallel, multiple lines of evidence have implicated the motor cortico-basal ganglia-thalamo-cortical (CBGTC) loop in TS. Finally, several studies demonstrate that patients with TS exhibit exaggerated habit learning. This talk will present a computational theory of TS that ties together these multiple findings.MethodsThe computational theory builds on computational reinforcement-learning models, and more specifically on a recent model of the role of the direct and indirect basal-ganglia pathways in learning from positive and negative outcomes, respectively.ResultsA model defined by a small set of equations that characterize the role of dopamine in modulating learning and excitability in the direct and indirect pathways explains, in an integrated way: (1) the role of dopamine in the development of tics; (2) the relation between dopaminergic disturbances, involvement of the motor CBGTC loop, and excessive habit learning in TS; (3) the mechanism of action of antipsychotics in TS; and (4) the psychological and neural mechanisms of action of habit-reversal training, the main behavioral therapy for TS.ConclusionsA simple computational model, thoroughly grounded on computational theory and basic-science findings concerning dopamine and the basal ganglia, provides an integrated, rigorous mathematical explanation for a broad range of empirical findings in TS.Disclosure of interestThe author has not supplied his declaration of competing interest.",
        "link": "http://dx.doi.org/10.1016/j.eurpsy.2017.01.083"
    },
    {
        "id": 28908,
        "title": "Vibration Suppression for Large-Scale Flexible Structures Using Deep Reinforcement Learning Based on Cable-Driven Parallel Robots",
        "authors": "Haining Sun, Xiaoqiang Tang, Jinhao Wei",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1115/1.0003804v"
    },
    {
        "id": 28909,
        "title": "Reinforcement Learning with Temporal Logic Constraints",
        "authors": "Bengt Lennartson, Qing-Shan Jia",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2021.04.044"
    },
    {
        "id": 28910,
        "title": "A Layered Reference Model for Penetration Testing with Reinforcement Learning and Attack Graphs",
        "authors": "Tyler Cody",
        "published": "2022-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/stc55697.2022.00015"
    },
    {
        "id": 28911,
        "title": "Towards Multi-agent Reinforcement Learning for Wireless Network Protocol Synthesis",
        "authors": "Hrishikesh Dutta, Subir Biswas",
        "published": "2021-1-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/comsnets51098.2021.9352858"
    },
    {
        "id": 28912,
        "title": "Global Routing Under a Congestion-Aware Reinforcement Learning Model",
        "authors": "Lin Li, Yici Cai, Qiang Zhou",
        "published": "2023-5-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iseda59274.2023.10218371"
    },
    {
        "id": 28913,
        "title": "Reinforcement learning applied to games",
        "authors": "João Crespo, Andreas Wichert",
        "published": "2020-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42452-020-2560-3"
    },
    {
        "id": 28914,
        "title": "Simulation and Comparison of Reinforcement Learning Algorithms",
        "authors": "Konstantinos Spyropoulos, Dionysios N. Sotiropoulos",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisa59645.2023.10345966"
    },
    {
        "id": 28915,
        "title": "Control of Shared Energy Storage Assets Within Building Clusters Using Reinforcement Learning",
        "authors": "Philip Odonkor, Kemper Lewis",
        "published": "2018-8-26",
        "citations": 5,
        "abstract": "This work leverages the current state of the art in reinforcement learning for continuous control, the Deep Deterministic Policy Gradient (DDPG) algorithm, towards the optimal 24-hour dispatch of shared energy assets within building clusters. The modeled DDPG agent interacts with a battery environment, designed to emulate a shared battery system. The aim here is to not only learn an efficient charged/discharged policy, but to also address the continuous domain question of how much energy should be charged or discharged. Experimentally, we examine the impact of the learned dispatch strategy towards minimizing demand peaks within the building cluster. Our results show that across the variety of building cluster combinations studied, the algorithm is able to learn and exploit energy arbitrage, tailoring it into battery dispatch strategies for peak demand shifting.",
        "link": "http://dx.doi.org/10.1115/detc2018-86094"
    },
    {
        "id": 28916,
        "title": "Modeling variation in empathic sensitivity using go/no-go social reinforcement learning",
        "authors": "Katherine O'Connell, Marissa Walsh, Brandon Padgett, Sarah Connell, Abigail Marsh",
        "published": "No Date",
        "citations": 1,
        "abstract": "Empathic experiences shape social behaviors and display considerable individual variation. Recent advances in computational behavioral modeling can help rigorously quantify individual differences, but remain understudied in the context of empathy and antisocial behavior. We adapted a go/no-go reinforcement learning task across social and non-social contexts such that monetary gains and losses explicitly impacted the subject, a study partner, or no one. Empathy was operationalized as sensitivity to others’ rewards, sensitivity to others’ losses, and as the Pavlovian influence of empathic outcomes on approach and avoidance behavior. Results showed that 61 subjects learned for a partner in a way that was computationally similar to how they learned for themselves. Results supported the psychometric value of individualized model parameters such as sensitivity to others’ loss, which was inversely associated with antisociality. Modeled empathic sensitivity also mapped onto motivation ratings, but was not associated with self-reported trait empathy. This work is the first to apply a social reinforcement learning task that spans affect and action requirement (go/no-go) to measure multiple facets of empathic sensitivity.",
        "link": "http://dx.doi.org/10.31234/osf.io/drcq6"
    },
    {
        "id": 28917,
        "title": "Towards High-Level Intrinsic Exploration in Reinforcement Learning",
        "authors": "Nicolas Bougie, Ryutaro Ichise",
        "published": "2020-7",
        "citations": 2,
        "abstract": "Deep reinforcement learning (DRL) methods traditionally struggle with tasks where environment rewards are sparse or delayed, which entails that exploration remains one of the key challenges of DRL. Instead of solely relying on extrinsic rewards, many state-of-the-art methods use intrinsic curiosity as exploration signal. While they hold promise of better local exploration, discovering global exploration strategies is beyond the reach of current methods. We propose a novel end-to-end intrinsic reward formulation that introduces high-level exploration in reinforcement learning. Our curiosity signal is driven by a fast reward that deals with local exploration and a slow reward that incentivizes long-time horizon exploration strategies. We formulate curiosity as the error in an agent’s ability to reconstruct the observations given their contexts. Experimental results show that this high-level exploration enables our agents to outperform prior work in several Atari games.",
        "link": "http://dx.doi.org/10.24963/ijcai.2020/733"
    },
    {
        "id": 28918,
        "title": "Rule-Based System Against Reinforcement Learning*",
        "authors": "Bozhan Orozov, Daniela Orozova",
        "published": "2021-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3472410.3472437"
    },
    {
        "id": 28919,
        "title": "Model-Free Reinforcement Learning-Based Control for Continuous-Time Systems",
        "authors": "Kyriakos G. Vamvoudakis",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4471-5102-9_100065-1"
    },
    {
        "id": 28920,
        "title": "Further Exploration and Next Steps",
        "authors": "Nimish Sanghi",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-6809-4_10"
    },
    {
        "id": 28921,
        "title": "Reinforcement Learning Based Multi-Access Control with Energy Harvesting",
        "authors": "Man Chu, Hang Li, Xuewen Liao, Shuguang Cui",
        "published": "2018-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/glocom.2018.8647438"
    },
    {
        "id": 28922,
        "title": "Dynamic Channel Access and Power Control via Deep Reinforcement Learning",
        "authors": "Ziyang Lu, M. Cenk Gursoy",
        "published": "2019-9",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtcfall.2019.8891391"
    },
    {
        "id": 28923,
        "title": "Reinforcement Learning for Multi-Well SAGD Optimization: A Policy Gradient Approach",
        "authors": "J. L. Guevara, J. Trivedi",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "Abstract\nFinding an optimal steam injection strategy for a SAGD process is considered a major challenge due to the complex dynamics of the physical phenomena. Recently, reinforcement learning (RL) has been presented as alternative to conventional methods (e.g., adjoint-optimization, model predictive control) as an effective way to address the cited challenge. In general, RL represents a model-free strategy where an agent is trained to find the optimal policy - the action at every time step that will maximize cumulative long-term performance of a given process- only by continuous interactions with the environment (e.g., SAGD process). This environment is modeled as a Markov-Decision-Process (MDP) and a state must be defined to characterize it. During the interaction process, at each time step, the agent executes an action, receives a scalar reward (e.g., net present value) due to the action taken and observes the new state (e.g., pressure distribution of the reservoir) of the environment. This process continuous for a number of simulations or episodes until convergence is achieved. One approach to solve the RL problem is to parametrize the policy using well-known methods, e.g., linear functions, SVR, neural networks, etc. This approach is based on maximizing the performance of the process with respect to the parameters of the policy. Using the Monte Carlo algorithm, after every episode a long-term performance of the process is obtained and the parameters of the policy are updated using gradient-ascent methods. In this work policy gradient is used to find the steam injection policy that maximizes cumulative net present value of a SAGD process. The environment is represented by a reservoir simulation model inspired by northern Alberta reservoir and the policy is parametrized using a deep neural network. Results show that optimal steam injection can be characterized in two regions: 1) an increase or slight increase of steam injection rates, and 2) a sharp decrease until reaching the minimum value. Furthermore, the first region's objective appears to be more of pressure maintenance using high steam injection rates. In the second region, the objective is to collect more reward or achieve high values of daily net present value due to the reduction of steam injection while keeping high oil production values.",
        "link": "http://dx.doi.org/10.2118/213104-ms"
    },
    {
        "id": 28924,
        "title": "Prioritization Techniques for Android Test Suites Generated by a Reinforcement Learning Algorithm",
        "authors": "Md Khorrom Khan, Ryan Michaels, Farhan Rahman Arnob, Renée Bryce",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4450321"
    },
    {
        "id": 28925,
        "title": "Pseudo Random Number Generation through Reinforcement Learning and Recurrent Neural Networks",
        "authors": "Luca Pasqualini, Maurizio Parton",
        "published": "2020-11-23",
        "citations": 2,
        "abstract": "A Pseudo-Random Number Generator (PRNG) is any algorithm generating a sequence of numbers approximating properties of random numbers. These numbers are widely employed in mid-level cryptography and in software applications. Test suites are used to evaluate the quality of PRNGs by checking statistical properties of the generated sequences. These sequences are commonly represented bit by bit. This paper proposes a Reinforcement Learning (RL) approach to the task of generating PRNGs from scratch by learning a policy to solve a partially observable Markov Decision Process (MDP), where the full state is the period of the generated sequence, and the observation at each time-step is the last sequence of bits appended to such states. We use Long-Short Term Memory (LSTM) architecture to model the temporal relationship between observations at different time-steps by tasking the LSTM memory with the extraction of significant features of the hidden portion of the MDP’s states. We show that modeling a PRNG with a partially observable MDP and an LSTM architecture largely improves the results of the fully observable feedforward RL approach introduced in previous work.",
        "link": "http://dx.doi.org/10.3390/a13110307"
    },
    {
        "id": 28926,
        "title": "PokerBot: Hand Strength Reinforcement Learning",
        "authors": "Angela Ramirez, Solomon Reinman, Narges Norouzi",
        "published": "2019-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/inista.2019.8778267"
    },
    {
        "id": 28927,
        "title": "Reinforcement Learning for Smart Charging of Electric Buses in Smart Grid",
        "authors": "Wenzhuo Chen, Peng Zhuang, Hao Liang",
        "published": "2019-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9014160"
    },
    {
        "id": 28928,
        "title": "Reinforcement Learning Method with Internal World Model Training",
        "authors": "Kenji Hirata, Hiroyuki Iizuka, Masahito Yamamoto",
        "published": "2020-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sii46433.2020.9026225"
    },
    {
        "id": 28929,
        "title": "Planning with a Model: AlphaZero",
        "authors": "Michael Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-9606-6_14"
    },
    {
        "id": 28930,
        "title": "Deep Reinforcement Learning Based Collision Avoidance System for Autonomous Ships",
        "authors": "Yong Wang, Haixiang Xu, Hui Feng, Jianhua He, Haojie Yang, Lian Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4566668"
    },
    {
        "id": 28931,
        "title": "Deep Reinforcement Learning based Resource Allocation in NOMA",
        "authors": "N. Iswarya, R. Venkateswari",
        "published": "2022-9-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciiet55458.2022.9967604"
    },
    {
        "id": 28932,
        "title": "Reinforcement Learning Based Autonomous Vehicles Lateral Control",
        "authors": "Eslam Mahmoud, Dalil Ichalal, Saïd Mammar",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnsc58704.2023.10318986"
    },
    {
        "id": 28933,
        "title": "Sentence Selective Neural Extractive Summarization with Reinforcement Learning",
        "authors": "Laifu Chen, Minh Le Nguyen",
        "published": "2019-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/kse.2019.8919490"
    },
    {
        "id": 28934,
        "title": "A Data-Driven Choice of Misfit Function for FWI Using Reinforcement Learning",
        "authors": "B. Sun, T. Alkhalifah",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202010203"
    },
    {
        "id": 28935,
        "title": "Building load management clusters using reinforcement learning",
        "authors": "Shamwilu Ahmed, Francois Bouffard",
        "published": "2017-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iemcon.2017.8117147"
    },
    {
        "id": 28936,
        "title": "A Practical Deep Reinforcement Learning Approach to Semiconductor Equipment Scheduling",
        "authors": "Changhee Lee, Sunghee Lee",
        "published": "2021-3-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit46573.2021.9453533"
    },
    {
        "id": 28937,
        "title": "State Complexity Reduction in Reinforcement Learning based Adaptive Traffic Signal Control",
        "authors": "Mladen Miletic, Kresimir Kusic, Martin Greguric, Edouard Ivanjko",
        "published": "2020-9",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elmar49956.2020.9219024"
    },
    {
        "id": 28938,
        "title": "Channel Pruning via Lookahead Search Guided Reinforcement Learning",
        "authors": "Zi Wang, Chengcheng Li",
        "published": "2022-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wacv51458.2022.00357"
    },
    {
        "id": 28939,
        "title": "Multi-Agent Deep Reinforcement Learning-Based Maintenance Optimization for Multi-Dependent Component Systems",
        "authors": "Van Thai Nguyen, Phuc Do, Alexandre Voisin, benoit IUNG",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4496792"
    },
    {
        "id": 28940,
        "title": "Low-Latency Virtual Network Function Scheduling Algorithm Based on Deep Reinforcement Learning",
        "authors": "Zhiwei Liu, Zhaogang Shu, Shuwu Chen, Yiwen Zhong, Jiaxiang Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4563908"
    },
    {
        "id": 28941,
        "title": "A Lunar Robot Obstacle Avoidance Planning Method Using Deep Reinforcement Learning for Data Fusion",
        "authors": "Ruijun Hu, Zhaokui Wang",
        "published": "2019-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac48633.2019.8997266"
    },
    {
        "id": 28942,
        "title": "Multi-Agent Reinforcement Learning for Dynamic Spectrum Access",
        "authors": "Huijuan Jiang, Tianyu Wang, Shaowei Wang",
        "published": "2019-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2019.8761786"
    },
    {
        "id": 28943,
        "title": "Multi-Constraints Guidance and Maneuvering Penetration Strategy via Meta Deep Reinforcement Learning",
        "authors": "Sibo Zhao, JianWen Zhu, Weimin Bao, Xiaoping Li, Haifeng Sun",
        "published": "No Date",
        "citations": 1,
        "abstract": "In response to the issue of vehicle escape guidance, this manuscript proposes a unified intelligent control strategy synthesizing optimal guidance and Meta Deep Reinforcement Learning (DRL). Optimal control with minor energy consumption is introduced to meet terminal latitude, longitude and altitude. Maneuvering escape is realized by adding longitudinal and lateral direction maneuver overloads. Maneuver command decision model is calculated based on Soft-Actor-Critic (SAC) networks. Meta learning is introduced to enhance autonomous escape capability, which improves generalization performance to time-varying scenarios not encountered in the training process. In order to obtain training samples at a faster speed, this manuscript uses the prediction method to solve reward values, which avoiding a large number of numerical integration. The simulation results manifest that the proposed intelligent strategy can achieve high precise guidance and effective escape.",
        "link": "http://dx.doi.org/10.20944/preprints202308.1512.v1"
    },
    {
        "id": 28944,
        "title": "Harnessing Deep Reinforcement Learning to Construct Time-Dependent Optimal Fields for Quantum Control Dynamics",
        "authors": "Yuanqi Gao, Xian Wang, Nanpeng Yu, Bryan Wong",
        "published": "No Date",
        "citations": 0,
        "abstract": "We present an efficient deep reinforcement learning (DRL) approach to automatically construct time-dependent optimal control fields that enable desired transitions in reduced-dimensional chemical systems. Our DRL approach gives impressive performance in autonomously and efficiently constructing optimal control fields, even for cases that are difficult to converge with existing gradient-based approaches. We provide a detailed description of the algorithms and hyperparameters as well as performance metrics for our DRL-based approach. Our results demonstrate that DRL can be employed as an effective artificial intelligence approach to efficiently and autonomously design control fields in continuous quantum dynamical chemical systems.",
        "link": "http://dx.doi.org/10.26434/chemrxiv-2022-kh0jk"
    },
    {
        "id": 28945,
        "title": "Flexibility in valenced reinforcement learning computations across development",
        "authors": "Kate Nussenbaum, Juan A. Velez, Bradli T. Washington, Hannah E. Hamling, Catherine A. Hartley",
        "published": "No Date",
        "citations": 1,
        "abstract": "Optimal integration of positive and negative outcomes during learning varies depending on an environment’s reward statistics. The present study investigated the extent to which children, adolescents, and adults (N = 142 8 - 25 year-olds, 55% female, 42% White, 31% Asian, 17% mixed race, and 8% Black) adapt their weighting of better-than-expected and worse-than-expected outcomes when learning from reinforcement. Participants made a series of choices across two contexts: one in which weighting positive outcomes more heavily than negative outcomes led to better performance, and one in which the reverse was true. Reinforcement learning modeling revealed that across age, participants shifted their valence biases in accordance with the structure of the environment. Exploratory analyses revealed increases in context-dependent flexibility with age.",
        "link": "http://dx.doi.org/10.31234/osf.io/5f9uc"
    },
    {
        "id": 28946,
        "title": "Permutation Invariant Agent-Specific Centralized Critic in Multi-Agent Reinforcement Learning",
        "authors": "Patsornchai Noppakun, Khajonpong Akkarajitsakul",
        "published": "2022-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/incit56086.2022.10067429"
    },
    {
        "id": 28947,
        "title": "Reinforcement Learning Based Resource Allocation for Network Slicing in 5G C-RAN",
        "authors": "Xiaofei Wang, Tiankui Zhang",
        "published": "2019-10",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/comcomap46287.2019.9018774"
    },
    {
        "id": 28948,
        "title": "Canonical Problems and Applications",
        "authors": "",
        "published": "2022-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119815068.ch2"
    },
    {
        "id": 28949,
        "title": "Distributed Intelligence for Dynamic Task Migration in the 6G User Plane using Deep Reinforcement Learning",
        "authors": "Sayantini Majumdar, Susanna Schwarzmann, Riccardo Trivisonno, Georg Carle",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In-Network Computing (INC) is a currently emerging paradigm. Realizing INC in 6G networks could mean that user plane entities (UPEs) carry out computations on packets while transmitting them. These computations may have specific requirements in terms of their completion time. In case of high compute pressure at one UPE, migrating computations to another UPE may be beneficial, in order to avoid exceeding the completion time requirement. Centralized migration approaches suffer from increased signaling and are prone to react too slow. Therefore, this paper investigates the applicability of distributed intelligence to tackle the problem of compute task migration in the 6G user plane. Each UPE is equipped with an intelligent agent, enabling autonomous decisions on whether computations should be migrated to another UPE. To enable the intelligent agents to learn and apply an optimal task migration policy, we investigate and compare of two state-of-the-art Deep Reinforcement Learning (DRL) approaches: Advantage Actor-Critic (A2C) and Double Deep Q-Network (DDQN). We show, via simulations, that the performance of both solutions, in terms of the percentage of tasks exceeding their completion time requirement, is near-optimal and training A2C is at least 60% faster. </p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24459487.v1"
    },
    {
        "id": 28950,
        "title": "Requirements Business Value and Test Case Attributes based Test Case Prioritization Using Deep Reinforcement Learning.",
        "authors": "Tamim Ahmed Khan, Zobia Jabeen Akhtar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTesting is a cost-intensive process whereas regression testing is used for the purpose of finding out if there is a regression in a post-evolution or post-maintenance scenario. It is expensive to run all test cases for regression testing and therefore we require a test case selection strategy so that we may select a subset of test cases from a previously available set of all test cases. Test case prioritization is a mechanism that can be used as a test subset selection strategy for regression testing. We investigate how requirements priority and importance as well as test case parameters such as fault severity, fault count, and their inter-dependency can be used for proposing a reward function to improve results. We use a deep reinforcement learning approach to optimize or prioritize the test cases using our reward function. We make use of an off-policy, model-free, and value-based test case prioritization approach considering a deep Q-Network (DQN) agent and a reward system based on a business value of test cases that prioritize the test cases against those business values. We use performance metrics to measure the performance and effectiveness of our research. We have compared our results with the published research and conclude that the results achieved from our approach outperform the previously published results of state-of-the-art studies.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3497933/v1"
    },
    {
        "id": 28951,
        "title": "RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning",
        "authors": "Dongsheng Zuo, Yikang Ouyang, Yuzhe Ma",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dac56929.2023.10247941"
    },
    {
        "id": 28952,
        "title": "Is Reinforcement Learning a Panacea for Solving All Contingencies in UAVs?",
        "authors": "Chiman Kwan",
        "published": "2019-12-6",
        "citations": 0,
        "abstract": "We would like to point out that reinforcement learning (RL) needs to be used with caution in contingency planning for UAVs. A more practical approach is proposed in this opinion.",
        "link": "http://dx.doi.org/10.19080/raej.2019.05.555655"
    },
    {
        "id": 28953,
        "title": "Autonomous Car Parking System using Deep Reinforcement Learning",
        "authors": "Rikuya Takehara, Tad Gonsalves",
        "published": "2021-9-23",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icitech50181.2021.9590169"
    },
    {
        "id": 28954,
        "title": "An inductive bias for slowly changing features in human reinforcement learning",
        "authors": "Noa L. Hedrich, Eric Schulz, Sam Hall-McMaster, Nicolas W. Schuck",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractIdentifying goal-relevant features in novel environments is a central challenge for efficient behaviour. We asked whether humans address this challenge by relying on prior knowledge about common properties of reward-predicting features. One such property is the rate of change of features, given that behaviourally relevant processes tend to change on a slower timescale than noise. Hence, we asked whether humans are biased to learn more when task-relevant features are slow rather than fast. To test this idea, 100 human participants were asked to learn the rewards of two-dimensional bandits when either a slowly or quickly changing feature of the bandit predicted reward. Participants accrued more reward and achieved better generalisation to unseen feature values when a bandit’s relevant feature changed slowly, and its irrelevant feature quickly, as compared to the opposite. Participants were also more likely to incorrectly base their choices on the irrelevant feature when it changed slowly versus quickly. These effects were stronger when participants experienced the feature speed before learning about rewards. Modelling this behaviour with a set of four function approximation Kalman filter models that embodied alternative hypotheses about how feature speed could affect learning revealed that participants had a higher learning rate for the slow feature, and adjusted their learning to both the relevance and the speed of feature changes. The larger the improvement in participants’ performance for slow compared to fast bandits, the more strongly they adjusted their learning rates. These results provide evidence that human reinforcement learning favours slower features, suggesting a bias in how humans approach reward learning.Author SummaryLearning experiments in the laboratory are often assumed to exist in a vacuum, where participants solve a given task independently of how they learn in more natural circumstances. But humans and other animals are in fact well known to “meta learn”, i.e. to leverage generalisable assumptions abouthow to learnfrom other experiences. Taking inspiration from a well-known machine learning technique known as slow feature analysis, we investigated one specific instance of such an assumption in learning: the possibility that humans tend to focus on slowly rather than quickly changing features when learning about rewards. To test this, we developed a task where participants had to learn the value of stimuli composed of two features. Participants indeed learned better from a slowly rather than quickly changing feature that predicted reward and were more distracted by the reward-irrelevant feature when it changed slowly. Computational modelling of participant behaviour indicated that participants had a higher learning rate for slowly changing features from the outset. Hence, our results support the idea that human reinforcement learning reflects a priori assumptions about the reward structure in natural environments.",
        "link": "http://dx.doi.org/10.1101/2024.01.24.576910"
    },
    {
        "id": 28955,
        "title": "Robust Control of An Inverted Pendulum System Based on Policy Iteration in Reinforcement Learning",
        "authors": "Xu Dengguo, Ma Yan, Huang Jiashun, Li Yahui",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper is primarily focused on the robust control of an inverted pendulum system based \r\non the policy iteration in reinforcement learning. First, a mathematical model of the single inverted \r\npendulum system is established through a force analysis of the pendulum and trolley. Second,  \r\nbased on the theory of robust optimal control, the robust control of the uncertain linear inverted  \r\npendulum system is transformed into an optimal control problem with an appropriate performance  \r\nindex. Moreover, for the uncertain linear and nonlinear systems, two reinforcement-learning control  \r\nalgorithms are proposed using the policy iteration method. Finally, two numerical examples are  \r\nprovided to validate the reinforcement learning algorithms for the robust control of the inverted  \r\npendulum systems.",
        "link": "http://dx.doi.org/10.20944/preprints202310.1100.v1"
    },
    {
        "id": 28956,
        "title": "Deep Reinforcement Learning for Intelligent Penetration Testing Path Design",
        "authors": "Junkai Yi, Xiaoyan Liu",
        "published": "2023-8-21",
        "citations": 1,
        "abstract": "Penetration testing is an important method to evaluate the security degree of a network system. The importance of penetration testing attack path planning lies in its ability to simulate attacker behavior, identify vulnerabilities, reduce potential losses, and continuously improve security strategies. By systematically simulating various attack scenarios, it enables proactive risk assessment and the development of robust security measures. To address the problems of inaccurate path prediction and difficult convergence in the training process of attack path planning, an algorithm which combines attack graph tools (i.e., MulVAL, multi-stage vulnerability analysis language) and the double deep Q network is proposed. This algorithm first constructs an attack tree, searches paths in the attack graph, and then builds a transfer matrix based on depth-first search to obtain all reachable paths in the target system. Finally, the optimal path for target system attack path planning is obtained by using the deep double Q network (DDQN) algorithm. The MulVAL double deep Q network(MDDQN) algorithm is tested in different scale penetration testing environments. The experimental results show that, compared with the traditional deep Q network (DQN) algorithm, the MDDQN algorithm is able to reach convergence faster and more stably and improve the efficiency of attack path planning.",
        "link": "http://dx.doi.org/10.3390/app13169467"
    },
    {
        "id": 28957,
        "title": "On Computational Models of Theory of Mind and the Imitative Reinforcement Learning in Spiking Neural Networks",
        "authors": "Mohammad Ganjtabesh, Ashena G. Mohammadi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTheory of Mind is referred to the ability of inferring other's mental states, and it plays a crucial role in social cognition and learning. Biological evidences indicate that complex circuits are involved in this ability, including the mirror neuron system. The mirror neuron system influences imitation abilities and action understanding, leading to learn through observing others. To simulate this imitative learning behavior, a Theory-of-Mind-based Imitative Reinforcement Learning (ToM-based ImRL) framework is proposed. Employing the bio-inspired spiking neural networks and the mechanisms of the mirror neuron system, ToM-based ImRL is a bio-inspired computational model which enables an agent to effectively learn how to act in an interactive environment through observing an expert, inferring its goals, and imitating its behaviors. The aim of this paper is to review some computational attempts in modeling ToM and to explain the proposed ToM-based ImRL framework which is tested in the environment of River Raid game from Atari 2600 series.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3341817/v1"
    },
    {
        "id": 28958,
        "title": "5G Network Slice Admission Control Using Optimization and Reinforcement Learning",
        "authors": "Md Ariful Haque, Vassilka Kirova",
        "published": "2022-4-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc51071.2022.9771643"
    },
    {
        "id": 28959,
        "title": "On-Policy Deep Reinforcement Learning Approach to Multi Agent Problems",
        "authors": "Ziya Tan, Mehmet Karakose",
        "published": "2021-9-14",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003202240-58"
    },
    {
        "id": 28960,
        "title": "Transfer Reinforcement Learning under Unobserved Contextual Information",
        "authors": "Yan Zhang, Michael M. Zavlanos",
        "published": "2020-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccps48487.2020.00015"
    },
    {
        "id": 28961,
        "title": "Reinforcement Learning for Mobile Robot Navigation: An overview",
        "authors": "Nessrine Khlif, Nahla Khraief, Safya Belghith",
        "published": "2022-7-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsis56166.2022.10118362"
    },
    {
        "id": 28962,
        "title": "Inclined Quadrotor Landing using Deep Reinforcement Learning",
        "authors": "Jacob E. Kooi, Robert Babuska",
        "published": "2021-9-27",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros51168.2021.9636096"
    },
    {
        "id": 28963,
        "title": "A Policy Search Method For Temporal Logic Specified Reinforcement Learning Tasks",
        "authors": "Xiao Li, Yao Ma, Calin Belta",
        "published": "2018-6",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2018.8431181"
    },
    {
        "id": 28964,
        "title": "Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Yumeya Yamamori, Oliver J Robinson, Jonathan P Roiser",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractAlthough avoidance is a prevalent feature of anxiety-related psychopathology, differences in the measurement of avoidance between humans and non-human animals hinder our progress in its theoretical understanding and treatment. To address this, we developed a novel translational measure of anxiety-related avoidance in the form of an approach-avoidance reinforcement learning task, by adapting a paradigm from the non-human animal literature to study the same cognitive processes in human participants. We used computational modelling to probe the putative cognitive mechanisms underlying approach-avoidance behaviour in this task and investigated how they relate to subjective task-induced anxiety. In a large online study (n = 372), participants who experienced greater task-induced anxiety avoided choices associated with punishment, even when this resulted in lower overall reward. Computational modelling revealed that this effect was explained by greater individual sensitivities to punishment relative to rewards. We replicated these findings in an independent sample (n = 627) and we also found fair-to-excellent reliability of measures of task performance in a sub-sample retested one week later (n = 57). Our findings demonstrate the potential of approach-avoidance reinforcement learning tasks as translational and computational models of anxiety-related avoidance. Future studies should assess the predictive validity of this approach in clinical samples and experimental manipulations of anxiety.",
        "link": "http://dx.doi.org/10.1101/2023.04.04.535526"
    },
    {
        "id": 28965,
        "title": "The segregation of vocal circuits solves a credit assignment problem associated with multi-objective reinforcement learning",
        "authors": "Don Murdoch, Ruidong Chen, Jesse Goldberg",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMotor circuits vary in topographic organization, ranging from a coarse relationship between neuron location and function to highly localized regions controlling specific behaviors. For unclear reasons, vocal learning circuits lie at this second extreme: they repeatedly evolved to be spatially segregated from other parts of the motor system. Here we show that spatially segregated motor circuits can solve a specific problem that arises when an animal tries to learn two things at once. We trained songbirds in vocal and place learning paradigms with brief strobe light flashes and noise bursts. Strobe light negatively reinforced place learning but did not affect song syllable learning. Noise bursts positively reinforced place preference but negatively reinforced syllable learning. These double dissociations indicate that vocalization-related reinforcement signals specifically target the vocal motor system, while place-related reinforcement signals specifically target the navigation system. Non-global, target-specific reinforcement signals have established utility in machine implementation of multi-objective learning. In vocal learners, such signals could enable an animal to practice vocalizing as it does other things such as forage for food or learn to walk.",
        "link": "http://dx.doi.org/10.1101/236273"
    },
    {
        "id": 28966,
        "title": "Computational characteristics of the striatal dopamine system described by reinforcement learning with fast generalization",
        "authors": "Yoshihisa Fujita, Sho Yagishita, Haruo Kasai, Shin Ishii",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractGeneralization enables applying past experience to similar but nonidentical situations. Therefore, it may be essential for adaptive behaviors. Recent neurobiological observation indicates that the striatal dopamine system achieves generalization and subsequent discrimination by updating corticostriatal synaptic connections in differential response to reward and punishment. To analyze how the computational characteristics in this system affect behaviors, we proposed a novel reinforcement learning model with multilayer neural networks in which the synaptic weights of only the last layer are updated according to the prediction error. We set fixed connections between the input and hidden layers so as to maintain the similarity of inputs in the hidden-layer representation. This network enabled fast generalization, and thereby facilitated safe and efficient exploration in reinforcement learning tasks, compared to algorithms which do not show generalization. However, disturbance in the network induced aberrant valuation. In conclusion, the unique computation suggested by corticostriatal plasticity has the advantage of providing safe and quick adaptations to unknown environments, but on the other hand has the potential defect which can induce maladaptive behaviors like delusional symptoms of psychiatric disorders.Author summaryThe brain has an ability to generalize knowledge obtained from reward- and punishment-related learning. Animals that have been trained to associate a stimulus with subsequent reward or punishment respond not only to the same stimulus but also to resembling stimuli. How does generalization affect behaviors in situations where individuals are required to adapt to unknown environments? It may enable efficient learning and promote adaptive behaviors, but inappropriate generalization may disrupt behaviors by associating reward or punishment with irrelevant stimuli. The effect of generalization here should depend on computational characteristics of underlying biological basis in the brain, namely, the striatal dopamine system. In this research, we made a novel computational model based on the characteristics of the striatal dopamine system. Our model enabled fast generalization and showed its advantage of providing safe and quick adaptation to unknown environments. By contrast, disturbance of our model induced abnormal behaviors. The results suggested the advantage and the shortcoming of generalization by the striatal dopamine system.",
        "link": "http://dx.doi.org/10.1101/2019.12.12.873950"
    },
    {
        "id": 28967,
        "title": "Cooperative reinforcement learning for multiple units combat in starCraft",
        "authors": "Kun Shao, Yuanheng Zhu, Dongbin Zhao",
        "published": "2017-11",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci.2017.8280949"
    },
    {
        "id": 28968,
        "title": "Reinforcement learning strategies for vessel navigation",
        "authors": "Andrius Daranda, Gintautas Dzemyda",
        "published": "2022-11-24",
        "citations": 3,
        "abstract": "Safe navigation at sea is more important than ever. Cargo is usually transported by vessel because it makes economic sense. However, marine accidents can cause huge losses of people, cargo, and the vessel itself, as well as irreversible ecological disasters. These are the reasons to strive for safe vessel navigation. The navigator shall ensure safe vessel navigation. He must plan every maneuver and act safely. At the same time, he must evaluate and predict the actions of other vessels in dense maritime traffic. This is a complicated process and requires constant human concentration. It is a very tiring and long-lasting duty. Therefore, human error is the main reason of collisions between vessels. In this paper, different reinforcement learning strategies have been explored in order to find the most appropriate one for the real-life problem of ensuring safe maneuvring in maritime traffic. An experiment using different algorithms was conducted to discover a suitable method for autonomous vessel navigation. The experiments indicate that the most effective algorithm (Deep SARSA) allows reaching 92.08% accuracy. The efficiency of the proposed model is demonstrated through a real-life collision between two vessels and how it could have been avoided.",
        "link": "http://dx.doi.org/10.3233/ica-220688"
    },
    {
        "id": 28969,
        "title": "Approach-avoidance reinforcement learning as a translational and computational model of anxiety-related avoidance",
        "authors": "Yumeya Yamamori, Oliver J Robinson, Jonathan P Roiser",
        "published": "No Date",
        "citations": 0,
        "abstract": "Although avoidance is a prevalent feature of anxiety-related psychopathology, differences in existing measures of avoidance between humans and non-human animals impede progress in its theoretical understanding and treatment. To address this, we developed a novel translational measure of anxiety-related avoidance in the form of an approach-avoidance reinforcement learning task, by adapting a paradigm from the non-human animal literature to study the same cognitive processes in human participants. We used computational modelling to probe the putative cognitive mechanisms underlying approach-avoidance behaviour in this task and investigated how they relate to subjective task-induced anxiety. In a large online study, participants (n = 372) who experienced greater task-induced anxiety avoided choices associated with punishment, even when this resulted in lower overall reward. Computational modelling revealed that this effect was explained by greater individual sensitivities to punishment relative to rewards. We replicated these findings in an independent sample (n = 627) and we also found fair-to-excellent reliability of measures of task performance in a sub-sample retested one week later (n = 57). Our findings demonstrate the potential of approach-avoidance reinforcement learning tasks as translational and computational models of anxiety-related avoidance. Future studies should assess the predictive validity of this approach in clinical samples and experimental manipulations of anxiety.",
        "link": "http://dx.doi.org/10.7554/elife.87720.2"
    },
    {
        "id": 28970,
        "title": "Optimizing Reinforcement Learning-Based Visual Navigation for Resource-Constrained Devices",
        "authors": "U. Vijetha, V. Geetha",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3323801"
    },
    {
        "id": 28971,
        "title": "Application of Deep Reinforcement Learning in Werewolf Game Agents",
        "authors": "Tianhe Wang, Tomoyuki Kaneko",
        "published": "2018-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/taai.2018.00016"
    },
    {
        "id": 28972,
        "title": "Reinforcement Learning for Long-term Reward Optimization in Recommender Systems",
        "authors": "Anton Dorozhko",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sibircon48586.2019.8958202"
    },
    {
        "id": 28973,
        "title": "FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance",
        "authors": "Xiao-Yang Liu",
        "published": "No Date",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3737257"
    },
    {
        "id": 28974,
        "title": "Task-Oriented Query Reformulation with Reinforcement Learning",
        "authors": "Rodrigo Nogueira, Kyunghyun Cho",
        "published": "2017",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d17-1061"
    },
    {
        "id": 28975,
        "title": "Efficient hyperparameter optimization through model-based reinforcement learning",
        "authors": "Jia Wu, SenPeng Chen, XiYuan Liu",
        "published": "2020-10",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2020.06.064"
    },
    {
        "id": 28976,
        "title": "Application of Reinforcement Learning in SINS/GNSS/DVL Integrated Navigation",
        "authors": "Xiangyuan Li, Xuesong Tang, Xingshu Wang, Wenfeng Tan, Jiaxing Zheng, Yingwei Zhao",
        "published": "2022-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlccim55934.2022.00066"
    },
    {
        "id": 28977,
        "title": "Deep reinforcement learning collision avoidance using policy gradient optimisation and Q-learning",
        "authors": "Shady A. Maged, Bishoy H. Mikhail",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijcvr.2020.107253"
    },
    {
        "id": 28978,
        "title": "Multiagent Coordination Systems Based on Neuro-Fuzzy Models with Reinforcement Learning",
        "authors": "Leonardo Alfredo Mendoza, Evelyn Batista, Harold Dias De Mello, Marco Aurelio Pacheco",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00151"
    },
    {
        "id": 28979,
        "title": "A Study on Application of Curriculum Learning in Deep Reinforcement Learning : Action Acquisition in Shooting Game AI as Example",
        "authors": "Ikumi Kodaka, Fumiaki Saitoh",
        "published": "2021-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwcia52852.2021.9626020"
    },
    {
        "id": 28980,
        "title": "Implementation of Reinforcement Learning Simulated Madel on Physical UGV Using Robot Operating System for Continual Learning",
        "authors": "Edgar M. Perez, Abhijit Majumdar, Patrick Benavidez, Mo Jamshidi",
        "published": "2019-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sysose.2019.8753801"
    },
    {
        "id": 28981,
        "title": "Deep Reinforcement Learning for Optimization",
        "authors": "Md Mahmudul Hasan, Md Shahinur Rahman, Adrian Bell",
        "published": "2019",
        "citations": 1,
        "abstract": "Deep reinforcement learning (DRL) has transformed the field of artificial intelligence (AI) especially after the success of Google DeepMind. This branch of machine learning epitomizes a step toward building autonomous systems by understanding of the visual world. Deep reinforcement learning (RL) is currently applied to different sorts of problems that were previously obstinate. In this chapter, at first, the authors started with an introduction of the general field of RL and Markov decision process (MDP). Then, they clarified the common DRL framework and the necessary components RL settings. Moreover, they analyzed the stochastic gradient descent (SGD)-based optimizers such as ADAM and a non-specific multi-policy selection mechanism in a multi-objective Markov decision process. In this chapter, the authors also included the comparison for different Deep Q networks. In conclusion, they describe several challenges and trends in research within the deep reinforcement learning field. ",
        "link": "http://dx.doi.org/10.4018/978-1-5225-7862-8.ch011"
    },
    {
        "id": 28982,
        "title": "Coordinated Voltage Regulation of Microgrid Clusters Based on Deep Reinforcement Learning Approach",
        "authors": "Xiaozhe Xue, Hui Ge",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10166110"
    },
    {
        "id": 28983,
        "title": "When Does Communication Learning Need Hierarchical Multi-Agent Deep Reinforcement Learning",
        "authors": "Marie Ossenkopf, Mackenzie Jorgensen, Kurt Geihs",
        "published": "2019-11-17",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/01969722.2019.1677335"
    },
    {
        "id": 28984,
        "title": "Correction: Optimum trajectory learning in musculoskeletal systems with model predictive control and deep reinforcement learning",
        "authors": "Berat Denizdurduran, Henry Markram, Marc-Oliver Gewaltig",
        "published": "2022-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00422-022-00949-2"
    },
    {
        "id": 28985,
        "title": "Developing multi-agent adversarial environment using reinforcement learning and imitation learning",
        "authors": "Ziyao Han, Yupeng Liang, Kazuhiro Ohkura",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10015-023-00912-9"
    },
    {
        "id": 28986,
        "title": "Learning to Grasp on the Moon from 3D Octree Observations with Deep Reinforcement Learning",
        "authors": "Andrej Orsula, Simon Bøgh, Miguel Olivares-Mendez, Carol Martinez",
        "published": "2022-10-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981661"
    },
    {
        "id": 28987,
        "title": "Reinforcement Learning-Based Network Slice Resource Allocation for Federated Learning Applications",
        "authors": "Zhouxiang Wu, Genya Ishigaki, Riti Gour, Congzhou Li, Jason P. Jue",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001715"
    },
    {
        "id": 28988,
        "title": "A Curriculum Learning Based Multi-agent Reinforcement Learning Method for Realtime Strategy Game",
        "authors": "Dayu Zhang, Weidong Bao, Wenqian Liang, Guanlin Wu, Jiang Cao",
        "published": "2022-8-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdia56350.2022.9874056"
    },
    {
        "id": 28989,
        "title": "Cooperative zone-based rebalancing of idle overhead hoist transportations using multi-agent reinforcement learning with graph representation learning",
        "authors": "Kyuree Ahn, Jinkyoo Park",
        "published": "2021-2-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/24725854.2020.1851823"
    },
    {
        "id": 28990,
        "title": "Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-Resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning",
        "authors": "Md Tamjid Hossain, John W. T. Lee",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmlc58545.2023.10327979"
    },
    {
        "id": 28991,
        "title": "Learning to navigate a crystallization model with Deep Reinforcement Learning",
        "authors": "Vidhyadhar Manee, Roberto Baratti, Jose A. Romagnoli",
        "published": "2022-2",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cherd.2021.12.005"
    },
    {
        "id": 28992,
        "title": "Learning Human-Aware Robot Navigation from Physical Interaction via Inverse Reinforcement Learning",
        "authors": "Marina Kollmitz, Torsten Koller, Joschka Boedecker, Wolfram Burgard",
        "published": "2020-10-24",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros45743.2020.9340865"
    },
    {
        "id": 28993,
        "title": "Learning from Unreliable Human Action Advice in Interactive Reinforcement Learning",
        "authors": "Lisa Scherf, Cigdem Turan, Dorothea Koert",
        "published": "2022-11-28",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/humanoids53995.2022.10000078"
    },
    {
        "id": 28994,
        "title": "MASK-RL: Multiagent Video Object Segmentation Framework Through Reinforcement Learning",
        "authors": "Giuseppe Vecchio, Simone Palazzo, Daniela Giordano, Francesco Rundo, Concetto Spampinato",
        "published": "2020-12",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2019.2963282"
    },
    {
        "id": 28995,
        "title": "Toward a Reinforcement Learning Environment Toolbox for Intelligent Electric Motor Control",
        "authors": "Arne Traue, Gerrit Book, Wilhelm Kirchgassner, Oliver Wallscheid",
        "published": "2022-3",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2020.3029573"
    },
    {
        "id": 28996,
        "title": "Learning What to Share in Online Social Networks Using Deep Reinforcement Learning",
        "authors": "Shatha Jaradat, Nima Dokoohaki, Mihhail Matskin, Elena Ferrari",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-89932-9_6"
    },
    {
        "id": 28997,
        "title": "WITHDRAWN: Preliminary study of embedding two-level reinforcement learning to enhance the functionality of setting objectives compared with machine learning",
        "authors": "Shantanu Lohi, Nandita Tiwari",
        "published": "2021-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.matpr.2021.01.508"
    },
    {
        "id": 28998,
        "title": "Applying Reinforcement Learning on Real-World Data with Practical Examples in Python",
        "authors": "Philip Osborne, Kajal Singh, Matthew E. Taylor",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-79167-3"
    },
    {
        "id": 28999,
        "title": "Robot Control Using Model-Based Reinforcement Learning With Inverse Kinematics",
        "authors": "Dario Luipers, Nicolas Kaulen, Oliver Chojnowski, Sebastian Schneider, Anja Richert, Sabina Jeschke",
        "published": "2022-9-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl53763.2022.9962215"
    },
    {
        "id": 29000,
        "title": "Reinforcement Learning Method with Dynamic Learning Rate for Real-Time Route Guidance Based on SUMO",
        "authors": "Yuzhen Li, Jiawen Tang, Han Zhao, Ruikang Luo",
        "published": "2022-12-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icarcv57592.2022.10004227"
    },
    {
        "id": 29001,
        "title": "Deep Reinforcement Q-Learning for Intelligent Traffic Control in Mass Transit",
        "authors": "Shurok Khozam, Nadir Farhi",
        "published": "2023-7-14",
        "citations": 0,
        "abstract": "Traffic control in mass transit consists of the regulation of both vehicle dynamics and passenger flows. While most of the existing approaches focus on the optimization of vehicle dwell time, vehicle time headway, and passenger stocks, we propose in this article an approach which also includes the optimization of the passenger inflows to the platforms. We developed in this work a deep reinforcement Q-learning model for the traffic control in a mass transit line. We first propose a new mathematical traffic model for the train and passengers dynamics. The model combines a discrete-event description of the vehicle dynamics, with a macroscopic model for the passenger flows. We use this new model as the environment of the traffic in mass transit for the reinforcement learning optimization. For this aim, we defined, under the new traffic model, the state variables as well as the control ones, including in particular the number of running vehicles, the vehicle dwell times at stations, and the passenger inflow to platforms. Second, we present our new deep Q-network (DQN) model for the reinforcement learning (RL) with the state representation, action space, and reward function definitions. We also provide the neural network architecture as well as the main hyper-parameters. Finally, we give an evaluation of the model under multiple scenarios. We show in particular the efficiency of the control of the passenger inflows into the platforms.",
        "link": "http://dx.doi.org/10.3390/su151411051"
    },
    {
        "id": 29002,
        "title": "Reinforcement Learning Consensus Control for Discrete-Time Multi-Agent Systems",
        "authors": "Xiaoxia Zhu, Xin Yuan, Yuanda Wang, Changyin Sun",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8865975"
    },
    {
        "id": 29003,
        "title": "Rainbow with Episodic Memory in Deep Reinforcement Learning",
        "authors": "Daiki Kuyoshi, Toi Tsuneda, Satoshi Yamane",
        "published": "2020-10-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce50665.2020.9291872"
    },
    {
        "id": 29004,
        "title": "Reinforcement Learning for Generating Secure Configurations",
        "authors": "Shuvalaxmi Dass, Akbar Siami Namin",
        "published": "2021-9-30",
        "citations": 1,
        "abstract": "Many security problems in software systems are because of vulnerabilities caused by improper configurations. A poorly configured software system leads to a multitude of vulnerabilities that can be exploited by adversaries. The problem becomes even more serious when the architecture of the underlying system is static and the misconfiguration remains for a longer period of time, enabling adversaries to thoroughly inspect the software system under attack during the reconnaissance stage. Employing diversification techniques such as Moving Target Defense (MTD) can minimize the risk of exposing vulnerabilities. MTD is an evolving defense technique through which the attack surface of the underlying system is continuously changing. However, the effectiveness of such dynamically changing platform depends not only on the goodness of the next configuration setting with respect to minimization of attack surfaces but also the diversity of set of configurations generated. To address the problem of generating a diverse and large set of secure software and system configurations, this paper introduces an approach based on Reinforcement Learning (RL) through which an agent is trained to generate the desirable set of configurations. The paper reports the performance of the RL-based secure and diverse configurations through some case studies.",
        "link": "http://dx.doi.org/10.3390/electronics10192392"
    },
    {
        "id": 29005,
        "title": "Mesh Based Analysis of Low Fractal Dimension Reinforcement Learning Policies",
        "authors": "Sean Gillen, Katie Byl",
        "published": "2021-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9561874"
    },
    {
        "id": 29006,
        "title": "Prioritized Stochastic Memory Management for Enhanced Reinforcement Learning",
        "authors": "Taehwan Kwon, Dong Eui Chang",
        "published": "2018-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icce-asia.2018.8552124"
    },
    {
        "id": 29007,
        "title": "Robot PID Control Using Reinforcement Learning",
        "authors": "Guillermo Puriel, Xiaoou Li, Brisbane Ovilla-Martinez, Wen Yu",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10371985"
    },
    {
        "id": 29008,
        "title": "Portfolio Optimization Under the Framework of Reinforcement Learning",
        "authors": "Li Xucheng, Peng Zhihao",
        "published": "2019-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmtma.2019.00180"
    },
    {
        "id": 29009,
        "title": "Mpc-Based Model-Based Reinforcement Learning and Planning For Auv Path Following",
        "authors": "Zheng Zhang, Tianhao Chen, Dong Jiang, Zheng Fang, Guangliang Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4349138"
    },
    {
        "id": 29010,
        "title": "Autonomous Navigation of Wheelchairs in Indoor Environments using Deep Reinforcement Learning and Computer Vision",
        "authors": "Paulo de Almeida Afonso, Paulo R. Ferreira",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWe introduce an innovative approach to autonomous navigation for motorized wheelchairs in shared indoor spaces with human presence, combining Deep Reinforcement Learning and Computer Vision. The central aim of this study is to enhance the well-being of individuals with disabilities who rely on such assistance for their mobility needs. Our methodology merges the Deep Deterministic Policy Gradient (DDPG) algorithm with advanced computer vision techniques, empowering motorized wheelchairs to navigate through environments that encompass both stationary and moving people. Comparative tests were conducted between the DDPG and Deep Q-Network (DQN) algorithms, spanning four distinct stages. Each stage represented two different scenarios from the training environment, characterized by complexity levels exceeding those to which the robot had been trained.\nThe DDPG algorithm showcased its superior efficiency and stability compared to DQN. Across all analyzed stages, DDPG exhibited higher average success rates: 98\\% (Stage 01), 89\\% (Stage 02), 86\\% (Stage 03), and 86\\% (Stage 04), demonstrating exceptional generalization capabilities and consistently outperforming the training environment in diverse settings. In contrast, DQN struggled to avoid collisions, resulting in significantly lower average success rates: 3\\% (Stage 02), 14\\% (Stage 03), and 29\\% (Stage 04). Our findings underscore the promising potential of our proposed solution, thereby contributing to the progress of research in this particular domain.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3287103/v1"
    },
    {
        "id": 29011,
        "title": "Double Riss Assisted Task Offloading for NOMA-MEC with Action-Constrained Deep Reinforcement Learning",
        "authors": "Junli Fang, Baoshan Lu, Xuemin Hong, Jianghong Shi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4602610"
    },
    {
        "id": 29012,
        "title": "Reinforcement Learning Exploration Algorithms for Energy Harvesting Communications Systems",
        "authors": "Alaeddin Masadeh, Zhengdao Wang, Ahmed E. Kamal",
        "published": "2018-5",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2018.8422710"
    },
    {
        "id": 29013,
        "title": "TCP Congestion Avoidance in Data Centres using Reinforcement Learning",
        "authors": "Ali Hassan, Shahram Shah Heydari",
        "published": "2022-2-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/icact53585.2022.9728923"
    },
    {
        "id": 29014,
        "title": "Automated design of search algorithms based on reinforcement learning",
        "authors": "Wenjie Yi, Rong Qu",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.119639"
    },
    {
        "id": 29015,
        "title": "Optimal Control of a Wind Farm in Time-Varying Wind Using Deep Reinforcement Learning",
        "authors": "Taewan Kim, Changwook Kim, Jeonghwan Song, Donghyun You",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4581760"
    },
    {
        "id": 29016,
        "title": "DSTS: A hybrid optimal and deep reinforcement learning for dynamic scalable task scheduling on container cloud environment",
        "authors": "M Saravanan, R Vignesh",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nContainers have grown into the most dependable and lightweight virtualization platform for delivering cloud services, offering flexible sorting, portability, and scalability. In cloud container services, planner components play a critical role. This enhances cloud resource workloads and diversity performance while lowering costs. We present a hybrid optimum and deep reinforcement learning approach for dynamic scalable task scheduling (DSTS) in a container cloud environment in this research. To expand containers virtual resources, we first offer a modified multi-swarm coyote optimization (MMCO) method, which improves customer service level agreements. Then, to assure priority-based scheduling, we create a modified pigeon-inspired optimization (MPIO) method for task clustering and a rapid adaptive feedback recurrent neural network (FARNN) for pre-virtual CPU allocation. Meanwhile, the task load monitoring system is built on a deep convolutional neural network (DCNN), which allows for dynamic priority-based scheduling. Finally, the presentation of the planned DSTS methodology will be estimated utilizing various test vectors, and the results will be associated to present state-of-the-art techniques.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1431790/v1"
    },
    {
        "id": 29017,
        "title": "Latent Structure Matching for Knowledge Transfer in Reinforcement Learning",
        "authors": "Yi Zhou, Fenglei Yang",
        "published": "2020-2-13",
        "citations": 1,
        "abstract": "Reinforcement learning algorithms usually require a large number of empirical samples and give rise to a slow convergence in practical applications. One solution is to introduce transfer learning: Knowledge from well-learned source tasks can be reused to reduce sample request and accelerate the learning of target tasks. However, if an unmatched source task is selected, it will slow down or even disrupt the learning procedure. Therefore, it is very important for knowledge transfer to select appropriate source tasks that have a high degree of matching with target tasks. In this paper, a novel task matching algorithm is proposed to derive the latent structures of value functions of tasks, and align the structures for similarity estimation. Through the latent structure matching, the highly-matched source tasks are selected effectively, from which knowledge is then transferred to give action advice, and improve exploration strategies of the target tasks. Experiments are conducted on the simulated navigation environment and the mountain car environment. The results illustrate the significant performance gain of the improved exploration strategy, compared with traditional    ϵ   -greedy exploration strategy. A theoretical proof is also given to verify the improvement of the exploration strategy based on latent structure matching.",
        "link": "http://dx.doi.org/10.3390/fi12020036"
    },
    {
        "id": 29018,
        "title": "Autonomous Surface Vessel Obstacle Avoidance Based on Hierarchical Reinforcement Learning",
        "authors": "Chang Zhou, Lei Wang, Shangyu Yu, Huacheng He",
        "published": "2020-8-3",
        "citations": 1,
        "abstract": "Abstract\nThe obstacle avoidance problem of autonomous surface vessels (ASV) has attracted the attention of the marine control research community for long years. Out of safety consideration, it is important for ASV to avoid all kinds of obstacles like shores, cliffs, floaters and other vessels. Developing a heading and path planning strategy for ASV is the main task and the remaining challenge. Traditional obstacle avoidance algorithms lead to too much computing in working environment. The issue of computation cost can be solved by training obstacle avoidance models with reinforcement learning (RL). By using the RL method, the ASV will choose the most efficient action according to the ASV’s experience it learned from the past. In this paper, RL is adopted to design a decision-making agent for obstacle avoidance. To train the obstacle avoidance model under a sparse feedback environment, hierarchical reinforcement learning (HRL) method is applied. Using this algorithm, better obstacle avoidance performance and longer survival time can be achieved. Memory pool modification and target network modification are also used to smooth the training process of the ASV. Simulation results demonstrate that HRL can make the learning process of un-manned ship’s obstacle avoidance smoother and more effective. Also, the living time of ASVs is improved.",
        "link": "http://dx.doi.org/10.1115/omae2020-18454"
    },
    {
        "id": 29019,
        "title": "Coordinated Multi-Agent Reinforcement Learning for Swarm Battery Control",
        "authors": "Niklas Ebell, Marco Pruckner",
        "published": "2018-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccece.2018.8447851"
    },
    {
        "id": 29020,
        "title": "Reinforcement Learning-Based Underwater Acoustic Channel Tracking for Correlated Time-Varying Channels",
        "authors": "Yuhang Wang, Wei Li, Qihang Huang",
        "published": "2021-9-20",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/oceans44145.2021.9705830"
    },
    {
        "id": 29021,
        "title": "Deep Reinforcement Learning for the Fighter Theater",
        "authors": "Guoshen Li, Wenlin Zhang",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iaeac47372.2019.8997984"
    },
    {
        "id": 29022,
        "title": "Reinforcement Learning with Discrete Event Simulation: The Premise, Reality, and Promise",
        "authors": "Sahil Belsare, Emily Diaz Badilla, Mohammad Dehghanimohammadabadi",
        "published": "2022-12-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc57314.2022.10015503"
    },
    {
        "id": 29023,
        "title": "Deep Reinforcement Learning for Queue-Time Management in Semiconductor Manufacturing",
        "authors": "Harel Yedidsion, Prafulla Dawadi, David Norman, Emrah Zarifoglu",
        "published": "2022-12-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc57314.2022.10015463"
    },
    {
        "id": 29024,
        "title": "Mobility-Aware Centralized Reinforcement Learning for Dynamic Resource Allocation in HetNets",
        "authors": "Jie Liu, Xiaoming Tao, Jianhua Lu",
        "published": "2019-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9013191"
    },
    {
        "id": 29025,
        "title": "Millimeter Wave Beamforming Training: A Reinforcement Learning Approach",
        "authors": "Ehab Mahmoud Mohamed",
        "published": "2020-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24425/ijet.2021.135949"
    },
    {
        "id": 29026,
        "title": "5G Resource Scheduling for Low-latency Communication: A Reinforcement Learning Approach",
        "authors": "Qian Huang, Michel Kadoch",
        "published": "2020-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2020-fall49728.2020.9348718"
    },
    {
        "id": 29027,
        "title": "Emergent Resource Exchange and Tolerated Theft Behavior Using Multiagent Reinforcement Learning",
        "authors": "Jack Garbus, Jordan Pollack",
        "published": "2024-1-19",
        "citations": 0,
        "abstract": "Abstract\nFor decades, the evolution of cooperation has piqued interest in numerous academic disciplines, such as game theory, economics, biology, and computer science. In this work, we demonstrate the emergence of a novel and effective resource exchange protocol formed by dropping and picking up resources in a foraging environment. This form of cooperation is made possible by the introduction of a campfire, which adds an extended period of congregation and downtime for agents to explore otherwise unlikely interactions. We find that the agents learn to avoid getting cheated by their exchange partners, but not always from a third party. We also observe the emergence of behavior analogous to tolerated theft, despite the lack of any punishment, combat, or larceny mechanism in the environment.",
        "link": "http://dx.doi.org/10.1162/artl_a_00423"
    },
    {
        "id": 29028,
        "title": "Dynamic Pricing Based Electric Vehicle Charging Station Location Strategy Using Reinforcement Learning",
        "authors": "YanBin Li, JiaNi Wang, WeiYe Wang, Chang Liu, Yun Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4376344"
    },
    {
        "id": 29029,
        "title": "Optimal Robust Control of Nonlinear Uncertain System via Off-Policy Integral Reinforcement Learning",
        "authors": "Xiaoyang Wang, Xiufen Ye",
        "published": "2020-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9189626"
    },
    {
        "id": 29030,
        "title": "Degradation of Performance in Reinforcement Learning with State Measurement Uncertainty",
        "authors": "Mark McKenzie, Mark D. McDonnell",
        "published": "2019-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/milcis.2019.8930725"
    },
    {
        "id": 29031,
        "title": "Hybrid Online POMDP Planning and Deep Reinforcement Learning for Safer Self-Driving Cars",
        "authors": "Florian Pusse, Matthias Klusch",
        "published": "2019-6",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2019.8814125"
    },
    {
        "id": 29032,
        "title": "Neurofuzzy Reinforcement Learning Control Schemes for Optimized Dynamical Performance",
        "authors": "Mohammed Abouheaf, Wail Gueaieb",
        "published": "2019-6",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rose.2019.8790424"
    },
    {
        "id": 29033,
        "title": "Multi-Agent Deep Reinforcement Learning with Emergent Communication",
        "authors": "David Simoes, Nuno Lau, Luis Paulo Reis",
        "published": "2019-7",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2019.8852293"
    },
    {
        "id": 29034,
        "title": "Sequential Banner Design Optimization with Deep Reinforcement Learning",
        "authors": "Yusuke Kondo, Xueting Wang, Hiroyuki Seshime, Toshihiko Yamasaki",
        "published": "2021-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ism52913.2021.00050"
    },
    {
        "id": 29035,
        "title": "Experiments Focused on Exploration in Deep Reinforcement Learning",
        "authors": "Martin Kaloev, Georgi Krastev",
        "published": "2021-10-21",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ismsit52890.2021.9604690"
    },
    {
        "id": 29036,
        "title": "A Reinforcement Learning based Computing Offloading and Resource Allocation Scheme in F-RAN",
        "authors": "Fan Jiang, Rongxin Ma, Youjun Gao, Zesheng Gu",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nThis paper investigates a computing offloading policy and the allocation of computational resource for multiple user equipments (UEs) in Device-to-Device (D2D) aided fog radio access networks (F-RANs). Concerning the dynamically changing wireless environment where the channel state information (CSI) is difficult to predict and know exactly, we formulate the problem of task offloading and resource optimization as a mixed-integer nonlinear programming problem to maximize the total utility of all UEs. Concerning the non-convex property of the formulated problem, we decouple the original problem into two phases to solve. Firstly, a centralized deep reinforcement learning (DRL) algorithm called Dueling Deep Q-Network (DDQN) is utilized to obtain the most suitable offloading mode for each UE. Particularly, to reduce the complexity of the proposed offloading scheme based DDQN algorithm, a pre-processing procedure is adopted. Then a distributed Deep Q-Network (DQN)algorithm based on the training result of the DDQN algorithm is further proposed to allocate the appropriate computational resource for each UE. Combining these two phases, the optimal offloading policy and resource allocation for each UE are finally achieved. Simulation results demonstrate the performance gains of the proposed scheme compared with other existing baseline schemes.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-483062/v1"
    },
    {
        "id": 29037,
        "title": "Unmanned Aerial Vehicle Angular Velocity Control via Reinforcement Learning in Dimension Reduced Search Spaces",
        "authors": "Qiang Li, Yunjun Xu",
        "published": "2020-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147911"
    },
    {
        "id": 29038,
        "title": "Robotics with Multi-Fingered Grippers and Deep Reinforcement Learning in Unity",
        "authors": "Jwu-Sheng Hu, Li-Jing Zheng",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cacs60074.2023.10326170"
    },
    {
        "id": 29039,
        "title": "A Graph-Based Spatial-Temporal Deep Reinforcement Learning Model for Edge Caching",
        "authors": "Jiacheng Hou, Amiya Nayak",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436966"
    },
    {
        "id": 29040,
        "title": "Partially Observable Reinforcement Learning for Blood Glucose Control Under Missing Data",
        "authors": "Haiyan Yu, Jiao Xiang, Nan Kong, Li Luo, Ching-Chi Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4647947"
    },
    {
        "id": 29041,
        "title": "Transferable dynamics models for efficient object-oriented reinforcement learning",
        "authors": "Ofir Marom, Benjamin Rosman",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.artint.2024.104079"
    },
    {
        "id": 29042,
        "title": "A Deep Reinforcement Learning Approach to Asset-Liability Management",
        "authors": "Alan Fontoura, Diego Haddad, Eduardo Bezerra",
        "published": "2019-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bracis.2019.00046"
    },
    {
        "id": 29043,
        "title": "Adversarial Proximal Policy Optimisation for Robust Reinforcement Learning",
        "authors": "Bilkan Ince, Hyo-Sang Shin, Antonios Tsourdos",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2024-1697"
    },
    {
        "id": 29044,
        "title": "The Neural Architecture of Theory-based Reinforcement Learning",
        "authors": "Momchil S. Tomov, Pedro A. Tsividis, Thomas Pouncy, Joshua B. Tenenbaum, Samuel J. Gershman",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractHumans learn internal models of the environment that support efficient planning and flexible generalization in complex, real-world domains. Yet it remains unclear how such internal models are represented and learned in the brain. We approach this question within the framework of theory-based reinforcement learning, a strong form of model-based reinforcement learning in which the model is an intuitive theory – a rich, abstract, causal model of the environment built on a natural ontology of physical objects, intentional agents, relations, and goals. We used a theory-based reinforcement learning model to analyze brain data from human participants learning to play different Atari-style video games while undergoing functional MRI. Theories inferred by the theory-based model explained the signal in inferior frontal gyrus and other prefrontal areas better than several alternative models. Brain activity increased in response to theory update events in inferior frontal gyrus, occipital cortex, and fusiform gyrus, with separate learning signals for different theory components. This corresponded with a transient strengthening of theory representations in those regions. Finally, the effective connectivity pattern during theory updating suggests that information flows top-down from theory-coding regions in the prefrontal cortex to theory updating regions in occipital and temporal cortex. These results are consistent with a neural architecture in which top-down theory representations originating in prefrontal regions shape sensory predictions in visual areas, where factorized theory prediction errors are computed and in turn trigger bottom-up updates of the theory. This initial sketch provides a foundation for understanding of the neural representations and computations that support efficient theory-based reinforcement learning in complex, naturalistic environments.",
        "link": "http://dx.doi.org/10.1101/2022.06.14.496001"
    },
    {
        "id": 29045,
        "title": "Pedestrian Simulation with Reinforcement Learning: A Curriculum-Based Approach",
        "authors": "Giuseppe Vizzari, Thomas Cecconello",
        "published": "2022-12-27",
        "citations": 3,
        "abstract": "Pedestrian simulation is a consolidated but still lively area of research. State of the art models mostly take an agent-based perspective, in which pedestrian decisions are made according to a manually defined model. Reinforcement learning (RL), on the other hand, is used to train an agent situated in an environment how to act so as to maximize an accumulated numerical reward signal (a feedback provided by the environment to every chosen action). We explored the possibility of applying RL to pedestrian simulation. We carefully defined a reward function combining elements related to goal orientation, basic proxemics, and basic way-finding considerations. The proposed approach employs a particular training curriculum, a set of scenarios growing in difficulty supporting an incremental acquisition of general movement competences such as orientation, walking, and pedestrian interaction. The learned pedestrian behavioral model is applicable to situations not presented to the agents in the training phase, and seems therefore reasonably general. This paper describes the basic elements of the approach, the training procedure, and an experimentation within a software framework employing Unity and ML-Agents.",
        "link": "http://dx.doi.org/10.3390/fi15010012"
    },
    {
        "id": 29046,
        "title": "Self-imitation guided goal-conditioned reinforcement learning",
        "authors": "Yao Li, YuHui Wang, XiaoYang Tan",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patcog.2023.109845"
    },
    {
        "id": 29047,
        "title": "Reinforcement Learning Based Controller for a Soft Continuum Robot",
        "authors": "Anirudh Mazumder",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bdkcse59280.2023.10339746"
    },
    {
        "id": 29048,
        "title": "Reinforcement Learning Based Routing Protocol for Wireless Body Sensor Networks",
        "authors": "Farzad Kiani",
        "published": "2017-11",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sc2.2017.18"
    },
    {
        "id": 29049,
        "title": "HJB-RL: Initializing Reinforcement Learning with Optimal Control Policies Applied to Autonomous Drone Racing",
        "authors": "Keiko Nagami, Mac Schwager",
        "published": "2021-7-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2021.xvii.062"
    },
    {
        "id": 29050,
        "title": "Autonomous Control of Fixed-wing Unmanned Aerial System by Reinforcement Learning",
        "authors": "Hao Wang, Weijia Wang",
        "published": "2020-11-27",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus50048.2020.9274858"
    },
    {
        "id": 29051,
        "title": "The Effects of Memory Replay in Reinforcement Learning",
        "authors": "Ruishan Liu, James Zou",
        "published": "2018-10",
        "citations": 39,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/allerton.2018.8636075"
    },
    {
        "id": 29052,
        "title": "Safe Reinforcement Learning on Autonomous Vehicles",
        "authors": "David Isele, Alireza Nakhaei, Kikuo Fujimura",
        "published": "2018-10",
        "citations": 33,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2018.8593420"
    },
    {
        "id": 29053,
        "title": "A Multi-phase Intersection Traffic Signal Control Strategy with Deep Reinforcement Learning",
        "authors": "Congcong Li, Yuan Li, Guihua Liu",
        "published": "2018-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2018.8623383"
    },
    {
        "id": 29054,
        "title": "DVNE-DRL: dynamic virtual network embedding algorithm based on deep reinforcement learning",
        "authors": "Xiancui Xiao",
        "published": "2023-11-13",
        "citations": 0,
        "abstract": "AbstractVirtual network embedding (VNE), as the key challenge of network resource management technology, lies in the contradiction between online embedding decision and pursuing long-term average revenue goals. Most of the previous work ignored the dynamics in Virtual Network (VN) modeling, or could not automatically detect the complex and time-varying network state to provide a reasonable network embedding scheme. In view of this, we model a network embedding framework where the topology and resource allocation change dynamically with the number of network users and workload, and then introduce a deep reinforcement learning method to solve the VNE problem. Further, a dynamic virtual network embedding algorithm based on Deep Reinforcement Learning (DRL), named DVNE-DRL, is proposed. In DVNE-DRL, VNE is modeled as a Markov Decision Process (MDP), and then deep learning is introduced to perceive the current network state through historical data and embedded knowledge, while utilizing reinforcement learning decision-making capabilities to implement the network embedding process. In addition, we improve the method of feature extraction and matrix optimization, and consider the characteristics of virtual network and physical network together to alleviate the problem of redundancy and slow convergence. The simulation results show that compared with the existing advanced algorithms, the acceptance rate and average revenue of DVNE-DRL are increased by about 25% and 35%, respectively.",
        "link": "http://dx.doi.org/10.1038/s41598-023-47195-5"
    },
    {
        "id": 29055,
        "title": "Lane-Merging Using Policy-based Reinforcement Learning and Post-Optimization",
        "authors": "Patrick Hart, Leonard Rychly, Alois Knoll",
        "published": "2019-10",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8917002"
    },
    {
        "id": 29056,
        "title": "Improving Reinforcement Learning with Confidence-Based Demonstrations",
        "authors": "Zhaodong Wang, Matthew E. Taylor",
        "published": "2017-8",
        "citations": 16,
        "abstract": "Reinforcement learning has had many successes, but in practice it often requires significant amounts of data to learn high-performing policies. One common way to improve learning is to allow a trained (source) agent to assist a new (target) agent. The goals in this setting are to 1) improve the target agent's performance, relative to learning unaided, and 2) allow the target agent to outperform the source agent. Our approach leverages source agent demonstrations, removing any requirements on the source agent's learning algorithm or representation. The target agent then estimates the source agent's policy and improves upon it. The key contribution of this work is to show that leveraging the target agent's uncertainty in the source agent's policy can significantly improve learning in two complex simulated domains, Keepaway and Mario.",
        "link": "http://dx.doi.org/10.24963/ijcai.2017/422"
    },
    {
        "id": 29057,
        "title": "Graph Signal Sampling via Reinforcement Learning",
        "authors": "Oleksii Abramenko, Alexander Jung",
        "published": "2019-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp.2019.8683181"
    },
    {
        "id": 29058,
        "title": "An adversarial reinforcement learning based system for cyber security",
        "authors": "Song Xia, Meikang Qiu, Hao Jiang",
        "published": "2019-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smartcloud.2019.00046"
    },
    {
        "id": 29059,
        "title": "Human Motion Control of Quadrupedal Robots using Deep Reinforcement Learning",
        "authors": "Sunwoo Kim, Maks Sorokin, Jehee Lee, Sehoon Ha",
        "published": "2022-6-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2022.xviii.021"
    },
    {
        "id": 29060,
        "title": "Deep Reinforcement Learning for Video Prediction",
        "authors": "Yung-Han Ho, Chuan-Yuan Cho, Wen-Hsiao Peng",
        "published": "2019-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2019.8803825"
    },
    {
        "id": 29061,
        "title": "Competition and Cooperation Between Multiple Reinforcement Learning Systems",
        "authors": "Wouter Kool, Fiery A. Cushman, Samuel J. Gershman",
        "published": "2018",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-812098-9.00007-3"
    },
    {
        "id": 29062,
        "title": "Parameter and model recovery of reinforcement learning models for restless bandit problems",
        "authors": "Ludwig Danwitz, David Mathar, Elke Smith, Deniz Tuzsus, Jan Peters",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMulti-armed restless bandit tasks are regularly applied in psychology and cognitive neuroscience to assess exploration and exploitation behavior in structured environments. These models are also readily applied to examine effects of (virtual) brain lesions on performance, and to infer neurocomputational mechanisms using neuroimaging or pharmacological approaches. However, to infer individual, psychologically meaningful parameters from such data, computational cognitive modeling is typically applied. Recent studies indicate that softmax (SM) decision rule models that include a representation of environmental dynamics (e.g. the Kalman Filter) and additional parameters for modeling exploration and perseveration (Kalman SMEP) fit human bandit task data better than competing models. Parameter and model recovery are two central requirements for computational models: parameter recovery refers to the ability to recover true data-generating parameters; model recovery refers to the ability to correctly identify the true data generating model using model comparison techniques. Here we comprehensively examined parameter and model recovery of the Kalman SMEP model as well as nested model versions, i.e. models without the additional parameters, using simulation and Bayesian inference. Parameter recovery improved with increasing trial numbers, from around .8 for 100 trials to around .93 for 300 trials. Model recovery analyses likewise confirmed acceptable recovery of the Kalman SMEP model. Model recovery was lower for nested Kalman filter models as well as delta rule models with fixed learning rates.Exploratory analyses examined associations of model parameters with model-free performance metrics. Random exploration, captured by the inverse softmax temperature, was associated with lower accuracy and more switches. For the exploration bonus parameter modeling directed exploration, we confirmed an inverse-U-shaped association with accuracy, such that both an excess and a lack of directed exploration reduced accuracy. Taken together, these analyses underline that the Kalman SMEP model fulfills basic requirements of a cognitive model.",
        "link": "http://dx.doi.org/10.1101/2021.10.27.466089"
    },
    {
        "id": 29063,
        "title": "Primal-Dual Algorithm for Distributed Reinforcement Learning: Distributed GTD",
        "authors": "Donghwan Lee, Hyungjin Yoon, Naira Hovakimyan",
        "published": "2018-12",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc.2018.8619839"
    },
    {
        "id": 29064,
        "title": "A Mean-VaR Based Deep Reinforcement Learning Framework for Practical Algorithmic Trading",
        "authors": "Boyi Jin",
        "published": "2023",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3259108"
    },
    {
        "id": 29065,
        "title": "Reinforcement Learning in Railway Timetable Rescheduling",
        "authors": "Yongqiu Zhu, Hongrui Wang, Rob M.P. Goverde",
        "published": "2020-9-20",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc45102.2020.9294188"
    },
    {
        "id": 29066,
        "title": "Resource Allocation Based on Reinforcement Learning for Heterogeneous Air Network",
        "authors": "Shuai Wu",
        "published": "2023-6-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bmsb58369.2023.10211128"
    },
    {
        "id": 29067,
        "title": "DECORE: Deep Compression with Reinforcement Learning",
        "authors": "Manoj Alwani, Yang Wang, Vashisht Madhavan",
        "published": "2022-6",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.01203"
    },
    {
        "id": 29068,
        "title": "Application of deep reinforcement learning in mobile robot path planning",
        "authors": "Jing Xin, Huan Zhao, Ding Liu, Minqi Li",
        "published": "2017-10",
        "citations": 55,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2017.8244061"
    },
    {
        "id": 29069,
        "title": "Deep Reinforcement Learning for Navigation in Cluttered Environments",
        "authors": "Peter Regier, Lukas Gesing, Maren Bennewitz",
        "published": "2020-9-26",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5121/csit.2020.101117"
    },
    {
        "id": 29070,
        "title": "Distributional Deep Reinforcement Learning with a Mixture of Gaussians",
        "authors": "Yunho Choi, Kyungjae Lee, Songhwai Oh",
        "published": "2019-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8793505"
    },
    {
        "id": 29071,
        "title": "Novelty-Guided Reinforcement Learning via Encoded Behaviors",
        "authors": "Rajkumar Ramamurthy, Rafet Sifa, Max Lubbering, Christian Bauckhage",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9206982"
    },
    {
        "id": 29072,
        "title": "TCP Congestion Avoidance in Data Centres using Reinforcement Learning",
        "authors": "Ali Hassan, Shahram Shah Heydari",
        "published": "2021-2-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/icact51234.2021.9370861"
    },
    {
        "id": 29073,
        "title": "Partially Explainable Big Data Driven Deep Reinforcement Learning for Green 5G UAV",
        "authors": "Weisi Guo",
        "published": "2020-6",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc40277.2020.9149151"
    },
    {
        "id": 29074,
        "title": "Deep Reinforcement Learning Based Container Cluster Placement Strategy in Edge Computing Environment",
        "authors": "Zhuo Chen, Bowen Zhu",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001234"
    },
    {
        "id": 29075,
        "title": "Circuit Driving of RC Scale Cars using Reinforcement Learning",
        "authors": "Minhyeok Kwon, Yongsoon Eun",
        "published": "2022-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iccas55662.2022.10003730"
    },
    {
        "id": 29076,
        "title": "Design of microfluidic chromatographs through reinforcement learning",
        "authors": "Mohammad Shahab, Raghunathan Rengaswamy",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.dche.2024.100141"
    },
    {
        "id": 29077,
        "title": "Faculty Opinions recommendation of Orbitofrontal Circuits Control Multiple Reinforcement-Learning Processes.",
        "authors": "David Morilak",
        "published": "2019-9-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3410/f.736041230.793565498"
    },
    {
        "id": 29078,
        "title": "Deep Reinforcement Learning Attitude Control of Fixed-Wing UAVs",
        "authors": "Yan Zhen, Mingrui Hao, Wendi Sun",
        "published": "2020-11-27",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus50048.2020.9274875"
    },
    {
        "id": 29079,
        "title": "Computational modeling of Choice-Induced Preference Change: A Reinforcement-Learning-based approach",
        "authors": "Jianhong Zhu, Junya Hashimoto, Kentaro Katahira, Makoto Hirakawa, Takashi Nakao",
        "published": "No Date",
        "citations": 0,
        "abstract": "The value learning process has been investigated using decision-making tasks with a correct answer specified by the external environment (externally guided decision-making, EDM). In EDM, people are required to adjust their choices based on feedback, and the learning process is generally explained by the reinforcement learning (RL) model. In addition to EDM, value is learned through internally guided decision-making (IDM), in which no correct answer defined by external circumstances is available, such as preference judgment. In IDM, it has been believed that the value of the chosen item is increased and that of the rejected item is decreased (choice-induced preference change; CIPC). An RL-based model called the choice-based learning (CBL) model had been proposed to describe CIPC, in which the values of chosen and/or rejected items are updated as if own choice were the correct answer. However, the validity of the CBL model has not been confirmed by fitting the model to IDM behavioral data. The present study aims to examine the CBL model in IDM. We conducted simulations, a preference judgment task for novel contour shapes, and applied computational model analyses to the behavioral data. The results showed that the CBL model with both the chosen and rejected value’s updated were a good fit for the IDM behavioral data compared to the other candidate models. Although previous studies using subjective preference ratings had repeatedly reported changes only in one of the values of either the chosen or rejected items, we demonstrated for the first time both items’ value changes were based solely on IDM choice behavioral data with computational model analyses.",
        "link": "http://dx.doi.org/10.31234/osf.io/zqrvw"
    },
    {
        "id": 29080,
        "title": "Reinforcement Learning for Combinatorial Optimization of Train Timetable Rescheduling",
        "authors": "Qi Shi, Xuewu Dai, Dongliang Cui, Lijuan Cheng",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/swc57546.2023.10449236"
    },
    {
        "id": 29081,
        "title": "Growing Robot Navigation Based on Deep Reinforcement Learning",
        "authors": "Ahmad Ataka, Andreas P. Sandiwan",
        "published": "2023-4-21",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccar57134.2023.10151740"
    },
    {
        "id": 29082,
        "title": "Reinforcement Learning for Signal Temporal Logic using Funnel-Based Approach",
        "authors": "Naman Saxena, Gorantla Sandeep, Pushpak Jagtap",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10442833"
    },
    {
        "id": 29083,
        "title": "Multi-agent Deep Reinforcement Learning for Zero Energy Communities",
        "authors": "Amit Prasad, Ivana Dusparic",
        "published": "2019-9",
        "citations": 27,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgteurope.2019.8905628"
    },
    {
        "id": 29084,
        "title": "Vision-based Navigation Using Deep Reinforcement Learning",
        "authors": "Jonas Kulhanek, Erik Derner, Tim de Bruin, Robert Babuska",
        "published": "2019-9",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecmr.2019.8870964"
    },
    {
        "id": 29085,
        "title": "Contention Window Optimization in IEEE 802.11ax Networks with Deep Reinforcement Learning",
        "authors": "Witold Wydmanski, Szymon Szott",
        "published": "2021-3-29",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc49053.2021.9417575"
    },
    {
        "id": 29086,
        "title": "UAV Path Planning Based on Multi-Layer Reinforcement Learning Technique",
        "authors": "Zhengyang Cui, Yong Wang",
        "published": "2021",
        "citations": 44,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3073704"
    },
    {
        "id": 29087,
        "title": "Attention-Aware Deep Reinforcement Learning for Video Face Recognition",
        "authors": "Yongming Rao, Jiwen Lu, Jie Zhou",
        "published": "2017-10",
        "citations": 109,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2017.424"
    },
    {
        "id": 29088,
        "title": "Reinforcement Learning in the Load Balancing Problem for the iFDAQ of the COMPASS Experiment at CERN",
        "authors": "Ondřej Šubrt, Martin Bodlák, Matouš Jandek, Vladimír Jarý, Antonín Květoň, Josef Nový, Miroslav Virius, Martin Zemko",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009035107340741"
    },
    {
        "id": 29089,
        "title": "Real-time Allocation of Shared Parking Spaces Based on Deep Reinforcement Learning",
        "authors": "Minghai Yuan Minghai Yuan, Chenxi Zhang Minghai Yuan, Kaiwen Zhou Chenxi Zhang, Fengque Pei Kaiwen Zhou",
        "published": "2023-1",
        "citations": 0,
        "abstract": "\n                        <p>Aiming at the parking space heterogeneity problem in shared parking space management, a multi-objective optimization model for parking space allocation is constructed with the optimization objectives of reducing the average walking distance of users and improving the utilization rate of parking spaces, a real-time allocation method for shared parking spaces based on deep reinforcement learning is proposed, which includes a state space for heterogeneous regions, an action space based on policy selection and a reward function with variable coefficients. To accurately evaluate the model performance, dynamic programming is used to derive the theoretical optimal values. Simulation results show that the improved algorithm not only improves the training success rate, but also increases the Agent performance by at least 12.63% and maintains the advantage for different sizes of parking demand, reducing the user walking distance by 53.58% and improving the parking utilization by 6.67% on average, and keeping the response time less than 0.2 seconds.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/160792642023012401004"
    },
    {
        "id": 29090,
        "title": "DERLight: A Deep Reinforcement Learning Traffic Light Control Algorithm with Dual Experience Replay",
        "authors": "Zhichao Yang Zhichao Yang, Yan Kong Zhichao Yang, Chih-Hsien Hsia Yan Kong",
        "published": "2024-1",
        "citations": 0,
        "abstract": "\n                        <p>In recent years, with the increasingly severe traffic environment, most cities are facing various traffic congestion problems, and the demand for intelligent regulation of traffic signals is also increasing. In this study, we propose a new intelligent traffic light control algorithm, dual experience replay light (DERLight), which innovatively and efficiently designs a dual experience replay training mechanism based on the classic deep Q network (DQN) framework and considers the dynamic epoch function. As results show that compared with some state-of-the-art algorithms, DERLight can shorten the average travel time of vehicles, increase the throughput at intersections, and also speed up the convergence of the network. In addition, the design of this algorithm framework is not only limited to the field of intelligent transportation, but also has transferability for some other fields.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/160792642024012501007"
    },
    {
        "id": 29091,
        "title": "A Study on Application of Curriculum Learning in Deep Reinforcement Learning : Action Acquisition in Shooting Game AI as Example",
        "authors": "Ikumi Kodaka, Fumiaki Saitoh",
        "published": "2021-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwcia52852.2021.9626020"
    },
    {
        "id": 29092,
        "title": "Implementation of Reinforcement Learning Simulated Madel on Physical UGV Using Robot Operating System for Continual Learning",
        "authors": "Edgar M. Perez, Abhijit Majumdar, Patrick Benavidez, Mo Jamshidi",
        "published": "2019-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sysose.2019.8753801"
    },
    {
        "id": 29093,
        "title": "Application of Reinforcement Learning in SINS/GNSS/DVL Integrated Navigation",
        "authors": "Xiangyuan Li, Xuesong Tang, Xingshu Wang, Wenfeng Tan, Jiaxing Zheng, Yingwei Zhao",
        "published": "2022-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlccim55934.2022.00066"
    },
    {
        "id": 29094,
        "title": "Deep reinforcement learning collision avoidance using policy gradient optimisation and Q-learning",
        "authors": "Shady A. Maged, Bishoy H. Mikhail",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijcvr.2020.107253"
    },
    {
        "id": 29095,
        "title": "Reinforcement Learning-Based Network Slice Resource Allocation for Federated Learning Applications",
        "authors": "Zhouxiang Wu, Genya Ishigaki, Riti Gour, Congzhou Li, Jason P. Jue",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001715"
    },
    {
        "id": 29096,
        "title": "Multiagent Coordination Systems Based on Neuro-Fuzzy Models with Reinforcement Learning",
        "authors": "Leonardo Alfredo Mendoza, Evelyn Batista, Harold Dias De Mello, Marco Aurelio Pacheco",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00151"
    },
    {
        "id": 29097,
        "title": "A Curriculum Learning Based Multi-agent Reinforcement Learning Method for Realtime Strategy Game",
        "authors": "Dayu Zhang, Weidong Bao, Wenqian Liang, Guanlin Wu, Jiang Cao",
        "published": "2022-8-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdia56350.2022.9874056"
    },
    {
        "id": 29098,
        "title": "Learning to navigate a crystallization model with Deep Reinforcement Learning",
        "authors": "Vidhyadhar Manee, Roberto Baratti, Jose A. Romagnoli",
        "published": "2022-2",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cherd.2021.12.005"
    },
    {
        "id": 29099,
        "title": "Deep Reinforcement Learning for Optimization",
        "authors": "Md Mahmudul Hasan, Md Shahinur Rahman, Adrian Bell",
        "published": "2019",
        "citations": 1,
        "abstract": "Deep reinforcement learning (DRL) has transformed the field of artificial intelligence (AI) especially after the success of Google DeepMind. This branch of machine learning epitomizes a step toward building autonomous systems by understanding of the visual world. Deep reinforcement learning (RL) is currently applied to different sorts of problems that were previously obstinate. In this chapter, at first, the authors started with an introduction of the general field of RL and Markov decision process (MDP). Then, they clarified the common DRL framework and the necessary components RL settings. Moreover, they analyzed the stochastic gradient descent (SGD)-based optimizers such as ADAM and a non-specific multi-policy selection mechanism in a multi-objective Markov decision process. In this chapter, the authors also included the comparison for different Deep Q networks. In conclusion, they describe several challenges and trends in research within the deep reinforcement learning field. ",
        "link": "http://dx.doi.org/10.4018/978-1-5225-7862-8.ch011"
    },
    {
        "id": 29100,
        "title": "Learning Human-Aware Robot Navigation from Physical Interaction via Inverse Reinforcement Learning",
        "authors": "Marina Kollmitz, Torsten Koller, Joschka Boedecker, Wolfram Burgard",
        "published": "2020-10-24",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros45743.2020.9340865"
    },
    {
        "id": 29101,
        "title": "Attention-Aware Deep Reinforcement Learning for Video Face Recognition",
        "authors": "Yongming Rao, Jiwen Lu, Jie Zhou",
        "published": "2017-10",
        "citations": 109,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2017.424"
    },
    {
        "id": 29102,
        "title": "Reinforcement Learning in the Load Balancing Problem for the iFDAQ of the COMPASS Experiment at CERN",
        "authors": "Ondřej Šubrt, Martin Bodlák, Matouš Jandek, Vladimír Jarý, Antonín Květoň, Josef Nový, Miroslav Virius, Martin Zemko",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009035107340741"
    },
    {
        "id": 29103,
        "title": "Real-time Allocation of Shared Parking Spaces Based on Deep Reinforcement Learning",
        "authors": "Minghai Yuan Minghai Yuan, Chenxi Zhang Minghai Yuan, Kaiwen Zhou Chenxi Zhang, Fengque Pei Kaiwen Zhou",
        "published": "2023-1",
        "citations": 0,
        "abstract": "\n                        <p>Aiming at the parking space heterogeneity problem in shared parking space management, a multi-objective optimization model for parking space allocation is constructed with the optimization objectives of reducing the average walking distance of users and improving the utilization rate of parking spaces, a real-time allocation method for shared parking spaces based on deep reinforcement learning is proposed, which includes a state space for heterogeneous regions, an action space based on policy selection and a reward function with variable coefficients. To accurately evaluate the model performance, dynamic programming is used to derive the theoretical optimal values. Simulation results show that the improved algorithm not only improves the training success rate, but also increases the Agent performance by at least 12.63% and maintains the advantage for different sizes of parking demand, reducing the user walking distance by 53.58% and improving the parking utilization by 6.67% on average, and keeping the response time less than 0.2 seconds.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/160792642023012401004"
    },
    {
        "id": 29104,
        "title": "DERLight: A Deep Reinforcement Learning Traffic Light Control Algorithm with Dual Experience Replay",
        "authors": "Zhichao Yang Zhichao Yang, Yan Kong Zhichao Yang, Chih-Hsien Hsia Yan Kong",
        "published": "2024-1",
        "citations": 0,
        "abstract": "\n                        <p>In recent years, with the increasingly severe traffic environment, most cities are facing various traffic congestion problems, and the demand for intelligent regulation of traffic signals is also increasing. In this study, we propose a new intelligent traffic light control algorithm, dual experience replay light (DERLight), which innovatively and efficiently designs a dual experience replay training mechanism based on the classic deep Q network (DQN) framework and considers the dynamic epoch function. As results show that compared with some state-of-the-art algorithms, DERLight can shorten the average travel time of vehicles, increase the throughput at intersections, and also speed up the convergence of the network. In addition, the design of this algorithm framework is not only limited to the field of intelligent transportation, but also has transferability for some other fields.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/160792642024012501007"
    },
    {
        "id": 29105,
        "title": "A Study on Application of Curriculum Learning in Deep Reinforcement Learning : Action Acquisition in Shooting Game AI as Example",
        "authors": "Ikumi Kodaka, Fumiaki Saitoh",
        "published": "2021-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwcia52852.2021.9626020"
    },
    {
        "id": 29106,
        "title": "Implementation of Reinforcement Learning Simulated Madel on Physical UGV Using Robot Operating System for Continual Learning",
        "authors": "Edgar M. Perez, Abhijit Majumdar, Patrick Benavidez, Mo Jamshidi",
        "published": "2019-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sysose.2019.8753801"
    },
    {
        "id": 29107,
        "title": "Application of Reinforcement Learning in SINS/GNSS/DVL Integrated Navigation",
        "authors": "Xiangyuan Li, Xuesong Tang, Xingshu Wang, Wenfeng Tan, Jiaxing Zheng, Yingwei Zhao",
        "published": "2022-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlccim55934.2022.00066"
    },
    {
        "id": 29108,
        "title": "Deep reinforcement learning collision avoidance using policy gradient optimisation and Q-learning",
        "authors": "Shady A. Maged, Bishoy H. Mikhail",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijcvr.2020.107253"
    },
    {
        "id": 29109,
        "title": "Reinforcement Learning-Based Network Slice Resource Allocation for Federated Learning Applications",
        "authors": "Zhouxiang Wu, Genya Ishigaki, Riti Gour, Congzhou Li, Jason P. Jue",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001715"
    },
    {
        "id": 29110,
        "title": "Multiagent Coordination Systems Based on Neuro-Fuzzy Models with Reinforcement Learning",
        "authors": "Leonardo Alfredo Mendoza, Evelyn Batista, Harold Dias De Mello, Marco Aurelio Pacheco",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla.2018.00151"
    },
    {
        "id": 29111,
        "title": "A Curriculum Learning Based Multi-agent Reinforcement Learning Method for Realtime Strategy Game",
        "authors": "Dayu Zhang, Weidong Bao, Wenqian Liang, Guanlin Wu, Jiang Cao",
        "published": "2022-8-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdia56350.2022.9874056"
    },
    {
        "id": 29112,
        "title": "Learning to navigate a crystallization model with Deep Reinforcement Learning",
        "authors": "Vidhyadhar Manee, Roberto Baratti, Jose A. Romagnoli",
        "published": "2022-2",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cherd.2021.12.005"
    },
    {
        "id": 29113,
        "title": "Deep Reinforcement Learning for Optimization",
        "authors": "Md Mahmudul Hasan, Md Shahinur Rahman, Adrian Bell",
        "published": "2019",
        "citations": 1,
        "abstract": "Deep reinforcement learning (DRL) has transformed the field of artificial intelligence (AI) especially after the success of Google DeepMind. This branch of machine learning epitomizes a step toward building autonomous systems by understanding of the visual world. Deep reinforcement learning (RL) is currently applied to different sorts of problems that were previously obstinate. In this chapter, at first, the authors started with an introduction of the general field of RL and Markov decision process (MDP). Then, they clarified the common DRL framework and the necessary components RL settings. Moreover, they analyzed the stochastic gradient descent (SGD)-based optimizers such as ADAM and a non-specific multi-policy selection mechanism in a multi-objective Markov decision process. In this chapter, the authors also included the comparison for different Deep Q networks. In conclusion, they describe several challenges and trends in research within the deep reinforcement learning field. ",
        "link": "http://dx.doi.org/10.4018/978-1-5225-7862-8.ch011"
    },
    {
        "id": 29114,
        "title": "Learning Human-Aware Robot Navigation from Physical Interaction via Inverse Reinforcement Learning",
        "authors": "Marina Kollmitz, Torsten Koller, Joschka Boedecker, Wolfram Burgard",
        "published": "2020-10-24",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros45743.2020.9340865"
    },
    {
        "id": 29115,
        "title": "Coordinated Voltage Regulation of Microgrid Clusters Based on Deep Reinforcement Learning Approach",
        "authors": "Xiaozhe Xue, Hui Ge",
        "published": "2023-5-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls58216.2023.10166110"
    },
    {
        "id": 29116,
        "title": "When Does Communication Learning Need Hierarchical Multi-Agent Deep Reinforcement Learning",
        "authors": "Marie Ossenkopf, Mackenzie Jorgensen, Kurt Geihs",
        "published": "2019-11-17",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/01969722.2019.1677335"
    },
    {
        "id": 29117,
        "title": "Correction: Optimum trajectory learning in musculoskeletal systems with model predictive control and deep reinforcement learning",
        "authors": "Berat Denizdurduran, Henry Markram, Marc-Oliver Gewaltig",
        "published": "2022-10-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00422-022-00949-2"
    },
    {
        "id": 29118,
        "title": "Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-Resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning",
        "authors": "Md Tamjid Hossain, John W. T. Lee",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmlc58545.2023.10327979"
    },
    {
        "id": 29119,
        "title": "Cooperative zone-based rebalancing of idle overhead hoist transportations using multi-agent reinforcement learning with graph representation learning",
        "authors": "Kyuree Ahn, Jinkyoo Park",
        "published": "2021-2-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/24725854.2020.1851823"
    },
    {
        "id": 29120,
        "title": "Learning to Grasp on the Moon from 3D Octree Observations with Deep Reinforcement Learning",
        "authors": "Andrej Orsula, Simon Bøgh, Miguel Olivares-Mendez, Carol Martinez",
        "published": "2022-10-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981661"
    },
    {
        "id": 29121,
        "title": "Developing multi-agent adversarial environment using reinforcement learning and imitation learning",
        "authors": "Ziyao Han, Yupeng Liang, Kazuhiro Ohkura",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10015-023-00912-9"
    },
    {
        "id": 29122,
        "title": "Learning from Unreliable Human Action Advice in Interactive Reinforcement Learning",
        "authors": "Lisa Scherf, Cigdem Turan, Dorothea Koert",
        "published": "2022-11-28",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/humanoids53995.2022.10000078"
    },
    {
        "id": 29123,
        "title": "Path planning of mobile robot based on deep reinforcement learning with transfer learning strategy",
        "authors": "Jie Zhu, Chuanhai Yang, Zhaodong Liu, Chengdong Yang",
        "published": "2022-11-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/yac57282.2022.10023708"
    },
    {
        "id": 29124,
        "title": "Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks",
        "authors": "Yijie Guo, Qiucheng Wu, Honglak Lee",
        "published": "2022-6-28",
        "citations": 1,
        "abstract": "Meta reinforcement learning (meta-RL) aims to learn a policy solving a set of training tasks simultaneously and quickly adapting to new tasks. It requires massive amounts of data drawn from training tasks to infer the common structure shared among tasks. Without heavy reward engineering, the sparse rewards in long-horizon tasks exacerbate the problem of sample efficiency in meta-RL. Another challenge in meta-RL is the discrepancy of difficulty level among tasks, which might cause one easy task dominating learning of the shared policy and thus preclude policy adaptation to new tasks. This work introduces a novel objective function to learn an action translator among training tasks. We theoretically verify that the value of the transferred policy with the action translator can be close to the value of the source policy and our objective function (approximately) upper bounds the value difference. We propose to combine the action translator with context-based meta-RL algorithms for better data collection and moreefficient exploration during meta-training. Our approach em-pirically improves the sample efficiency and performance ofmeta-RL algorithms on sparse-reward tasks.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i6.20635"
    },
    {
        "id": 29125,
        "title": "Reinforcement Learning for PHEV Route Choice Based on Congestion Game",
        "authors": "Huiwei Wang, Huaqing Li, Bo Zhou",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-4528-7_9"
    },
    {
        "id": 29126,
        "title": "Correction: Optimum trajectory learning in musculoskeletal systems with model predictive control and deep reinforcement learning",
        "authors": "Berat Denizdurduran, Henry Markram, Marc-Oliver Gewaltig",
        "published": "2022-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00422-022-00947-4"
    },
    {
        "id": 29127,
        "title": "Guest editorial: special issue on reinforcement learning for real life",
        "authors": "Yuxi Li, Alborz Geramifard, Lihong Li, Csaba Szepesvari, Tao Wang",
        "published": "2021-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-021-06041-3"
    },
    {
        "id": 29128,
        "title": "Critical State Detection for Adversarial Attacks in Deep Reinforcement Learning",
        "authors": "R Praveen Kumar, I Niranjan Kumar, Sujith Sivasankaran, A Mohan Vamsi, Vineeth Vijayaraghavan",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00279"
    },
    {
        "id": 29129,
        "title": "Reinforcement Learning PCG",
        "authors": "Matthew Guzdial, Sam Snodgrass, Adam J. Summerville",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-16719-5_10"
    },
    {
        "id": 29130,
        "title": "Learning and Transfer of Movement Gaits Using Reinforcement Learning",
        "authors": "David Waidner, Marcus Strand",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-86294-7_34"
    },
    {
        "id": 29131,
        "title": "Learning to Walk: Spike Based Reinforcement Learning for Hexapod Robot Central Pattern Generation",
        "authors": "Ashwin Sanjay Lele, Yan Fang, Justin Ting, Arijit Raychowdhury",
        "published": "2020-8",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aicas48895.2020.9073987"
    },
    {
        "id": 29132,
        "title": "Interpretable Machine Learning for Reinforcement Learning based Control Strategy Optimization of Industrial Energy Supply Systems",
        "authors": "Arthur Arthur, Arthur Stobert, Heiko Ranzau, Inga Pfenning, Matthias Weigold",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46855/energy-proceedings-11066"
    },
    {
        "id": 29133,
        "title": "Deep Flamingo Search and Reinforcement Learning Based Recommendation System for E-Learning Platform using Social Media",
        "authors": "N Vedavathi, R Suhas Bharadwaj",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2022.12.022"
    },
    {
        "id": 29134,
        "title": "Optimal Hierarchical Learning Path Design With Reinforcement Learning",
        "authors": "Xiao Li, Hanchen Xu, Jinming Zhang, Hua-hua Chang",
        "published": "2021-1",
        "citations": 5,
        "abstract": " E-learning systems are capable of providing more adaptive and efficient learning experiences for learners than traditional classroom settings. A key component of such systems is the learning policy. The learning policy is an algorithm that designs the learning paths or rather it selects learning materials for learners based on information such as the learners’ current progresses and skills, learning material contents. In this article, the authors address the problem of finding the optimal learning policy. To this end, a model for learners’ hierarchical skills in the E-learning system is first developed. Based on the hierarchical skill model and the classical cognitive diagnosis model, a framework to model various mastery levels related to hierarchical skills is further developed. The optimal learning path in consideration of the hierarchical structure of skills is found by applying a model-free reinforcement learning method, which does not require any assumption about learners’ learning transition processes. The effectiveness of the proposed framework is demonstrated via simulation studies. ",
        "link": "http://dx.doi.org/10.1177/0146621620947171"
    },
    {
        "id": 29135,
        "title": "Enhancing Machine Learning Based Malware Detection Model by Reinforcement Learning",
        "authors": "Cangshuai Wu, Jiangyong Shi, Yuexiang Yang, Wenhua Li",
        "published": "2018-11-2",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3290480.3290494"
    },
    {
        "id": 29136,
        "title": "Automated Aircraft Stall Recovery using Reinforcement Learning and Supervised Learning Techniques",
        "authors": "Dheerenrda Singh Tomar, Jason Gauci, Alexiei Dingli, Alan Muscat, David Zammit Mangion",
        "published": "2021-10-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dasc52595.2021.9594316"
    },
    {
        "id": 29137,
        "title": "Tabular Q-learning Based Reinforcement Learning Agent for Autonomous Vehicle Drift Initiation and Stabilization",
        "authors": "Szilárd H. Tóth, Ádám Bárdos, Zsolt J. Viharos",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.10.1261"
    },
    {
        "id": 29138,
        "title": "Automated clash resolution for reinforcement steel design in precast concrete wall panels via generative adversarial network and reinforcement learning",
        "authors": "Pengkun Liu, Hongtuo Qi, Jiepeng Liu, Liang Feng, Dongsheng Li, Jingjing Guo",
        "published": "2023-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.aei.2023.102131"
    },
    {
        "id": 29139,
        "title": "Hierarchical Adversarial Inverse Reinforcement Learning",
        "authors": "Jiayu Chen, Tian Lan, Vaneet Aggarwal",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3305983"
    },
    {
        "id": 29140,
        "title": "The Implementation of Deep Reinforcement Learning in E-Learning and Distance Learning: Remote Practical Work",
        "authors": "Abdelali El Gourari, Mustapha Raoufi, Mohammed Skouri, Fahd Ouatik",
        "published": "2021-6-25",
        "citations": 10,
        "abstract": "The world has seen major developments in the field of e-learning and distance learning, especially during the COVID-19 crisis, which revealed the importance of these two types of education and the fruitful benefits they have offered in a group of countries, especially those that have excellent infrastructure. At the Faculty of Sciences Semlalia, Cadi Ayyad University Marrakech, Morocco, we have created a simple electronic platform for remote practical work (RPW), and its results have been good in terms of student interaction and even facilitating the employment of a professor. The objective of this work is to propose a recommendation system based on deep quality-learning networks (DQNs) to recommend and direct students in advance of doing the RPW according to their skills of each mouse or keyboard click per student. We are focusing on this technology because it has strong, tremendous visibility and problem-solving ability that we will demonstrate in the result section. Our platform has enabled us to collect a range of students’ and teachers’ information and their interactions with the learning content we will rely on as inputs (a large number of images per second for each mouse or keyboard click per student) into our new system for output (doing the RPW). This technique is reflected in an attempt to embody the virtual teacher’s image within the platform and then adequately trained with DQN technology to perform the RPW.",
        "link": "http://dx.doi.org/10.1155/2021/9959954"
    },
    {
        "id": 29141,
        "title": "Learning a Low-Dimensional Representation of a Safe Region for Safe Reinforcement Learning on Dynamical Systems",
        "authors": "Zhehua Zhou, Ozgur S. Oguz, Marion Leibold, Martin Buss",
        "published": "2023-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3106818"
    },
    {
        "id": 29142,
        "title": "Learning What to Share in Online Social Networks Using Deep Reinforcement Learning",
        "authors": "Shatha Jaradat, Nima Dokoohaki, Mihhail Matskin, Elena Ferrari",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-89932-9_6"
    },
    {
        "id": 29143,
        "title": "Deep reinforcement learning collision avoidance using policy gradient optimisation and Q-learning",
        "authors": "Bishoy H. Mikhail, Shady A. Maged",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1504/ijcvr.2020.10029037"
    },
    {
        "id": 29144,
        "title": "An improvement of the learning speed through Influence Map on Reinforcement Learning",
        "authors": "Yong-Woo Shin",
        "published": "2017-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7583/jkgs.2017.17.4.109"
    },
    {
        "id": 29145,
        "title": "TeachMe: Three-phase learning framework for robotic motion imitation based on interactive teaching and reinforcement learning",
        "authors": "Taewoo Kim, Joo-Haeng Lee",
        "published": "2019-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ro-man46459.2019.8956326"
    },
    {
        "id": 29146,
        "title": "An online scalarization multi-objective reinforcement learning algorithm: TOPSIS Q-learning",
        "authors": "Mohammad Mirzanejad, Morteza Ebrahimi, Peter Vamplew, Hadi Veisi",
        "published": "2022",
        "citations": 3,
        "abstract": "Abstract\nConventional reinforcement learning focuses on problems with single objective. However, many problems have multiple objectives or criteria that may be independent, related, or contradictory. In such cases, multi-objective reinforcement learning is used to propose a compromise among the solutions to balance the objectives. TOPSIS is a multi-criteria decision method that selects the alternative with minimum distance from the positive ideal solution and the maximum distance from the negative ideal solution, so it can be used effectively in the decision-making process to select the next action. In this research a single-policy algorithm called TOPSIS Q-Learning is provided with focus on its performance in online mode. Unlike all single-policy methods, in the first version of the algorithm, there is no need for the user to specify the weights of the objectives. The user’s preferences may not be completely definite, so all weight preferences are combined together as decision criteria and a solution is generated by considering all these preferences at once and user can model the uncertainty and weight changes of objectives around their specified preferences of objectives. If the user only wants to apply the algorithm for a specific set of weights the second version of the algorithm efficiently accomplishes that.",
        "link": "http://dx.doi.org/10.1017/s0269888921000163"
    },
    {
        "id": 29147,
        "title": "Uncertainty-Aware Autonomous Mobile Robot Navigation with Deep Reinforcement Learning",
        "authors": "Lynnette González-Rodríguez, Armando Plasencia-Salgueiro",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-77939-9_7"
    },
    {
        "id": 29148,
        "title": "Human-Centered AI using Ethical Causality and Learning Representation for Multi-Agent Deep Reinforcement Learning",
        "authors": "Joshua Ho, Chien-Min Wang",
        "published": "2021-9-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ichms53169.2021.9582667"
    },
    {
        "id": 29149,
        "title": "Unmanned Aerial Vehicles (UAV) relay in Vehicular Ad Hoc Network (VANETs) against smart jamming with reinforcement learning/deep learning",
        "authors": "",
        "published": "2022-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/pbtr036e_ch5"
    },
    {
        "id": 29150,
        "title": "Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning",
        "authors": "Linhai Xie, Sen Wang, Stefano Rosa, Andrew Markham, Niki Trigoni",
        "published": "2018-5",
        "citations": 47,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2018.8461203"
    },
    {
        "id": 29151,
        "title": "Learning to Design Games: Strategic Environments in Reinforcement Learning",
        "authors": "Haifeng Zhang, Jun Wang, Zhiming Zhou, Weinan Zhang, Yin Wen, Yong Yu, Wenxin Li",
        "published": "2018-7",
        "citations": 6,
        "abstract": "In typical reinforcement learning (RL), the environment is assumed given and the goal of the learning is to identify an optimal policy for the agent taking actions through its interactions with the environment.  In this paper, we extend this setting by considering the environment is not given, but controllable and learnable through its interaction with the agent at the same time. This extension is motivated by environment design scenarios in the real-world, including game design, shopping space design and traffic signal design. Theoretically, we find a dual Markov decision process (MDP) w.r.t. the environment to that w.r.t. the agent, and derive a policy gradient solution to optimizing the parametrized environment. Furthermore, discontinuous environments are addressed by a proposed general generative framework. Our experiments on a Maze game design task show the effectiveness of the proposed algorithms in generating diverse and challenging Mazes against various agent settings.",
        "link": "http://dx.doi.org/10.24963/ijcai.2018/426"
    },
    {
        "id": 29152,
        "title": "BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?",
        "authors": "Xingmeng Zhao, Tongnian Wang, Sheri Osborn, Anthony Rios",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-babylm.16"
    },
    {
        "id": 29153,
        "title": "Fairness in Learning-Based Sequential Decision Algorithms: A Survey",
        "authors": "Xueru Zhang, Mingyan Liu",
        "published": "2021",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-60990-0_18"
    },
    {
        "id": 29154,
        "title": "Train timetabling with the general learning environment and multi-agent deep reinforcement learning",
        "authors": "Wenqing Li, Shaoquan Ni",
        "published": "2022-3",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.trb.2022.02.006"
    },
    {
        "id": 29155,
        "title": "Parallel reward and punishment control in humans and robots: Safe reinforcement learning using the MaxPain algorithm",
        "authors": "Stefan Elfwing, Ben Seymour",
        "published": "2017-9",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/devlrn.2017.8329799"
    },
    {
        "id": 29156,
        "title": "Nondominated Policy-Guided Learning in Multi-Objective Reinforcement Learning",
        "authors": "Man-Je Kim, Hyunsoo Park, Chang Wook Ahn",
        "published": "2022-3-28",
        "citations": 1,
        "abstract": "Control intelligence is a typical field where there is a trade-off between target objectives, and researchers in this field have longed for artificial intelligence that achieves the target objectives. Multi-objective deep reinforcement learning was sufficient to satisfy this need. In particular, multi-objective deep reinforcement learning methods based on policy optimization are leading the optimization of control intelligence. However, multi-objective reinforcement learning has difficulties when finding various Pareto optimals of multi-objectives due to the greedy nature of reinforcement learning. We propose a method of policy assimilation to solve this problem. This method was applied to MO-V-MPO, one of preference-based multi-objective reinforcement learning, to increase diversity. The performance of this method has been verified through experiments in a continuous control environment.",
        "link": "http://dx.doi.org/10.3390/electronics11071069"
    },
    {
        "id": 29157,
        "title": "A reinforcement learning approach for thermostat setpoint preference learning",
        "authors": "Hussein Elehwany, Mohamed Ouf, Burak Gunay, Nunzio Cotrufo, Jean-Simon Venne",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s12273-023-1056-7"
    },
    {
        "id": 29158,
        "title": "K-Qbot: Language Learning Chatbot Based on Reinforcement Learning",
        "authors": "Nurziya Oralbayeva, Aidar Shakerimov, Shamil Sarmonov, Kanagat Kantoreyeva, Fatima Dadebayeva, Nuray Serkali, Anara Sandygulova",
        "published": "2022-3-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hri53351.2022.9889428"
    },
    {
        "id": 29159,
        "title": "Deep Reinforcement Learning for Quadrotor Path Following and Obstacle Avoidance",
        "authors": "Bartomeu Rubí, Bernardo Morcego, Ramon Pérez",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-77939-9_17"
    },
    {
        "id": 29160,
        "title": "Modeling the Development of Infant Imitation using Inverse Reinforcement Learning",
        "authors": "Ahmet E. Tekden, Emre Ugur, Yukie Nagai, Erhan Oztop",
        "published": "2018-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/devlrn.2018.8761045"
    },
    {
        "id": 29161,
        "title": "A New Asynchronous Architecture for Tabular Reinforcement Learning Algorithms",
        "authors": "Xingyu Zhao, Shifei Ding, Yuexuan An",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-01520-6_15"
    },
    {
        "id": 29162,
        "title": "Application of Deep Reinforcement Learning and Transfer Learning for Optimization of Geometry Parameters of Corrugated Wing",
        "authors": "Toshikazu Noda, Kie Okabayashi, Shintaro Takeuchi, Takeo Kajishima",
        "published": "2022-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2022-0458"
    },
    {
        "id": 29163,
        "title": "A behavioral-economic analysis of demand and preference for social and food reinforcement in rats",
        "authors": "Cyrus Kirkman, Haoran Wan, Timothy D. Hackenberg",
        "published": "2022-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.lmot.2021.101780"
    },
    {
        "id": 29164,
        "title": "Correction to: Model-free inverse reinforcement learning with multi-intention, unlabeled, and overlapping demonstrations",
        "authors": "Ariyan Bighashdel, Pavol Jancura, Gijs Dubbelman",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-022-06298-2"
    },
    {
        "id": 29165,
        "title": "Macquarie University at BioASQ 6b: Deep learning and deep reinforcement learning for query-based summarisation",
        "authors": "Diego Mollá",
        "published": "2018",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w18-5303"
    },
    {
        "id": 29166,
        "title": "Reinforcement learning for evolutionary distance metric learning systems improvement",
        "authors": "Bassel Ali, Wasin Kalintha, Koichi Moriyama, Masayuki Numao, Ken-ichi Fukui",
        "published": "2018-7-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3205651.3205675"
    },
    {
        "id": 29167,
        "title": "Automatic construction of Markov decision process models for multi-agent reinforcement learning",
        "authors": "Darrell L. Young, Chris Eccles",
        "published": "2020-4-21",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2557823"
    },
    {
        "id": 29168,
        "title": "Exploiting Multi-Task Learning to Achieve Effective Transfer Deep Reinforcement Learning in Elastic Optical Networks",
        "authors": "Xiaoliang Chen, Roberto Proietti, Che-Yu Liu, Zuqing Zhu, S. J. Ben Yoo",
        "published": "2020",
        "citations": 10,
        "abstract": "We propose a multi-task-learning-aided knowledge transferring approach for effective and scalable deep reinforcement learning in EONs. Case studies with RMSA show that this approach can achieve ∼ 4× learning time reduction and ∼17.7% lower blocking probability.",
        "link": "http://dx.doi.org/10.1364/ofc.2020.m1b.3"
    },
    {
        "id": 29169,
        "title": "Learning Network Representation Through Reinforcement Learning",
        "authors": "Siqi Shen, Yongquan Fu, Adele Lu Jia, Huayou Su, Qinglin Wang, Chengsong Wang, Yong Dou",
        "published": "2020-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp40776.2020.9053879"
    },
    {
        "id": 29170,
        "title": "An Efficiency Framework for Task Allocation Based on Reinforcement Learning",
        "authors": "Jun Xu, Weiwei Li, Jun Yao",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicml60161.2023.10424951"
    },
    {
        "id": 29171,
        "title": "A novel implicit hybrid machine learning model and its application for reinforcement learning",
        "authors": "Derek Machalek, Titus Quah, Kody M. Powell",
        "published": "2021-12",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compchemeng.2021.107496"
    },
    {
        "id": 29172,
        "title": "When Learning Joins Edge: Real-Time Proportional Computation Offloading via Deep Reinforcement Learning",
        "authors": "Ning Chen, Sheng Zhang, Zhuzhong Qian, Jie Wu, Sanglu Lu",
        "published": "2019-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpads47876.2019.00066"
    },
    {
        "id": 29173,
        "title": "Evaluating skills in hierarchical reinforcement learning",
        "authors": "Marzieh Davoodabadi Farahani, Nasser Mozayani",
        "published": "2020-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s13042-020-01141-3"
    },
    {
        "id": 29174,
        "title": "Integration into a Flexible Manufacturing System",
        "authors": "Schirin Bär",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-39179-9_7"
    },
    {
        "id": 29175,
        "title": "Towards Bootstrapping Biomedical Named Entity Recognition using Reinforcement Learning",
        "authors": "Dongsheng Wang, Hongjie Fan, Junfei Liu",
        "published": "2020-12-16",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm49941.2020.9313571"
    },
    {
        "id": 29176,
        "title": "Intersection Navigation Under Dynamic Constraints Using Deep Reinforcement Learning",
        "authors": "Ali Demir, Volkan Sezer",
        "published": "2018-10",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ceit.2018.8751788"
    },
    {
        "id": 29177,
        "title": "3  Reinforcement Learning am Digitalen Zwilling für die Handhabung flexibler Objekte",
        "authors": "F. Jaensch, A. Verl",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.51202/9783186709028-23"
    },
    {
        "id": 29178,
        "title": "Interactions between supervised and reinforcement learning processes in a neurorobotic model",
        "authors": "Adriano Capirchio, Chiara Ponte, Gianluca Baldassarre, Francesco Mannella, Elisa Pelosin, Daniele Caligiore",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractSeveral influential works propose that the acquisition of motor behavior involves different learning mechanisms in the brain, in particular supervised and reinforcement learning, that are respectively associated with cerebellar-thalamocortical and basal ganglia-thalamocortical networks. Despite increasing evidence suggesting anatomical and functional interactions between these circuits, the learning processes operating within them are studied in isolation, neglecting their strong interdependence. This article proposes a bio-inspired neurorobotic model implementing a possible cooperation mechanism between supervised and reinforcement learning. The model, validated with empirical data from healthy participants and patients with cerebellar ataxia, shows how the integration of the two learning processes could lead to benefit both learning performance and movement accuracy.",
        "link": "http://dx.doi.org/10.1101/2022.09.30.510289"
    },
    {
        "id": 29179,
        "title": "Reinforcement Learning Based Automatic Personal Mashup Generation",
        "authors": "Panwen Hu, Jiazhen Liu, Tianyu Cao, Rui Huang",
        "published": "2021-7-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme51207.2021.9428357"
    },
    {
        "id": 29180,
        "title": "Token-Based Deep Reinforcement Learning for Heterogeneous Vrp with Service Time",
        "authors": "Yujun Wang, Xiaopeng HONG, Yabin Wang, Junzhou Zhao, Guanghui Sun, Baoxing Qin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4476906"
    },
    {
        "id": 29181,
        "title": "Evaluation and Potential Improvements of a Deep Reinforcement Learning Model for Automated Stock Trading",
        "authors": "Rainer Andreas Jager",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3786304"
    },
    {
        "id": 29182,
        "title": "Adaptive Flexible Switching Mode Control of the Aircraft Skin Inspection Robot Using Integral Reinforcement Learning",
        "authors": "Xuewei Wu, Congqing Wang",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9327735"
    },
    {
        "id": 29183,
        "title": "Experiments of Power-Frequency Constant Estimation of Load-Frequency Control Using Reinforcement Learning",
        "authors": "Shota Nishimura, Nobuyuki Yamaguchi",
        "published": "2020-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/psc50246.2020.9131330"
    },
    {
        "id": 29184,
        "title": "Safe residual reinforcement learning for helicopter aerial refueling",
        "authors": "Damsara Jayarathne, Santiago Paternain, Sandipan Mishra",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aim46323.2023.10196137"
    },
    {
        "id": 29185,
        "title": "Safe and Sample-Efficient Reinforcement Learning Algorithms for Factored Environments",
        "authors": "Thiago D. Simão",
        "published": "2019-8",
        "citations": 0,
        "abstract": "Reinforcement Learning (RL) deals with problems that can be modeled as a Markov Decision Process (MDP) where the transition function is unknown. In situations where an arbitrary policy pi is already in execution and the experiences with the environment were recorded in a batch D, an RL algorithm can use D to compute a new policy pi'. However, the policy computed by traditional RL algorithms might have worse performance compared to pi. Our goal is to develop safe RL algorithms, where the agent has a high confidence that the performance of pi' is better than the performance of pi given D. To develop sample-efficient and safe RL algorithms we combine ideas from exploration strategies in RL with a safe policy improvement method.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/919"
    },
    {
        "id": 29186,
        "title": "Reinforcement Learning",
        "authors": "Merin Deora, Sumit Mathur",
        "published": "2017-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17148/ijarcce.2017.6433"
    },
    {
        "id": 29187,
        "title": "Optimal entscheiden in einer bekannten Umwelt",
        "authors": "Uwe Lorenz",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-662-61651-2_3"
    },
    {
        "id": 29188,
        "title": "Reinforcement learning for tiled aperture beam combining (Conference Presentation)",
        "authors": "Henrik Tünnermann, Akira Shirakawa",
        "published": "2020-3-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2545578"
    },
    {
        "id": 29189,
        "title": "HeuRL: A Heuristically Initialized Reinforcement Learning Method for Autonomous Driving Control Task",
        "authors": "Jiaxuan Xu, Jian Yuan",
        "published": "2018-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccr.2018.8534494"
    },
    {
        "id": 29190,
        "title": "Adaptive Sliding Mode Disturbance Observer and Deep Reinforcement Learning based Robust Motion Control for Micropositioners",
        "authors": "Shiyun Liang, Ruidong Xi, Xiao Xiao, Zhixin Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "The robust control of high precision electromechanical systems, such as micropositioners, is challenging in terms of the inherent high nonlinearity, the sensitivity to external interference, and the complexity of accurate identification of the model parameters. To cope with these problems, this work investigates a disturbance observer-based deep reinforcement learning control strategy to realize high robustness and precise tracking performance. Reinforcement learning has shown great potential as optimal control scheme, however, its application in micropositioning systems is still rare. Therefore, embedded with the integral differential compensator (ID), deep deterministic policy gradient (DDPG) is utilized in this work with the ability to not only decrease the state error but also improves the transient response speed. In addition, an adaptive sliding mode disturbance observer (ASMDO) is proposed to further eliminate the collective effect caused by the lumped disturbances. The sterling performance is revealed with intensive tracking simulation experiments and demonstrates the improvement in the accuracy and response time of the controller.",
        "link": "http://dx.doi.org/10.20944/preprints202203.0199.v1"
    },
    {
        "id": 29191,
        "title": "Prefrontal Solution to the Bias-Variance Tradeoff During Reinforcement Learning",
        "authors": "Dongjae Kim, Jaeseung Jeong, Sang Wan Lee",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3811830"
    },
    {
        "id": 29192,
        "title": "Quantification of Joint Redundancy considering Dynamic Feasibility using Deep Reinforcement Learning",
        "authors": "Jiazheng Chai, Mitsuhiro Hayashibe",
        "published": "2021-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9561048"
    },
    {
        "id": 29193,
        "title": "Review on Reinforcement Learning-Based Energy Management Strategies for Hybrid Electric Vehicles",
        "authors": "Hwan-Sik Yoon",
        "published": "2022-2-7",
        "citations": 0,
        "abstract": "Hybrid Electric Vehicles (HEVs) achieve better fuel economy than conventional vehicles by employing two different power sources: a mechanical engine and an electrical motor. These power sources have conventionally been controlled by a rule-based algorithm or optimization-based control. Besides these conventional approaches, reinforcement learning-based control algorithms have actively been studied recently. Reinforcement learning, which is one of three machine learning paradigms, has the capability of determining optimal control actions to maximize a vehicle’s fuel economy without the vehicle model nor a priori driving route information. To provide a useful reference to researchers interested in this technology, this article reviews reinforcement learning-based energy management strategies for HEVs with their advantages and disadvantages.",
        "link": "http://dx.doi.org/10.31031/eme.2022.04.000579"
    },
    {
        "id": 29194,
        "title": "A3C in Code",
        "authors": "Mohit Sewak",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-13-8285-7_12"
    },
    {
        "id": 29195,
        "title": "Mathematical Reinforcement to the Minibatch of Deep Learning",
        "authors": "Kazuyuki Fujii",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4236/apm.2018.83016"
    },
    {
        "id": 29196,
        "title": "Optimization for reinforcement learning based 3D animation exercise",
        "authors": "Ximan Shi",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s12065-022-00740-z"
    },
    {
        "id": 29197,
        "title": "Distributed Reinforcement Learning Based Adaptive Antenna Pattern Selection for Wireless Communication Networks",
        "authors": "Yu-An Chen",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icce-taiwan58799.2023.10226834"
    },
    {
        "id": 29198,
        "title": "Mobile Service Robot Path Planning Using Deep Reinforcement Learning",
        "authors": "A. A. Nippun Kumaar, Sreeja Kochuvila",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3311519"
    },
    {
        "id": 29199,
        "title": "Hybrid Car Following Control for Cavs: Integrating Linear Feedback and Deep Reinforcement Learning to Stabilize Mixed Traffic",
        "authors": "Ximin Yue, Haotian Shi, Yang Zhou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4686157"
    },
    {
        "id": 29200,
        "title": "Compulsivity is linked to maladaptive choice variability but unaltered reinforcement learning under uncertainty",
        "authors": "Junseok K. Lee, Marion Rouault, Valentin Wyart",
        "published": "No Date",
        "citations": 2,
        "abstract": "Compulsivity has been associated with variable behavior under uncertainty. However, previous work has not distinguished between two main sources of behavioral variability: the stochastic selection of choice options that do not maximize expected reward (choice variability), and random noise in the reinforcement learning process that updates option values from choice outcomes (learning variability). Here we studied the relation between dimensional compulsivity and behavioral variability, using a computational model which dissociates its two sources. We found that compulsivity is associated with more frequent switches between options, triggered by increased choice variability but no change in learning variability. This effect of compulsivity on the ‘trait’ component of choice variability is observed even in conditions where this source of behavioral variability yields no cognitive benefits. These findings indicate that compulsive individuals make variable and maladaptive choices under uncertainty, but do not hold degraded representations of option values.",
        "link": "http://dx.doi.org/10.1101/2023.01.05.522867"
    },
    {
        "id": 29201,
        "title": "Application of Reinforcement Learning in Code Repair",
        "authors": "Young Kim",
        "published": "2022-5-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26821/ijshre.10.4.2022.100204"
    },
    {
        "id": 29202,
        "title": "Control of Shared Production Buffers: A Reinforcement Learning Approach",
        "authors": "N. Krippendorff, C. Schwindt",
        "published": "2021-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieem50564.2021.9673034"
    },
    {
        "id": 29203,
        "title": "Blind Adaptive Gait Planning on Non-stationary Environments via Continual Reinforcement Learning",
        "authors": "Hao Hu, Yang Liu",
        "published": "2021-10-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus52573.2021.9641095"
    },
    {
        "id": 29204,
        "title": "Reinforcement-learning calibration of coherent-state receivers on variable-loss optical channels",
        "authors": "M. Bilkis, Matteo Rosati, John Calsamiglia",
        "published": "2021-10-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itw48936.2021.9611396"
    },
    {
        "id": 29205,
        "title": "Using Reinforcement Learning to Increase Grid Security Under Contingency Conditions",
        "authors": "Stephen Verzi, Ross Guttromson, Ace Sorensen",
        "published": "2022-4-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/kpec54747.2022.9814746"
    },
    {
        "id": 29206,
        "title": "Applying Quantum REINFORCE to the Information Game",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1_7"
    },
    {
        "id": 29207,
        "title": "Reinforcement Learning Applied to Cognitive Space Communications",
        "authors": "Carson D. Schubert, Rigoberto Roche, Janette C. Briones",
        "published": "2019-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccaaw.2019.8904912"
    },
    {
        "id": 29208,
        "title": "TrackDQN: Visual Tracking via Deep Reinforcement Learning",
        "authors": "Pei Yang, Jiyue Huang",
        "published": "2019-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccasit48058.2019.8973189"
    },
    {
        "id": 29209,
        "title": "Exploiting Action-Value Uncertainty to Drive Exploration in Reinforcement Learning",
        "authors": "Carlo D'Eramo, Andrea Cini, Marcello Restelli",
        "published": "2019-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2019.8852326"
    },
    {
        "id": 29210,
        "title": "Temporal encoding in deep reinforcement learning agents",
        "authors": "Dongyan Lin, Ann Zixiang Huang, Blake Aaron Richards",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "AbstractNeuroscientists have observed both cells in the brain that fire at specific points in time, known as “time cells”, and cells whose activity steadily increases or decreases over time, known as “ramping cells”. It is speculated that time and ramping cells support temporal computations in the brain and carry mnemonic information. However, due to the limitations in animal experiments, it is difficult to determine how these cells really contribute to behavior. Here, we show that time cells and ramping cells naturally emerge in the recurrent neural networks of deep reinforcement learning models performing simulated interval timing and working memory tasks, which have learned to estimate expected rewards in the future. We show that these cells do indeed carry information about time and items stored in working memory, but they contribute to behavior in large part by providing a dynamic representation on which policy can be computed. Moreover, the information that they do carry depends on both the task demands and the variables provided to the models. Our results suggest that time cells and ramping cells could contribute to temporal and mnemonic calculations, but the way in which they do so may be complex and unintuitive to human observers.",
        "link": "http://dx.doi.org/10.1038/s41598-023-49847-y"
    },
    {
        "id": 29211,
        "title": "A New Reinforcement Learning Algorithm Based on Counterfactual Experience Replay",
        "authors": "Li Menglin, Chen Jing, Chen Shaofei, Gao Wei",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9189606"
    },
    {
        "id": 29212,
        "title": "Robotic Grasping in Simulation Using Deep Reinforcement Learning",
        "authors": "Musab Coskun, Ozal Yildirim, Yakup Demir",
        "published": "2022-9-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ubmk55850.2022.9919482"
    },
    {
        "id": 29213,
        "title": "A Discrete-Continuous Reinforcement Learning Algorithm for Unit Commitment and Dispatch Problem",
        "authors": "Ping Zheng, Yuezu Lv",
        "published": "2022-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus55513.2022.9987086"
    },
    {
        "id": 29214,
        "title": "Variable Speed Limit Control Based on Deep Reinforcement Learning: A Possible Implementation",
        "authors": "Martin Greguric, Kresimir Kusic, Filip Vrbanic, Edouard Ivanjko",
        "published": "2020-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/elmar49956.2020.9219031"
    },
    {
        "id": 29215,
        "title": "Deep reinforcement learning based multi-layered traffic scheduling scheme in data center networks",
        "authors": "Guihua Wu",
        "published": "2022-2-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11276-021-02883-w"
    },
    {
        "id": 29216,
        "title": "Advancements in Reinforcement Learning Algorithms and their Applications in Various Domains",
        "authors": "",
        "published": "2023-6-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.56726/irjmets40341"
    },
    {
        "id": 29217,
        "title": "Deep Reinforcement Learning based Aggressive Flight Trajectory Tracker",
        "authors": "Omar Shadeed, Mehmet Hasanzade, Emre Koyuncu",
        "published": "2021-1-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2021-0777"
    },
    {
        "id": 29218,
        "title": "Interactive Reinforcement Learning-Based API Recommendation",
        "authors": "Ming Wan, Yuxiang Qiu",
        "published": "2022-4-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ctisc54888.2022.9849805"
    },
    {
        "id": 29219,
        "title": "Causal Deep Reinforcement Learning Using Observational Data",
        "authors": "Wenxuan Zhu, Chao Yu, Qiang Zhang",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Deep reinforcement learning (DRL) requires the collection of interventional data, which is sometimes expensive and even unethical in the real world, such as in the autonomous driving and the medical field. Offline reinforcement learning promises to alleviate this issue by exploiting the vast amount of observational data available in the real world. However, observational data may mislead the learning agent to undesirable outcomes if the behavior policy that generates the data depends on unobserved random variables (i.e., confounders). In this paper, we propose two deconfounding methods in DRL to address this problem. The methods first calculate the importance degree of different samples based on the causal inference technique, and then adjust the impact of different samples on the loss function by reweighting or resampling the offline dataset to ensure its unbiasedness. These deconfounding methods can be flexibly combined with existing model-free DRL algorithms such as soft actor-critic and deep Q-learning, provided that a weak condition can be satisfied by the loss functions of these algorithms. We prove the effectiveness of our deconfounding methods and validate them experimentally.",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/524"
    },
    {
        "id": 29220,
        "title": "Using Deep Reinforcement Learning for Routing in IP Networks",
        "authors": "Abhiram Singh, Sidharth Sharma, Ashwin Gumaste",
        "published": "2021-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccn52240.2021.9522197"
    },
    {
        "id": 29221,
        "title": "Unbalanced Web Phishing Classification through Deep Reinforcement Learning",
        "authors": "Antonio Maci, Alessandro Santorsola, Antonio Coscia, Andrea Iannacone",
        "published": "2023-6-9",
        "citations": 2,
        "abstract": "Web phishing is a form of cybercrime aimed at tricking people into visiting malicious URLs to exfiltrate sensitive data. Since the structure of a malicious URL evolves over time, phishing detection mechanisms that can adapt to such variations are paramount. Furthermore, web phishing detection is an unbalanced classification task, as legitimate URLs outnumber malicious ones in real-life cases. Deep learning (DL) has emerged as a promising technique to minimize concept drift to enhance web phishing detection. Deep reinforcement learning (DRL) combines DL with reinforcement learning (RL); that is, a sequential decision-making paradigm in which the problem to be addressed is expressed as a Markov decision process (MDP). Recent studies have proposed an ad hoc MDP formulation to tackle unbalanced classification tasks called the imbalanced classification Markov decision process (ICMDP). In this paper, we exploit the ICMDP to present a double deep Q-Network (DDQN)-based classifier to address the unbalanced web phishing classification problem. The proposed algorithm is evaluated on a Mendeley web phishing dataset, from which three different data imbalance scenarios are generated. Despite a significant training time, it results in better geometric mean, index of balanced accuracy, F1 score, and area under the ROC curve than other DL-based classifiers combined with data-level sampling techniques in all test cases.",
        "link": "http://dx.doi.org/10.3390/computers12060118"
    },
    {
        "id": 29222,
        "title": "Power Management in Smart Buildings Using Reinforcement Learning",
        "authors": "Zohreh Rostmnezhad, Louis Dessaint",
        "published": "2023-1-16",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgt51731.2023.10066398"
    },
    {
        "id": 29223,
        "title": "Flight Control of a Multicopter using Reinforcement Learning",
        "authors": "Francesco d’Apolito, Christoph Sulzbachner",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2021.10.454"
    },
    {
        "id": 29224,
        "title": "A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch",
        "authors": "Shengren Hou, Edgar Salazar, Peter Palensky, Pedro P.  Barrios Vergara",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4628278"
    },
    {
        "id": 29225,
        "title": "Adaptive Behavior Cloning Regularization for Stable Offline-to-Online Reinforcement Learning",
        "authors": "Yi Zhao, Rinu Boney, Alexander Ilin, Juho Kannala, Joni Pajarinen",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2022.es2022-110"
    },
    {
        "id": 29226,
        "title": "Parameter Tuning Method for Multi-agent Simulation using Reinforcement Learning",
        "authors": "Masanori Hirano, Kiyoshi Izumi",
        "published": "2022-10-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/besc57393.2022.9995509"
    },
    {
        "id": 29227,
        "title": "Safe Multi-Agent Reinforcement Learning for Price-Based Demand Response",
        "authors": "Hannah Markgraf, Matthias Althoff",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgteurope56780.2023.10407281"
    },
    {
        "id": 29228,
        "title": "Emergence of linguistic conventions in multi-agent reinforcement learning",
        "authors": "Dorota Lipowska, Adam Lipowski",
        "published": "2018-11-29",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1371/journal.pone.0208095"
    },
    {
        "id": 29229,
        "title": "Traffic Signal Control for An Isolated Intersection Using Reinforcement Learning",
        "authors": "Nandan Maiti, Bhargava Rama Chilukuri",
        "published": "2021-1-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/comsnets51098.2021.9352834"
    },
    {
        "id": 29230,
        "title": "Obtaining fault tolerance avoidance behavior using deep reinforcement learning",
        "authors": "Fidel Aznar, Mar Pujol, Ramón Rizo",
        "published": "2019-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2018.11.090"
    },
    {
        "id": 29231,
        "title": "Multi-agent simulation for strategic bidding in electricity markets using reinforcement learning",
        "authors": "",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17775/cseejpes.2020.02820"
    },
    {
        "id": 29232,
        "title": "Partially Observable Multi-Agent Deep Reinforcement Learning for Cognitive Resource Management",
        "authors": "Ning Yang, Haijun Zhang, Randall Berry",
        "published": "2020-12",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom42002.2020.9322150"
    },
    {
        "id": 29233,
        "title": "Optimization of Charging Strategies for New Energy Vehicles Based on Reinforcement Learning Algorithms",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23977/jaip.2024.070112"
    },
    {
        "id": 29234,
        "title": "Reinforcement Learning Algorithms: Survey and Classification",
        "authors": "N. R. Ravishankar, M. V. Vijayakumar",
        "published": "2017-1-11",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17485/ijst/2017/v10i1/109385"
    },
    {
        "id": 29235,
        "title": "ShrinkML: End-to-End ASR Model Compression Using Reinforcement Learning",
        "authors": "Łukasz Dudziak, Mohamed S. Abdelfattah, Ravichander Vipperla, Stefanos Laskaridis, Nicholas D. Lane",
        "published": "2019-9-15",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-2811"
    },
    {
        "id": 29236,
        "title": "Multi-Agent Reinforcement Learning Based Cognitive Anti-Jamming",
        "authors": "Mohamed A. Aref, Sudharman K. Jayaweera, Stephen Machuzak",
        "published": "2017-3",
        "citations": 73,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc.2017.7925694"
    },
    {
        "id": 29237,
        "title": "A Constraint Enforcement Deep Reinforcement Learning Framework  for Optimal Energy Storage Systems Dispatch",
        "authors": "Shengren Hou, Edgar Salazar, Peter Palensky, Pedro P.  Barrios Vergara",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4724680"
    },
    {
        "id": 29238,
        "title": "Adaptive beamforming based on the deep reinforcement learning",
        "authors": "Chuanhui Hao, Xubao Sun, Yidong Liu",
        "published": "2022-12-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnsc55942.2022.10004128"
    },
    {
        "id": 29239,
        "title": "Civic education reform based on deep reinforcement learning model",
        "authors": "Dan Peng",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "Abstract\nThe integration of artificial intelligence technology into education is an inevitable trend of scientific progress and educational reform, and how to use artificial intelligence technology and ideological and political education reform is called a key research direction in the education sector. Aiming at the problems of cold start in personalized recommendation system, lack of interpretability of recommendation results, and ignoring the implicit features of the course for better acceptance of recommendation results by learners, the BPRMF model based on deep learning is proposed to be applied to the problem of recommendation of Civics and Political Science course, which not only models learners’ preferences and combines with course attribute features to generate recommendation rating ranking list and provide personalized recommendation service. Then the study of Civics education reform is conducted, mainly analyzing the change in teaching methods based on big data, machine learning, and deep learning technologies to promote secondary school students. The performance of the BPRMF model is evaluated in comparison with the BPRMF model under different k values. It is concluded that the accuracy rate of the BPRMF model is 8.9%~12.01% higher than UBCF and 8.07%~10.26% higher than IBCF, but with the increase of k value, the recall rate will gradually pull away from other models and optimize the recommendation system to some extent. This study is beneficial to ideological education in the implementation process to better utilize the opportunities, meet the challenges, and develop efficiently.",
        "link": "http://dx.doi.org/10.2478/amns.2023.2.00417"
    },
    {
        "id": 29240,
        "title": "Evaluation of Blood Glucose Level Control in Type 1 Diabetic Patients Using Deep Reinforcement Learning",
        "authors": "Phuwadol Viroonluecha, Esteban Egea-Lopez, Jose Santa",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDiabetes mellitus is a disease associated with abnormally high levels of blood glucose due to lack of insulin. Combining an insulin pump and continuous glucose monitor with a control algorithm to deliver insulin is an alternative to patient self-management of insulin doses to control blood glucose levels in diabetes mellitus patients. In this work we propose a closed-loop control for blood glucose levels based on deep reinforcement learning. We describe the initial evaluation of several alternatives conducted on a realistic simulator of the glucoregulatory system and propose a particular implementation strategy based on reducing the frequency of the observations and rewards passed to the agent, and using a simple reward function. We train agents with that strategy for three groups of patient classes, evaluate and compare it with alternative control baselines. Our results show that our method is able to outperform baselines as well as similar recent proposals, by achieving longer periods of safe glycemic state and low risk.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1095721/v1"
    },
    {
        "id": 29241,
        "title": "A Study of Deep Reinforcement Learning Based Recommender Systems",
        "authors": "Garima Gupta, Rahul Katarya",
        "published": "2021-5-21",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsccc51823.2021.9478178"
    },
    {
        "id": 29242,
        "title": "Reinforcement Learning with Self-Attention Networks for Cryptocurrency Trading",
        "authors": "Carlos Betancourt, Wen-Hui Chen",
        "published": "2021-8-11",
        "citations": 6,
        "abstract": "This work presents an application of self-attention networks for cryptocurrency trading. Cryptocurrencies are extremely volatile and unpredictable. Thus, cryptocurrency trading is challenging and involves higher risks than trading traditional financial assets such as stocks. To overcome the aforementioned problems, we propose a deep reinforcement learning (DRL) approach for cryptocurrency trading. The proposed trading system contains a self-attention network trained using an actor-critic DRL algorithm. Cryptocurrency markets contain hundreds of assets, allowing greater investment diversification, which can be accomplished if all the assets are analyzed against one another. Self-attention networks are suitable for dealing with the problem because the attention mechanism can process long sequences of data and focus on the most relevant parts of the inputs. Transaction fees are also considered in formulating the studied problem. Systems that perform trades in high frequencies cannot overlook this issue, since, after many trades, small fees can add up to significant expenses. To validate the proposed approach, a DRL environment is built using data from an important cryptocurrency market. We test our method against a state-of-the-art baseline in two different experiments. The experimental results show the proposed approach can obtain higher daily profits and has several advantages over existing methods.",
        "link": "http://dx.doi.org/10.3390/app11167377"
    },
    {
        "id": 29243,
        "title": "Fear-Neuro-Inspired Reinforcement Learning for Safe Autonomous Driving",
        "authors": "Xiangkun He, Jingda Wu, Zhiyu Huang, Zhongxu Hu, Jun Wang, Alberto Sangiovanni-Vincentelli, Chen Lv",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Ensuring safety and achieving human-level driving performance remain challenges for autonomous vehicles, especially in safety-critical situations. As a key component of artificial intelligence, reinforcement learning is promising and has shown great potential in many complex tasks; however, its lack of safety guarantees limits its real-world applicability. Hence, further advancing reinforcement learning, especially from the safety perspective, is of great importance for autonomous driving. As revealed by cognitive neuroscientists, the amygdala of the brain can elicit defensive responses against threats or hazards, which is crucial for survival in and adaptation to risky environments. Drawing inspiration from this scientific discovery, we present a fear-neuro-inspired reinforcement learning framework to realize safe autonomous driving through modeling the amygdala functionality. This new technique facilitates an agent to learn defensive behaviors and achieve safe decision making with fewer safety violations. Through experimental tests, we show that the proposed approach enables the autonomous driving agent to attain state-of-the-art performance compared to the baseline agents and perform comparably to 30 certified human drivers, across various safety-critical scenarios. The results demonstrate the feasibility and effectiveness of our framework while also shedding light on the crucial role of simulating the amygdala function in the application of reinforcement learning to safety-critical autonomous driving domains.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24289108.v1"
    },
    {
        "id": 29244,
        "title": "Deep Reinforcement Learning Empowered Particle Swarm Optimization for Aerial Base Station Deployment",
        "authors": "Jinpeng Song, Bo Zhang, Junfeng Lia",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apscon56343.2023.10101173"
    },
    {
        "id": 29245,
        "title": "FedPreM: A Novel Federated Reinforcement Learning Framework for Predictive Maintenance",
        "authors": "Lu Yang, Chen-Khong Tham, Songtao Guo",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436788"
    },
    {
        "id": 29246,
        "title": "Invited- NVCell: Standard Cell Layout in Advanced Technology Nodes with Reinforcement Learning",
        "authors": "Haoxing Ren, Matthew Fojtik",
        "published": "2021-12-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dac18074.2021.9586188"
    },
    {
        "id": 29247,
        "title": "Deep Reinforcement Learning for Bandit Arm Localization",
        "authors": "Wenbin Du, Huaqing Jin, Chao Yu, Guosheng Yin",
        "published": "2022-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata55660.2022.10020647"
    },
    {
        "id": 29248,
        "title": "2-DOF Robot Optimal Control via Artificial Neural Network Reinforcement Learning",
        "authors": "Yuriy Romasevych, Viatcheslav Loveikin",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/khpiweek61412.2023.10312842"
    },
    {
        "id": 29249,
        "title": "Improving Inertial-Based UAV Localization using Data-Efficient Deep Reinforcement Learning",
        "authors": "Dimitrios Tsiakmakis, Nikolaos Passalis, Anastasios Tefas",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10289910"
    },
    {
        "id": 29250,
        "title": "Socially Aware Robot Navigation Using Deep Reinforcement Learning",
        "authors": "Truong Xuan Tung, Trung Dung Ngo",
        "published": "2018-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccece.2018.8447854"
    },
    {
        "id": 29251,
        "title": "Deep Multi-User Reinforcement Learning for Dynamic Spectrum Access in Multichannel Wireless Networks",
        "authors": "Oshri Naparstek, Kobi Cohen",
        "published": "2017-12",
        "citations": 73,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/glocom.2017.8254101"
    },
    {
        "id": 29252,
        "title": "A Novel Algorithmic Trading Approach Based on Reinforcement Learning",
        "authors": "Li Xucheng, Peng Zhihao",
        "published": "2019-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmtma.2019.00093"
    },
    {
        "id": 29253,
        "title": "Reinforcement Learning for Interference Avoidance Game in RF-Powered Backscatter Communications",
        "authors": "Ali Rahmati, Huaiyu Dai",
        "published": "2019-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2019.8761145"
    },
    {
        "id": 29254,
        "title": "Relational Abstractions for Generalized Reinforcement Learning on Symbolic Problems",
        "authors": "Rushang Karia, Siddharth Srivastava",
        "published": "2022-7",
        "citations": 1,
        "abstract": "Reinforcement learning in problems with symbolic state spaces is challenging due to the need for reasoning over long horizons. This paper presents a new approach that utilizes relational abstractions in conjunction with deep learning to learn a generalizable Q-function for such problems. The learned Q-function can be efficiently transferred to related problems that have different object names and object quantities, and thus, entirely different state spaces. We show that the learned, generalized Q-function can be utilized for zero-shot transfer to related problems without an explicit, hand-coded curriculum. Empirical evaluations on a range of problems show that our method facilitates efficient zero-shot transfer of learned knowledge to much larger problem instances containing many objects.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/435"
    },
    {
        "id": 29255,
        "title": "Continuous Policy Multi-Agent Deep Reinforcement Learning with Generalizable Episodic Memory",
        "authors": "Wenjing Ni, Bo Wang, Hua Zhong, Xiang Guo",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055953"
    },
    {
        "id": 29256,
        "title": "Visual Local Path Planning Based on Deep Reinforcement Learning",
        "authors": "Xuanru Shen",
        "published": "2023-2-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eebda56825.2023.10090576"
    },
    {
        "id": 29257,
        "title": "Differentiable Quantum Architecture Search for Quantum Reinforcement Learning",
        "authors": "Yize Sun, Yunpu Ma, Volker Tresp",
        "published": "2023-9-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/qce57702.2023.10177"
    },
    {
        "id": 29258,
        "title": "Reinforcement Learning Based Resource Management for Network Slicing",
        "authors": "Yohan Kim, Sunyong Kim, Hyuk Lim",
        "published": "2019-6-9",
        "citations": 29,
        "abstract": "Network slicing to create multiple virtual networks, called network slice, is a promising technology to enable networking resource sharing among multiple tenants for the 5th generation (5G) networks. By offering a network slice to slice tenants, network slicing supports parallel services to meet the service level agreement (SLA). In legacy networks, every tenant pays a fixed and roughly estimated monthly or annual fee for shared resources according to a contract signed with a provider. However, such a fixed resource allocation mechanism may result in low resource utilization or violation of user quality of service (QoS) due to fluctuations in the network demand. To address this issue, we introduce a resource management system for network slicing and propose a dynamic resource adjustment algorithm based on reinforcement learning approach from each tenant’s point of view. First, the resource management for network slicing is modeled as a Markov Decision Process (MDP) with the state space, action space, and reward function. Then, we propose a Q-learning-based dynamic resource adjustment algorithm that aims at maximizing the profit of tenants while ensuring the QoS requirements of end-users. The numerical simulation results demonstrate that the proposed algorithm can significantly increase the profit of tenants compared to existing fixed resource allocation methods while satisfying the QoS requirements of end-users.",
        "link": "http://dx.doi.org/10.3390/app9112361"
    },
    {
        "id": 29259,
        "title": "Deep Reinforcement Learning Approach for Trading Automation in the Stock Market",
        "authors": "Taylan Kabbani, Ekrem Duman",
        "published": "2022",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3203697"
    },
    {
        "id": 29260,
        "title": "A Deep Reinforcement Learning Approach to Traffic Signal Control",
        "authors": "Aquib Junaid Razack, Vysyakh Ajith, Rajiv Gupta",
        "published": "2021-4-22",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sustech51236.2021.9467450"
    },
    {
        "id": 29261,
        "title": "Deep Reinforcement Learning Based Multi-User Anti-Jamming Strategy",
        "authors": "Yue Bi, Yue Wu, Cunqing Hua",
        "published": "2019-5",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2019.8761848"
    },
    {
        "id": 29262,
        "title": "Resource Allocation in Vehicular Communications Using Graph and Deep Reinforcement Learning",
        "authors": "Sohan Gyawali, Yi Qian, Rose Qingyang Hu",
        "published": "2019-12",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9013594"
    },
    {
        "id": 29263,
        "title": "Multi-Objective Mission planning for UAV Swarm Based on Deep Reinforcement Learning",
        "authors": "Sun Yu, Dai Dingcheng",
        "published": "2023-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus58632.2023.10318490"
    },
    {
        "id": 29264,
        "title": "Reinforcement Learning PCG",
        "authors": "Matthew Guzdial, Sam Snodgrass, Adam J. Summerville",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-16719-5_10"
    },
    {
        "id": 29265,
        "title": "Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks",
        "authors": "Yijie Guo, Qiucheng Wu, Honglak Lee",
        "published": "2022-6-28",
        "citations": 1,
        "abstract": "Meta reinforcement learning (meta-RL) aims to learn a policy solving a set of training tasks simultaneously and quickly adapting to new tasks. It requires massive amounts of data drawn from training tasks to infer the common structure shared among tasks. Without heavy reward engineering, the sparse rewards in long-horizon tasks exacerbate the problem of sample efficiency in meta-RL. Another challenge in meta-RL is the discrepancy of difficulty level among tasks, which might cause one easy task dominating learning of the shared policy and thus preclude policy adaptation to new tasks. This work introduces a novel objective function to learn an action translator among training tasks. We theoretically verify that the value of the transferred policy with the action translator can be close to the value of the source policy and our objective function (approximately) upper bounds the value difference. We propose to combine the action translator with context-based meta-RL algorithms for better data collection and moreefficient exploration during meta-training. Our approach em-pirically improves the sample efficiency and performance ofmeta-RL algorithms on sparse-reward tasks.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i6.20635"
    },
    {
        "id": 29266,
        "title": "Tabular Q-learning Based Reinforcement Learning Agent for Autonomous Vehicle Drift Initiation and Stabilization",
        "authors": "Szilárd H. Tóth, Ádám Bárdos, Zsolt J. Viharos",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.10.1261"
    },
    {
        "id": 29267,
        "title": "Correction: Optimum trajectory learning in musculoskeletal systems with model predictive control and deep reinforcement learning",
        "authors": "Berat Denizdurduran, Henry Markram, Marc-Oliver Gewaltig",
        "published": "2022-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00422-022-00947-4"
    },
    {
        "id": 29268,
        "title": "Guest editorial: special issue on reinforcement learning for real life",
        "authors": "Yuxi Li, Alborz Geramifard, Lihong Li, Csaba Szepesvari, Tao Wang",
        "published": "2021-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-021-06041-3"
    },
    {
        "id": 29269,
        "title": "Learning and Transfer of Movement Gaits Using Reinforcement Learning",
        "authors": "David Waidner, Marcus Strand",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-86294-7_34"
    },
    {
        "id": 29270,
        "title": "Interpretable Machine Learning for Reinforcement Learning based Control Strategy Optimization of Industrial Energy Supply Systems",
        "authors": "Arthur Arthur, Arthur Stobert, Heiko Ranzau, Inga Pfenning, Matthias Weigold",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.46855/energy-proceedings-11066"
    },
    {
        "id": 29271,
        "title": "Learning to Walk: Spike Based Reinforcement Learning for Hexapod Robot Central Pattern Generation",
        "authors": "Ashwin Sanjay Lele, Yan Fang, Justin Ting, Arijit Raychowdhury",
        "published": "2020-8",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aicas48895.2020.9073987"
    },
    {
        "id": 29272,
        "title": "Critical State Detection for Adversarial Attacks in Deep Reinforcement Learning",
        "authors": "R Praveen Kumar, I Niranjan Kumar, Sujith Sivasankaran, A Mohan Vamsi, Vineeth Vijayaraghavan",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00279"
    },
    {
        "id": 29273,
        "title": "Reinforcement Learning for PHEV Route Choice Based on Congestion Game",
        "authors": "Huiwei Wang, Huaqing Li, Bo Zhou",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-33-4528-7_9"
    },
    {
        "id": 29274,
        "title": "Deep Flamingo Search and Reinforcement Learning Based Recommendation System for E-Learning Platform using Social Media",
        "authors": "N Vedavathi, R Suhas Bharadwaj",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2022.12.022"
    },
    {
        "id": 29275,
        "title": "Path planning of mobile robot based on deep reinforcement learning with transfer learning strategy",
        "authors": "Jie Zhu, Chuanhai Yang, Zhaodong Liu, Chengdong Yang",
        "published": "2022-11-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/yac57282.2022.10023708"
    },
    {
        "id": 29276,
        "title": "Optimal Hierarchical Learning Path Design With Reinforcement Learning",
        "authors": "Xiao Li, Hanchen Xu, Jinming Zhang, Hua-hua Chang",
        "published": "2021-1",
        "citations": 5,
        "abstract": " E-learning systems are capable of providing more adaptive and efficient learning experiences for learners than traditional classroom settings. A key component of such systems is the learning policy. The learning policy is an algorithm that designs the learning paths or rather it selects learning materials for learners based on information such as the learners’ current progresses and skills, learning material contents. In this article, the authors address the problem of finding the optimal learning policy. To this end, a model for learners’ hierarchical skills in the E-learning system is first developed. Based on the hierarchical skill model and the classical cognitive diagnosis model, a framework to model various mastery levels related to hierarchical skills is further developed. The optimal learning path in consideration of the hierarchical structure of skills is found by applying a model-free reinforcement learning method, which does not require any assumption about learners’ learning transition processes. The effectiveness of the proposed framework is demonstrated via simulation studies. ",
        "link": "http://dx.doi.org/10.1177/0146621620947171"
    },
    {
        "id": 29277,
        "title": "Automated Aircraft Stall Recovery using Reinforcement Learning and Supervised Learning Techniques",
        "authors": "Dheerenrda Singh Tomar, Jason Gauci, Alexiei Dingli, Alan Muscat, David Zammit Mangion",
        "published": "2021-10-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dasc52595.2021.9594316"
    },
    {
        "id": 29278,
        "title": "Enhancing Machine Learning Based Malware Detection Model by Reinforcement Learning",
        "authors": "Cangshuai Wu, Jiangyong Shi, Yuexiang Yang, Wenhua Li",
        "published": "2018-11-2",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3290480.3290494"
    },
    {
        "id": 29279,
        "title": "Proximal Gradient Temporal Difference Learning: Stable Reinforcement Learning with Polynomial Sample Complexity",
        "authors": "Bo Liu, Ian Gemp, Mohammad Ghavamzadeh, Ji Liu, Sridhar Mahadevan, Marek Petrik",
        "published": "2018-11-15",
        "citations": 3,
        "abstract": "\r\n\r\n\r\nIn this paper, we introduce proximal gradient temporal difference learning, which provides a principled way of designing and analyzing true stochastic gradient temporal difference learning algorithms. We show how gradient TD (GTD) reinforcement learning methods can be formally derived, not by starting from their original objective functions, as previously attempted, but rather from a primal-dual saddle-point objective function. We also conduct a saddle-point error analysis to obtain finite-sample bounds on their performance. Previous analyses of this class of algorithms use stochastic approximation techniques to prove asymptotic convergence, and do not provide any finite-sample analysis. We also propose an accelerated algorithm, called GTD2-MP, that uses proximal \"mirror maps\" to yield an improved convergence rate. The results of our theoretical analysis imply that the GTD family of algorithms are comparable and may indeed be preferred over existing least squares TD methods for off-policy learning, due to their linear complexity. We provide experimental results showing the improved performance of our accelerated gradient TD methods.\r\n\r\n\r\n",
        "link": "http://dx.doi.org/10.1613/jair.1.11251"
    },
    {
        "id": 29280,
        "title": "A Sublinear-Regret Reinforcement Learning Algorithm on Constrained Markov Decision Processes with reset action",
        "authors": "Takashi Watanabe, Takashi Sakuragawa",
        "published": "2020-1-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3380688.3380706"
    },
    {
        "id": 29281,
        "title": "Model-free inverse reinforcement learning with multi-intention, unlabeled, and overlapping demonstrations",
        "authors": "Ariyan Bighashdel, Pavol Jancura, Gijs Dubbelman",
        "published": "2023-7",
        "citations": 0,
        "abstract": "AbstractIn this paper, we define a novel inverse reinforcement learning (IRL) problem where the demonstrations are multi-intention, i.e., collected from multi-intention experts, unlabeled, i.e., without intention labels, and partially overlapping, i.e., shared between multiple intentions. In the presence of overlapping demonstrations, current IRL methods, developed to handle multi-intention and unlabeled demonstrations, cannot successfully learn the underlying reward functions. To solve this limitation, we propose a novel clustering-based approach to disentangle the observed demonstrations and experimentally validate its advantages. Traditional clustering-based approaches to multi-intention IRL, which are developed on the basis of model-based Reinforcement Learning (RL), formulate the problem using parametric density estimation. However, in high-dimensional environments and unknown system dynamics, i.e., model-free RL, the solution of parametric density estimation is only tractable up to the density normalization constant. To solve this, we formulate the problem as a mixture of logistic regressions to directly handle the unnormalized density. To research the challenges faced by overlapping demonstrations, we introduce the concepts of shared pair, which is a state-action pair that is shared in more than one intention, and separability, which resembles how well the multiple intentions can be separated in the joint state-action space. We provide theoretical analyses under the global optimality condition and the existence of shared pairs. Furthermore, we conduct extensive experiments on four simulated robotics tasks, extended to accept different intentions with specific levels of separability, and a synthetic driver task developed to directly control the separability. We evaluate the existing baselines on our defined problem and demonstrate, theoretically and experimentally, the advantages of our clustering-based solution, especially when the separability of the demonstrations decreases.",
        "link": "http://dx.doi.org/10.1007/s10994-022-06273-x"
    },
    {
        "id": 29282,
        "title": "Design and Development of Chatbot Based on Reinforcement Learning",
        "authors": "Hemlata M. Jadhav, Altaf Mulani, Makarand M. Jadhav",
        "published": "2022-12-13",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119861850.ch12"
    },
    {
        "id": 29283,
        "title": "UAV Head-On Situation Maneuver Generation Using Transfer-Learning-Based Deep Reinforcement Learning",
        "authors": "Insu Hwang, Jung Ho Bae",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "AbstractRecently, the demand for unmanned aerial vehicle technology has increased. In particular, AI pilots through reinforcement learning (RL) are more flexible than those using rule-based methods. Further, AI pilots with RL are expected to replace human pilots in the future. In a recent study, rather than completely replacing humans, studies on AI pilots are conducted toward the collaboration between man and unmanned aircraft. AI pilots have several advantages over humans. For example, on the one hand, human pilots avoid head-on situations due to collision. On the other hand, AI pilots may prefer head-on situations to finish the episode quickly. This study proposes a two-circle-based transfer learning method to demonstrate excellent performance in head-on situations. Based on the experimental results, the proposed two-circle-based multi-task transfer learning model outperforms the model without transfer learning-based RL. A study on transfer-learning-based learning technique has been conducted. However, it had a one-circle-based learning technique was specialized only for tail-chasing, limiting its application (Bae et al. in IEEE Access 11:26427–26440, 2023). Practically, the proposed two-circle-based learning technique outperforms the one-circle-based transfer learning technique in head-on situations.",
        "link": "http://dx.doi.org/10.1007/s42405-023-00695-0"
    },
    {
        "id": 29284,
        "title": "Reinforcement learning decoders for fault-tolerant quantum computation",
        "authors": "Ryan Sweke, Markus S Kesselring, Evert P L van Nieuwenburg, Jens Eisert",
        "published": "2021-6-1",
        "citations": 29,
        "abstract": "Abstract\nTopological error correcting codes, and particularly the surface code, currently provide the most feasible road-map towards large-scale fault-tolerant quantum computation. As such, obtaining fast and flexible decoding algorithms for these codes, within the experimentally realistic and challenging context of faulty syndrome measurements, without requiring any final read-out of the physical qubits, is of critical importance. In this work, we show that the problem of decoding such codes can be naturally reformulated as a process of repeated interactions between a decoding agent and a code environment, to which the machinery of reinforcement learning can be applied to obtain decoding agents. While in principle this framework can be instantiated with environments modelling circuit level noise, we take a first step towards this goal by using deepQ learning to obtain decoding agents for a variety of simplified phenomenological noise models, which yield faulty syndrome measurements without including the propagation of errors which arise in full circuit level noise models.",
        "link": "http://dx.doi.org/10.1088/2632-2153/abc609"
    },
    {
        "id": 29285,
        "title": "Learning to Manipulate Tools Using Deep Reinforcement Learning and Anchor Information",
        "authors": "Junhang Wei, Shaowei Cui, Peng Hao, Shuo Wang",
        "published": "2022-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio55434.2022.10012027"
    },
    {
        "id": 29286,
        "title": "Learning to Ascend Stairs and Ramps: Deep Reinforcement Learning for a Physics-Based Human Musculoskeletal Model",
        "authors": "Aurelien J. C. Adriaenssens, Vishal Raveendranathan, Raffaella Carloni",
        "published": "2022-11-3",
        "citations": 4,
        "abstract": "This paper proposes to use deep reinforcement learning to teach a physics-based human musculoskeletal model to ascend stairs and ramps. The deep reinforcement learning architecture employs the proximal policy optimization algorithm combined with imitation learning and is trained with experimental data of a public dataset. The human model is developed in the open-source simulation software OpenSim, together with two objects (i.e., the stairs and ramp) and the elastic foundation contact dynamics. The model can learn to ascend stairs and ramps with muscle forces comparable to healthy subjects and with a forward dynamics comparable to the experimental training data, achieving an average correlation of 0.82 during stair ascent and of 0.58 during ramp ascent across both the knee and ankle joints.",
        "link": "http://dx.doi.org/10.3390/s22218479"
    },
    {
        "id": 29287,
        "title": "Robust Reinforcement Learning: A Review of Foundations and Recent Advances",
        "authors": "Janosch Moos, Kay Hansel, Hany Abdulsamad, Svenja Stark, Debora Clever, Jan Peters",
        "published": "2022-3-19",
        "citations": 20,
        "abstract": "Reinforcement learning (RL) has become a highly successful framework for learning in Markov decision processes (MDP). Due to the adoption of RL in realistic and complex environments, solution robustness becomes an increasingly important aspect of RL deployment. Nevertheless, current RL algorithms struggle with robustness to uncertainty, disturbances, or structural changes in the environment. We survey the literature on robust approaches to reinforcement learning and categorize these methods in four different ways: (i) Transition robust designs account for uncertainties in the system dynamics by manipulating the transition probabilities between states; (ii) Disturbance robust designs leverage external forces to model uncertainty in the system behavior; (iii) Action robust designs redirect transitions of the system by corrupting an agent’s output; (iv) Observation robust designs exploit or distort the perceived system state of the policy. Each of these robust designs alters a different aspect of the MDP. Additionally, we address the connection of robustness to the risk-based and entropy-regularized RL formulations. The resulting survey covers all fundamental concepts underlying the approaches to robust reinforcement learning and their recent advances.",
        "link": "http://dx.doi.org/10.3390/make4010013"
    },
    {
        "id": 29288,
        "title": "Emotion in reinforcement learning agents and robots: a survey",
        "authors": "Thomas M. Moerland, Joost Broekens, Catholijn M. Jonker",
        "published": "2018-2",
        "citations": 98,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-017-5666-0"
    },
    {
        "id": 29289,
        "title": "Learning a Low-Dimensional Representation of a Safe Region for Safe Reinforcement Learning on Dynamical Systems",
        "authors": "Zhehua Zhou, Ozgur S. Oguz, Marion Leibold, Martin Buss",
        "published": "2023-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3106818"
    },
    {
        "id": 29290,
        "title": "Disturbance Observer-based Reinforcement Learning Control and the Application to a Nonlinear Dyna...",
        "authors": "",
        "published": "2022-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2022-1586.vid"
    },
    {
        "id": 29291,
        "title": "PAC Reinforcement Learning With an Imperfect Model",
        "authors": "Nan Jiang",
        "published": "2018-4-29",
        "citations": 0,
        "abstract": "\n      \n        Reinforcement learning (RL) methods have proved to be successful in many simulated environments. The common approaches, however, are often too sample intensive to be applied directly in the real world. A promising approach to addressing this issue is to train an RL agent in a simulator and transfer the solution to the real environment. When a high-fidelity simulator is available we would expect significant reduction in the amount of real trajectories needed for learning. In this work we aim at better understanding the theoretical nature of this approach. We start with a perhaps surprising result that, even if the approximate model (e.g., a simulator) only differs from the real environment in a single state-action pair (but which one is unknown), such a model could be information-theoretically useless and the sample complexity (in terms of real trajectories) still scales with the total number of states in the worst case. We investigate the hard instances and come up with natural conditions that avoid the pathological situations. We then propose two conceptually simple algorithms that enjoy polynomial sample complexity guarantees with no dependence on the size of the state-action space, and prove some foundational results to provide insights into this important problem.\n      \n    ",
        "link": "http://dx.doi.org/10.1609/aaai.v32i1.11594"
    },
    {
        "id": 29292,
        "title": "Plucking a string or playing a G? Choice type impacts human reinforcement learning",
        "authors": "Milena Rmus, Amy Zou, Anne G.E. Collins",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractIn reinforcement learning (RL) experiments, participants learn to make rewarding choices in response to different stimuli; RL models use outcomes to estimate stimulus-response values which change incrementally. RL models consider any response type indiscriminately, ranging from more concretely defined motor choices (e.g. pressing a key with the index finger), to more general choices that can be executed in a number of ways (e.g. getting dinner at the restaurant). But does the learning process vary as a function of the choice type? In Experiment 1, we show that it does: participants were slower and less accurate in learning correct choices of a general format compared to learning more concrete, motor actions. Using computational modeling, we show that two mechanisms contribute to this. First, there was evidence of irrelevant credit assignment: the values of motor actions interfered with the values of other choice dimensions, resulting in more incorrect choices when the correct response is not defined by a single motor action; second, information integration for relevant general choices was slower. In Experiment 2, we replicated and further extended the findings from Experiment 1, by showing that slowed learning was attributable to weaker working memory use, rather than slowed RL learning. In both experiments we ruled out the explanation that the difference in performance between two condition types was driven by difficulty/different levels of complexity. We conclude that defining a more abstract choice space used by multiple learning systems for credit assignment recruits executive resources, limiting how much such processes then contribute to fast learning.",
        "link": "http://dx.doi.org/10.1101/2021.08.25.457707"
    },
    {
        "id": 29293,
        "title": "Active Measure Reinforcement Learning for Observation Cost Minimization",
        "authors": "Colin Bellinger, Rory Coles, Mark Crowley, Isaac Tamblyn",
        "published": "2021-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/594757db.72846d04"
    },
    {
        "id": 29294,
        "title": "Application of reinforcement learning to detect and mitigate airspace loss of separation events",
        "authors": "Megan Hawley, Raj Bharadwaj",
        "published": "2018-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnsurv.2018.8384897"
    },
    {
        "id": 29295,
        "title": "Evaluating Quantum REINFORCE on IBM’s Quantum Hardware",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1_8"
    },
    {
        "id": 29296,
        "title": "Mood modelling within reinforcement learning",
        "authors": "Joe Collenette, Katie Atkinson, Daan Bloembergen, Karl Tuyls",
        "published": "2017-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/ecal_a_021"
    },
    {
        "id": 29297,
        "title": "Attention manipulation in reinforcement learning agents",
        "authors": "Oriol Corcoll, Abdullah Makkeh, Jaan Aru, Dirk Oliver Theis, Raul Vicente Zafra",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2019.1274-0"
    },
    {
        "id": 29298,
        "title": "Collective Behavior Acquisition of Real Robotic Swarms Using Deep Reinforcement Learning",
        "authors": "Toshiyuki Yasuda, Kazuhiro Ohkura",
        "published": "2018-1",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/irc.2018.00038"
    },
    {
        "id": 29299,
        "title": "A Multi-Agent Reinforcement Learning Framework for Lithium-ion Battery Scheduling Problems",
        "authors": "Yu Sui, Shiming Song",
        "published": "2020-4-17",
        "citations": 14,
        "abstract": "This paper presents a reinforcement learning framework for solving battery scheduling problems in order to extend the lifetime of batteries used in electrical vehicles (EVs), cellular phones, and embedded systems. Battery pack lifetime has often been the limiting factor in many of today’s smart systems, from mobile devices and wireless sensor networks to EVs. Smart charge-discharge scheduling of battery packs is essential to obtain super linear gain of overall system lifetime, due to the recovery effect and nonlinearity in the battery characteristics. Additionally, smart scheduling has also been shown to be beneficial for optimizing the system’s thermal profile and minimizing chances of irreversible battery damage. The recent rapidly-growing community and development infrastructure have added deep reinforcement learning (DRL) to the available tools for designing battery management systems. Through leveraging the representation powers of deep neural networks and the flexibility and versatility of reinforcement learning, DRL offers a powerful solution to both roofline analysis and real-world deployment on complicated use cases. This work presents a DRL-based battery scheduling framework to solve battery scheduling problems, with high flexibility to fit various battery models and application scenarios. Through the discussion of this framework, comparisons have also been made between conventional heuristics-based methods and DRL. The experiments demonstrate that DRL-based scheduling framework achieves battery lifetime comparable to the best weighted-k round-robin (kRR) heuristic scheduling algorithm. In the meantime, the framework offers much greater flexibility in accommodating a wide range of battery models and use cases, including thermal control and imbalanced battery.",
        "link": "http://dx.doi.org/10.3390/en13081982"
    },
    {
        "id": 29300,
        "title": "Deep Reinforcement Learning for Carrier-borne Aircraft Support Operation Scheduling",
        "authors": "Haifeng Feng, Wei Zeng",
        "published": "2021-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaa53760.2021.00169"
    },
    {
        "id": 29301,
        "title": "Active Measure Reinforcement Learning for Observation Cost Minimization",
        "authors": "Colin Bellinger, Rory Coles, Mark Crowley, Isaac Tamblyn",
        "published": "2021-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/594757db.72846d04"
    },
    {
        "id": 29302,
        "title": "Deep Reinforcement Learning Algorithms for Ship Navigation in Restricted Waters",
        "authors": "Jonathas Marcelo Pereira Figueiredo, Rodrigo Pereira Abou Rejaili",
        "published": "2018-12-29",
        "citations": 2,
        "abstract": "Reinforcement Learning has not been fully explored for the automated control of ships maneuvering movements in restricted waters. Nevertheless, more robust and efficient control can be achieved with such algorithms. This paper presents the use of Deep Q Network and Deep Deterministic Policy Gradient methods with a numerical simulator for ship maneuvers to develop control laws. Both methods proved to be efficient in navigational control through a channel. A comparison of response and control behavior resulting from each of the methods is presented.",
        "link": "http://dx.doi.org/10.11606/issn.2526-8260.mecatrone.2018.151953"
    },
    {
        "id": 29303,
        "title": "Deep Reinforcement Learning for Autonomous Model-Free Navigation with Partial Observability",
        "authors": "Daniel Tapia, Juan Parras, Santiago Zazo",
        "published": "2019-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco.2019.8902933"
    },
    {
        "id": 29304,
        "title": "Neurorobotic reinforcement learning for domains with parametrical uncertainty",
        "authors": "Camilo Amaya, Axel von Arnim",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "Neuromorphic hardware paired with brain-inspired learning strategies have enormous potential for robot control. Explicitly, these advantages include low energy consumption, low latency, and adaptability. Therefore, developing and improving learning strategies, algorithms, and neuromorphic hardware integration in simulation is a key to moving the state-of-the-art forward. In this study, we used the neurorobotics platform (NRP) simulation framework to implement spiking reinforcement learning control for a robotic arm. We implemented a force-torque feedback-based classic object insertion task (“peg-in-hole”) and controlled the robot for the first time with neuromorphic hardware in the loop. We therefore provide a solution for training the system in uncertain environmental domains by using randomized simulation parameters. This leads to policies that are robust to real-world parameter variations in the target domain, filling the sim-to-real gap.To the best of our knowledge, it is the first neuromorphic implementation of the peg-in-hole task in simulation with the neuromorphic Loihi chip in the loop, and with scripted accelerated interactive training in the Neurorobotics Platform, including randomized domains.",
        "link": "http://dx.doi.org/10.3389/fnbot.2023.1239581"
    },
    {
        "id": 29305,
        "title": "Application of reinforcement learning to detect and mitigate airspace loss of separation events",
        "authors": "Megan Hawley, Raj Bharadwaj",
        "published": "2018-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnsurv.2018.8384897"
    },
    {
        "id": 29306,
        "title": "Evaluating Quantum REINFORCE on IBM’s Quantum Hardware",
        "authors": "Leonhard Kunczik",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-658-37616-1_8"
    },
    {
        "id": 29307,
        "title": "Mood modelling within reinforcement learning",
        "authors": "Joe Collenette, Katie Atkinson, Daan Bloembergen, Karl Tuyls",
        "published": "2017-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/ecal_a_021"
    },
    {
        "id": 29308,
        "title": "Attention manipulation in reinforcement learning agents",
        "authors": "Oriol Corcoll, Abdullah Makkeh, Jaan Aru, Dirk Oliver Theis, Raul Vicente Zafra",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2019.1274-0"
    },
    {
        "id": 29309,
        "title": "Collective Behavior Acquisition of Real Robotic Swarms Using Deep Reinforcement Learning",
        "authors": "Toshiyuki Yasuda, Kazuhiro Ohkura",
        "published": "2018-1",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/irc.2018.00038"
    },
    {
        "id": 29310,
        "title": "Neural Network and Reinforcement Learning based Energy Management Strategy for Battery/Supercapacitor HEV",
        "authors": "Jili Tao, Guohua Chen, Ruibin Gao",
        "published": "2021-10-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9728529"
    },
    {
        "id": 29311,
        "title": "Deep Reinforcement Learning Based Sensor Data Management for Vehicles",
        "authors": "Jeongmin Moon, Mukoe Cheong, Ikjun Yeom, Honguk Woo",
        "published": "2019-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoin.2019.8718108"
    },
    {
        "id": 29312,
        "title": "Enhancing Digital Twins through Reinforcement Learning",
        "authors": "Constantin Cronrath, Abolfazl R. Aderiani, Bengt Lennartson",
        "published": "2019-8",
        "citations": 34,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/coase.2019.8842888"
    },
    {
        "id": 29313,
        "title": "Deep Reinforcement Learning for Carrier-borne Aircraft Support Operation Scheduling",
        "authors": "Haifeng Feng, Wei Zeng",
        "published": "2021-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaa53760.2021.00169"
    },
    {
        "id": 29314,
        "title": "Hybrid Robotic Reinforcement Learning for Inspection/Correction Tasks",
        "authors": "Hoda Nasereddin, Gerald M. Knapp",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.promfg.2020.01.384"
    },
    {
        "id": 29315,
        "title": "Hierarchical Task and Motion Planning through Deep Reinforcement Learning",
        "authors": "Abdullah Al Redwan Newaz, Tauhidul Alam",
        "published": "2021-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/irc52146.2021.00023"
    },
    {
        "id": 29316,
        "title": "Geometric deep reinforcement learning for dynamic DAG scheduling",
        "authors": "Nathan Grinsztajn, Olivier Beaumont, Emmanuel Jeannot, Philippe Preux",
        "published": "2020-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci47803.2020.9308278"
    },
    {
        "id": 29317,
        "title": "Cuems: Deep Reinforcement Learning for Community Control of Energy Management Systems in Microgrids",
        "authors": "Jianbin Li, Zeshuo Jiang, Zhiqiang Chen, Jinwei Liu, Long Cheng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4517399"
    },
    {
        "id": 29318,
        "title": "Model for Cooking Recipe Generation using Reinforcement Learning",
        "authors": "Jumpei Fujita, Masahiro Sato, Hajime Nobuhara",
        "published": "2021-4",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdew53142.2021.00007"
    },
    {
        "id": 29319,
        "title": "Meta Reinforcement Learning Based Underwater Manipulator Control",
        "authors": "Jiyoun Moon, Sung-Hoon Bae, Michael Cashmore",
        "published": "2021-10-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iccas52745.2021.9650009"
    },
    {
        "id": 29320,
        "title": "Deep Reinforcement Learning-Based Topology Optimization for Self-Organized Wireless Sensor Networks",
        "authors": "Xiangyue Meng, Hazer Inaltekin, Brian Krongold",
        "published": "2019-12",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9014179"
    },
    {
        "id": 29321,
        "title": "OFFER: Off-Environment Reinforcement Learning",
        "authors": "Kamil Ciosek, Shimon Whiteson",
        "published": "2017-2-13",
        "citations": 8,
        "abstract": "\n      \n        Policy gradient methods have been widely applied in reinforcement learning. For reasons of safety and cost, learning is often conducted using a simulator. However, learning in simulation does not traditionally utilise the opportunity to improve learning by adjusting certain environment variables - state features that are randomly determined by the environment in a physical setting but controllable in a simulator. Exploiting environment variables is crucial in domains containing significant rare events (SREs), e.g., unusual wind conditions that can crash a helicopter, which are rarely observed under random sampling but have a considerable impact on expected return. We propose off environment reinforcement learning (OFFER), which addresses such cases by simultaneously optimising the policy and a proposal distribution over environment variables. We prove that OFFER converges to a locally optimal policy and show experimentally that it learns better and faster than a policy gradient baseline.\n      \n    ",
        "link": "http://dx.doi.org/10.1609/aaai.v31i1.10810"
    },
    {
        "id": 29322,
        "title": "Age of Information Minimization for Wireless Ad Hoc Networks: A Deep Reinforcement Learning Approach",
        "authors": "Shiyang Leng, Aylin Yener",
        "published": "2019-12",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9013454"
    },
    {
        "id": 29323,
        "title": "Deep differentiable reinforcement learning and optimal trading",
        "authors": "Thibault Jaisson",
        "published": "2022-8-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/14697688.2022.2062431"
    },
    {
        "id": 29324,
        "title": "Research on Mobile Robot Path Planning Based on Deep Reinforcement Learning",
        "authors": "XiuFen Ye, Shuo Zhang",
        "published": "2020-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icma49215.2020.9233738"
    },
    {
        "id": 29325,
        "title": "Evolutionary reinforcement learning",
        "authors": "Danilo Vasconcellos Vargas",
        "published": "2018-7-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3205651.3207865"
    },
    {
        "id": 29326,
        "title": "A Reinforcement Learning Approach for Efficient Opportunistic Vehicle-to-Cloud Data Transfer",
        "authors": "Benjamin Sliwa, Christian Wietfeld",
        "published": "2020-5",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc45663.2020.9120681"
    },
    {
        "id": 29327,
        "title": "MODEL-BASED SECURITY ANALYSIS OF FPGA DESIGNS THROUGH REINFORCEMENT LEARNING",
        "authors": "Michael Vetter",
        "published": "2019-11-1",
        "citations": 0,
        "abstract": "Finding potential security weaknesses in any complex IT system is an important and often challenging task best started in the early stages of the development process. We present a method that transforms this task for FPGA designs into a reinforcement learning (RL) problem. This paper introduces a method to generate a Markov Decision Process based RL model from a formal, high-level system description (formulated in the domain-specific language) of the system under review and different, quantified assumptions about the system’s security. Probabilistic transitions and the reward function can be used to model the varying resilience of different elements against attacks and the capabilities of an attacker. This information is then used to determine a plausible data exfiltration strategy. An example with multiple scenarios illustrates the workflow. A discussion of supplementary techniques like hierarchical learning and deep neural networks concludes this paper.",
        "link": "http://dx.doi.org/10.14311/ap.2019.59.0518"
    },
    {
        "id": 29328,
        "title": "A Multi-Level Reinforcement-Learning Model of Wisconsin Card Sorting Test Performance",
        "authors": "Alexander Steinke, Florian Lange, Bruno Kopp",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2019.1030-0"
    },
    {
        "id": 29329,
        "title": "A Reinforcement Learning-Based Energy Management Strategy for Fuel Cell Electric Vehicle Considering Coupled-Energy Sources Degradations",
        "authors": "Weiwei Huo, Liu Teng, Fajia Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4442266"
    },
    {
        "id": 29330,
        "title": "Generating Multi-agent Patrol Areas by Reinforcement Learning",
        "authors": "Bumjin Park, Cheongwoong Kang, Jaesik Choi",
        "published": "2021-10-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iccas52745.2021.9650047"
    },
    {
        "id": 29331,
        "title": "Fog Fragment Cooperation on Bandwidth Management Based on Reinforcement Learning",
        "authors": "Motahareh Mobasheri, Yangwoo Kim, Woongsup Kim",
        "published": "2020-12-4",
        "citations": 3,
        "abstract": "The term big data has emerged in network concepts since the Internet of Things (IoT) made data generation faster through various smart environments. In contrast, bandwidth improvement has been slower; therefore, it has become a bottleneck, creating the need to solve bandwidth constraints. Over time, due to smart environment extensions and the increasing number of IoT devices, the number of fog nodes has increased. In this study, we introduce fog fragment computing in contrast to conventional fog computing. We address bandwidth management using fog nodes and their cooperation to overcome the extra required bandwidth for IoT devices with emergencies and bandwidth limitations. We formulate the decision-making problem of the fog nodes using a reinforcement learning approach and develop a Q-learning algorithm to achieve efficient decisions by forcing the fog nodes to help each other under special conditions. To the best of our knowledge, there has been no research with this objective thus far. Therefore, we compare this study with another scenario that considers a single fog node to show that our new extended method performs considerably better.",
        "link": "http://dx.doi.org/10.3390/s20236942"
    },
    {
        "id": 29332,
        "title": "A Reinforcement Learning Based Low-Delay Scheduling With Adaptive Transmission",
        "authors": "Yu Zhao, Joohyun Lee",
        "published": "2019-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictc46691.2019.8939680"
    },
    {
        "id": 29333,
        "title": "A LOGIC BASED AFFECTIVE TUTORING SYSTEM THAT USES REINFORCEMENT LEARNING FOR DISCOVERING TEACHING STRATEGIES",
        "authors": "Achilles Dougalis, Dimitris Plexousakis",
        "published": "2022-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21125/edulearn.2022.1085"
    },
    {
        "id": 29334,
        "title": "Deep Reinforcement Learning for Data Freshness-oriented Scheduling in Industrial IoT",
        "authors": "Jiaping Li, Jianhua Tang, Zilong Liu",
        "published": "2022-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001430"
    },
    {
        "id": 29335,
        "title": "A survey of benchmarks for reinforcement learning algorithms",
        "authors": "Belinda Stapelberg, Katherine Mary Malan",
        "published": "2020-12-8",
        "citations": 1,
        "abstract": "Reinforcement learning has recently experienced increased prominence in the machine learning community. There are many approaches to solving reinforcement learning problems with new techniques developed constantly. When solving problems using reinforcement learning, there are various difficult challenges to overcome. \\par To ensure progress in the field, benchmarks are important for testing new algorithms and comparing with other approaches. The reproducibility of results for fair comparison is therefore vital in ensuring that improvements are accurately judged. This paper provides an overview of different contributions to reinforcement learning benchmarking and discusses how they can assist researchers to address the challenges facing reinforcement learning. The contributions discussed are the most used and recent in the literature. The paper discusses the contributions in terms of implementation, tasks and provided algorithm implementations with benchmarks. \\par The survey aims to bring attention to the wide range of reinforcement learning benchmarking tasks available and to encourage research to take place in a standardised manner. Additionally, this survey acts as an overview for researchers not familiar with the different tasks that can be used to develop and test new reinforcement learning algorithms.",
        "link": "http://dx.doi.org/10.18489/sacj.v32i2.746"
    },
    {
        "id": 29336,
        "title": "Admission Control for 5G Network Slicing based on (Deep) Reinforcement Learning",
        "authors": "William Fernando Villota Jácome, Oscar Mauricio Caicedo Rendon, Nelson Luis Saldanha da Fonseca",
        "published": "No Date",
        "citations": 0,
        "abstract": "Network Slicing is a promising technology for\nproviding customized logical and virtualized networks for the\nindustry’s vertical segments.This paper proposes SARA and DSARA for the performance of admission control and resource allocation for network slice requests of eMBB, URLLC, and MIoT type in the 5G core network. SARA introduced a Q-learning based algorithm and DSARA a DQN-based algorithm to select the most profitable requests from a set that arrived in given time windows. These algorithms are model-free, meaning they do not make assumptions about the substrate network as do optimization based approaches.",
        "link": "http://dx.doi.org/10.36227/techrxiv.14498190.v1"
    },
    {
        "id": 29337,
        "title": "The Use of Reinforcement Learning in Gaming The Breakout Game Case Study.pdf",
        "authors": "Ao Chen, Taresh Dewan, Manva Trivedi, Danning Jiang, Aloukik Aditya, Sabah Mohammed",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper provides a comparative analysis between Deep Q Network (DQN) and Double Deep Q Network (DDQN) algorithms  based  on  their hit rate, out of which DDQN proved to be better for Breakout game. DQN is chosen over Basic Q learning because    it understands policy learning using its neural network which is good for complex environment and DDQN is chosen as it solves overestimation problem (agent always choses non-optimal action for any state just because it has maximum Q-value) occurring    in basic Q-learning.",
        "link": "http://dx.doi.org/10.36227/techrxiv.12061728.v1"
    },
    {
        "id": 29338,
        "title": "Injection Mold Production Sustainable Scheduling Using Deep Reinforcement Learning",
        "authors": "Seunghoon Lee, Yongju Cho, Young Hoon Lee",
        "published": "2020-10-21",
        "citations": 15,
        "abstract": "In the injection mold industry, it is important for manufacturers to satisfy the delivery date for the products that customers order. The mold products are diverse, and each product has a different manufacturing process. Owing to the nature of mold, mold manufacturing is a complex and dynamic environment. To meet the delivery date of the customers, the scheduling of mold production is important and is required to be sustainable and intelligent even in the complicated system and dynamic situation. To address this, in this paper, deep reinforcement learning (RL) is proposed for injection mold production scheduling. Before presenting the RL algorithm, a mathematical model for the mold scheduling problem is presented, and a Markov decision process framework is proposed for RL. The deep Q-network, which is an algorithm for RL, is employed to find the scheduling policy to minimize the total weighted tardiness. The results of experiments demonstrate that the proposed deep RL method outperforms the dispatching rules that are presented for minimizing the total weighted tardiness.",
        "link": "http://dx.doi.org/10.3390/su12208718"
    },
    {
        "id": 29339,
        "title": "Action-Bounding for Reinforcement Learning in Energy Harvesting Communication Systems",
        "authors": "Heasung Kim, Heecheol Yang, Yeongmo Kim, Jungwoo Lee",
        "published": "2018-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/glocom.2018.8647681"
    },
    {
        "id": 29340,
        "title": "Tabular Reinforcement Learning in Real-Time Strategy Games via Options",
        "authors": "Anderson R. Tavares, Luiz Chaimowicz",
        "published": "2018-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2018.8490427"
    },
    {
        "id": 29341,
        "title": "Self-timed Reinforcement Learning using Tsetlin Machine",
        "authors": "Adrian Wheeldon, Alex Yakovlev, Rishad Shafik",
        "published": "2021-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/async48570.2021.00014"
    },
    {
        "id": 29342,
        "title": "Contracts for Difference: A Reinforcement Learning Approach",
        "authors": "Nico Zengeler, Uwe Handmann",
        "published": "2020-4-17",
        "citations": 2,
        "abstract": "We present a deep reinforcement learning framework for an automatic trading of contracts for difference (CfD) on indices at a high frequency. Our contribution proves that reinforcement learning agents with recurrent long short-term memory (LSTM) networks can learn from recent market history and outperform the market. Usually, these approaches depend on a low latency. In a real-world example, we show that an increased model size may compensate for a higher latency. As the noisy nature of economic trends complicates predictions, especially in speculative assets, our approach does not predict courses but instead uses a reinforcement learning agent to learn an overall lucrative trading policy. Therefore, we simulate a virtual market environment, based on historical trading data. Our environment provides a partially observable Markov decision process (POMDP) to reinforcement learners and allows the training of various strategies.",
        "link": "http://dx.doi.org/10.3390/jrfm13040078"
    },
    {
        "id": 29343,
        "title": "Reinforcement Learning Based Inter-User-Interference Suppression in Full-Duplex Networks",
        "authors": "Dani Korpi, Mikko A. Uusitalo",
        "published": "2021-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2021-spring51267.2021.9448931"
    },
    {
        "id": 29344,
        "title": "Reinforcement Learning with Symbolic Input-Output Models",
        "authors": "Erik Derner, Jiri Kubalik, Robert Babuska",
        "published": "2018-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2018.8593881"
    },
    {
        "id": 29345,
        "title": "Attentional Reinforcement Learning in the Brain",
        "authors": "Hiroshi Yamakawa",
        "published": "2020-3",
        "citations": 8,
        "abstract": "AbstractRecently, attention mechanisms have significantly boosted the performance of natural language processing using deep learning. An attention mechanism can select the information to be used, such as by conducting a dictionary lookup; this information is then used, for example, to select the next utterance word in a sentence. In neuroscience, the basis of the function of sequentially selecting words is considered to be the cortico-basal ganglia-thalamocortical loop. Here, we first show that the attention mechanism used in deep learning corresponds to the mechanism in which the cerebral basal ganglia suppress thalamic relay cells in the brain. Next, we demonstrate that, in neuroscience, the output of the basal ganglia is associated with the action output in the actor of reinforcement learning. Based on these, we show that the aforementioned loop can be generalized as reinforcement learning that controls the transmission of the prediction signal so as to maximize the prediction reward. We call this attentional reinforcement learning (ARL). In ARL, the actor selects the information transmission route according to the attention, and the prediction signal changes according to the context detected by the information source of the route. Hence, ARL enables flexible action selection that depends on the situation, unlike traditional reinforcement learning, wherein the actor must directly select an action.",
        "link": "http://dx.doi.org/10.1007/s00354-019-00081-z"
    },
    {
        "id": 29346,
        "title": "Predictive Control of a Robot Manipulator with Deep Reinforcement Learning",
        "authors": "Eduardo Bejar, Antonio Moran",
        "published": "2021-4-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccar52225.2021.9463462"
    },
    {
        "id": 29347,
        "title": "Enhanced Smart Home Architecture using Deep Reinforcement Learning and Blockchain",
        "authors": " Subhita,  Divya,  Kavita",
        "published": "2023-3-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscon57294.2023.10112192"
    },
    {
        "id": 29348,
        "title": "16. Processes Basic to Learning and Reinforcement",
        "authors": "",
        "published": "2017-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12987/9780300129359-019"
    },
    {
        "id": 29349,
        "title": "Migration Towards Decentral Material Flow Systems for Volatile Production Environments Using Multi-Agent Reinforcement Learning",
        "authors": "Silvester Rotenhan, Florian Rothmeyer, Florian Ried, Johannes Fottner",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4716184"
    },
    {
        "id": 29350,
        "title": "An Improved Energy Management Strategy for Hybrid Electric Powered Aircraft Based on Deep Reinforcement Learning",
        "authors": "Liaolei He, Fang Chen, Peidong Tian, Huaxing Gou",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4720851"
    },
    {
        "id": 29351,
        "title": "Tackling Real-World Autonomous Driving using Deep Reinforcement Learning",
        "authors": "Paolo Maramotti, Alessandro Paolo Capasso, Giulio Bacchiani, Alberto Broggi",
        "published": "2022-6-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv51971.2022.9827302"
    },
    {
        "id": 29352,
        "title": "Deep Reinforcement and Machine Learning for Seismic Data Processing and Automated QC",
        "authors": "J. Brittan, R. O’Driscoll, J. Walpole",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202112542"
    },
    {
        "id": 29353,
        "title": "Multi-agent Cooperation Models by Reinforcement Learning (MCMRL)",
        "authors": "Deepak A., Parag Kulkarni",
        "published": "2017-10-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5120/ijca2017915511"
    },
    {
        "id": 29354,
        "title": "Forschungsmethodik",
        "authors": "Martin Sterzel",
        "published": "2023",
        "citations": 0,
        "abstract": "ZusammenfassungZur algorithmischen Interpretation von Effectuation kommen Ansätze agentenbasierter Modellierung zum Einsatz. Zur Abbildung effektuativen Lernens werden Methoden des Reinforcement Learnings zur Anwendung gebracht. Anhand einer protypischen Gründungssituation wird ein Zustandsraum gebildet, der als Lernumgebung des effektuativen Agenten dient. Durch Modellierung einer Belohnungsfunktion erhält der entrepreneuriale Agent die Möglichkeit Effectuation zu erlernen. Die Verwendung des Q-Learning-Algorithmus als Lernstrategie erlaubt die Modellierung der Kernelemente des entrepreneurialen Problemraums.",
        "link": "http://dx.doi.org/10.1007/978-3-658-39251-2_4"
    },
    {
        "id": 29355,
        "title": "A deep reinforcement learning approach for portfolio optimization and risk management – Case studies",
        "authors": "Filip Wójcik",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780367854690-7"
    },
    {
        "id": 29356,
        "title": "AutoPhoto: Aesthetic Photo Capture using Reinforcement Learning",
        "authors": "Hadi AlZayer, Hubert Lin, Kavita Bala",
        "published": "2021-9-27",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros51168.2021.9636788"
    },
    {
        "id": 29357,
        "title": "Reinforcement Learning and Distributed Model Predictive Control for Conflict Resolution in Highly Constrained Spaces",
        "authors": "Xu Shen, Francesco Borrelli",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iv55152.2023.10186560"
    },
    {
        "id": 29358,
        "title": "A Modified Multi-Agent Reinforcement Learning Protocol Based on Prediction for UAANETs",
        "authors": "Chao Li, Jing Liu",
        "published": "2020-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2020-fall49728.2020.9348732"
    },
    {
        "id": 29359,
        "title": "Reinforcement Learning of Space Robotic Manipulation with Multiple Safety Constraints",
        "authors": "Linfeng Li, Yongchun Xie, Yong Wang, Ao Chen",
        "published": "2022-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9902690"
    },
    {
        "id": 29360,
        "title": "Reinforcement learning beyond the Bellman equation: Exploring critic objectives using evolution",
        "authors": "Abe Leite, Madhavun Candadai, Eduardo J. Izquierdo",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1162/isal_a_00338"
    },
    {
        "id": 29361,
        "title": "Collaborative Filtering Guided Deep Reinforcement Learning for Sequential Recommendations",
        "authors": "Vahid Azizi, Saayan Mitra, Xiang Chen",
        "published": "2022-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata55660.2022.10020921"
    },
    {
        "id": 29362,
        "title": "Considerations of Reinforcement Learning within Real-Time Wireless Communication Systems",
        "authors": "Alyse M. Jones, William C. Headley",
        "published": "2022-11-28",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/milcom55135.2022.10017303"
    },
    {
        "id": 29363,
        "title": "Dynamic Scheduling of Maintenance by a Reinforcement Learning Approach - A Semiconductor Simulation Study",
        "authors": "Michael Geurtsen, Ivo Adan, Zumbul Atan",
        "published": "2022-12-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc57314.2022.10015402"
    },
    {
        "id": 29364,
        "title": "Deep Reinforcement Learning Aided Direct Torque Control for Induction Motor based on Pulse Width Modulation and Speed Controller",
        "authors": "S. Suresh, B. karthik",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe authors have requested that this preprint be removed from Research Square.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2290567/v1"
    },
    {
        "id": 29365,
        "title": "Multibit Tries Packet Classification with Deep Reinforcement Learning",
        "authors": "Hasibul Jamil, Ning Weng",
        "published": "2020-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hpsr48589.2020.9098974"
    },
    {
        "id": 29366,
        "title": "Attention-Based Curiosity in Multi-Agent Reinforcement Learning Environments",
        "authors": "Marton Szemenyei, Patrik Reizinger",
        "published": "2019-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccairo47923.2019.00035"
    },
    {
        "id": 29367,
        "title": "Reinforcement Learning-Based Control for Waste Biorefining Processes Under Uncertainty",
        "authors": "Ji Gao, Abigael Wahlen, Caleb Ju, Yongsheng Chen, Guanghui Lan, Zhaohui tong",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWaste biorefining processes face significant challenges related to the variability of feedstocks. The supply and composition of multiple feedstocks in these processes can be uncertain, making it difficult to achieve economically feasible and sustainable waste valorization for large-scale production. Here, we introduce a reinforcement learning-based framework that aims to control these uncertainties and improve the efficiency of the process. The framework is tested on an anaerobic digestion process and is found to perform better than traditional control strategies. In the short term, it achieves faster target tracking with increased precision and accuracy, while in the long term, it shows adaptive and robust behavior even under additional seasonal supply variability, meeting downstream demand with high probability. This reinforcement learning-based framework offers a promising and scalable solution to address uncertainty issues in real-world biorefining processes. If implemented, this framework could contribute to sustainable waste management practices globally, making waste biorefining processes more economically viable and environmentally friendly.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2860936/v1"
    },
    {
        "id": 29368,
        "title": "Agent behavior modeling method based on reinforcement learning and human in the loop",
        "authors": "Lin Huang, Li Gong",
        "published": "2023-6-1",
        "citations": 0,
        "abstract": "Computer generated force (CGF) is one of the increasingly important research topics in the field of simulation. However, low modeling efficiency and lack of adaptability are acute problems of traditional CGF modeling. In this study, a method for modeling the agent behavior based on reinforcement learning and human in the loop is proposed to improve the ability and efficiency of agent behavior modeling. First, an overall framework for modeling the behavior of intelligent agents is constructed based on the deep reinforcement learning algorithm Soft Actor Critic (SAC) framework. Second, in order to overcome the slow convergence speed of the SAC framework, a method for human interaction and value evaluation in the loop is introduced, and the specific algorithm flow is designed. Third, in order to verify the performance of the proposed method, experiments are conducted and compared with algorithms using a pure SAC framework based on an example of agent completing specific tasks. Result shows that after 100 episodes of training, the task completion rate of the agent can approach 100% while a pure SAC framework require at least 500 episodes of training to gradually improve the completion rate. Finally, the results demonstrate that the proposed method can significantly improve the efficiency of agent behavior modeling and the task completion rate increases with the number of human interventions in the loop.",
        "link": "http://dx.doi.org/10.1063/5.0152822"
    },
    {
        "id": 29369,
        "title": "Coexistence of LTE-Unlicensed and WiFi: A Reinforcement Learning Framework",
        "authors": "K. P. Naveen, Chaitanya Amballa",
        "published": "2021-1-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/comsnets51098.2021.9352942"
    },
    {
        "id": 29370,
        "title": "Novel multi-agent reinforcement learning for maximizing throughput in UAV-Enabled 5G networks",
        "authors": "Kuan Li",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11276-023-03560-w"
    },
    {
        "id": 29371,
        "title": "On Robust Model-Free Reduced-Dimensional Reinforcement Learning Control for Singularly Perturbed Systems",
        "authors": "Sayak Mukherjee, He Bai, Aranya Chakrabortty",
        "published": "2020-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147523"
    },
    {
        "id": 29372,
        "title": "An improved disjunctive graph and deep reinforcement learning for FJSP",
        "authors": "Changshun Shao, Zhenglin Yu, Jianyin Tang, Bin Zhou, Hongchang Ding, Kaifang Ding",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the current industrial manufacturing environment, the flexible job shop scheduling problem (FJSP), which is an extension of the job shop scheduling problem, holds significant research significance. While intelligent algorithms are commonly used to solve the FJSP problem, this paper proposes the use of reinforcement learning algorithms. The FJSP is first transformed into a Markov decision process, and an improved FJSP disjunction graph structure is introduced. Subsequently, 8 state features are designed, and a combination of gated attention network and multi-layer perceptron is employed to extract these features. For the decision-making processes of FJSP, a 3-tuple is designed as the action space. Finally, a reward function is formulated to optimize the objective of minimizing the maximum makespan. The training algorithm utilizes the Advantage Actor-Critic algorithm to establish an end-to-end deep reinforcement learning framework. Experimental results demonstrate that this algorithm exhibits significant advantages in terms of FJSP solution quality and efficiency. In conclusion, the reinforcement learning method proposed in this article presents a novel approach to address the FJSP problem and holds promising application prospects.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3872784/v1"
    },
    {
        "id": 29373,
        "title": "Dynamically Changing Sequencing Rules with Reinforcement Learning in a Job Shop System With Stochastic Influences",
        "authors": "Jens Heger, Thomas Voss",
        "published": "2020-12-14",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc48552.2020.9383903"
    },
    {
        "id": 29374,
        "title": "Reinforcement Learning Aided UAV Base Station Location Optimization for Rate Maximization",
        "authors": "Sudheesh Puthenveettil Gopi, Maurizio Magarini",
        "published": "2021-11-27",
        "citations": 8,
        "abstract": "The application of unmanned aerial vehicles (UAV) as base station (BS) is gaining popularity. In this paper, we consider maximization of the overall data rate by intelligent deployment of UAV BS in the downlink of a cellular system. We investigate a reinforcement learning (RL)-aided approach to optimize the position of flying BSs mounted on board UAVs to support a macro BS (MBS). We propose an algorithm to avoid collision between multiple UAVs undergoing exploratory movements and to restrict UAV BSs movement within a predefined area. Q-learning technique is used to optimize UAV BS position, where the reward is equal to sum of user equipment (UE) data rates. We consider a framework where the UAV BSs carry out exploratory movements in the beginning and exploitary movements in later stages to maximize the overall data rate. Our results show that a cellular system with three UAV BSs and one MBS serving 72 UE reaches 69.2% of the best possible data rate, which is identified by brute force search. Finally, the RL algorithm is compared with a K-means algorithm to study the need of accurate UE locations. Our results show that the RL algorithm outperforms the K-means clustering algorithm when the measure of imperfection is higher. The proposed algorithm can be made use of by a practical MBS–UAV BSs–UEs system to provide protection to UAV BSs while maximizing data rate.",
        "link": "http://dx.doi.org/10.3390/electronics10232953"
    },
    {
        "id": 29375,
        "title": "Feature-Based Interpretable Reinforcement Learning based on State-Transition Models",
        "authors": "Omid Davoodi, Majid Komeili",
        "published": "2021-10-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc52423.2021.9658917"
    },
    {
        "id": 29376,
        "title": "Automatic Market Making System with Offline Reinforcement Learning",
        "authors": "Hong Guo, Yue Zhao, Jianwu Lin",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394135"
    },
    {
        "id": 29377,
        "title": "DreamingV2: Reinforcement Learning with Discrete World Models without Reconstruction",
        "authors": "Masashi Okada, Tadahiro Taniguchi",
        "published": "2022-10-23",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981405"
    },
    {
        "id": 29378,
        "title": "Deep reinforcement learning for frontal view person shooting using drones",
        "authors": "Nikolaos Passalis, Anastasios Tefas",
        "published": "2018-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eais.2018.8397177"
    },
    {
        "id": 29379,
        "title": "Improvement Reinforcement Learning for Solving Multisensor Collaborative Scheduling Problems",
        "authors": "Zhaoyu Zhang, Nan Li",
        "published": "2023-8-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icicn59530.2023.10392612"
    },
    {
        "id": 29380,
        "title": "Heuristic Reward Function for Reinforcement Learning Based Manipulator Motion Planning",
        "authors": "Jiawei Zhang, Jifeng Guo, Chengchao Bai",
        "published": "2022-10-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus55513.2022.9986816"
    },
    {
        "id": 29381,
        "title": "Channel Selection and Power Control for D2D Communication via Online Reinforcement Learning",
        "authors": "Zhenfeng Sun, Mohammad Reza Nakhai",
        "published": "2021-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc42927.2021.9501055"
    },
    {
        "id": 29382,
        "title": "Deep Reinforcement Learning for Autonomous Traffic Light Control",
        "authors": "Deepeka Garg, Maria Chli, George Vogiatzis",
        "published": "2018-9",
        "citations": 27,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icite.2018.8492537"
    },
    {
        "id": 29383,
        "title": "Implicit counterfactual effect in partial feedback reinforcement learning: behavioral and modeling approach",
        "authors": "Zahra Barakchian, Abdol-hossein Vahabie, Majid Nili Ahmadabadi",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractContext by distorting values of options with respect to the distribution of available alternatives, remarkably affects learning behavior. Providing an explicit counterfactual component, outcome of unchosen option alongside with the chosen one (Complete feedback), would increase the contextual effect by inducing comparison-based strategy during learning. But It is not clear in the conditions where the context consists only of the juxtaposition of a series of options, and there is no such explicit counterfactual component (Partial feedback), whether and how the relativity will be emerged. Here for investigating whether and how implicit and explicit counterfactual components can affect reinforcement learning, we used two Partial and Complete feedback paradigms, in which options were associated with some reward distributions. Our modeling analysis illustrates that the model which uses the outcome of chosen option for updating values of both chosen and unchosen options, which is in line with diffusive function of dopamine on the striatum, can better account for the behavioral data. We also observed that size of this bias depends on the involved systems in the brain, such that this effect is larger in the transfer phase where subcortical systems are more involved, and is smaller in the deliberative value estimation phase where cortical system is more needed. Furthermore, our data shows that contextual effect is not only limited to probabilistic reward but also it extends to reward with amplitude. These results show that by extending counterfactual concept, we can better account for why there is contextual effect in a condition where there is no extra information of unchosen outcome.",
        "link": "http://dx.doi.org/10.1101/2020.09.30.320135"
    },
    {
        "id": 29384,
        "title": "Quantum Virtual Link Generation via Reinforcement Learning",
        "authors": "Ramon Aparicio-Pardo, Antoine Cousson, Redha A. Alliche",
        "published": "2023-7-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icton59386.2023.10207249"
    },
    {
        "id": 29385,
        "title": "A Comprehensive Discussion on Deep Reinforcement Learning",
        "authors": "Weikang Xu, Linbo Chen, Hongyu Yang",
        "published": "2021-5-14",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cisce52179.2021.9445914"
    },
    {
        "id": 29386,
        "title": "Closed-Loop Deep Brain Stimulation with Reinforcement Learning and Neural Simulation",
        "authors": "Chia-Hung Cho, Chii-Wann Lin, Pin-Jui Huang, Meng-Chao Chen",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170654748.84453385/v1"
    },
    {
        "id": 29387,
        "title": "When Being Selfish Prevails: The Impact of Sociality Regimes on Heterogeneous Cooperative-Competitive Multi-agent Reinforcement Learning",
        "authors": "Yue Zhao, José Hernández-Orallo",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nWork in multi-agent reinforcement learning (MARL) tends to compare algorithms choosing some particular sociality regime for sharing (or not) the rewards, assuming this choice is optimal. For instance, the popular multi-agent deep deterministic policy gradient algorithm (MADDPG), when evaluated in a predator and prey game, used a different regime when acting as a predator than when acting as a prey. In this paper, we question what kind of sociality regimes (selfish, egalitarian, or altruistic) should be used in multi-agent algorithms. In the past, choices of the sociality regime have been done only for homogeneous teams, and without normalising the effect of the sociality regimes. Here we introduce a normalisation of the weights of the sociality regimes so that the sum of all rewards is independent of the sociality regime. With this, we extend the analysis to heterogeneous teams in cooperative-competitive MARL situations since team composition is very relevant to the sociality choice. We find that the selfish regime is advantageous for both prey and predator teams, and for both homogeneous and heterogeneous teams. The takeaway message is that, unlike common practice in MARL, any study of homogeneous and heterogeneous cooperative-competitive multi-agent reinforcement learning teams should also take into account the sociality regimes before making conclusions on the preference of any algorithm.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2411520/v1"
    },
    {
        "id": 29388,
        "title": "CVA Hedging with Reinforcement Learning",
        "authors": "Roberto Daluiso, Marco Pinciroli, Michele Trapletti, Edoardo Vittori",
        "published": "2023-11-27",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3604237.3626852"
    },
    {
        "id": 29389,
        "title": "Inverse Reinforcement Learning Approach for Elicitation of Preferences in Multi-objective Sequential Optimization",
        "authors": "Akiko Ikenaga, Sachiyo Arai",
        "published": "2018-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/agents.2018.8460075"
    },
    {
        "id": 29390,
        "title": "Interterminal Truck Routing Optimization Using Deep Reinforcement Learning",
        "authors": "Taufik Nur Adi, Yelita Anggiane Iskandar, Hyerim Bae",
        "published": "2020-10-13",
        "citations": 17,
        "abstract": "The continued growth of the volume of global containerized transport necessitates that most of the major ports in the world improve port productivity by investing in more interconnected terminals. The development of the multiterminal system escalates the complexity of the container transport process and increases the demand for container exchange between different terminals within a port, known as interterminal transport (ITT). Trucks are still the primary modes of freight transportation to transport containers among most terminals. A trucking company needs to consider proper truck routing planning because, based on several studies, it played an essential role in coordinating ITT flows. Furthermore, optimal truck routing in the context of ITT significantly affects port productivity and efficiency. The study of deep reinforcement learning in truck routing optimization is still limited. In this study, we propose deep reinforcement learning to provide truck routes of a given container transport order by considering several significant factors such as order origin, destination, time window, and due date. To assess its performance, we compared between the proposed method and two approaches that are used to solve truck routing problems. The experiment results showed that the proposed method obtains considerably better results compared to the other algorithms.",
        "link": "http://dx.doi.org/10.3390/s20205794"
    },
    {
        "id": 29391,
        "title": "Reinforcement Learning Based Power Control for VANET Broadcast against Jamming",
        "authors": "Canhuang Dai, Xingyu Xiao, Liang Xiao, Peng Cheng",
        "published": "2018-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/glocom.2018.8647273"
    },
    {
        "id": 29392,
        "title": "A Deep Reinforcement Learning Approach for Active SLAM",
        "authors": "Julio A. Placed, José A. Castellanos",
        "published": "2020-11-25",
        "citations": 18,
        "abstract": "In this paper, we formulate the active SLAM paradigm in terms of model-free Deep Reinforcement Learning, embedding the traditional utility functions based on the Theory of Optimal Experimental Design in rewards, and therefore relaxing the intensive computations of classical approaches. We validate such formulation in a complex simulation environment, using a state-of-the-art deep Q-learning architecture with laser measurements as network inputs. Trained agents become capable not only to learn a policy to navigate and explore in the absence of an environment model but also to transfer their knowledge to previously unseen maps, which is a key requirement in robotic exploration.",
        "link": "http://dx.doi.org/10.3390/app10238386"
    },
    {
        "id": 29393,
        "title": "Adaptive operator selection with reinforcement learning",
        "authors": "Rafet Durgut, Mehmet Emin Aydin, Ibrahim Atli",
        "published": "2021-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ins.2021.10.025"
    },
    {
        "id": 29394,
        "title": "Towards Adaptive Packet Scheduler with Deep-Q Reinforcement Learning",
        "authors": "Qiwei Wang, Thinh Nguyen, Bella Bose",
        "published": "2020-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnc47757.2020.9049807"
    },
    {
        "id": 29395,
        "title": "Robust Model Predictive Shielding for Safe Reinforcement Learning with Stochastic Dynamics",
        "authors": "Shuo Li, Osbert Bastani",
        "published": "2020-5",
        "citations": 26,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra40945.2020.9196867"
    },
    {
        "id": 29396,
        "title": "Smooth Path Planning of 6-DOF Robot Based on Reinforcement Learning",
        "authors": "Jiawei Tian, Dazi Li",
        "published": "2022-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccr55715.2022.10053875"
    },
    {
        "id": 29397,
        "title": "Reproducibility in Deep Reinforcement Learning with Maximum Entropy",
        "authors": "Tudor-Andrei Paleu, Carlos Pascal",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icstcc59206.2023.10308431"
    },
    {
        "id": 29398,
        "title": "REST: Constructing Rectilinear Steiner Minimum Tree via Reinforcement Learning",
        "authors": "Jinwei Liu, Gengjie Chen, Evangeline F.Y. Young",
        "published": "2021-12-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dac18074.2021.9586209"
    },
    {
        "id": 29399,
        "title": "Deep reinforcement learning based Adaptive Modulation for OFDM Underwater Acoustic Communication System",
        "authors": "Xuerong Cui, Peihao Yan, Juan Li, Shibao Li, Jianhang Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDue to the time-varying and space-varying characteristics of the underwater acoustic channel, the communication process may be seriously disturbed. Thus, the underwater acoustic communication system is facing the challenges of alleviating interference and improving communication quality and communication efficiency through adaptive modulation. In order to select the optimal modulation mode adaptively and maximize the system throughput ensuring that the bit error rate (BER) meets the transmission requirements, this paper introduces deep reinforcement learning (DRL) into orthogonal frequency division multiplexing (OFDM) acoustic communication system. The adaptive modulation is mapped into a Markov decision process with unknown state transition probability. Thereby, the underwater communication channel environment is regarded as the state of DRL, and the modulation mode is regarded as action. The system returns channel state information (CSI) and signal noise ratio (SNR) in every time slot through the feedback link. Because the Deep Q-Network (DQN) optimizes in the changing state space of each time slot, it is suitable for a variety of different CSI. Finally, simulations in different underwater environments (SWellEx-96) show that the proposed adaptive modulation scheme can obtain lower BER, and improve the system throughput effectively.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1657231/v1"
    },
    {
        "id": 29400,
        "title": "Distributed Policy Gradient with Heterogeneous Computations for Federated Reinforcement Learning",
        "authors": "Ye Zhu, Xiaowen Gong",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089771"
    },
    {
        "id": 29401,
        "title": "Quantum Virtual Link Generation via Reinforcement Learning",
        "authors": "Ramon Aparicio-Pardo, Antoine Cousson, Redha A. Alliche",
        "published": "2023-7-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icton59386.2023.10207249"
    },
    {
        "id": 29402,
        "title": "Distributed Policy Gradient with Heterogeneous Computations for Federated Reinforcement Learning",
        "authors": "Ye Zhu, Xiaowen Gong",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089771"
    },
    {
        "id": 29403,
        "title": "Deep reinforcement learning based Adaptive Modulation for OFDM Underwater Acoustic Communication System",
        "authors": "Xuerong Cui, Peihao Yan, Juan Li, Shibao Li, Jianhang Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDue to the time-varying and space-varying characteristics of the underwater acoustic channel, the communication process may be seriously disturbed. Thus, the underwater acoustic communication system is facing the challenges of alleviating interference and improving communication quality and communication efficiency through adaptive modulation. In order to select the optimal modulation mode adaptively and maximize the system throughput ensuring that the bit error rate (BER) meets the transmission requirements, this paper introduces deep reinforcement learning (DRL) into orthogonal frequency division multiplexing (OFDM) acoustic communication system. The adaptive modulation is mapped into a Markov decision process with unknown state transition probability. Thereby, the underwater communication channel environment is regarded as the state of DRL, and the modulation mode is regarded as action. The system returns channel state information (CSI) and signal noise ratio (SNR) in every time slot through the feedback link. Because the Deep Q-Network (DQN) optimizes in the changing state space of each time slot, it is suitable for a variety of different CSI. Finally, simulations in different underwater environments (SWellEx-96) show that the proposed adaptive modulation scheme can obtain lower BER, and improve the system throughput effectively.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1657231/v1"
    },
    {
        "id": 29404,
        "title": "Robust Decision Making for Autonomous Vehicles at Highway On-Ramps: A Constrained Adversarial Reinforcement Learning Approach",
        "authors": "Xiangkun He, Baichuan Lou, Haohan Yang, Chen Lv",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning has demonstrated its potential in a series of challenging domains.</p>\n<p>However, many real-world decision making tasks involve unpredictable environmental changes or unavoidable perception errors that are often enough to mislead an agent into making suboptimal decisions and even cause catastrophic failures.</p>\n<p>In light of these potential risks, reinforcement learning with application in safety-critical autonomous driving domain remains tricky without ensuring robustness against environmental uncertainties (e.g., road adhesion changes or measurement noises). </p>\n<p>Therefore, this paper proposes a novel constrained adversarial reinforcement learning approach for robust decision making of autonomous vehicles at highway on-ramps.</p>\n<p>Environmental disturbance is modelled as an adversarial agent that can learn an optimal adversarial policy to thwart the autonomous driving agent.</p>\n<p>Meanwhile, observation perturbation is approximated to maximize the variation of the perturbed policy through a white-box adversarial attack technique.</p>\n<p>Furthermore, a constrained adversarial actor-critic algorithm is presented to optimize an on-ramp merging policy while keeping the variations of the attacked driving policy and action-value function within bounds.</p>\n<p>Finally, the proposed robust highway on-ramp merging decision making method of autonomous vehicles is evaluated in three stochastic mixed traffic flows with different densities, and its effectiveness is demonstrated in comparison with the competitive baselines.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21757526"
    },
    {
        "id": 29405,
        "title": "Smooth Path Planning of 6-DOF Robot Based on Reinforcement Learning",
        "authors": "Jiawei Tian, Dazi Li",
        "published": "2022-12-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccr55715.2022.10053875"
    },
    {
        "id": 29406,
        "title": "Towards Adaptive Packet Scheduler with Deep-Q Reinforcement Learning",
        "authors": "Qiwei Wang, Thinh Nguyen, Bella Bose",
        "published": "2020-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnc47757.2020.9049807"
    },
    {
        "id": 29407,
        "title": "Enhancing the Performance of Multi-Agent Reinforcement Learning for Controlling HVAC Systems",
        "authors": "Daniel Bayer, Marco Pruckner",
        "published": "2022-4-21",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sustech53338.2022.9794179"
    },
    {
        "id": 29408,
        "title": "Deep reinforcement learning identifies personalized intermittent androgen deprivation therapy for prostate cancer",
        "authors": "Yitao Lu, Qian Chu, Zhen Li, Mengdi Wang, Qingpeng Zhang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe evolution of drug resistance leads to treatment failure and tumor progression. Intermittent androgen deprivation therapy (IADT) helps responsive cancer cells compete with resistant cancer cells in intratumoral competition. However, conventional IADT is population-based and ignores the heterogeneous phenotypes of individual patients. To address this challenge, we developed a time-varied, mixed-effect, and generative Lotka-Volterra (tM-GLV) model to account for the heterogeneity of the evolution mechanism and the pharmacokinetics of individual patients. Then, we proposed a reinforcement learningenabled individualized IADT framework, namely, I2ADT, to learn the patient-specific tumor dynamics and derive the optimal drug administration policy. Experiments with clinical trial data demonstrated that the proposed I2ADT can significantly prolong the time to progression of prostate cancer patients with reduced cumulative drug dosage. This research elucidates the application of reinforcement learning techniques to identify personalized adaptive cancer therapy.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1573462/v2"
    },
    {
        "id": 29409,
        "title": "RL-LOGO: Deep Reinforcement Learning Localization for Logo Recognition",
        "authors": "Masato Fujitake",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447388"
    },
    {
        "id": 29410,
        "title": "Autonomous Learning and Navigation of Mobile Robots Based on Deep Reinforcement Learning",
        "authors": "Zhiqiang Lai, Zhiwei Jia, Man Chen",
        "published": "2022-1-1",
        "citations": 1,
        "abstract": "Abstract\nAiming at the problems of convergence difficulties faced by deep reinforcement learning algorithms in dynamic pedestrian environments, and insufficient reward and feedback mechanisms, a data-driven and model-driven navigation algorithm which named GRRL has been proposed. In order to enrich and perfect the reward feedback mechanism, we designed a dynamic reward function. The reward function fully considers the relationship between the robot and the pedestrian and the target position. It mainly includes three parts. The experimental results show that the autonomous learning efficiency and the average navigation success rate of the mobile robot driven by the GRRL algorithm are improved, the average navigation time is shorter. The dynamic reward function we designed has a certain improvement effect on robot navigation.",
        "link": "http://dx.doi.org/10.1088/1742-6596/2171/1/012024"
    },
    {
        "id": 29411,
        "title": "Scoped literature review of artificial intelligence marketing adoptions for ad optimization with reinforcement learning",
        "authors": "Johannes Sahlin, Håkan Sundell, Gideon Mbiydzenyuy, Jesper Holgersson",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811269264_0049"
    },
    {
        "id": 29412,
        "title": "Reinforcement Learning–Based Approach Towards Switch Migration for Load-Balancing in SDN",
        "authors": "Abha Kumari, Shubham Gupta, Joydeep Chandra, Ashok Singh Sairam",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003212249-3"
    },
    {
        "id": 29413,
        "title": "An Empirically Grounding Analytics (EGA) Approach to Hog Farm Finishing Stage Management: Deep Reinforcement Learning as Decision Support and Managerial Learning Tool",
        "authors": "Panos Kouvelis, Ye Liu, Danko Turcic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4617964"
    },
    {
        "id": 29414,
        "title": "A Hybrid Multi-Task Learning Approach for Optimizing Deep Reinforcement Learning Agents",
        "authors": "Nelson Vithayathil Varghese, Qusay H. Mahmoud",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3065710"
    },
    {
        "id": 29415,
        "title": "Single stock trading with deep reinforcement learning: A comparative study",
        "authors": "Jun GE, Yuanqi QIN, Yaling Li, yanjia Huang, Hao Hu",
        "published": "2022-2-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3529836.3529857"
    },
    {
        "id": 29416,
        "title": "Temporal Logic Guided Safe Model-Based Reinforcement Learning",
        "authors": "Max Cohen, Calin Belta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-29310-8_9"
    },
    {
        "id": 29417,
        "title": "Learning behavior of hopping with robotic leg on particular height using model free reinforcement learning",
        "authors": "Shiva Pandey, Avinash Bhashkar, Anuj Kumar Sharma",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/5.0189079"
    },
    {
        "id": 29418,
        "title": "Reinforcement Learning for Energy-Efficient Cloud Offloading of Mobile Embedded Applications",
        "authors": "Aditya Khune, Sudeep Pasricha",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-40677-5_7"
    },
    {
        "id": 29419,
        "title": "Continuous Control for Autonomous Underwater Vehicle Path Following Using Deep Interactive Reinforcement Learning",
        "authors": "Qilei Zhang, Chunxi Cheng, Zheng Fang, Dong Jiang, Bo He, Guangliang Li",
        "published": "2022-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlcr57210.2022.00013"
    },
    {
        "id": 29420,
        "title": "Domains as Objectives: Multi-Domain Reinforcement Learning with Convex-Coverage Set Learning for Domain Uncertainty Awareness",
        "authors": "Wendyam Eric Lionel Ilboudo, Taisuke Kobayashi, Takamitsu Matsubara",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10342236"
    },
    {
        "id": 29421,
        "title": "Learning Synergies Between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning",
        "authors": "Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser",
        "published": "2018-10",
        "citations": 306,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2018.8593986"
    },
    {
        "id": 29422,
        "title": "Sample efficient reinforcement learning with active learning for molecular design",
        "authors": "Michael Dodds, Jeff Guo, Thomas Löhr, Alessandro Tibo, Ola Engkvist, Jon Paul Janet",
        "published": "2024",
        "citations": 0,
        "abstract": "Active learning accelerates the design of molecules during generative reinforcement learning by creating surrogate models of expensive reward functions, obtaining a 4- to 64-fold reduction in computational effort per hit.",
        "link": "http://dx.doi.org/10.1039/d3sc04653b"
    },
    {
        "id": 29423,
        "title": "Deep learning and reinforcement learning approach on microgrid",
        "authors": "Kumar Chandrasekaran, Prabaakaran Kandasamy, Srividhya Ramanathan",
        "published": "2020-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531"
    },
    {
        "id": 29424,
        "title": "Research on Constant Perturbation Strategy for Deep Reinforcement Learning",
        "authors": "Jiamin Shen, Li Xu, Xu Wan, Jixuan Chai, Chunlong Fan",
        "published": "2023-3-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3590003.3590101"
    },
    {
        "id": 29425,
        "title": "Perturbational Complexity by Distribution Mismatch: A Systematic Analysis of Reinforcement Learning in Reproducing Kernel Hilbert Space",
        "authors": "Jihao Long null, Jiequn Han",
        "published": "2022-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4208/jml.220114"
    },
    {
        "id": 29426,
        "title": "Human Action Recognition Based on YOLOv7",
        "authors": "Chenwei Liang, Wei Qi Yan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Human action recognition is a fundamental research problem in computer vision. The accuracy of human action recognition has important applications. In this book chapter, the authors use a YOLOv7-based model for human action recognition. To evaluate the performance of the model, the action recognition results of YOLOv7 were compared with those using CNN+LSTM, YOLOv5, and YOLOv4. Furthermore, a small human action dataset suitable for YOLO model training is designed. This data set is composed of images extracted from KTH, Weizmann, MSR data sets. In this book chapter, the authors make use of this data set to verify the experimental results. The final experimental results show that using the YOLOv7 model for human action recognition is very convenient and effective, compared with the previous YOLO model.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch006"
    },
    {
        "id": 29427,
        "title": "Reinforcement Learning Algorithm for Mixed Mean Field Control Games",
        "authors": "Andrea Angiuli, Nils Detering, Jean-Pierre Fouque, Mathieu Laurière null, Jimin Lin",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4208/jml.220915"
    },
    {
        "id": 29428,
        "title": "Universal Reinforcement Learning Algorithms: Survey and Experiments",
        "authors": "John Aslanides, Jan Leike, Marcus Hutter",
        "published": "2017-8",
        "citations": 4,
        "abstract": "Many state-of-the-art reinforcement learning (RL) algorithms typically assume that the environment is an ergodic Markov Decision Process (MDP). In contrast, the field of universal reinforcement learning (URL) is concerned with algorithms that make as few assumptions as possible about the environment. The universal Bayesian agent AIXI and a family of related URL algorithms have been developed in this setting. While numerous theoretical optimality results have been proven for these agents, there has been no empirical investigation of their behavior to date. We present a short and accessible survey of these URL algorithms under a unified notation and framework, along with results of some experiments that qualitatively illustrate some properties of the resulting policies, and their relative performance on partially-observable gridworld environments. We also present an open- source reference implementation of the algorithms which we hope will facilitate further understanding of, and experimentation with, these ideas.",
        "link": "http://dx.doi.org/10.24963/ijcai.2017/194"
    },
    {
        "id": 29429,
        "title": "Scalable Deep Reinforcement Learning for Ride-Hailing",
        "authors": "Jiekun Feng, Mark Gluzman, J. G. Dai",
        "published": "2021-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9483145"
    },
    {
        "id": 29430,
        "title": "Intra-RAN Online Distributed Reinforcement Learning For Uplink Power Control in 5G Cellular Networks",
        "authors": "Majid Butt, Jian Song, Jens Steiner, Klaus Pedersen, Istvan Kovacs",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Uplink power control plays a significant role in maintaining a good signal quality at the serving cell while minimizing interference to neighboring cells, thus maximizing the system performance. Traditionally, a single value open-loop power control (OLPC) parameter, P0, is configured for all the user equipments (UEs) in a cell, and often same setting is used for similar cells. Recent studies have demonstrated that optimal P0 depends on many factors, which yields a complex multidimensional optimization problem and there are no efficient approaches known to solve it under practical system-level settings. In this paper, we propose a solution based on reinforcement learning (RL) where each BS autonomously adjusts its P0 setting to maximize its throughput performance. As compared to conventional sub-optimal approach, our solution encompasses a smart clustering of UEs, where each cluster specifies its own P0. The proposed solution is evaluated by extensive system level simulations, where our results demonstrate a potential performance enhancement as compared to the baseline proposals.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19369046"
    },
    {
        "id": 29431,
        "title": "Data-Driven Robust Control Using Reinforcement Learning",
        "authors": "Phuong D. Ngo, Miguel Tejedor, Fred Godtliebsen",
        "published": "2022-2-21",
        "citations": 0,
        "abstract": "This paper proposes a robust control design method using reinforcement learning for controlling partially-unknown dynamical systems under uncertain conditions. The method extends the optimal reinforcement learning algorithm with a new learning technique based on the robust control theory. By learning from the data, the algorithm proposes actions that guarantee the stability of the closed-loop system within the uncertainties estimated also from the data. Control policies are calculated by solving a set of linear matrix inequalities. The controller was evaluated using simulations on a blood glucose model for patients with Type 1 diabetes. Simulation results show that the proposed methodology is capable of safely regulating the blood glucose within a healthy level under the influence of measurement and process noises. The controller has also significantly reduced the post-meal fluctuation of the blood glucose. A comparison between the proposed algorithm and the existing optimal reinforcement learning algorithm shows the improved robustness of the closed-loop system using our method.",
        "link": "http://dx.doi.org/10.3390/app12042262"
    },
    {
        "id": 29432,
        "title": "Admission Control for 5G Network Slicing based on (Deep) Reinforcement Learning",
        "authors": "William Fernando Villota Jácome, Oscar Mauricio Caicedo Rendon, Nelson Luis Saldanha da Fonseca",
        "published": "No Date",
        "citations": 0,
        "abstract": "Network Slicing is a promising technology for\nproviding customized logical and virtualized networks for the\nindustry’s vertical segments.This paper proposes SARA and DSARA for the performance of admission control and resource allocation for network slice requests of eMBB, URLLC, and MIoT type in the 5G core network. SARA introduced a Q-learning based algorithm and DSARA a DQN-based algorithm to select the most profitable requests from a set that arrived in given time windows. These algorithms are model-free, meaning they do not make assumptions about the substrate network as do optimization based approaches.",
        "link": "http://dx.doi.org/10.36227/techrxiv.14498190"
    },
    {
        "id": 29433,
        "title": "Viznav: A Modular Off-Policy Deep Reinforcement Learning Framework for Vision-Based Autonomous Uav Navigation in 3d Dynamic Environments",
        "authors": "Fadi AlMahamid, Katarina Grolinger",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4573189"
    },
    {
        "id": 29434,
        "title": "Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control",
        "authors": "Jiayu Chen, Tian Lan, Vaneet Aggarwal",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160374"
    },
    {
        "id": 29435,
        "title": "Localisation-Safe Reinforcement Learning for Mapless Navigation",
        "authors": "Feiqiang Lin, Ze Ji, Changyun Wei, Raphael Grech",
        "published": "2022-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio55434.2022.10011937"
    },
    {
        "id": 29436,
        "title": "Neural Network Pruning Through Constrained Reinforcement Learning",
        "authors": "Shehryar Malik, Muhammad Umair Haider, Omer Iqbal, Murtaza Taj",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956050"
    },
    {
        "id": 29437,
        "title": "Distributed Online Service Coordination Using Deep Reinforcement Learning",
        "authors": "Stefan Schneider, Haydar Qarawlus, Holger Karl",
        "published": "2021-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdcs51616.2021.00058"
    },
    {
        "id": 29438,
        "title": "Replication Package for Article",
        "authors": "He Zhu, Zikang Xiong, Stephen Magill, Suresh Jagannathan",
        "published": "2019-4-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3325983"
    },
    {
        "id": 29439,
        "title": "Research on Machine Translation (MT) System Based on Deep Reinforcement Learning",
        "authors": "Junchen He",
        "published": "2022-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccsmt58129.2022.00108"
    },
    {
        "id": 29440,
        "title": "3D Network-On-Chip Data Acquisition System Mapping Based on Reinforcement Learning and Improved Attention Mechanism",
        "authors": "Chuanpei Xu, Xiuli Shi, Yang Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe 3D Network-on-Chip (NoC) data acquisition system utilizes NoC technology to establish a time-interleaved data acquisition system The mapping scheme determines the location of each Intellectual Property (IP) node in the NoC topology. The optimization of the mapping algorithm is one of the important means to reduce the communication delay of the acquisition system. The abundance of functional IP nodes in the 3D NoC data acquisition system creates a mapping challenge. To address this, we propose a mapping algorithm called Reinforcement Learning and an improved Attention Mechanism Mapping algorithm (RA-Map). The RA-Map mapping algorithm employs node function encoding and node position encoding to express the properties of an IP node in the task graph preprocessing. The local attention mechanism is used in the mapping network encoder, and the fusion of dynamic key node information is proposed in the decoder. The mapping result evaluation network achieves unsupervised training of the mapping network. These targeted improvements ultimately lead to an enhancement in mapping quality. Experimental results demonstrate that when compared to the discrete particle swarm algorithm and simulated annealing algorithm, the RA-Map mapping algorithm reduces the average communication cost by 6.5% and 8.5%, respectively. Furthermore, while ensuring mapping quality, it also shortens the mapping time.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3142584/v1"
    },
    {
        "id": 29441,
        "title": "Optimal Operation of Integrated Energy System Based on Deep Reinforcement Learning",
        "authors": "Fangchi Zhang, Fan Zhou, Jun Zhao, Wei Wang",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9326839"
    },
    {
        "id": 29442,
        "title": "Tracking Locomotion using Reinforcement Learning",
        "authors": "Rutuja Jadhav",
        "published": "2022-7-31",
        "citations": 0,
        "abstract": "Abstract: This article presents the concept of reinforcement learning, which prepares a static direct approach for consistent control problems, and adjusts cutting-edge techniques for testing effectiveness in benchmark Mujoco locomotion tasks. This model was designed and developed to use the Mujoco Engine to track the movement of robotic structures and eliminate problems with assessment calculations using perceptron’s and random search algorithms. Here, the machine learning model is trained to make a series of decisions. The humanoid model is considered to be one of the most difficult and ongoing problems to solve by applying state-of-the-art RL technology. The field of machine learning has a great influence on the training model of the RL environment. Here we use random seed values to provide continuous input to achieve optimized results. The goal of this project is to use the Mujoco engine in a specific context to automatically determine the ideal behavior of the robot in an augmented reality environment. Enhanced random search was introduced to train linear guidelines for achieving the efficiency of Mujoco roaming tasks. The results of these models highlight the variability of the Mujoco benchmark task and lead to efficiently optimized rewards",
        "link": "http://dx.doi.org/10.22214/ijraset.2022.45509"
    },
    {
        "id": 29443,
        "title": "Deep reinforcement learning framework for controlling infectious disease outbreaks in the context of multi-jurisdictions",
        "authors": "Seyedeh Nazanin Khatami, Chaitra Gopalappa",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractIn the absence of pharmaceutical interventions, social distancing and lockdown have been key options for controlling new or reemerging respiratory infectious disease outbreaks. The timely implementation of these interventions is vital for effectively controlling and safeguarding the economy.Motivated by the COVID-19 pandemic, we evaluated whether, when, and to what level lockdowns are necessary to minimize epidemic and economic burdens of new disease outbreaks. We formulated the question as a sequential decision-making Markov Decision Process and solved it using deep Q-network algorithm. We evaluated the question under two objective functions: a 2-objective function to minimize economic burden and hospital capacity violations, suitable for diseases with severe health risks but with minimal death, and a 3-objective function that additionally minimizes the number of deaths, suitable for diseases that have high risk of mortality. A key feature of the model is that we evaluated the above questions in the context of two-geographical jurisdictions that interact through travel but make autonomous and independent decisions, evaluating under cross-jurisdictional cooperation and non-cooperation.In the 2-objective function under cross-jurisdictional cooperation, the optimal policy was to aim for shutdowns at 50% and 25% per day. Though this policy avoided hospital capacity violations, the shutdowns extended until a large proportion of the population reached herd immunity. Delays in initiating this optimal policy or non-cooperation from an outside jurisdiction required shutdowns at a higher level of 75% per day, thus adding to economic burdens. In the 3-objective function, the optimal policy under cross-jurisdictional cooperation was to aim for shutdowns of up to 75% per day to prevent deaths by reducing infected cases. This optimal policy continued for the entire duration of the simulation, suggesting that, until pharmaceutical interventions such as treatment or vaccines become available, contact reductions through physical distancing would be necessary to minimize deaths. Deviating from this policy increased the number of shutdowns and led to several deaths.In summary, we present a decision-analytic methodology for identifying optimal lockdown strategy under the context of interactions between jurisdictions that make autonomous and independent decisions. The numerical analysis outcomes are intuitive and, as expected, serve as proof of the feasibility of such a model.",
        "link": "http://dx.doi.org/10.1101/2022.10.18.22281063"
    },
    {
        "id": 29444,
        "title": "The Harm and Countermeasures of Automobile Exhaust Pollution Based on Deep Reinforcement Learning",
        "authors": "",
        "published": "2021-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.38007/ajeb.2021.020405"
    },
    {
        "id": 29445,
        "title": "Digital Campus Financial Data Sharing Based on Distributed Reinforcement Learning Algorithm",
        "authors": "Yuejun Hu",
        "published": "2022-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiars57204.2022.00019"
    },
    {
        "id": 29446,
        "title": "Choosing the Right Method",
        "authors": "Ken Ramirez",
        "published": "2020-1-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781118968543.ch4"
    },
    {
        "id": 29447,
        "title": "Multi-rotor UAV reinforcement learning control modeling and algorithm design",
        "authors": "X. Xu, Z. Liao",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.2541"
    },
    {
        "id": 29448,
        "title": "A Comprehensive Survey on Security Attacks to Edge Server of IoT Devices through Reinforcement Learning",
        "authors": "Anit Kumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4253769"
    },
    {
        "id": 29449,
        "title": "Multi-step reinforcement learning for medical image super-resolution",
        "authors": "Alix Bouffard, Mihaela Pop, Mehran Ebrahimi",
        "published": "2023-4-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2653655"
    },
    {
        "id": 29450,
        "title": "Phrase-Level Action Reinforcement Learning for Neural Dialog Response Generation",
        "authors": "Takato Yamazaki, Akiko Aizawa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-acl.446"
    },
    {
        "id": 29451,
        "title": "Deep reinforcement learning and automatic target recognition: synergies and opportunities",
        "authors": "Miguel Morales",
        "published": "2021-4-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2595804"
    },
    {
        "id": 29452,
        "title": "Galvanometer Motor Control Based on Reinforcement Learning",
        "authors": "Kainan Liu, Xiaoshi Cai, Xiaojun Ban, Jian Zhang",
        "published": "2022-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoias56028.2022.9931291"
    },
    {
        "id": 29453,
        "title": "Deep Reinforcement One-Shot Learning for Change Point Detection",
        "authors": "Anton Puzanov, Kobi Cohen",
        "published": "2018-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/allerton.2018.8635928"
    },
    {
        "id": 29454,
        "title": "Security Service-aware Reinforcement Learning for Efficient Network Service Provisioning",
        "authors": "Hyeonjun Jo, Kyungbaek Kim",
        "published": "2022-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/apnoms56106.2022.9919928"
    },
    {
        "id": 29455,
        "title": "Reinforcement Learning Versus Model Predictive Control on Greenhouse Climate Control",
        "authors": "Bernardo Morcego, Wenjie Yin, Sjoerd Boersma, Eldert  van Henten, Vicenç Puig, congcong sun",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4525429"
    },
    {
        "id": 29456,
        "title": "Reinforcement Learning Scheduler for Vehicle-to-Vehicle Communications Outside Coverage",
        "authors": "Taylan Sahin, Ramin Khalili, Mate Boban, Adam Wolisz",
        "published": "2018-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vnc.2018.8628366"
    },
    {
        "id": 29457,
        "title": "Cooperative reinforcement learning based throughput optimization in energy harvesting wireless sensor networks",
        "authors": "Yin Wu, Kun Yang",
        "published": "2018-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wocc.2018.8372691"
    },
    {
        "id": 29458,
        "title": "An Online Crowd Semantic Segmentation Method Based on Reinforcement Learning",
        "authors": "Yu Cheng, Hua Yang, Lin Chen",
        "published": "2019-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2019.8803324"
    },
    {
        "id": 29459,
        "title": "Action Spaces in Deep Reinforcement Learning to Mimic Human Input Devices",
        "authors": "Marco Pleines, Frank Zimmer, Vincent-Pierre Berges",
        "published": "2019-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8848080"
    },
    {
        "id": 29460,
        "title": "Reinforcement-learning-based miniature UAV identification",
        "authors": "She Xiaoyu, Guan Zhenyu, Mao Ruizhi, Li Jie, Yang Chengwei",
        "published": "2017-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus.2017.8278347"
    },
    {
        "id": 29461,
        "title": "Adaptive exploration strategies for reinforcement learning",
        "authors": "Kao-Shing Hwang, Chih-Wen Li, Wei-Cheng Jiang",
        "published": "2017-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsse.2017.8030828"
    },
    {
        "id": 29462,
        "title": "Velocity Regulation for Automatic Train Operation via Meta-Reinforcement Learning",
        "authors": "Feiran Zhao, Keyou You, Yunxin Fan, Gang Yan",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9188581"
    },
    {
        "id": 29463,
        "title": "A Hybrid MPC for Constrained Deep Reinforcement Learning applied for Planar Robotic Arm",
        "authors": "Mostafa Al-Gabalawy",
        "published": "2021-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.isatra.2021.03.046"
    },
    {
        "id": 29464,
        "title": "Reinforcement learning for dynamic optimization problems",
        "authors": "Abdennour Boulesnane, Souham Meshoul",
        "published": "2021-7-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3449726.3459543"
    },
    {
        "id": 29465,
        "title": "Experiment of reinforcement learning with extremum seeking",
        "authors": "Megumi Miyashita, Ryo Hirotani, Shiro Yano, Toshiyuki Kondo",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ict-ispc.2017.8075301"
    },
    {
        "id": 29466,
        "title": "Design of Transfer Reinforcement Learning Mechanisms for Autonomous Collision Avoidance",
        "authors": "Xiongqing Liu, Yan Jin",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-05363-5_17"
    },
    {
        "id": 29467,
        "title": "Deep Reinforcement Learning Based Coalition Formation for Energy Trading in Smart Grid",
        "authors": "Mohammad Sadeghi, Melike Erol-Kantarci",
        "published": "2021-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/5gwf52925.2021.00042"
    },
    {
        "id": 29468,
        "title": "Reinforcement-Learning Based Threshold Policies for Continuous Intraday Electricity Market Trading",
        "authors": "Gilles Bertrand, Anthony Papavasiliou",
        "published": "2019-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm40551.2019.8973602"
    },
    {
        "id": 29469,
        "title": "CVA Hedging by Risk-Averse Stochastic-Horizon Reinforcement Learning",
        "authors": "Roberto Daluiso, Marco Pinciroli, Michele Trapletti, Edoardo Vittori",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4673150"
    },
    {
        "id": 29470,
        "title": "A Precision Advertising Strategy Based on Deep Reinforcement Learning",
        "authors": "Haiqing Liang",
        "published": "2020-6-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18280/isi.250316"
    },
    {
        "id": 29471,
        "title": "Reliable Off-Policy Evaluation for Reinforcement Learning",
        "authors": "Jie Wang, Rui Gao, Hongyuan Zha",
        "published": "2022-10-11",
        "citations": 0,
        "abstract": " Off-policy evaluation is an important topic in reinforcement learning, which estimates the expected cumulative reward of a target policy using logged trajectory data generated from a different behavior policy, without execution of the target policy. It is imperative to quantify the uncertainty of the off-policy estimate before deployment of the target policy. Here we leverage methodologies from (Wasserstein) distributionally robust optimization to provide robust and optimistic cumulative reward estimates. With proper selection of the size of the distributional uncertainty set, these estimates serve as confidence bounds with nonasymptotic and asymptotic guarantees under stochastic or adversarial environments. We also generalize those results to batch reinforcement learning. ",
        "link": "http://dx.doi.org/10.1287/opre.2022.2382"
    },
    {
        "id": 29472,
        "title": "Reinforcement Learning-Driven Continuous and Crashless Load Test Architecture",
        "authors": "Tolga Buyuktanir, Batiray Erbay, Mert Altun",
        "published": "2022-5-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu55565.2022.9864834"
    },
    {
        "id": 29473,
        "title": "Curriculum Offline Reinforcement Learning with Progressive Action Space in Intelligent Healthcare Decision-Making",
        "authors": "Chao Yu, Qikai Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4167820"
    },
    {
        "id": 29474,
        "title": "Dynamic Pricing Based on Demand Response Using Actor–Critic Agent Reinforcement Learning",
        "authors": "Ahmed Ismail, Mustafa Baysal",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "Eco-friendly technologies for sustainable energy development require the efficient utilization of energy resources. Real-time pricing (RTP), also known as dynamic pricing, offers advantages over other pricing systems by enabling demand response (DR) actions. However, existing methods for determining and controlling DR have limitations in managing an increasing demand and predicting future pricing. This paper presents a novel approach to address the limitations of existing methods for determining and controlling demand response (DR) in the context of dynamic pricing systems for sustainable energy development. By leveraging actor–critic agent reinforcement learning (RL) techniques, a dynamic pricing DR model is proposed for efficient energy management. The model’s learning framework was trained using DR and real-time pricing data extracted from the Australian Energy Market Operator (AEMO) spanning a period of 17 years. The efficacy of the RL-based dynamic pricing approach was evaluated through two predicting cases: actual-predicted demand and actual-predicted price. Initially, long short-term memory (LSTM) models were employed to predict price and demand, and the results were subsequently enhanced using the deep RL model. Remarkably, the proposed approach achieved an impressive accuracy of 99% for every 30 min future price prediction. The results demonstrated the efficiency of the proposed RL-based model in accurately predicting both demand and price for effective energy management.",
        "link": "http://dx.doi.org/10.3390/en16145469"
    },
    {
        "id": 29475,
        "title": "Brief Survey of Model-Based Reinforcement Learning Techniques",
        "authors": "Constantin-Valentin Pal, Florin Leon",
        "published": "2020-10-8",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icstcc50638.2020.9259716"
    },
    {
        "id": 29476,
        "title": "Deep reinforcement learning identifies personalized intermittent androgen deprivation therapy for prostate cancer",
        "authors": "Yitao Lu, Qian Chu, Zhen Li, Mengdi Wang, Qingpeng Zhang",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nThe evolution of drug resistance leads to treatment failure and tumor progression. Intermittent androgen deprivation therapy (IADT) helps responsive cancer cells compete with resistant cancer cells in intratumoral competition. However, conventional IADT is population-based and ignores the heterogeneous phenotypes of individual patients. To address this challenge, we developed a time-varied, mixed-effect, and generative Lotka-Volterra (tM-GLV) model to account for the heterogeneity of the evolution mechanism and the pharmacokinetics of individual patients. Then, we proposed a reinforcement learningenabled individualized IADT framework, namely, I2ADT, to learn the patient-specific tumor dynamics and derive the optimal drug administration policy. Experiments with clinical trial data demonstrated that the proposed I2ADT can significantly prolong the time to progression of prostate cancer patients with reduced cumulative drug dosage. This research elucidates the application of reinforcement learning techniques to identify personalized adaptive cancer therapy.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1573462/v1"
    },
    {
        "id": 29477,
        "title": "Tracking control for networked control systems with DoS attacks via reinforcement learning method",
        "authors": "Jinliang Liu, Yanhui Dong, Lijuan Zha, Xiangpeng Xie, Engang Tian",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper is concerned with the tracking control problem for a class of\nnetworked systems subject to denial-of-service (DoS) attacks using\nreinforcement learning methods. Taking the effects of DoS attacks into\nconsideration, a novel value function is proposed, which considers the\ncost of the control input, external disturbance and tracking error.\nThen, using the structure of the value function, the tracking Bellman\nequation and Hamilton function are defined. By employing the Bellman\noptimality theory, the optimal control strategy and the game algebraic\nRiccati equation (GARE) are solved with the Hamilton function. Next, the\ndesired tracking performance is guaranteed as the solution of the GARE\nis found. Furthermore, an attacks-based Q-learning algorithm is\nprojected to find the solution to the optimal tracking problem without\nthe system dynamics and the convergence of the Q-learning algorithm is\ngiven. Finally, the F-404 aircraft engine system is given to verify the\neffectiveness of the proposed control strategy.",
        "link": "http://dx.doi.org/10.22541/au.168172563.35721111/v1"
    },
    {
        "id": 29478,
        "title": "Reinforcement learning-based tracking control for AUVs subject to disturbances",
        "authors": "Guangcang Wang, Dianfeng Zhang, Zhaojing Wu",
        "published": "2022-8-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc55256.2022.10033544"
    },
    {
        "id": 29479,
        "title": "Research on Intelligent Optimization Algorithm of Arranging Based on Reinforcement Learning",
        "authors": "Fang He",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/netcit57419.2022.00087"
    },
    {
        "id": 29480,
        "title": "Production flow control through the use of reinforcement learning",
        "authors": "Tomé Silva, Américo Azevedo",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.promfg.2020.01.026"
    },
    {
        "id": 29481,
        "title": "Navigation of Ground Robots with Reinforcement Learning",
        "authors": "Sharanya S, Divya Radhakrishna Varma, Ajay Paul",
        "published": "2023-7-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acirs58671.2023.10240285"
    },
    {
        "id": 29482,
        "title": "Fault Localization for Reinforcement Learning",
        "authors": "Jesús Morán, Antonia Bertolino, Claudio De La Riva, Javier Tuya",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aitest58265.2023.00016"
    },
    {
        "id": 29483,
        "title": "Push Recovery Control for Humanoid Robot Using Reinforcement Learning",
        "authors": "Harin Kim, Donghyeon Seo, Donghan Kim",
        "published": "2019-2",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/irc.2019.00102"
    },
    {
        "id": 29484,
        "title": "Deep Reinforcement Learning Based Virtual Network Embedding for 6G Satellite Networks",
        "authors": "Ruijie Zhu, Gong Li, Peisen Wang, Junling Yuan",
        "published": "2021",
        "citations": 1,
        "abstract": "We establish a satellite network and propose a deep reinforcement learning based virtual network embedding (DRVE-SN) algorithm. Simulation results show that it performs better than the state-of-art algorithm in blocking probability and average resource utilization.",
        "link": "http://dx.doi.org/10.1364/oecc.2021.js2a.16"
    },
    {
        "id": 29485,
        "title": "On Simple Reactive Neural Networks for Behaviour-Based Reinforcement Learning",
        "authors": "Ameya Pore, Gerardo Aragon-Camarasa",
        "published": "2020-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra40945.2020.9197262"
    },
    {
        "id": 29486,
        "title": "Intra-RAN Online Distributed Reinforcement Learning For Uplink Power Control in 5G Cellular Networks",
        "authors": "Majid Butt, Jian Song, Jens Steiner, Klaus Pedersen, Istvan Kovacs",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Uplink power control plays a significant role in maintaining a good signal quality at the serving cell while minimizing interference to neighboring cells, thus maximizing the system performance. Traditionally, a single value open-loop power control (OLPC) parameter, P0, is configured for all the user equipments (UEs) in a cell, and often same setting is used for similar cells. Recent studies have demonstrated that optimal P0 depends on many factors, which yields a complex multidimensional optimization problem and there are no efficient approaches known to solve it under practical system-level settings. In this paper, we propose a solution based on reinforcement learning (RL) where each BS autonomously adjusts its P0 setting to maximize its throughput performance. As compared to conventional sub-optimal approach, our solution encompasses a smart clustering of UEs, where each cluster specifies its own P0. The proposed solution is evaluated by extensive system level simulations, where our results demonstrate a potential performance enhancement as compared to the baseline proposals.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19369046.v1"
    },
    {
        "id": 29487,
        "title": "Vision-based deep reinforcement learning to control a manipulator",
        "authors": "Wonchul Kim, Taewan Kim, Jonggu Lee, H. Jin Kim",
        "published": "2017-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ascc.2017.8287315"
    },
    {
        "id": 29488,
        "title": "Singular violations of transitivity disrupt inferred relational knowledge in humans and reinforcement learning models",
        "authors": "Thomas Graham, Bernhard Spitzer",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1279-0"
    },
    {
        "id": 29489,
        "title": "Morphing Aircraft Adaptive Attitude Control Based on Deep Reinforcement Learning",
        "authors": "Shaojie Ma, Junpeng Hui, Yuhang Wang, Xuan Zhang",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10241099"
    },
    {
        "id": 29490,
        "title": "Deep Reinforcement Learning for Wireless Resource Allocation Using Buffer State Information",
        "authors": "Eike-Manuel Bansbach, Victor Eliachevitch, Laurent Schmalen",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685702"
    },
    {
        "id": 29491,
        "title": "Real-Time Parameter Identification for Forging Machine Using Reinforcement Learning",
        "authors": "Dapeng Zhang, Lifeng Du, Zhiwei Gao",
        "published": "2021-10-18",
        "citations": 3,
        "abstract": "It is a challenge to identify the parameters of a mechanism model under real-time operating conditions disrupted by uncertain disturbances due to the deviation between the design requirement and the operational environment. In this paper, a novel approach based on reinforcement learning is proposed for forging machines to achieve the optimal model parameters by applying the raw data directly instead of observation window. This approach is an online parameter identification algorithm in one period without the need of the labelled samples as training database. It has an excellent ability against unknown distributed disturbances in a dynamic process, especially capable of adapting to a new process without historical data. The effectiveness of the algorithm is demonstrated and validated by a simulation of acquiring the parameter values of a forging machine.",
        "link": "http://dx.doi.org/10.3390/pr9101848"
    },
    {
        "id": 29492,
        "title": "Integral Reinforcement Learning-Based Angular Acceleration Autopilot for High Dynamic Flight Vehicles",
        "authors": "Yingxin Liu, Yuhui Hu, Kai Shen, Jiatai Qiu, Konstantin A. Neusypin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4646396"
    },
    {
        "id": 29493,
        "title": "Massive MIMO Power Allocation Using Deep Reinforcement Learning",
        "authors": "Huynh Vu Hoang Phuc, Ha Hoang Kha",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isee59483.2023.10299873"
    },
    {
        "id": 29494,
        "title": "Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective",
        "authors": "Nelson Vadori",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3604237.3626837"
    },
    {
        "id": 29495,
        "title": "Notice of Removal: Organizational Resource Scheduling using Deep Reinforcement Learning",
        "authors": "Lihi Idan",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dsaa60987.2023.10302505"
    },
    {
        "id": 29496,
        "title": "Market Making with Deep Reinforcement Learning from Limit Order Books",
        "authors": "Hong Guo, Jianwu Lin, Fanlin Huang",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191123"
    },
    {
        "id": 29497,
        "title": "Multi-day Residential EV Charging Strategy Using Reinforcement Learning",
        "authors": "Dominic Goh, Peter Sokolowski, Mahdi Jalili",
        "published": "2021-6-20",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isie45552.2021.9576168"
    },
    {
        "id": 29498,
        "title": "Leveraging Lstm and Reinforcement Learning to Enhance Adaptive Sensing in Multi-Sensing Nodes",
        "authors": "Sushmita Ghosh, Siamak Layeghy, Swades De, Shouri Chatterjee, Marius Portmann",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4666282"
    },
    {
        "id": 29499,
        "title": "A Reinforcement Learning based End-to-End Algorithm for Confrontation Problem",
        "authors": "Siqiang WANG, Haodi YAO, Yu YAO, Fenghua HE",
        "published": "2019-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8866337"
    },
    {
        "id": 29500,
        "title": "Controlling the Risk of Conversational Search via Reinforcement Learning",
        "authors": "Zhenduo Wang, Qingyao Ai",
        "published": "2021-4-19",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3442381.3449893"
    },
    {
        "id": 29501,
        "title": "Xairec: Explainable Ai-Driven Recommender Systems with Knowledge Graphs and Reinforcement Learning",
        "authors": "Neeraj Tiwary, Shahrul Azman Mohd Noah, Fariza Fauzi, Tan  Siok Yee",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4758301"
    },
    {
        "id": 29502,
        "title": "Blockchain-Based Zero Trust Supply Chain Security Integrated with Deep Reinforcement Learning",
        "authors": "Shereen Ismail, Hajar Moudoud, Diana Dawoud, Hassan Reza",
        "published": "No Date",
        "citations": 0,
        "abstract": "The modern supply chain (SC) is growing in terms of data, devices, users, and stakeholders, which introduced new security challenges and threats, especially with the reliance on centralized servers or cloud platforms. In addition, increased trust among system participants exposes the SC to a higher risk of vulnerabilities which require strong security measures. This article proposes a hybrid security framework for SC systems, BC-DRLzSC, that integrates Blockchain (BC) and Deep Reinforcement Learning (DRL) designed to operate in a zero trust (ZT) environment. In particular, we propose a decentralized BC-based approach integrated with smart contracts to manage system participant registration and authentication and to control access to system resources. BC-DRLzSC adopts a ZT architecture to reinforce SC security, which can be achieved with an advocate to verify each entity&rsquo;s trustworthiness before granting or retaining access to system resources. Incorporating the ZT architecture, with BC and DRL, can potentially and significantly bolster SC system security. DRL is employed to develop a proactive attack detection model that continuously monitors the incoming traffic from authenticated nodes within the network and predicts any malicious actions. Finally, we evaluate the performance of our proposed DRL solution using the NSL-KDD dataset.",
        "link": "http://dx.doi.org/10.20944/preprints202403.0714.v1"
    },
    {
        "id": 29503,
        "title": "A Potential Role for Reinforcement Learning in Speech Production",
        "authors": "Benjamin Parrell",
        "published": "2021-5-31",
        "citations": 4,
        "abstract": "Abstract\nReinforcement learning, the ability to change motor behavior based on external reward, has been suggested to play a critical role in early stages of speech motor development and is widely used in clinical rehabilitation for speech motor disorders. However, no current evidence exists that demonstrates the capability of reinforcement to drive changes in human speech behavior. Speech provides a unique test of the universality of reinforcement learning across motor domains: Speech is a complex, high-dimensional motor task whose goals do not specify a task to be performed in the environment but ultimately must be self-generated by each speaker such that they are understood by those around them. Across four experiments, we examine whether reinforcement learning alone is sufficient to drive changes in speech behavior and parametrically test two features known to affect reinforcement learning in reaching: how informative the reinforcement signal is as well as the availability of sensory feedback about the outcomes of one's motor behavior. We show that learning does occur and is more likely when participants receive auditory feedback that gives an implicit target for production, although they do not explicitly imitate that target. Contrary to results from upper limb control, masking feedback about movement outcomes has no effect on speech learning. Together, our results suggest a potential role for reinforcement learning in speech but that it likely operates differently than in other motor domains.",
        "link": "http://dx.doi.org/10.1162/jocn_a_01742"
    },
    {
        "id": 29504,
        "title": "Federated Reinforcement Learning for Fast Personalization",
        "authors": "Chetan Nadiger, Anil Kumar, Sherine Abdelhak",
        "published": "2019-6",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aike.2019.00031"
    },
    {
        "id": 29505,
        "title": "Explainability in deep reinforcement learning",
        "authors": "Alexandre Heuillet, Fabien Couthouis, Natalia Díaz-Rodríguez",
        "published": "2021-2",
        "citations": 130,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2020.106685"
    },
    {
        "id": 29506,
        "title": "Decentralized Reinforcement Learning Based MAC Optimization",
        "authors": "Eleni Nisioti, Nikolaos Thomos",
        "published": "2018-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pimrc.2018.8580848"
    },
    {
        "id": 29507,
        "title": "SparRL: Graph Sparsification via Deep Reinforcement Learning",
        "authors": "Ryan Wickman",
        "published": "2022-6-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3514221.3520254"
    },
    {
        "id": 29508,
        "title": "Overcoming Obstacles With a Reconfigurable Robot Using Reinforcement Learning",
        "authors": "Liran Yehezkel, Sigal Berman, David Zarrouk",
        "published": "2020",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2020.3040896"
    },
    {
        "id": 29509,
        "title": "Master-Slave Curriculum Design for Reinforcement Learning",
        "authors": "Yuechen Wu, Wei Zhang, Ke Song",
        "published": "2018-7",
        "citations": 4,
        "abstract": "Curriculum learning is often introduced as a leverage to improve the agent training for complex tasks, where the goal is to generate a sequence of easier subasks for an agent to train on, such that final performance or learning speed is improved. However, conventional curriculum is mainly designed for one agent with fixed action space and sequential simple-to-hard training manner. Instead, we present a novel curriculum learning strategy by introducing the concept of master-slave agents and enabling flexible action setting for agent training. Multiple agents, referred as master agent for the target task and slave agents for the subtasks, are trained concurrently within different action spaces by sharing a perception network with an asynchronous strategy. Extensive evaluation on the VizDoom platform demonstrates the joint learning of master agent and slave agents mutually benefit each other. Significant improvement is obtained over A3C in terms of learning speed and performance.",
        "link": "http://dx.doi.org/10.24963/ijcai.2018/211"
    },
    {
        "id": 29510,
        "title": "A Constrained Reinforcement Learning Based Approach for Network Slicing",
        "authors": "Yongshuai Liu, Jiaxin Ding, Xin Liu",
        "published": "2020-10-13",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnp49622.2020.9259378"
    },
    {
        "id": 29511,
        "title": "Reinforcement Learning-Enabled Seamless Microgrids Interconnection",
        "authors": "Yan Li, Zihao Xu, Kenneth B Bowes, Lingyu Ren",
        "published": "2021-7-26",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm46819.2021.9637836"
    },
    {
        "id": 29512,
        "title": "Power Flow Management in Electric Vehicles Charging Station Using Reinforcement Learning",
        "authors": "Arwa O. Erick, Komla A. Folly",
        "published": "2020-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec48606.2020.9185652"
    },
    {
        "id": 29513,
        "title": "Multi-agent Cooperative Search based on Reinforcement Learning",
        "authors": "Yinjiang Sun, Rui Zhang, Wenbao Liang, Cheng Xu",
        "published": "2020-11-27",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus50048.2020.9275003"
    },
    {
        "id": 29514,
        "title": "Deep reinforcement learning approaches for process control",
        "authors": "S.P.K. Spielberg, R.B. Gopaluni, P.D. Loewen",
        "published": "2017-5",
        "citations": 71,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/adconip.2017.7983780"
    },
    {
        "id": 29515,
        "title": "Multi-Agent Energy Management Strategy for Multi-Microgrids Using Reinforcement Learning",
        "authors": "Mohammad Safayet Hossain, Chinwendu Enyioha",
        "published": "2023-2-13",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpec56611.2023.10078538"
    },
    {
        "id": 29516,
        "title": "Autonomous Learning and Navigation of Mobile Robots Based on Deep Reinforcement Learning",
        "authors": "Zhiqiang Lai, Zhiwei Jia, Man Chen",
        "published": "2022-1-1",
        "citations": 1,
        "abstract": "Abstract\nAiming at the problems of convergence difficulties faced by deep reinforcement learning algorithms in dynamic pedestrian environments, and insufficient reward and feedback mechanisms, a data-driven and model-driven navigation algorithm which named GRRL has been proposed. In order to enrich and perfect the reward feedback mechanism, we designed a dynamic reward function. The reward function fully considers the relationship between the robot and the pedestrian and the target position. It mainly includes three parts. The experimental results show that the autonomous learning efficiency and the average navigation success rate of the mobile robot driven by the GRRL algorithm are improved, the average navigation time is shorter. The dynamic reward function we designed has a certain improvement effect on robot navigation.",
        "link": "http://dx.doi.org/10.1088/1742-6596/2171/1/012024"
    },
    {
        "id": 29517,
        "title": "Scoped literature review of artificial intelligence marketing adoptions for ad optimization with reinforcement learning",
        "authors": "Johannes Sahlin, Håkan Sundell, Gideon Mbiydzenyuy, Jesper Holgersson",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811269264_0049"
    },
    {
        "id": 29518,
        "title": "Reinforcement Learning–Based Approach Towards Switch Migration for Load-Balancing in SDN",
        "authors": "Abha Kumari, Shubham Gupta, Joydeep Chandra, Ashok Singh Sairam",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003212249-3"
    },
    {
        "id": 29519,
        "title": "An Empirically Grounding Analytics (EGA) Approach to Hog Farm Finishing Stage Management: Deep Reinforcement Learning as Decision Support and Managerial Learning Tool",
        "authors": "Panos Kouvelis, Ye Liu, Danko Turcic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4617964"
    },
    {
        "id": 29520,
        "title": "A Hybrid Multi-Task Learning Approach for Optimizing Deep Reinforcement Learning Agents",
        "authors": "Nelson Vithayathil Varghese, Qusay H. Mahmoud",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3065710"
    },
    {
        "id": 29521,
        "title": "Single stock trading with deep reinforcement learning: A comparative study",
        "authors": "Jun GE, Yuanqi QIN, Yaling Li, yanjia Huang, Hao Hu",
        "published": "2022-2-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3529836.3529857"
    },
    {
        "id": 29522,
        "title": "Temporal Logic Guided Safe Model-Based Reinforcement Learning",
        "authors": "Max Cohen, Calin Belta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-29310-8_9"
    },
    {
        "id": 29523,
        "title": "Learning behavior of hopping with robotic leg on particular height using model free reinforcement learning",
        "authors": "Shiva Pandey, Avinash Bhashkar, Anuj Kumar Sharma",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/5.0189079"
    },
    {
        "id": 29524,
        "title": "Reinforcement Learning for Energy-Efficient Cloud Offloading of Mobile Embedded Applications",
        "authors": "Aditya Khune, Sudeep Pasricha",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-40677-5_7"
    },
    {
        "id": 29525,
        "title": "Continuous Control for Autonomous Underwater Vehicle Path Following Using Deep Interactive Reinforcement Learning",
        "authors": "Qilei Zhang, Chunxi Cheng, Zheng Fang, Dong Jiang, Bo He, Guangliang Li",
        "published": "2022-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlcr57210.2022.00013"
    },
    {
        "id": 29526,
        "title": "Domains as Objectives: Multi-Domain Reinforcement Learning with Convex-Coverage Set Learning for Domain Uncertainty Awareness",
        "authors": "Wendyam Eric Lionel Ilboudo, Taisuke Kobayashi, Takamitsu Matsubara",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10342236"
    },
    {
        "id": 29527,
        "title": "Learning Synergies Between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning",
        "authors": "Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser",
        "published": "2018-10",
        "citations": 306,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2018.8593986"
    },
    {
        "id": 29528,
        "title": "Sample efficient reinforcement learning with active learning for molecular design",
        "authors": "Michael Dodds, Jeff Guo, Thomas Löhr, Alessandro Tibo, Ola Engkvist, Jon Paul Janet",
        "published": "2024",
        "citations": 0,
        "abstract": "Active learning accelerates the design of molecules during generative reinforcement learning by creating surrogate models of expensive reward functions, obtaining a 4- to 64-fold reduction in computational effort per hit.",
        "link": "http://dx.doi.org/10.1039/d3sc04653b"
    },
    {
        "id": 29529,
        "title": "Deep learning and reinforcement learning approach on microgrid",
        "authors": "Kumar Chandrasekaran, Prabaakaran Kandasamy, Srividhya Ramanathan",
        "published": "2020-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531"
    },
    {
        "id": 29530,
        "title": "Research on Constant Perturbation Strategy for Deep Reinforcement Learning",
        "authors": "Jiamin Shen, Li Xu, Xu Wan, Jixuan Chai, Chunlong Fan",
        "published": "2023-3-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3590003.3590101"
    },
    {
        "id": 29531,
        "title": "Perturbational Complexity by Distribution Mismatch: A Systematic Analysis of Reinforcement Learning in Reproducing Kernel Hilbert Space",
        "authors": "Jihao Long null, Jiequn Han",
        "published": "2022-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4208/jml.220114"
    },
    {
        "id": 29532,
        "title": "Human Action Recognition Based on YOLOv7",
        "authors": "Chenwei Liang, Wei Qi Yan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Human action recognition is a fundamental research problem in computer vision. The accuracy of human action recognition has important applications. In this book chapter, the authors use a YOLOv7-based model for human action recognition. To evaluate the performance of the model, the action recognition results of YOLOv7 were compared with those using CNN+LSTM, YOLOv5, and YOLOv4. Furthermore, a small human action dataset suitable for YOLO model training is designed. This data set is composed of images extracted from KTH, Weizmann, MSR data sets. In this book chapter, the authors make use of this data set to verify the experimental results. The final experimental results show that using the YOLOv7 model for human action recognition is very convenient and effective, compared with the previous YOLO model.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch006"
    },
    {
        "id": 29533,
        "title": "Reinforcement Learning Algorithm for Mixed Mean Field Control Games",
        "authors": "Andrea Angiuli, Nils Detering, Jean-Pierre Fouque, Mathieu Laurière null, Jimin Lin",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4208/jml.220915"
    },
    {
        "id": 29534,
        "title": "Literaturverzeichnis",
        "authors": "Alexander Zai, Brandon Brown",
        "published": "2020-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3139/9783446466081.013"
    },
    {
        "id": 29535,
        "title": "Reinforcement Learning Versus Generative Adversarial Networks for Video Summarization",
        "authors": "Mohamed Aboelenien, Mohammed A.-M. Salem",
        "published": "2021-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/miucc52538.2021.9447645"
    },
    {
        "id": 29536,
        "title": "Optimizing Enhanced Cost Per Click via Reinforcement Learning Without Exploration",
        "authors": "Sinan Li, Chun Yuan, Xin Zhu",
        "published": "2021-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9534126"
    },
    {
        "id": 29537,
        "title": "Accelerating Lifelong Reinforcement Learning via Reshaping Rewards",
        "authors": "Kun Chu, Xianchao Zhu, William Zhu",
        "published": "2021-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc52423.2021.9659064"
    },
    {
        "id": 29538,
        "title": "Speeding up Reinforcement Learning for Inference and Control of Gene Regulatory Networks",
        "authors": "Rodrigo Bonini",
        "published": "2019-12-8",
        "citations": 0,
        "abstract": "Motivated by the desire to understand genomic functions through interactions between genes and gene products, the research in the area of gene regulatory networks has become a very important object of study in recent years. Probabilistic Boolean Networks (PBN), which are rules-based dynamic systems, are some of the most studied mathematical models to represent networks and their regulations. However, frequently their representation, regulation, and interactions between genes are overly complex to learn and control, requiring a complex model. Reinforcement Learning (RL) is an interesting technique to deal with this problem because it can learn solutions without the need of a model. This approach is used to train autonomous agents who can find solutions to complex problems, including those of regulation and relationships between genes. But its classical approaches are slow when having to learn tasks with many states especially when these tasks have multiple goals and agents. Besides that, learning bad solutions can make the learning process even more difficult and slow. Therefore, some RL approaches and techniques need to be tested in order to verify the best way to flexibilize, adapt and improve them to intervene and control the gene networks.",
        "link": "http://dx.doi.org/10.52591/lxai2019120821"
    },
    {
        "id": 29539,
        "title": "Reinforcement Learning Based Prioritized Cross-Area Resource Management for Vehicular Cloud Networks",
        "authors": "Zhuyue Yu, Yuliang Tang, Xing Tang",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcwkshps45667.2019.9024604"
    },
    {
        "id": 29540,
        "title": "Coordinated Frequency Control through Safe Reinforcement Learning",
        "authors": "Yi Zhou, Liangcai Zhou, Di Shi, Xiaoying Zhao",
        "published": "2022-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm48719.2022.9916894"
    },
    {
        "id": 29541,
        "title": "Improving HS-DACS Based Streaming Transformer ASR with Deep Reinforcement Learning",
        "authors": "Mohan Li, Rama Doddipatla",
        "published": "2021-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/asru51503.2021.9688268"
    },
    {
        "id": 29542,
        "title": "Network-Aware Online Charge Control with Reinforcement Learning",
        "authors": "Andrey Poddubnyy, Phuong Nguyen, Han Slootweg",
        "published": "2022-9-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sest53650.2022.9898460"
    },
    {
        "id": 29543,
        "title": "Reinforcement-Based Frugal Learning for Interactive Satellite Image Change Detection",
        "authors": "Sebastien Deschamps, Hichem Sahbi",
        "published": "2022-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss46834.2022.9883633"
    },
    {
        "id": 29544,
        "title": "Meta-Reinforcement Learning by Tracking Task Non-stationarity",
        "authors": "Riccardo Poiani, Andrea Tirinzoni, Marcello Restelli",
        "published": "2021-8",
        "citations": 1,
        "abstract": "Many real-world domains are subject to a structured non-stationarity which affects the agent's goals and the environmental dynamics. Meta-reinforcement learning (RL) has been shown successful for training agents that quickly adapt to related tasks. However, most of the existing meta-RL algorithms for non-stationary domains either make strong assumptions on the task generation process or require sampling from it at training time. In this paper, we propose a novel algorithm (TRIO) that optimizes for the future by explicitly tracking the task evolution through time. At training time, TRIO learns a variational module to quickly identify latent parameters from experience samples. This module is learned jointly with an optimal exploration policy that takes task uncertainty into account. At test time, TRIO tracks the evolution of the latent parameters online, hence reducing the uncertainty over future tasks and obtaining fast adaptation through the meta-learned policy. Unlike most existing methods, TRIO does not assume Markovian task-evolution processes, it does not require information about the non-stationarity at training time, and it captures complex changes undergoing in the environment. We evaluate our algorithm on different simulated problems and show it outperforms competitive baselines.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/399"
    },
    {
        "id": 29545,
        "title": "Advances in Preference-based Reinforcement Learning: A Review",
        "authors": "Youssef Abdelkareem, Shady Shehata, Fakhri Karray",
        "published": "2022-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53654.2022.9945333"
    },
    {
        "id": 29546,
        "title": "RL-CoPref: A Reinforcement Learning-based Coordinated Prefetching Controller for Multiple Prefetchers",
        "authors": "Huijing Yang, Juan Fang, Xing Su, Zhi Cai, Yuening Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nModern processors employ data prefetchers to alleviate the impact of long memory access latency. However, current prefetchers are designed for specific memory access patterns, which perform poorly on mixed applications with multiple memory access patterns. To address these issues, RL-CoPref, a reinforcement learning (RL)-based coordinated prefetching controller for multiple prefetchers, is proposed in this paper. RL-CoPref takes diverse program context information as the input, learns to maximize cumulative rewards, and evaluates prefetch quality based on prefetch hits/misses and memory bandwidth utilization. It can dynamically adjust the prefetch activation and prefetch degree, enabling multiple prefetchers to complement each other on mixed applications. Our extensive evaluation, utilizing the ChampSim simulator, demonstrates that RL-CoPref can effectively adapt to various workloads and system configurations, optimizing prefetch control. On average, RL-CoPref achieves 76.15% prefetch coverage, having 35.50% IPC improvement, outperforming state-of-the-art individual prefetchers by 5.91%−16.54% and outperforming SBP, a state-of-the-art (non-RL) prefetch controller, by 4.64%.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3174293/v1"
    },
    {
        "id": 29547,
        "title": "Multi-Agent Reinforcement Learning for Empty Container Repositioning",
        "authors": "Qiang Luo, Xiaojun Huang",
        "published": "2018-11",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsess.2018.8663934"
    },
    {
        "id": 29548,
        "title": "A Novel Reinforcement Learning-based Unsupervised Fault Detection for Industrial Manufacturing Systems",
        "authors": "Antonio Acernese, Amol Yerudkar, Carmen Del Vecchio",
        "published": "2022-6-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867763"
    },
    {
        "id": 29549,
        "title": "Multi-agent Collaborative Fire Rescue Based on Deep Reinforcement Learning",
        "authors": "Yiming Feng",
        "published": "2022-2-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/eebda53927.2022.9744860"
    },
    {
        "id": 29550,
        "title": "Reinforcement Learning Applied to Scrum Team towards Large-Scale Global Optimization",
        "authors": "Supakit Nootyaskool, Pimolrat Ounsrimuang",
        "published": "2018-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsp.2018.8652286"
    },
    {
        "id": 29551,
        "title": "Improve Deep Reinforcement Learning-Based Perimeter Metering Control Methods with Domain Control Knowledge",
        "authors": "Dongqin Zhou, Vikash Gayah",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4025074"
    },
    {
        "id": 29552,
        "title": "A Reinforcement Learning Approach for Energy Efficient Beamforming in NOMA Systems",
        "authors": "Yuqin Liu, Ruikang Zhong, Mona Jaber",
        "published": "2022-12-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10000828"
    },
    {
        "id": 29553,
        "title": "Data-Driven Reinforcement Learning Design for Multi-agent Systems with Unknown Disturbances",
        "authors": "Xiangnan Zhong, Zhen Ni",
        "published": "2018-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489773"
    },
    {
        "id": 29554,
        "title": "A new thermal power generation control in reinforcement learning",
        "authors": "Luobao Zou, Zhiwei Zhuang, Yin Cheng, Yuexin Huang, Weidong Zhang",
        "published": "2018-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac.2018.8623337"
    },
    {
        "id": 29555,
        "title": "Navigation of Sounding Balloons with Deep Reinforcement Learning",
        "authors": "Marco Gannetti, Matteo Gemignani, Salvo Marcuccio",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/metroaerospace57412.2023.10189997"
    },
    {
        "id": 29556,
        "title": "Robust Optimal Control of Continuous Time Linear System using Reinforcement Learning",
        "authors": "Abdul Sami, Attaullah Y. Memon",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/anzcc.2018.8606607"
    },
    {
        "id": 29557,
        "title": "How can we model the brain when it goes awry? How Reinforcement Learning Models can shed light on Psychiatric Disorders that emerge during Development.",
        "authors": "",
        "published": "2021-1-26",
        "citations": 0,
        "abstract": "",
        "link": "http://dx.doi.org/10.13056/acamh.14458"
    },
    {
        "id": 29558,
        "title": "Free Gait Planning for a Hexapod Robot Based on Reinforcement Learning",
        "authors": "Manhong LI",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3901/jme.2019.05.036"
    },
    {
        "id": 29559,
        "title": "The Use of Reinforcement Learning in Gaming The Breakout Game Case Study.pdf",
        "authors": "Ao Chen, Taresh Dewan, Manva Trivedi, Danning Jiang, Aloukik Aditya, Sabah Mohammed",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper provides a comparative analysis between Deep Q Network (DQN) and Double Deep Q Network (DDQN) algorithms  based  on  their hit rate, out of which DDQN proved to be better for Breakout game. DQN is chosen over Basic Q learning because    it understands policy learning using its neural network which is good for complex environment and DDQN is chosen as it solves overestimation problem (agent always choses non-optimal action for any state just because it has maximum Q-value) occurring    in basic Q-learning.",
        "link": "http://dx.doi.org/10.36227/techrxiv.12061728"
    },
    {
        "id": 29560,
        "title": "Revolver: Vertex-Centric Graph Partitioning Using Reinforcement Learning",
        "authors": "Mohammad Hasanzadeh Mofrad, Rami Melhem, Mohammad Hammoud",
        "published": "2018-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cloud.2018.00111"
    },
    {
        "id": 29561,
        "title": "Developing Smart Air Purifier Control Strategies for Better IAQ and Energy Efficiency Using Reinforcement Learning",
        "authors": "Wenzhe Shang, Junjie Liu, Xilei Dai, Congcong Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4349006"
    },
    {
        "id": 29562,
        "title": "Intelligent Maritime Communications Enabled by Deep Reinforcement Learning",
        "authors": "Jiabo Li, Tingting Yang, Hailong Feng",
        "published": "2019-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccchina.2019.8855946"
    },
    {
        "id": 29563,
        "title": "Adaptive Fault-Tolerant Control for Spacecraft: A Dynamic Stackelberg Game Approach with A2c Reinforcement Learning",
        "authors": "Yizhen Meng, Chun Liu, Yangyang Liu, Longyu Tan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4683974"
    },
    {
        "id": 29564,
        "title": "Safety and Liveness Guarantees through Reach-Avoid Reinforcement Learning",
        "authors": "Kai-Chieh Hsu*, Vicenç Rubies-Royo*, Claire Tomlin, Jaime Fisac",
        "published": "2021-7-12",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2021.xvii.077"
    },
    {
        "id": 29565,
        "title": "Acute stress blunts prediction error signals in the dorsal striatum during reinforcement learning",
        "authors": "Joana Carvalheiro, Vasco A. Conceição, Ana Mesquita, Ana Seara-Cardoso",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractReinforcement learning, which implicates learning from the rewarding and punishing outcomes of our choices, is critical for adjusted behaviour. Acute stress seems to affect this ability but the neural mechanisms by which it disrupts this type of learning are still poorly understood. Here, we investigate whether and how acute stress blunts neural signalling of prediction errors during reinforcement learning using model-based functional magnetic resonance imaging. Male participants completed a well-established reinforcement learning task involving monetary gains and losses whilst under stress and control conditions. Acute stress impaired participants’ behavioural performance towards obtaining monetary gains, but not towards avoiding losses. Importantly, acute stress blunted signalling of prediction errors during gain and loss trials in the dorsal striatum — with subsidiary analyses suggesting that acute stress preferentially blunted signalling of positive prediction errors. Our results thus reveal a neurocomputational mechanism by which acute stress may impair reward learning.",
        "link": "http://dx.doi.org/10.1101/2021.02.11.430640"
    },
    {
        "id": 29566,
        "title": "A Deep-Reinforcement-Learning Assisted Microstrip BPF Design Approach for Multiple Specifications",
        "authors": "Masataka Ohira, Yuto Asai, Zhewang Ma",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/apmc57107.2023.10439910"
    },
    {
        "id": 29567,
        "title": "Cooperative Transport by Manipulators with Uncertainty-Aware Model-Based Reinforcement Learning",
        "authors": "Takumi Aotani, Taisuke Kobayashi",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sii58957.2024.10417389"
    },
    {
        "id": 29568,
        "title": "Author response for \"Discovering mechanisms for materials microstructure optimization via reinforcement learning of a generative model\"",
        "authors": " Rama K Vasudevan,  Erick Orozco,  Sergei V. Kalinin",
        "published": "2022-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/2632-2153/aca004/v2/response1"
    },
    {
        "id": 29569,
        "title": "Null Space Based Efficient Reinforcement Learning with Hierarchical Safety Constraints",
        "authors": "Quantao Yang, Johannes A. Stork, Todor Stoyanov",
        "published": "2021-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ecmr50962.2021.9568848"
    },
    {
        "id": 29570,
        "title": "Distributed Conflict Resolution at High Traffic Densities with Reinforcement Learning",
        "authors": "Marta Ribeiro, Joost Ellerbroek, Jacco Hoekstra",
        "published": "2022-8-25",
        "citations": 5,
        "abstract": "Future operations involving drones are expected to result in traffic densities that are orders of magnitude higher than any observed in manned aviation. Current geometric conflict resolution (CR) methods have proven to be very efficient at relatively moderate densities. However, at higher densities, performance is hindered by the unpredictable emergent behaviour from neighbouring aircraft. Reinforcement learning (RL) techniques are often capable of identifying emerging patterns through training in the environment. Although some work has started introducing RL to resolve conflicts and ensure separation between aircraft, it is not clear how to employ these methods with a higher number of aircraft, and whether these can compare to or even surpass the performance of current CR geometric methods. In this work, we employ an RL method for distributed conflict resolution; the method is completely responsible for guaranteeing minimum separation of all aircraft during operation. Two different action formulations are tested: (1) where the RL method controls heading, and speed variation; (2) where the RL method controls heading, speed, and altitude variation. The final safety values are directly compared to a state-of-the-art distributed CR algorithm, the Modified Voltage Potential (MVP) method. Although, overall, the RL method is not as efficient as MVP in reducing the total number of losses of minimum separation, its actions help identify favourable patterns to avoid conflicts. The RL method has a more preventive behaviour, defending in advance against nearby neighbouring aircraft not yet in conflict, and head-on conflicts while intruders are still far away.",
        "link": "http://dx.doi.org/10.3390/aerospace9090472"
    },
    {
        "id": 29571,
        "title": "Safety Enhancement for Deep Reinforcement Learning in Autonomous Separation Assurance",
        "authors": "Wei Guo, Marc Brittain, Peng Wei",
        "published": "2021-9-19",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9564466"
    },
    {
        "id": 29572,
        "title": "Conditioned Reinforcement",
        "authors": "Erin B. Rasmussen, Casey J. Clay, W. David Pierce, Carl D. Cheney",
        "published": "2022-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003202622-10"
    },
    {
        "id": 29573,
        "title": "Hypothesis Theory and Nonlearning Despite Ideal S–R-Reinforcement Contingencies\n                           *",
        "authors": "Marvin Levine",
        "published": "2022-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003316565-32"
    },
    {
        "id": 29574,
        "title": "Optimizing the Area Coverage of Networked UAVs using Multi-Agent Reinforcement Learning",
        "authors": "Tua A. Tamba",
        "published": "2021-8-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ica52848.2021.9625676"
    },
    {
        "id": 29575,
        "title": "Inertial Navigation Compensation with Reinforcement Learning",
        "authors": "Eric Bozeman, Minhdao Nguyen, Mohammad Alam, Jeffrey Onners",
        "published": "2022-5-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/inertial53425.2022.9787527"
    },
    {
        "id": 29576,
        "title": "Determinantal Reinforcement Learning",
        "authors": "Takayuki Osogami, Rudy Raymond",
        "published": "2019-7-17",
        "citations": 2,
        "abstract": "We study reinforcement learning for controlling multiple agents in a collaborative manner. In some of those tasks, it is insufficient for the individual agents to take relevant actions, but those actions should also have diversity. We propose the approach of using the determinant of a positive semidefinite matrix to approximate the action-value function in reinforcement learning, where we learn the matrix in a way that it represents the relevance and diversity of the actions. Experimental results show that the proposed approach allows the agents to learn a nearly optimal policy approximately ten times faster than baseline approaches in benchmark tasks of multi-agent reinforcement learning. The proposed approach is also shown to achieve the performance that cannot be achieved with conventional approaches in partially observable environment with exponentially large action space.",
        "link": "http://dx.doi.org/10.1609/aaai.v33i01.33014659"
    },
    {
        "id": 29577,
        "title": "Counterfactual based reinforcement learning for graph neural networks",
        "authors": "David Pham, Yongfeng Zhang",
        "published": "2022-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10479-022-04978-9"
    },
    {
        "id": 29578,
        "title": "Design of Deep Reinforcement Learning Approach for Traffic Signal Control at Three-way Crossroads",
        "authors": "Thanh Nguyen Canh, Anh Pham Tuan, Xiem HoangVan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nTraffic light control (TSC) is an important and challenging real-world problem with the aim of reducing travel time as well as saving energy. Recent researches have numerous attempts to apply intelligent methods for TSC at four-way crossroads to solve the traffic light scheduling problem. However, there is the limitation of researches on efficient TSC at three-way crossroads. Therefore, this paper introduces a novel TSC solution for three-way crossroad environment (TW-TSC). The proposed TSC method is designed based on a deep reinforcement learning approach, namely Soft Actor-Critic (TWSAC). Firstly, we create a simulation environment for three-way crossroads which consists of numerous transportation and two parallel lanes using Unity framework. Secondly, to achieve practical movements of transportation in three-way crossroads, we carefully design agents which have a high impact to the transportation movement, notably the time to wait for traffic light, the velocity of transportation, and the number of transportation passing successfully. Finally, to achieve TW-TSC efficiency, we propose a novel reward function together with a design of TWSAC algorithm. Experimental results show that the proposed TWSAC in TW-TSC achieves higher performance than both fixed-time TSC methods and relevant RL algorithms.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3128875/v1"
    },
    {
        "id": 29579,
        "title": "Quantum Reinforcement Learning with Quantum World Model",
        "authors": "Peigen Zeng, Ying He, F. Richard Yu, Victor C.M. Leung",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437803"
    },
    {
        "id": 29580,
        "title": "Policy Gradient using Weak Derivatives for Reinforcement Learning",
        "authors": "Sujay Bhatt, Alec Koppel, Vikram Krishnamurthy",
        "published": "2019-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciss.2019.8692920"
    },
    {
        "id": 29581,
        "title": "Efficient Symptom Inquiring and Diagnosis Via Adaptive Alignment of Reinforcement Learning and Classification",
        "authors": "Hongyi Yuan, Sheng Yu",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4268843"
    },
    {
        "id": 29582,
        "title": "Impact of Sensor and Actuator Clock Offsets on Reinforcement Learning",
        "authors": "Filippos Fotiadis, Aris Kanellopoulos, Kyriakos G. Vamvoudakis, Jerome Hugues",
        "published": "2022-6-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867823"
    },
    {
        "id": 29583,
        "title": "Energy-Aware Task Allocation for Mobile IoT by Online Reinforcement Learning",
        "authors": "Jingjing Yao, Nirwan Ansari",
        "published": "2019-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc.2019.8761509"
    },
    {
        "id": 29584,
        "title": "Online Reinforcement Learning in Markov Decision Process Using Linear Programming",
        "authors": "Vincent Leon, S. Rasoul Etesami",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383839"
    },
    {
        "id": 29585,
        "title": "Reinforcement Learning Based Demand Charge Minimization Using Energy Storage",
        "authors": "Lucas Weber, Ana Bušić, Jiamin Zhu",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383414"
    },
    {
        "id": 29586,
        "title": "Reinforcement learning with temporal logic rewards",
        "authors": "Xiao Li, Cristian-Ioan Vasile, Calin Belta",
        "published": "2017-9",
        "citations": 68,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2017.8206234"
    },
    {
        "id": 29587,
        "title": "Enhanced entropy based reinforcement learning hotel recommendation system",
        "authors": "G. Jai Arul Jose, Qasim AlAjmi",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11042-024-18732-9"
    },
    {
        "id": 29588,
        "title": "Viewpoint Selection for DermDrone using Deep Reinforcement Learning",
        "authors": "Mojtaba Ahangar Arzati, Siamak Arzanpour",
        "published": "2021-10-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iccas52745.2021.9649799"
    },
    {
        "id": 29589,
        "title": "A reinforcement learning-based network load balancing mechanism",
        "authors": "Jiawei Wang",
        "published": "2023-3-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2667915"
    },
    {
        "id": 29590,
        "title": "Reinforcement learning-based composite suboptimal control for Markov jump singularly perturbed systems with unknown dynamics",
        "authors": "Jiacheng Wu, Wenqian Li, Yun Wang, Hao Shen",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this article, a model-free parallel reinforcement learning method is\nproposed to solve the suboptimal control problem for the Markov jump\nsingularly perturbed systems. First, since fast and slow dynamics\ncoexist in Markov jump singularly perturbed systems, it may lead to\nill-conditioned numerical problems during the controller design process.\nTherefore, the original system can be decomposed into independent\nsubsystems at different time-scales by employing the reduced order\nmethod. Besides, a model-based parallel algorithm is designed to obtain\nthe optimal controllers of the fast and slow subsystems respectively.\nMoreover, within the framework of reinforcement learning, the composite\ncontroller of the Markov jump singularly perturbed systems can be\nobtained without system dynamics. Finally, a numerical example is\nintroduced to prove the effectiveness of proposed algorithms.",
        "link": "http://dx.doi.org/10.22541/au.169052592.23514237/v1"
    },
    {
        "id": 29591,
        "title": "Autonomous Pulse Control for Quantum Transducers with Deep Reinforcement Learning",
        "authors": "Mekena Metcalf, Huo Chen, Anastasiia Butko, Mariam Kiran",
        "published": "2022",
        "citations": 0,
        "abstract": "Quantum transducers are the back-bone technology and enabler for the Quantum Internet. We created a Deep Reinforcment Learning control framework to overcome current, low conversion efficiencies, bringing quantum transducers towards practical use.",
        "link": "http://dx.doi.org/10.1364/ofc.2022.m3z.16"
    },
    {
        "id": 29592,
        "title": "Robust Grasp Operation in Clutter for Multi-Objective Robotic Tasks Using Deep Reinforcement Learning",
        "authors": "Tengteng Zhang, Hongwei Mo",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10240314"
    },
    {
        "id": 29593,
        "title": "Autonomous Spacecraft Collision Avoidance with Multiple Space Debris Based on Reinforcement Learning",
        "authors": "Shuo Liu, Chaoxu Mu, Zhaoyang Liu",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10241000"
    },
    {
        "id": 29594,
        "title": "Deep Hierarchical Variational Autoencoders for World Models in Reinforcement Learning",
        "authors": "Sriharshitha Ayyalasomayajula, Banafsheh Rekabdar, Christos Mousas",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/transai60598.2023.00039"
    },
    {
        "id": 29595,
        "title": "Policy-guided Monte Carlo: Reinforcement-learning Markov chain dynamics",
        "authors": "Troels Arnfred Bojesen",
        "published": "2018-12-4",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1103/physreve.98.063303"
    },
    {
        "id": 29596,
        "title": "A REINFORCEMENT LEARNING-BASED ENERGY EFFICIENT AND QOS SUPPORTING MAC PROTOCOL FOR WIRELESS SENSOR NETWORKS",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.35741/issn.0258-2724.58.1.57"
    },
    {
        "id": 29597,
        "title": "Reinforcement learning: A brief guide for philosophers of mind",
        "authors": "Julia Haas",
        "published": "2022-9",
        "citations": 1,
        "abstract": "AbstractIn this opinionated review, I draw attention to some of the contributions reinforcement learning can make to questions in the philosophy of mind. In particular, I highlight reinforcement learning's foundational emphasis on the role of reward in agent learning, and canvass two ways in which the framework may advance our understanding of perception and motivation.",
        "link": "http://dx.doi.org/10.1111/phc3.12865"
    },
    {
        "id": 29598,
        "title": "DRSIR: A Deep Reinforcement Learning Approach for Routing in Software-Defined Networking",
        "authors": "Daniela Casas Velasco, Oscar Mauricio Caicedo Rendon, Nelson Luis Saldanha da Fonseca",
        "published": "No Date",
        "citations": 1,
        "abstract": "Traditional routing protocols employ limited information to make routing decisions which leads to slow adaptation to trafﬁc variability and restricted support to the quality of service requirements of the applications. To address these shortcomings, in previous work, we proposed RSIR, a routing solution based on Reinforcement Learning (RL) in SoftwareDeﬁned Networking (SDN). However, RL-based solutions usually suffer an increase in the learning process when dealing with large action and state spaces. This paper introduces a different routing approach called Deep Reinforcement Learning and SoftwareDeﬁned Networking Intelligent Routing (DRSIR). DRSIR deﬁnes a routing algorithm based on Deep RL (DRL) in SDN that overcomes the limitations of RL-based solutions. DRSIR considers path-state metrics to produce proactive, efﬁcient, and intelligent routing that adapts to dynamic trafﬁc changes. DRSIR was evaluated by emulation using real and synthetic  trafﬁc matrices. The results show that this solution outperforms the routing algorithms based on the Dijkstra’s algorithm and RSIR, in relation to stretching (stretch), packet loss, and delay. Moreover, the results obtained demonstrate that DRSIR provides a practical and viable  solution for routing in SDN.",
        "link": "http://dx.doi.org/10.36227/techrxiv.14501604.v1"
    },
    {
        "id": 29599,
        "title": "Robust Decision Making for Autonomous Vehicles at Highway On-Ramps: A Constrained Adversarial Reinforcement Learning Approach",
        "authors": "Xiangkun He, Baichuan Lou, Haohan Yang, Chen Lv",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Reinforcement learning has demonstrated its potential in a series of challenging domains.</p>\n<p>However, many real-world decision making tasks involve unpredictable environmental changes or unavoidable perception errors that are often enough to mislead an agent into making suboptimal decisions and even cause catastrophic failures.</p>\n<p>In light of these potential risks, reinforcement learning with application in safety-critical autonomous driving domain remains tricky without ensuring robustness against environmental uncertainties (e.g., road adhesion changes or measurement noises). </p>\n<p>Therefore, this paper proposes a novel constrained adversarial reinforcement learning approach for robust decision making of autonomous vehicles at highway on-ramps.</p>\n<p>Environmental disturbance is modelled as an adversarial agent that can learn an optimal adversarial policy to thwart the autonomous driving agent.</p>\n<p>Meanwhile, observation perturbation is approximated to maximize the variation of the perturbed policy through a white-box adversarial attack technique.</p>\n<p>Furthermore, a constrained adversarial actor-critic algorithm is presented to optimize an on-ramp merging policy while keeping the variations of the attacked driving policy and action-value function within bounds.</p>\n<p>Finally, the proposed robust highway on-ramp merging decision making method of autonomous vehicles is evaluated in three stochastic mixed traffic flows with different densities, and its effectiveness is demonstrated in comparison with the competitive baselines.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.21757526.v1"
    },
    {
        "id": 29600,
        "title": "Psychological Restrain for Fault Detection in Reinforcement Learning",
        "authors": "Himanshi Phour, Neha Gautam, Abhilash Gaurav",
        "published": "2023-11-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccis60361.2023.10425110"
    },
    {
        "id": 29601,
        "title": "Robot arm simulation based on model-free reinforcement learning",
        "authors": "Kun Li, Ke Wang",
        "published": "2021-6-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaica52286.2021.9498063"
    },
    {
        "id": 29602,
        "title": "A Reinforcement Learning Based Algorithm Towards Energy Efficient 5G Multi-Tier Network",
        "authors": "Nahina Islam, Ammar Alazab, Mamoun Alazab",
        "published": "2019-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccc.2019.000-2"
    },
    {
        "id": 29603,
        "title": "An Adaptive Analytical FPGA Placement flow based on Reinforcement Learning",
        "authors": "C. Barn, S. Vermeulen, S. Areibi, G. Grewal",
        "published": "2023-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icm60448.2023.10378891"
    },
    {
        "id": 29604,
        "title": "Active Tactile Exploration using Shape-Dependent Reinforcement Learning",
        "authors": "Shuo Jiang, Lawson L.S. Wong",
        "published": "2022-10-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9982266"
    },
    {
        "id": 29605,
        "title": "Deep Reinforcement Learning Based Tracking Control of Unmanned Vehicle with Safety Guarantee",
        "authors": "Zhongjing Luo, Jialing Zhou, Guanghui Wen",
        "published": "2022-5-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ascc56756.2022.9828057"
    },
    {
        "id": 29606,
        "title": "Measuring Data Quality for Dataset Selection in Offline Reinforcement Learning",
        "authors": "Phillip Swazinna, Steffen Udluft, Thomas Runkler",
        "published": "2021-12-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9660006"
    },
    {
        "id": 29607,
        "title": "Intelligent Processing of Data Streams on the Edge Using Reinforcement Learning",
        "authors": "Shubham Vaishnav, Sindri Magnússon",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccworkshops57953.2023.10283692"
    },
    {
        "id": 29608,
        "title": "Video Emotional Classification Based on Deep Reinforcement Learning",
        "authors": "Tingting Yuan, Yuyu Yuan",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acctcs58815.2023.00079"
    },
    {
        "id": 29609,
        "title": "A New Power System Dispatching Optimization Method Based on Reinforcement Learning",
        "authors": "Dabiao Wang",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acfpe59335.2023.10455427"
    },
    {
        "id": 29610,
        "title": "Goal-driven dimensionality reduction for reinforcement learning",
        "authors": "Simone Parisi, Simon Ramstedt, Jan Peters",
        "published": "2017-9",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2017.8206334"
    },
    {
        "id": 29611,
        "title": "Optimal Control of Battery System by Reinforcement Learning Considering Profitability",
        "authors": "Takuya Goto, Daisuke Kodaira",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pree57903.2023.10370554"
    },
    {
        "id": 29612,
        "title": "Data Efficient Safe Reinforcement Learning",
        "authors": "Sindhu Padakandla, Prabuchandran K J, Sourav Ganguly, Shalabh Bhatnagar",
        "published": "2022-10-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53654.2022.9945313"
    },
    {
        "id": 29613,
        "title": "Safety-informed mutations for evolutionary deep reinforcement learning",
        "authors": "Enrico Marchesini, Christopher Amato",
        "published": "2022-7-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3520304.3533980"
    },
    {
        "id": 29614,
        "title": "A Reinforcement Learning Hyper-heuristic for the optimisation of Flight Connections",
        "authors": "Yaroslav Pylyavskyy, Ahmed Kheiri, Leena Ahmed",
        "published": "2020-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec48606.2020.9185803"
    },
    {
        "id": 29615,
        "title": "Training a Reinforcement Learning Agent based on XCS in a Competitive Snake Environment",
        "authors": "Johannes Buttner, Sebastian Von Mammen",
        "published": "2021-8-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog52621.2021.9619161"
    },
    {
        "id": 29616,
        "title": "Improved Deep Reinforcement Learning for Intelligent Logistics Supply Chain Transportation Decision Model",
        "authors": "Qi Liu",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciics59993.2023.10421169"
    },
    {
        "id": 29617,
        "title": "CST-RL: Contrastive Spatio-Temporal Representations for Reinforcement Learning",
        "authors": "Chi-Kai Ho, Chung-Ta King",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3258540"
    },
    {
        "id": 29618,
        "title": "Multi-Objective Inverse Reinforcement Learning via Non-Negative Matrix Factorization",
        "authors": "Daiko Kishikawa, Sachiyo Arai",
        "published": "2021-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iiai-aai53430.2021.00078"
    },
    {
        "id": 29619,
        "title": "Deep Reinforcement Learning for Event-Triggered Control",
        "authors": "Dominik Baumann, Jia-Jie Zhu, Georg Martius, Sebastian Trimpe",
        "published": "2018-12",
        "citations": 42,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc.2018.8619335"
    },
    {
        "id": 29620,
        "title": "Stability Constrained Reinforcement Learning for Real-Time Voltage Control",
        "authors": "Yuanyuan Shi, Guannan Qu, Steven Low, Anima Anandkumar, Adam Wierman",
        "published": "2022-6-8",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867476"
    },
    {
        "id": 29621,
        "title": "Goal-Conditioned Reinforcement Learning: Problems and Solutions",
        "authors": "Minghuan Liu, Menghui Zhu, Weinan Zhang",
        "published": "2022-7",
        "citations": 15,
        "abstract": "Goal-conditioned reinforcement learning (GCRL), related to a set of complex RL problems, trains an agent to achieve different goals under particular scenarios. Compared to the standard RL solutions that learn a policy solely depending on the states or observations, GCRL additionally requires the agent to make decisions according to different goals.\n\nIn this survey, we provide a comprehensive overview of the challenges and algorithms for GCRL. Firstly, we answer what the basic problems are studied in this field. Then, we explain how goals are represented and present how existing solutions are designed from different points of view. Finally, we make the conclusion and discuss potential future prospects that recent researches focus on.",
        "link": "http://dx.doi.org/10.24963/ijcai.2022/770"
    },
    {
        "id": 29622,
        "title": "Deep reinforcement learning for extractive document summarization",
        "authors": "Kaichun Yao, Libo Zhang, Tiejian Luo, Yanjun Wu",
        "published": "2018-4",
        "citations": 49,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2018.01.020"
    },
    {
        "id": 29623,
        "title": "Application of reinforcement learning to detect and mitigate airspace loss of separation events",
        "authors": "M. Hawley, R. Bharadwaj",
        "published": "2018-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icnsurv.2018.8384989"
    },
    {
        "id": 29624,
        "title": "Predictability Analyzing: Deep Reinforcement Learning for Early Action Recognition",
        "authors": "Chen Xiaokai, Gao Ke, Cao Juan",
        "published": "2019-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icme.2019.00169"
    },
    {
        "id": 29625,
        "title": "Deep Reinforcement Learning with DQN vs. PPO in VizDoom",
        "authors": "Anton Zakharenkov, Ilya Makarov",
        "published": "2021-11-18",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cinti53070.2021.9668479"
    },
    {
        "id": 29626,
        "title": "Deep Reinforcement Learning of Region Proposal Networks for Object Detection",
        "authors": "Aleksis Pirinen, Cristian Sminchisescu",
        "published": "2018-6",
        "citations": 49,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2018.00726"
    },
    {
        "id": 29627,
        "title": "Use of Simulation-Aided Reinforcement Learning for Optimal Scheduling of Operations in Industrial Plants",
        "authors": "Satyavrat Wagle, Aditya A. Paranjape",
        "published": "2020-12-14",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc48552.2020.9383893"
    },
    {
        "id": 29628,
        "title": "A reinforcement learning method for multi-AGV scheduling in manufacturing",
        "authors": "Tianfang Xue, Peng Zeng, Haibin Yu",
        "published": "2018-2",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit.2018.8352413"
    },
    {
        "id": 29629,
        "title": "A New Maintenance Plan for Wind Turbine Farms Using Reinforcement Learning",
        "authors": "Hasan Rasay, Fatemeh Safaei, Sharareh Taghipour",
        "published": "2024-1-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rams51492.2024.10457834"
    },
    {
        "id": 29630,
        "title": "Generalizable Field Development Optimization Using Deep Reinforcement Learning with Field Examples",
        "authors": "Jincong He, Yusuf Nasir, Shusei Tanaka",
        "published": "2022-11-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003207009-20"
    },
    {
        "id": 29631,
        "title": "Medical Text Simplification Using Reinforcement Learning (TESLEA): Deep Learning–Based Text Simplification Approach",
        "authors": "Atharva Phatak, David W Savage, Robert Ohle, Jonathan Smith, Vijay Mago",
        "published": "2022-11-18",
        "citations": 0,
        "abstract": "\nBackground\nIn most cases, the abstracts of articles in the medical domain are publicly available. Although these are accessible by everyone, they are hard to comprehend for a wider audience due to the complex medical vocabulary. Thus, simplifying these complex abstracts is essential to make medical research accessible to the general public.\n\n\nObjective\nThis study aims to develop a deep learning–based text simplification (TS) approach that converts complex medical text into a simpler version while maintaining the quality of the generated text.\n\n\nMethods\nA TS approach using reinforcement learning and transformer–based language models was developed. Relevance reward, Flesch-Kincaid reward, and lexical simplicity reward were optimized to help simplify jargon-dense complex medical paragraphs to their simpler versions while retaining the quality of the text. The model was trained using 3568 complex-simple medical paragraphs and evaluated on 480 paragraphs via the help of automated metrics and human annotation.\n\n\nResults\nThe proposed method outperformed previous baselines on Flesch-Kincaid scores (11.84) and achieved comparable performance with other baselines when measured using ROUGE-1 (0.39), ROUGE-2 (0.11), and SARI scores (0.40). Manual evaluation showed that percentage agreement between human annotators was more than 70% when factors such as fluency, coherence, and adequacy were considered.\n\n\nConclusions\nA unique medical TS approach is successfully developed that leverages reinforcement learning and accurately simplifies complex medical paragraphs, thereby increasing their readability. The proposed TS approach can be applied to automatically generate simplified text for complex medical text data, which would enhance the accessibility of biomedical research to a wider audience.\n",
        "link": "http://dx.doi.org/10.2196/38095"
    },
    {
        "id": 29632,
        "title": "Learning to be a Bot: Reinforcement Learning in Shooter Games",
        "authors": "Michelle McPartland, Marcus Gallagher",
        "published": "2021-9-27",
        "citations": 1,
        "abstract": "This paper demonstrates the applicability of reinforcement learning for first person shooter bot artificial intelligence. Reinforcement learning is a machine learning technique where an agent learns a problem through interaction with the environment. The Sarsa(&lambda;) algorithm will be applied to a first person shooter bot controller to learn the tasks of (1) navigation and item collection, and (2) combat. The results will show the validity and diversity of reinforcement learning in a first person shooter environment.",
        "link": "http://dx.doi.org/10.1609/aiide.v4i1.18676"
    },
    {
        "id": 29633,
        "title": "Imitation learning based deep reinforcement learning for traffic signal control",
        "authors": "CunXiao Qiu, DaKe Zhou, Qingxian Wu, Tao Li",
        "published": "2023-8-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.3004832"
    },
    {
        "id": 29634,
        "title": "Brain Effective Connectivity Learning with Deep Reinforcement Learning",
        "authors": "Yilin Lu, Jinduo Liu, Junzhong Ji, Han Lv, Mengdi Huai",
        "published": "2022-12-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bibm55620.2022.9995284"
    },
    {
        "id": 29635,
        "title": "Deep reinforcement learning based trading agents: Risk curiosity driven learning for financial rules-based policy",
        "authors": "Badr Hirchoua, Brahim Ouhbi, Bouchra Frikh",
        "published": "2021-5",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.eswa.2020.114553"
    },
    {
        "id": 29636,
        "title": "Value learning from trajectory optimization and Sobolev descent: A step toward reinforcement learning with superlinear convergence properties",
        "authors": "Amit Parag, Sebastien Kleff, Leo Saci, Nicolas Mansard, Olivier Stasse",
        "published": "2022-5-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra46639.2022.9811993"
    },
    {
        "id": 29637,
        "title": "Reinforcement Learning in Traffic Prediction of Core Optical Networks using Learning Automata",
        "authors": "Anastasios Valkanis, Georgia A. Beletsioti, Petros Nicopolitidis, Georgios Papadimitriou, Emmanouel Varvarigos",
        "published": "2020-11-3",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccci49893.2020.9256655"
    },
    {
        "id": 29638,
        "title": "Learning From Oracle Demonstrations—A New Approach to Develop Autonomous Intersection Management Control Algorithms Based on Multiagent Deep Reinforcement Learning",
        "authors": "Antonio Guillen-Perez, Maria-Dolores Cano",
        "published": "2022",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3175493"
    },
    {
        "id": 29639,
        "title": "Machine Learning (Reinforcement Learning)-Based Steel Stock Yard Planning Algorithm",
        "authors": "Jong Hun Woo, Young In Cho, Sang Hyeon Yu, So Hyun Nam, Haoyu Zhu, Dong Hoon Kwak, Jong-Ho Nam",
        "published": "2020-12-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc48552.2020.9384049"
    },
    {
        "id": 29640,
        "title": "Auto-learning communication reinforcement learning for multi-intersection traffic light control",
        "authors": "Ruijie Zhu, Wenting Ding, Shuning Wu, Lulu Li, Ping Lv, Mingliang Xu",
        "published": "2023-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.110696"
    },
    {
        "id": 29641,
        "title": "Reinforcement learning for linear continuous-time systems: an incremental learning approach",
        "authors": "Tao Bian, Zhong-Ping Jiang",
        "published": "2019-3",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/jas.2019.1911390"
    },
    {
        "id": 29642,
        "title": "Deep Learning and Reinforcement Learning for Modeling Occupants’ Information in an Occupant-Centric Building Control: A Systematic Literature Review",
        "authors": "Rosina Adhikari, Yogesh Gautam, Houtan Jebelli, Willian E. Sitzabee",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1061/9780784485262.020"
    },
    {
        "id": 29643,
        "title": "A Survey on Reinforcement Learning for Recommender Systems",
        "authors": "Yuanguo Lin, Yong Liu, Fan Lin, Lixin Zou, Pengcheng Wu, Wenhua Zeng, Huanhuan Chen, Chunyan Miao",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3280161"
    },
    {
        "id": 29644,
        "title": "A Reinforcement Learning Approach to Price Cloud Resources With Provable Convergence Guarantees",
        "authors": "Hong Xie, John C. S. Lui",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3085088"
    },
    {
        "id": 29645,
        "title": "VASE: Variational Assorted Surprise Exploration for Reinforcement Learning",
        "authors": "Haitao Xu, Lech Szymanski, Brendan McCane",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3105140"
    },
    {
        "id": 29646,
        "title": "Autonomous Learning and Navigation of Mobile Robots Based on Deep Reinforcement Learning",
        "authors": "Zhiqiang Lai, Zhiwei Jia, Man Chen",
        "published": "2022-1-1",
        "citations": 1,
        "abstract": "Abstract\nAiming at the problems of convergence difficulties faced by deep reinforcement learning algorithms in dynamic pedestrian environments, and insufficient reward and feedback mechanisms, a data-driven and model-driven navigation algorithm which named GRRL has been proposed. In order to enrich and perfect the reward feedback mechanism, we designed a dynamic reward function. The reward function fully considers the relationship between the robot and the pedestrian and the target position. It mainly includes three parts. The experimental results show that the autonomous learning efficiency and the average navigation success rate of the mobile robot driven by the GRRL algorithm are improved, the average navigation time is shorter. The dynamic reward function we designed has a certain improvement effect on robot navigation.",
        "link": "http://dx.doi.org/10.1088/1742-6596/2171/1/012024"
    },
    {
        "id": 29647,
        "title": "Reinforcement Learning for Energy-Efficient Cloud Offloading of Mobile Embedded Applications",
        "authors": "Aditya Khune, Sudeep Pasricha",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-40677-5_7"
    },
    {
        "id": 29648,
        "title": "Scoped literature review of artificial intelligence marketing adoptions for ad optimization with reinforcement learning",
        "authors": "Johannes Sahlin, Håkan Sundell, Gideon Mbiydzenyuy, Jesper Holgersson",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811269264_0049"
    },
    {
        "id": 29649,
        "title": "Reinforcement Learning–Based Approach Towards Switch Migration for Load-Balancing in SDN",
        "authors": "Abha Kumari, Shubham Gupta, Joydeep Chandra, Ashok Singh Sairam",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003212249-3"
    },
    {
        "id": 29650,
        "title": "Sample efficient reinforcement learning with active learning for molecular design",
        "authors": "Michael Dodds, Jeff Guo, Thomas Löhr, Alessandro Tibo, Ola Engkvist, Jon Paul Janet",
        "published": "2024",
        "citations": 0,
        "abstract": "Active learning accelerates the design of molecules during generative reinforcement learning by creating surrogate models of expensive reward functions, obtaining a 4- to 64-fold reduction in computational effort per hit.",
        "link": "http://dx.doi.org/10.1039/d3sc04653b"
    },
    {
        "id": 29651,
        "title": "Learning behavior of hopping with robotic leg on particular height using model free reinforcement learning",
        "authors": "Shiva Pandey, Avinash Bhashkar, Anuj Kumar Sharma",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/5.0189079"
    },
    {
        "id": 29652,
        "title": "Research on Constant Perturbation Strategy for Deep Reinforcement Learning",
        "authors": "Jiamin Shen, Li Xu, Xu Wan, Jixuan Chai, Chunlong Fan",
        "published": "2023-3-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3590003.3590101"
    },
    {
        "id": 29653,
        "title": "An Empirically Grounding Analytics (EGA) Approach to Hog Farm Finishing Stage Management: Deep Reinforcement Learning as Decision Support and Managerial Learning Tool",
        "authors": "Panos Kouvelis, Ye Liu, Danko Turcic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4617964"
    },
    {
        "id": 29654,
        "title": "Human Action Recognition Based on YOLOv7",
        "authors": "Chenwei Liang, Wei Qi Yan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Human action recognition is a fundamental research problem in computer vision. The accuracy of human action recognition has important applications. In this book chapter, the authors use a YOLOv7-based model for human action recognition. To evaluate the performance of the model, the action recognition results of YOLOv7 were compared with those using CNN+LSTM, YOLOv5, and YOLOv4. Furthermore, a small human action dataset suitable for YOLO model training is designed. This data set is composed of images extracted from KTH, Weizmann, MSR data sets. In this book chapter, the authors make use of this data set to verify the experimental results. The final experimental results show that using the YOLOv7 model for human action recognition is very convenient and effective, compared with the previous YOLO model.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch006"
    },
    {
        "id": 29655,
        "title": "A Hybrid Multi-Task Learning Approach for Optimizing Deep Reinforcement Learning Agents",
        "authors": "Nelson Vithayathil Varghese, Qusay H. Mahmoud",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3065710"
    },
    {
        "id": 29656,
        "title": "Single stock trading with deep reinforcement learning: A comparative study",
        "authors": "Jun GE, Yuanqi QIN, Yaling Li, yanjia Huang, Hao Hu",
        "published": "2022-2-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3529836.3529857"
    },
    {
        "id": 29657,
        "title": "Temporal Logic Guided Safe Model-Based Reinforcement Learning",
        "authors": "Max Cohen, Calin Belta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-29310-8_9"
    },
    {
        "id": 29658,
        "title": "Domains as Objectives: Multi-Domain Reinforcement Learning with Convex-Coverage Set Learning for Domain Uncertainty Awareness",
        "authors": "Wendyam Eric Lionel Ilboudo, Taisuke Kobayashi, Takamitsu Matsubara",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10342236"
    },
    {
        "id": 29659,
        "title": "Deep learning and reinforcement learning approach on microgrid",
        "authors": "Kumar Chandrasekaran, Prabaakaran Kandasamy, Srividhya Ramanathan",
        "published": "2020-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531"
    },
    {
        "id": 29660,
        "title": "Reinforcement Learning Algorithm for Mixed Mean Field Control Games",
        "authors": "Andrea Angiuli, Nils Detering, Jean-Pierre Fouque, Mathieu Laurière null, Jimin Lin",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4208/jml.220915"
    },
    {
        "id": 29661,
        "title": "Learning Synergies Between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning",
        "authors": "Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser",
        "published": "2018-10",
        "citations": 306,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2018.8593986"
    },
    {
        "id": 29662,
        "title": "Continuous Control for Autonomous Underwater Vehicle Path Following Using Deep Interactive Reinforcement Learning",
        "authors": "Qilei Zhang, Chunxi Cheng, Zheng Fang, Dong Jiang, Bo He, Guangliang Li",
        "published": "2022-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlcr57210.2022.00013"
    },
    {
        "id": 29663,
        "title": "Perturbational Complexity by Distribution Mismatch: A Systematic Analysis of Reinforcement Learning in Reproducing Kernel Hilbert Space",
        "authors": "Jihao Long null, Jiequn Han",
        "published": "2022-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4208/jml.220114"
    },
    {
        "id": 29664,
        "title": "NavREn-Rl: Learning to fly in real environment via end-to-end deep reinforcement learning using monocular images",
        "authors": "Malik Aqeel Anwar, Arijit Raychowdhury",
        "published": "2018-11",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/m2vip.2018.8600838"
    },
    {
        "id": 29665,
        "title": "Structure Learning-Based Task Decomposition for Reinforcement Learning in Non-stationary Environments",
        "authors": "Honguk Woo, Gwangpyo Yoo, Minjong Yoo",
        "published": "2022-6-28",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) agents empowered by deep neural networks have been considered a feasible solution to automate control functions in a cyber-physical system. \n In this work, we consider an RL-based agent and address the issue of learning via continual interaction with a time-varying dynamic system modeled as a non-stationary Markov decision process (MDP). \n We view such a non-stationary MDP as a time series of conventional MDPs that can be parameterized by hidden variables. To infer the hidden parameters, we present a task decomposition method that exploits CycleGAN-based structure learning. \n This method enables the separation of time-variant tasks from a non-stationary MDP, establishing the task decomposition embedding specific to time-varying information. \n To mitigate the adverse effect due to inherent noises of task embedding, we also leverage continual learning on sequential tasks by adapting the orthogonal gradient descent scheme with a sliding window.\n Through various experiments, we demonstrate that our approach renders the RL agent adaptable to time-varying dynamic environment conditions, outperforming other methods including state-of-the-art non-stationary MDP algorithms.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i8.20844"
    },
    {
        "id": 29666,
        "title": "Maximum Entropy Inverse Reinforcement Learning Based on Behavior Cloning of Expert Examples",
        "authors": "Dazi Li, Jianghai Du",
        "published": "2021-5-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls52934.2021.9455476"
    },
    {
        "id": 29667,
        "title": "An online prediction algorithm for reinforcement learning with linear function approximation using cross entropy method",
        "authors": "Ajin George Joseph, Shalabh Bhatnagar",
        "published": "2018-9",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-018-5727-z"
    },
    {
        "id": 29668,
        "title": "Learning Skills to Navigate without a Master: A Sequential Multi-Policy Reinforcement Learning Algorithm",
        "authors": "Ambedkar Dukkipati, Rajarshi Banerjee, Ranga Shaarad Ayyagari, Dhaval Parmar Udaybhai",
        "published": "2022-10-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981607"
    },
    {
        "id": 29669,
        "title": "Linking Learning Fundamental Reinforcement Learning Concepts with Being Physically Active",
        "authors": "Ramakrishna Sai Annaluru, Christine Julien, Jamie Payton",
        "published": "2022-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3545947.3576316"
    },
    {
        "id": 29670,
        "title": "Adaptive Learning: A New Decentralized Reinforcement Learning Approach for Cooperative Multiagent Systems",
        "authors": "Meng-Lin Li, Shaofei Chen, Jing Chen",
        "published": "2020",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2020.2997899"
    },
    {
        "id": 29671,
        "title": "A Transfer Learning Strategy for Improving the Data Efficiency of Deep Reinforcement Learning Control in Smart Buildings",
        "authors": "Kadir Amasyali, Yan Liu, Helia Zandi",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgt59692.2024.10454120"
    },
    {
        "id": 29672,
        "title": "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks",
        "authors": "Vahid Behzadan, Arslan Munir",
        "published": "2017",
        "citations": 93,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-62416-7_19"
    },
    {
        "id": 29673,
        "title": "Learning to Calibrate Battery Models in Real-Time with Deep Reinforcement Learning",
        "authors": "Ajaykumar Unagar, Yuan Tian, Manuel Arias Chao, Olga Fink",
        "published": "2021-3-2",
        "citations": 17,
        "abstract": "Lithium-ion (Li-I) batteries have recently become pervasive and are used in many physical assets. For the effective management of the batteries, reliable predictions of the end-of-discharge (EOD) and end-of-life (EOL) are essential. Many detailed electrochemical models have been developed for the batteries. Their parameters are calibrated before they are taken into operation and are typically not re-calibrated during operation. However, the degradation of batteries increases the reality gap between the computational models and the physical systems and leads to inaccurate predictions of EOD/EOL. The current calibration approaches are either computationally expensive (model-based calibration) or require large amounts of ground truth data for degradation parameters (supervised data-driven calibration). This is often infeasible for many practical applications. In this paper, we introduce a reinforcement learning-based framework for reliably inferring calibration parameters of battery models in real time. Most importantly, the proposed methodology does not need any labeled data samples of observations and the ground truth parameters. The experimental results demonstrate that our framework is capable of inferring the model parameters in real time with better accuracy compared to approaches based on unscented Kalman filters. Furthermore, our results show better generalizability than supervised learning approaches even though our methodology does not rely on ground truth information during training.",
        "link": "http://dx.doi.org/10.3390/en14051361"
    },
    {
        "id": 29674,
        "title": "Learning from other minds: an optimistic critique of reinforcement learning models of social learning",
        "authors": "Natalia Vélez, Hyowon Gweon",
        "published": "2021-4",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cobeha.2021.01.006"
    },
    {
        "id": 29675,
        "title": "Reinforcement Learning-Based Adaptive Optimal Control for Partially Unknown Systems Using Differentiator",
        "authors": "Xinxin Guo, Weisheng Yan, Rongxin Cui",
        "published": "2018-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2018.8431133"
    },
    {
        "id": 29676,
        "title": "Reinforcement Learning for Turn-Based Strategy Game",
        "authors": "Gabriel Jonathan, Nur Ulfa Maulidevi",
        "published": "2023-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaicta59291.2023.10390086"
    },
    {
        "id": 29677,
        "title": "A Novel Deep Reinforcement Learning Strategy for Portfolio Management",
        "authors": "Yixin Qin, Fengchen Gu, Jionglong Su",
        "published": "2022-3-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icbda55095.2022.9760348"
    },
    {
        "id": 29678,
        "title": "Deep Reinforcement Learning in Smart Grid: Progress and Prospects",
        "authors": "Amila Akagic, Izudin Dzafic",
        "published": "2022-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icat54566.2022.9811131"
    },
    {
        "id": 29679,
        "title": "Hybridizing A Genetic Algorithm With Reinforcement Learning for Automated Design of Genetic Algorithms",
        "authors": "Ahmed Hassan, Nelishia Pillay",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec55065.2022.9870302"
    },
    {
        "id": 29680,
        "title": "A Deep Reinforcement Learning-based resource allocation mechanism for XR applications*",
        "authors": "Beining Feng",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bmsb58369.2023.10211567"
    },
    {
        "id": 29681,
        "title": "Hierarchical Temporal Memory with Reinforcement Learning",
        "authors": "Eduard Nugamanov, Aleksandr I. Panov",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2020.02.123"
    },
    {
        "id": 29682,
        "title": "Learning Strategies at SD 8 Muhammadiyah Reinforcement during the Covid-19 Pandemic",
        "authors": "Friska Aprilia, Muhlasin Amrullah",
        "published": "2021-8-26",
        "citations": 0,
        "abstract": "This research is motivated by the regulation of the Ministry of Education and Culture regarding the implementation of education during Covid-19 pandemic which requires the implementation of distance learning using online learning media. The purpose of this study was to find out the history of school formation, differences in learning before and after pandemic, challenges faced and learninf strategies implemented by SD 8 Muhammadiyah Tulangan during Covid-19 pandemic. The method used is qualitative with the observation, interview, question and answer. The results of thsi study can be concluded that SD 8 Muhammadiyah has a combination of learning strategies between face to face learning and online learning or what is commonly referred to blended learning method. The blended learning methods that is implemented in SD 8 Muhammadiyah Sidoarjo are conducting learning activities through zoom application for better material delivery and conducting study visits to students’ homes directly to share opinions and views between teacher, parents, and students as well as to find out advantages and disadvantages of online learning so far.",
        "link": "http://dx.doi.org/10.21070/icecrs20211153"
    },
    {
        "id": 29683,
        "title": "Reinforcement Learning Algorithms Performance Comparison on the Game of DeepRTS",
        "authors": "Safa Onur Şahin, Veysel Yücesoy",
        "published": "2021-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu53274.2021.9477938"
    },
    {
        "id": 29684,
        "title": "Application of Reinforcement Learning to a Robotic Drinking Assistant",
        "authors": "Tejas Kumar Shastha, Maria Kyrarini, Axel Gräser",
        "published": "2019-12-18",
        "citations": 12,
        "abstract": "Meal assistant robots form a very important part of the assistive robotics sector since self-feeding is a priority activity of daily living (ADL) for people suffering from physical disabilities like tetraplegia. A quick survey of the current trends in this domain reveals that, while tremendous progress has been made in the development of assistive robots for the feeding of solid foods, the task of feeding liquids from a cup remains largely underdeveloped. Therefore, this paper describes an assistive robot that focuses specifically on the feeding of liquids from a cup using tactile feedback through force sensors with direct human–robot interaction (HRI). The main focus of this paper is the application of reinforcement learning (RL) to learn what the best robotic actions are, based on the force applied by the user. A model of the application environment is developed based on the Markov decision process and a software training procedure is designed for quick development and testing. Five of the commonly used RL algorithms are investigated, with the intention of finding the best fit for training, and the system is tested in an experimental study. The preliminary results show a high degree of acceptance by the participants. Feedback from the users indicates that the assistive robot functions intuitively and effectively.",
        "link": "http://dx.doi.org/10.3390/robotics9010001"
    },
    {
        "id": 29685,
        "title": "Security Aware Virtual Network Embedding Algorithm Based on Reinforcement Learning",
        "authors": "Chunxiao Jiang, Peiying Zhang",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-5221-9_4"
    },
    {
        "id": 29686,
        "title": "Deriving optimal single-reservoir operating policies with reinforcement learning based approach incorporating uncertainties of demand and streamflow",
        "authors": "Divya Upadhyay, Sarth Dubey, Udit Bhatia",
        "published": "No Date",
        "citations": 0,
        "abstract": "Obtaining optimal reservoir operation policies is a challenging task and a strategic concern for policymakers. These policies are typically derived through a complex decision-making process with conflicting objectives, represented by nonlinear, nonconvex and multi-modal functions. The information on available inflow and various demands play a key role in developing optimal operation rules. However, they are characterized by various uncertainties which reduce the practical applicability of deterministic policy solutions. In literature, most of the studies handle streamflow uncertainty with single-demand scenarios. Although Stochastic Dynamic Programming (SDP) is a widely-used method for reservoir operations optimization under uncertainty, it suffers from the dual curses of dimensionality and modeling. This study considers the uncertainties for streamflow and various demands such as municipal, industrial, hydropower and irrigation water requirements. Here, we present a reinforcement learning framework that utilizes uncertainty-aware streamflow forecasts and demand requirements to yield optimal operation policies for Sardar Sarovar Dam, India. The proposed methodology incorporates the uncertainties of the underlying inflow and demand behavior, and demonstrates better performance than SDP in terms of net benefit. Overall, this work offers reliable techniques that can be used to develop multi-objective reservoir operation policies which are more adaptable in real-time.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu23-776"
    },
    {
        "id": 29687,
        "title": "General Real-Time Three-Dimensional Multi-Aircraft Conflict Resolution Method Using Multi-Agent Reinforcement Learning",
        "authors": "YUTONG CHEN, Yan Xu, Lei Yang, Minghua Hu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4423617"
    },
    {
        "id": 29688,
        "title": "Autonomous Navigation and Control of a Quadrotor Using Deep Reinforcement Learning",
        "authors": "Mohamed Mokhtar, Ayman El-Badawy",
        "published": "2023-6-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icuas57906.2023.10156126"
    },
    {
        "id": 29689,
        "title": "Combining primitive DQNs for improved reinforcement learning in Minecraft",
        "authors": "Matthew Reynard, Herman Kamper, Herman A. Engelbrecht, Benjamin Rosman",
        "published": "2020-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/saupec/robmech/prasa48453.2020.9041025"
    },
    {
        "id": 29690,
        "title": "Implementation of Language-Action Reward Network in Reinforcement Learning by Using Natural Language",
        "authors": "Sagiraju Hima Keerthi",
        "published": "2020-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indiscon50162.2020.00026"
    },
    {
        "id": 29691,
        "title": "Tire-Soil Tangential Force Reinforcement Learning Modeling",
        "authors": "Yingchun Qi, Jiaqi Zhao, Ye Zhuang",
        "published": "2022-10-10",
        "citations": 0,
        "abstract": "Tire-soil tangential interaction involves complex terramechanics. When modeling the tire-soil tangential forces, a great amount of experiments is needed to identify the parameters in the currently available models. The efficiency and accuracy of the current models is still time and cost consuming. The control of the terrain vehicle is introduced gradually to the off-road vehicles. Such application requires more accurate and real-time tire-soil force models. Therefore, the machine learning technique, the reinforcement learning algorithm, is introduced to the tire-soil tangential force modelling. First, the tire-soil rolling experiment is carried out under longitudinal and lateral slip condition with the tire-soil test facility. The tire-soil forces vs slip ratios test data is obtained on the sand and mud road surfaces. The reinforcement learning model, which including the physical interpretation and the uncertainty (with Gaussian Process), is proposed. The model parameters is identified through the supervised learning (training) by the model from the acquired experimental data. The model accuracy could be improved gradually with the iterative off-line learning (training). The trained model could calculate the force-vs-slip ratio relationship with high accuracy and efficiency. The proposed model could also be updated with the new data learning.",
        "link": "http://dx.doi.org/10.56884/rhbe9228"
    },
    {
        "id": 29692,
        "title": "You Only Train Once: A highly generalizable reinforcement learning method for dynamic job shop scheduling problem",
        "authors": "Yunhui Zeng, Zijun Liao, Xiu Li, Bo Yuan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Research in artificial intelligence demonstrates the applicability and flexibility of the reinforcement learning (RL) technique for the dynamic job shop scheduling problem (DJSP). However, the RL-based method will always overfit to the training environment and cannot generalize well to novel unseen situations at deployment time, which is unacceptable in real-world production. For this reason, this paper proposes a highly generalizable reinforcement learning framework named Train Once For All (TOFA) for the dynamic job shop scheduling problem. The trivial and non-trivial states are distinguished when the DJSP is formulated as a semi-Markov decision process, defining the size-agnostic state, action, and reward function. A novel graph representation learning method based on attention mechanism and spatial pyramid pooling is implemented to compress the disjunctive graphs of differentsize DJSP into fixed-length feature vectors. Combining the proposed dynamic frame skipping and an improved prioritized experience replay method that considers the sample quality difference at different training phases. TOFA shows superb generalization capability, outperforms practically favored dispatching rules and even instance-by-instance training RL-based schedulers on various benchmark DJSP. Additionally, we proved that TOFA acquires a transferable scheduling policy that can be used to schedule a whole new DJSP without additional training.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20324070"
    },
    {
        "id": 29693,
        "title": "Generate and Revise: Reinforcement Learning in Neural Poetry",
        "authors": "Andrea Zugarini, Luca Pasqualini, Stefano Melacci, Marco Maggini",
        "published": "2021-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533573"
    },
    {
        "id": 29694,
        "title": "Distributed Reinforcement Learning-based Memory Allocation for Edge-PLCs in Industrial IoT",
        "authors": "Tingting Fu, Yanjun Peng, Peng Liu, Haksrun Lao, Shaohua Wan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe rapid growth of the number of devices in the industrial Internet of things (IIoT) has a huge influence in the amount of data involved. In order to alleviate the computing load of cloud servers and reduce the delay of data processing, edge-cloud computing cooperation has been introduced to the IIoT. General programmable logic controllers (PLCs), which have been playing important roles in industrial control systems, start to gain the ability of processing large amount of industrial data and sharing the workload of cloud servers. This transforms them into edge-PLCs. However, continuous influx of multiple types of concurrent production data stream against the limited capacity of built-in memory in PLCs bring a huge challenge. Therefore, ability to reasonably allocate memory resources in edge-PLCs to ensure data utilization and real-time processing has become one of the core means in improving the efficiency of industrial processes. In this paper, to tackle dynamic changes of arrival data rate over time at each edge-PLC, we propose to optimize memory allocation with Q-learning distributedly. The simulation experiments verify that the method can effectively reduce the data loss probability while improve the system performance.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1934718/v1"
    },
    {
        "id": 29695,
        "title": "Reinforcement Learning of Whole-Body Control Strategies to Balance a Dynamically Stable Mobile Manipulator",
        "authors": "Vighnesh Vatsal, Balamuralidhar Purushothaman",
        "published": "2021-12-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc54714.2021.9703140"
    },
    {
        "id": 29696,
        "title": "Base Station Power Optimization for Green Networks Using Reinforcement Learning",
        "authors": "Semih AKTAŞ, Hande ALEMDAR",
        "published": "2021-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.35377/saucis...932709"
    },
    {
        "id": 29697,
        "title": "Proxy Functions for Approximate Reinforcement Learning",
        "authors": "Eduard Alibekov, Jiří Kubalík, Robert Babuška",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2019.09.145"
    },
    {
        "id": 29698,
        "title": "Comparing Actor-Critic Deep Reinforcement Learning Controllers for Enhanced Performance on a Ball-and-Plate System",
        "authors": "Daniel Udekwe, Ore-Ofe Ajayi, Osichinaka Ubadike",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4601865"
    },
    {
        "id": 29699,
        "title": "Visual Navigation with Actor-Critic Deep Reinforcement Learning",
        "authors": "Kun Shao, Dongbin Zhao, Yuanheng Zhu, Qichao Zhang",
        "published": "2018-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489185"
    },
    {
        "id": 29700,
        "title": "Reinforcement Learning: Advancements, Limitations, and Real-world Applications",
        "authors": "Avanthikaa Srinivasan",
        "published": "2023-8-7",
        "citations": 0,
        "abstract": "This paper aims to review the advancements, limitations, and real-world applications of RL. Additionally, it will explore the future of RL and the challenges that must be addressed to enhance its widespread applicability. By addressing these challenges, RL can be further harnessed to tackle complex real-world problems.",
        "link": "http://dx.doi.org/10.55041/ijsrem25118"
    },
    {
        "id": 29701,
        "title": "Deep Reinforcement Learning based Solution Approach for Unit Commitment under Demand and Wind Power Uncertainty",
        "authors": "Akshay Ajagekar, Fengqi You",
        "published": "2022-6-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867273"
    },
    {
        "id": 29702,
        "title": "Apply Deep Reinforcement Learning to NS-SHAFT Game Control",
        "authors": "BoYu Lin, ChingLung Chang, Chuan-Yu Chang",
        "published": "2020-9-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccs49175.2020.9231455"
    },
    {
        "id": 29703,
        "title": "Reinforcement-learning-based path planning for UAVs in intensive obstacle environment",
        "authors": "Miao Guo, Teng Long, Hui Li, Jingliang Sun",
        "published": "2021-10-22",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9727746"
    },
    {
        "id": 29704,
        "title": "Active Vision Control Policies for Face Recognition using Deep Reinforcement Learning",
        "authors": "Pavlos Tosidis, Nikolaos Passalis, Anastasios Tefas",
        "published": "2022-8-29",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco55093.2022.9909691"
    },
    {
        "id": 29705,
        "title": "Deep Reinforcement Learning with Feedback-based Exploration",
        "authors": "Jan Scholten, Daan Wout, Carlos Celemin, Jens Kober",
        "published": "2019-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc40024.2019.9029503"
    },
    {
        "id": 29706,
        "title": "A Deep Reinforcement Learning Approach for Solving the Pickup and Delivery Problem with Drones and Time Windows",
        "authors": "Fuqiang Lu, Nan Chen, Bihua Ling",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4684209"
    },
    {
        "id": 29707,
        "title": "Deep Reinforcement Learning Ensemble for Detecting Anomaly in Telemetry Water Level Data",
        "authors": "Thakolpat Khampuengson, Wenjia Wang",
        "published": "2022-8-13",
        "citations": 2,
        "abstract": "Water levels in rivers are measured by various devices installed mostly in remote locations along the rivers, and the collected data are then transmitted via telemetry systems to a data centre for further analysis and utilisation, including producing early warnings for risk situations. So, the data quality is essential. However, the devices in the telemetry station may malfunction and cause errors in the data, which can result in false alarms or missed true alarms. Finding these errors requires experienced humans with specialised knowledge, which is very time-consuming and also inconsistent. Thus, there is a need to develop an automated approach. In this paper, we firstly investigated the applicability of Deep Reinforcement Learning (DRL). The testing results show that whilst they are more accurate than some other machine learning models, particularly in identifying unknown anomalies, they lacked consistency. Therefore, we proposed an ensemble approach that combines DRL models to improve consistency and also accuracy. Compared with other models, including Multilayer Perceptrons (MLP) and Long Short-Term Memory (LSTM), our ensemble models are not only more accurate in most cases, but more importantly, more reliable.",
        "link": "http://dx.doi.org/10.3390/w14162492"
    },
    {
        "id": 29708,
        "title": "Online Energy Management in Commercial Buildings using Deep Reinforcement Learning",
        "authors": "Aviek Naug, Ibrahim Ahmed, Gautam Biswas",
        "published": "2019-6",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smartcomp.2019.00060"
    },
    {
        "id": 29709,
        "title": "Deep Reinforcement Learning-Based Computation Offloading and Distributed Edge Service Caching for Mobile Edge Computing",
        "authors": "Mande Xie, Jiefeng Ye, Guoping Zhang, Xueping Ni",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4725151"
    },
    {
        "id": 29710,
        "title": "Reinforcement learning with convolutional reservoir computing",
        "authors": "Hanten Chang, Katsuya Futagami",
        "published": "2020-8",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-020-01679-3"
    },
    {
        "id": 29711,
        "title": "Special issue on multi-objective reinforcement learning",
        "authors": "Madalina Drugan, Marco Wiering, Peter Vamplew, Madhu Chetty",
        "published": "2017-11",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2017.06.020"
    },
    {
        "id": 29712,
        "title": "Multi-modal Feedback for Affordance-driven Interactive Reinforcement Learning",
        "authors": "Francisco Cruz, German I. Parisi, Stefan Wermter",
        "published": "2018-7",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489237"
    },
    {
        "id": 29713,
        "title": "New Approach in Human-AI Interaction by Reinforcement-Imitation Learning",
        "authors": "Neda Navidi, Rene Landry",
        "published": "2021-3-30",
        "citations": 5,
        "abstract": "Reinforcement Learning (RL) provides effective results with an agent learning from a stand-alone reward function. However, it presents unique challenges with large amounts of environment states and action spaces, as well as in the determination of rewards. Imitation Learning (IL) offers a promising solution for those challenges using a teacher. In IL, the learning process can take advantage of human-sourced assistance and/or control over the agent and environment. A human teacher and an agent learner are considered in this study. The teacher takes part in the agent’s training towards dealing with the environment, tackling a specific objective, and achieving a predefined goal. This paper proposes a novel approach combining IL with different types of RL methods, namely, state-action-reward-state-action (SARSA) and Asynchronous Advantage Actor–Critic Agents (A3C), to overcome the problems of both stand-alone systems. How to effectively leverage the teacher’s feedback—be it direct binary or indirect detailed—for the agent learner to learn sequential decision-making policies is addressed. The results of this study on various OpenAI-Gym environments show that this algorithmic method can be incorporated with different combinations, and significantly decreases both human endeavors and tedious exploration process.",
        "link": "http://dx.doi.org/10.3390/app11073068"
    },
    {
        "id": 29714,
        "title": "Deep Reinforcement Learning Based Group Recommendation System with Multi-Head Attention Mechanism",
        "authors": "Saba Izadkhah, Banafsheh Reakbdar",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/transai60598.2023.00038"
    },
    {
        "id": 29715,
        "title": "A Deep Reinforcement Learning Method for 2D Irregular Packing with Dense Reward",
        "authors": "Viviana Crescitelli, Takashi Oshima",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/transai60598.2023.00037"
    },
    {
        "id": 29716,
        "title": "Reinforcement Learning for Turn-Based Strategy Game",
        "authors": "Gabriel Jonathan, Nur Ulfa Maulidevi",
        "published": "2023-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaicta59291.2023.10390086"
    },
    {
        "id": 29717,
        "title": "Model-Reference Reinforcement Learning Control of Autonomous Surface Vehicles",
        "authors": "Qingrui Zhang, Wei Pan, Vasso Reppa",
        "published": "2020-12-14",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc42340.2020.9304347"
    },
    {
        "id": 29718,
        "title": "Reinforcement-Learning-Based Test Program Generation for Software-Based Self-Test",
        "authors": "Ching-Yuan Chen, Jiun-Lang Huang",
        "published": "2019-12",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ats47505.2019.00013"
    },
    {
        "id": 29719,
        "title": "Safe Reinforcement Learning Control for Water Distribution Networks",
        "authors": "Jorge Val, Rafal Wisniewski, Carsten Skovmose Kallesoe",
        "published": "2021-8-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccta48906.2021.9659138"
    },
    {
        "id": 29720,
        "title": "Adaptive control for building energy management using reinforcement learning",
        "authors": "Lukas Eller, Lydia C. Siafara, Thilo Sauter",
        "published": "2018-2",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit.2018.8352414"
    },
    {
        "id": 29721,
        "title": "Dynamic Economic Optimization of a Continuously Stirred Tank Reactor Using Reinforcement Learning",
        "authors": "Derek Machalek, Titus Quah, Kody M. Powell",
        "published": "2020-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147706"
    },
    {
        "id": 29722,
        "title": "Application of Reinforcement Learning to a Robotic Drinking Assistant",
        "authors": "Tejas Kumar Shastha, Maria Kyrarini, Axel Gräser",
        "published": "2019-12-18",
        "citations": 12,
        "abstract": "Meal assistant robots form a very important part of the assistive robotics sector since self-feeding is a priority activity of daily living (ADL) for people suffering from physical disabilities like tetraplegia. A quick survey of the current trends in this domain reveals that, while tremendous progress has been made in the development of assistive robots for the feeding of solid foods, the task of feeding liquids from a cup remains largely underdeveloped. Therefore, this paper describes an assistive robot that focuses specifically on the feeding of liquids from a cup using tactile feedback through force sensors with direct human–robot interaction (HRI). The main focus of this paper is the application of reinforcement learning (RL) to learn what the best robotic actions are, based on the force applied by the user. A model of the application environment is developed based on the Markov decision process and a software training procedure is designed for quick development and testing. Five of the commonly used RL algorithms are investigated, with the intention of finding the best fit for training, and the system is tested in an experimental study. The preliminary results show a high degree of acceptance by the participants. Feedback from the users indicates that the assistive robot functions intuitively and effectively.",
        "link": "http://dx.doi.org/10.3390/robotics9010001"
    },
    {
        "id": 29723,
        "title": "Efficient Dimensionality Reduction Strategies for Quantum Reinforcement Learning",
        "authors": "Eva Andrés, M. P. Cuéllar, G. Navarro",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2023.3318173"
    },
    {
        "id": 29724,
        "title": "Identification of Animal Behavioral Strategies by Inverse Reinforcement Learning",
        "authors": "Shoichiro Yamaguchi, Honda Naoki, Muneki Ikeda, Yuki Tsukada, Shunji Nakano, Ikue Mori, Shin Ishii",
        "published": "No Date",
        "citations": 2,
        "abstract": "AbstractAnimals are able to reach a desired state in an environment by controlling various behavioral patterns. Identification of the behavioral strategy used for this control is important for understanding animals’ decision-making and is fundamental to dissect information processing done by the nervous system. However, methods for quantifying such behavioral strategies have not been fully established. In this study, we developed an inverse reinforcement-learning (IRL) framework to identify an animal’s behavioral strategy from behavioral time-series data. As a particular target, we applied this framework toC. elegansthermotactic behavior; after cultivation at a constant temperature with or without food, the fed and starved worms prefer and avoid from the cultivation temperature on a thermal gradient, respectively. Our IRL approach revealed that the fed worms used both absolute and temporal derivative of temperature and that their strategy comprised mixture of two strategies: directed migration (DM) and isothermal migration (IM). The DM is a strategy that the worms efficiently reach to specific temperature, which explained thermotactic behaviors of the fed worms. The IM is a strategy that the worms track along a constant temperature, which reflects isothermal tracking well observed in previous studies. We also showed the neural basis underlying the strategies, by applying our method to thermosensory neuron-deficient worms. In contrast to fed animals, the strategy of starved animals indicated that they escaped the cultivation temperature using only absolute, but not temporal derivative of temperature. Thus, our IRL-based approach is capable of identifying animal strategies from behavioral time-series data and will be applicable to wide range of behavioral studies, including decision-making of other organisms.Author SummaryUnderstanding animal decision-making has been a fundamental problem in neuroscience and behavioral ecology. Many studies analyze actions that represent decision-making in behavioral tasks, in which rewards are artificially designed with specific objectives. However, it is impossible to extend this artificially designed experiment to a natural environment, because in a natural environment, the rewards for freely-behaving animals cannot be clearly defined. To this end, we must reverse the current paradigm so that rewards are identified from behavioral data. Here, we propose a new reverse-engineering approach (inverse reinforcement learning) that can estimate a behavioral strategy from time-series data of freely-behaving animals. By applying this technique with thermotaxis inC. elegans, we successfully identified the reward-based behavioral strategy.",
        "link": "http://dx.doi.org/10.1101/129007"
    },
    {
        "id": 29725,
        "title": "Reinforcement learning based control of batch polymerisation processes",
        "authors": "Vikas Singh, Hariprasad Kodamana",
        "published": "2020",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2020.06.111"
    },
    {
        "id": 29726,
        "title": "Exploring State Transition Uncertainty in Variational Reinforcement Learning",
        "authors": "Jen-Tzung Chien, Wei-Lin Liao, Issam El Naqa",
        "published": "2021-1-24",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco47968.2020.9287440"
    },
    {
        "id": 29727,
        "title": "Mastering Fighting Game Using Deep Reinforcement Learning With Self-play",
        "authors": "Dae-Wook Kim, Sungyun Park, Seong-il Yang",
        "published": "2020-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog47356.2020.9231639"
    },
    {
        "id": 29728,
        "title": "Application-Layer DDoS Defense with Reinforcement Learning",
        "authors": "Yebo Feng, Jun Li, Thanh Nguyen",
        "published": "2020-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwqos49365.2020.9213026"
    },
    {
        "id": 29729,
        "title": "Reinforcement Learning for Optimal Protection Coordination",
        "authors": "Hasan Can Kilickiran, Bedri Kekezoglu, Nikolaos G. Paterakis",
        "published": "2018-9",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sest.2018.8495830"
    },
    {
        "id": 29730,
        "title": "Joint DNN Partitioning and Task Offloading in Mobile Edge Computing via Deep Reinforcement Learning",
        "authors": "Jianbing Zhang, Shufang Ma, Zexiao Yan, Jiwei Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs Artificial Intelligence (AI) becomes increasingly prevalent, Deep Neural Networks (DNNs) have become a crucial tool for developing and advancing AI applications. Considering limited computing and energy resources on mobile devices (MDs), it is a challenge to perform compute-intensive DNN tasks on MDs. To attack this challenge, mobile edge computing (MEC) provides a viable solution through DNN partitioning and task offloading. However, as the communication conditions between different devices change over time, DNN partitioning on different devices must also change synchronously. This is a dynamic process, which aggravates the complexity of DNN partitioning. In this paper, we delve into the issue of jointly optimizing energy and delay for DNN partitioning and task offloading in a dynamic MEC scenario where each MD and the server adopt the pre-trained DNNs for task inference. Taking advantage of the characteristics of DNN, we first propose a strategy for layered partitioning of DNN tasks to divide the task of each MD into subtasks that can be either processed on the MD or offloaded to the server for computation. Then, we formulate the trade-off between energy and delay as a joint optimization problem, which is further represented as a Markov decision process (MDP). To solve this, we design a DNN partitioning and task offloading (DPTO) algorithm utilizing deep reinforcement learning (DRL), which enables MDs to make optimal offloading decisions. Finally, experimental results demonstrate that our algorithm outperforms existing non-DRL and DRL algorithms with respect to processing delay and energy consumption, and can be applied to different DNN types.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2901233/v1"
    },
    {
        "id": 29731,
        "title": "Reinforcement Learning Training Environment for Fixed Wing UAV Collision Avoidance",
        "authors": "Francesco d'Apolito",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2022.12.035"
    },
    {
        "id": 29732,
        "title": "Approximate Logic Synthesis: A Reinforcement Learning-Based Technology Mapping Approach",
        "authors": "Ghasem Pasandi, Shahin Nazarian, Massoud Pedram",
        "published": "2019-3",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isqed.2019.8697679"
    },
    {
        "id": 29733,
        "title": "Autonomous Highway Driving using Deep Reinforcement Learning",
        "authors": "Subramanya Nageshrao, H. Eric Tseng, Dimitar Filev",
        "published": "2019-10",
        "citations": 67,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc.2019.8914621"
    },
    {
        "id": 29734,
        "title": "Multi-UAV Searching Trajectory Optimization Algorithm based on Deep Reinforcement Learning",
        "authors": "Bo Zhang, Kunhao Yang",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icct59356.2023.10419808"
    },
    {
        "id": 29735,
        "title": "A Reinforcement Learning Toolkit for Quadruped Robots With Pybullet",
        "authors": "Fuchun Liu, Suchen Zhang",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isrimt59937.2023.10428509"
    },
    {
        "id": 29736,
        "title": "Faculty Opinions recommendation of Discovering faster matrix multiplication algorithms with reinforcement learning.",
        "authors": "Sutirtha Chakraborty",
        "published": "2022-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3410/f.742353014.793595962"
    },
    {
        "id": 29737,
        "title": "Fear-Neuro-Inspired Reinforcement Learning for Safe Autonomous Driving",
        "authors": "Xiangkun He, Jingda Wu, Zhiyu Huang, Zhongxu Hu, Jun Wang, Alberto Sangiovanni-Vincentelli, Chen Lv",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Ensuring safety and achieving human-level driving performance remain challenges for autonomous vehicles, especially in safety-critical situations. As a key component of artificial intelligence, reinforcement learning is promising and has shown great potential in many complex tasks; however, its lack of safety guarantees limits its real-world applicability. Hence, further advancing reinforcement learning, especially from the safety perspective, is of great importance for autonomous driving. As revealed by cognitive neuroscientists, the amygdala of the brain can elicit defensive responses against threats or hazards, which is crucial for survival in and adaptation to risky environments. Drawing inspiration from this scientific discovery, we present a fear-neuro-inspired reinforcement learning framework to realize safe autonomous driving through modeling the amygdala functionality. This new technique facilitates an agent to learn defensive behaviors and achieve safe decision making with fewer safety violations. Through experimental tests, we show that the proposed approach enables the autonomous driving agent to attain state-of-the-art performance compared to the baseline agents and perform comparably to 30 certified human drivers, across various safety-critical scenarios. The results demonstrate the feasibility and effectiveness of our framework while also shedding light on the crucial role of simulating the amygdala function in the application of reinforcement learning to safety-critical autonomous driving domains.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.24289108"
    },
    {
        "id": 29738,
        "title": "Visual Navigation with Actor-Critic Deep Reinforcement Learning",
        "authors": "Kun Shao, Dongbin Zhao, Yuanheng Zhu, Qichao Zhang",
        "published": "2018-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2018.8489185"
    },
    {
        "id": 29739,
        "title": "Reinforcement learning of optimal active particle navigation",
        "authors": "Mahdi Nasiri, Benno Liebchen",
        "published": "2022-7-1",
        "citations": 16,
        "abstract": "Abstract\nThe development of self-propelled particles at the micro- and the nanoscale has sparked a huge potential for future applications in active matter physics, microsurgery, and targeted drug delivery. However, while the latter applications provoke the quest on how to optimally navigate towards a target, such as e.g. a cancer cell, there is still no simple way known to determine the optimal route in sufficiently complex environments. Here we develop a machine learning-based approach that allows us, for the first time, to determine the asymptotically optimal path of a self-propelled agent which can freely steer in complex environments. Our method hinges on policy gradient-based deep reinforcement learning techniques and, crucially, does not require any reward shaping or heuristics. The presented method provides a powerful alternative to current analytical methods to calculate optimal trajectories and opens a route towards a universal path planner for future intelligent active particles.",
        "link": "http://dx.doi.org/10.1088/1367-2630/ac8013"
    },
    {
        "id": 29740,
        "title": "Andes_gym: A Versatile Environment for Deep Reinforcement Learning in Power Systems",
        "authors": "Hantao Cui, Yichen Zhang",
        "published": "2022-7-17",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm48719.2022.9916967"
    },
    {
        "id": 29741,
        "title": "DeepWiERL: Bringing Deep Reinforcement Learning to the Internet of Self-Adaptive Things",
        "authors": "Francesco Restuccia, Tommaso Melodia",
        "published": "2020-7",
        "citations": 19,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infocom41043.2020.9155461"
    },
    {
        "id": 29742,
        "title": "Reinforcedet: Object Detection By Integrating Reinforcement Learning With Decoupled Pipeline",
        "authors": "Man Zhou, Liu Liu, Rujing Wang",
        "published": "2021-9-19",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip42928.2021.9506038"
    },
    {
        "id": 29743,
        "title": "Active Perception in Adversarial Scenarios using Maximum Entropy Deep Reinforcement Learning",
        "authors": "Macheng Shen, Jonathan P. How",
        "published": "2019-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8794389"
    },
    {
        "id": 29744,
        "title": "Adaptive Approximation Strategy to Reduce Approximation Error in Reinforcement Learning",
        "authors": "Min Li, William Zhu",
        "published": "2023-4-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccea58433.2023.10135234"
    },
    {
        "id": 29745,
        "title": "Deep reinforcement learning based missile guidance law design for maneuvering target interception",
        "authors": "Mingjian Du, Chi Peng, Jianjun Ma",
        "published": "2021-7-26",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc52363.2021.9549596"
    },
    {
        "id": 29746,
        "title": "Reinforcement Learning for Control of Building HVAC Systems",
        "authors": "Naren Srivaths Raman, Adithya M. Devraj, Prabir Barooah, Sean P. Meyn",
        "published": "2020-7",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc45564.2020.9147629"
    },
    {
        "id": 29747,
        "title": "Deep Reinforcement Learning for Agile Satellite Scheduling Problem",
        "authors": "Ming Chen, Yuning Chen, Yingwu Chen, Weihua Qi",
        "published": "2019-12",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci44817.2019.9002957"
    },
    {
        "id": 29748,
        "title": "Reinforcement Learning-based Event-Triggered Model Predictive Control for Autonomous Vehicle Path Following",
        "authors": "Jun Chen, Xiangyu Meng, Zhaojian Li",
        "published": "2022-6-8",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc53348.2022.9867347"
    },
    {
        "id": 29749,
        "title": "Safe Reinforcement Learning for Mixed-Autonomy Platoon Control",
        "authors": "Jingyuan Zhou, Longhao Yan, Kaidi Yang",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10422463"
    },
    {
        "id": 29750,
        "title": "Dynamic Reinforcement Learning based Scheduling for Energy-Efficient Edge-Enabled LoRaWAN",
        "authors": "Jui Mhatre, Ahyoung Lee",
        "published": "2022-11-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipccc55026.2022.9894340"
    },
    {
        "id": 29751,
        "title": "Reinforcement Learning Based Coexistence in Mixed 802.11ax and Legacy WLANs",
        "authors": "Fabián Frommel, Germán Capdehourat, Federico Larroca",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc55385.2023.10119114"
    },
    {
        "id": 29752,
        "title": "Reinforcement Learning based Coverage Planning for UAVs Fleets",
        "authors": "Cosimo Bromo, Simone Godio, Giorgio Guglieri",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2023-1149"
    },
    {
        "id": 29753,
        "title": "Maze Solver: A Federated Reinforcement Learning Approach",
        "authors": "Aayush Sharma, Harjeet Kaur, Deepak Prashar",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10307671"
    },
    {
        "id": 29754,
        "title": "Multi-Objective Reinforcement Learning for Autonomous Drone Navigation in Urban Area",
        "authors": "Jiahao Wu, Yang Ye, Jing Du",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1061/9780784485262.072"
    },
    {
        "id": 29755,
        "title": "A Survey on Reinforcement Learning for Recommender Systems",
        "authors": "Yuanguo Lin, Yong Liu, Fan Lin, Lixin Zou, Pengcheng Wu, Wenhua Zeng, Huanhuan Chen, Chunyan Miao",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3280161"
    },
    {
        "id": 29756,
        "title": "A Reinforcement Learning Approach to Price Cloud Resources With Provable Convergence Guarantees",
        "authors": "Hong Xie, John C. S. Lui",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3085088"
    },
    {
        "id": 29757,
        "title": "VASE: Variational Assorted Surprise Exploration for Reinforcement Learning",
        "authors": "Haitao Xu, Lech Szymanski, Brendan McCane",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3105140"
    },
    {
        "id": 29758,
        "title": "Autonomous Learning and Navigation of Mobile Robots Based on Deep Reinforcement Learning",
        "authors": "Zhiqiang Lai, Zhiwei Jia, Man Chen",
        "published": "2022-1-1",
        "citations": 1,
        "abstract": "Abstract\nAiming at the problems of convergence difficulties faced by deep reinforcement learning algorithms in dynamic pedestrian environments, and insufficient reward and feedback mechanisms, a data-driven and model-driven navigation algorithm which named GRRL has been proposed. In order to enrich and perfect the reward feedback mechanism, we designed a dynamic reward function. The reward function fully considers the relationship between the robot and the pedestrian and the target position. It mainly includes three parts. The experimental results show that the autonomous learning efficiency and the average navigation success rate of the mobile robot driven by the GRRL algorithm are improved, the average navigation time is shorter. The dynamic reward function we designed has a certain improvement effect on robot navigation.",
        "link": "http://dx.doi.org/10.1088/1742-6596/2171/1/012024"
    },
    {
        "id": 29759,
        "title": "Reinforcement Learning for Energy-Efficient Cloud Offloading of Mobile Embedded Applications",
        "authors": "Aditya Khune, Sudeep Pasricha",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-40677-5_7"
    },
    {
        "id": 29760,
        "title": "Scoped literature review of artificial intelligence marketing adoptions for ad optimization with reinforcement learning",
        "authors": "Johannes Sahlin, Håkan Sundell, Gideon Mbiydzenyuy, Jesper Holgersson",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811269264_0049"
    },
    {
        "id": 29761,
        "title": "Reinforcement Learning–Based Approach Towards Switch Migration for Load-Balancing in SDN",
        "authors": "Abha Kumari, Shubham Gupta, Joydeep Chandra, Ashok Singh Sairam",
        "published": "2023-5-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003212249-3"
    },
    {
        "id": 29762,
        "title": "Sample efficient reinforcement learning with active learning for molecular design",
        "authors": "Michael Dodds, Jeff Guo, Thomas Löhr, Alessandro Tibo, Ola Engkvist, Jon Paul Janet",
        "published": "2024",
        "citations": 0,
        "abstract": "Active learning accelerates the design of molecules during generative reinforcement learning by creating surrogate models of expensive reward functions, obtaining a 4- to 64-fold reduction in computational effort per hit.",
        "link": "http://dx.doi.org/10.1039/d3sc04653b"
    },
    {
        "id": 29763,
        "title": "Learning behavior of hopping with robotic leg on particular height using model free reinforcement learning",
        "authors": "Shiva Pandey, Avinash Bhashkar, Anuj Kumar Sharma",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/5.0189079"
    },
    {
        "id": 29764,
        "title": "Research on Constant Perturbation Strategy for Deep Reinforcement Learning",
        "authors": "Jiamin Shen, Li Xu, Xu Wan, Jixuan Chai, Chunlong Fan",
        "published": "2023-3-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3590003.3590101"
    },
    {
        "id": 29765,
        "title": "An Empirically Grounding Analytics (EGA) Approach to Hog Farm Finishing Stage Management: Deep Reinforcement Learning as Decision Support and Managerial Learning Tool",
        "authors": "Panos Kouvelis, Ye Liu, Danko Turcic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4617964"
    },
    {
        "id": 29766,
        "title": "Human Action Recognition Based on YOLOv7",
        "authors": "Chenwei Liang, Wei Qi Yan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Human action recognition is a fundamental research problem in computer vision. The accuracy of human action recognition has important applications. In this book chapter, the authors use a YOLOv7-based model for human action recognition. To evaluate the performance of the model, the action recognition results of YOLOv7 were compared with those using CNN+LSTM, YOLOv5, and YOLOv4. Furthermore, a small human action dataset suitable for YOLO model training is designed. This data set is composed of images extracted from KTH, Weizmann, MSR data sets. In this book chapter, the authors make use of this data set to verify the experimental results. The final experimental results show that using the YOLOv7 model for human action recognition is very convenient and effective, compared with the previous YOLO model.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch006"
    },
    {
        "id": 29767,
        "title": "A Hybrid Multi-Task Learning Approach for Optimizing Deep Reinforcement Learning Agents",
        "authors": "Nelson Vithayathil Varghese, Qusay H. Mahmoud",
        "published": "2021",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3065710"
    },
    {
        "id": 29768,
        "title": "Single stock trading with deep reinforcement learning: A comparative study",
        "authors": "Jun GE, Yuanqi QIN, Yaling Li, yanjia Huang, Hao Hu",
        "published": "2022-2-18",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3529836.3529857"
    },
    {
        "id": 29769,
        "title": "Temporal Logic Guided Safe Model-Based Reinforcement Learning",
        "authors": "Max Cohen, Calin Belta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-29310-8_9"
    },
    {
        "id": 29770,
        "title": "Domains as Objectives: Multi-Domain Reinforcement Learning with Convex-Coverage Set Learning for Domain Uncertainty Awareness",
        "authors": "Wendyam Eric Lionel Ilboudo, Taisuke Kobayashi, Takamitsu Matsubara",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10342236"
    },
    {
        "id": 29771,
        "title": "Deep learning and reinforcement learning approach on microgrid",
        "authors": "Kumar Chandrasekaran, Prabaakaran Kandasamy, Srividhya Ramanathan",
        "published": "2020-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/2050-7038.12531"
    },
    {
        "id": 29772,
        "title": "Reinforcement Learning Algorithm for Mixed Mean Field Control Games",
        "authors": "Andrea Angiuli, Nils Detering, Jean-Pierre Fouque, Mathieu Laurière null, Jimin Lin",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4208/jml.220915"
    },
    {
        "id": 29773,
        "title": "Learning Synergies Between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning",
        "authors": "Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser",
        "published": "2018-10",
        "citations": 306,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros.2018.8593986"
    },
    {
        "id": 29774,
        "title": "Continuous Control for Autonomous Underwater Vehicle Path Following Using Deep Interactive Reinforcement Learning",
        "authors": "Qilei Zhang, Chunxi Cheng, Zheng Fang, Dong Jiang, Bo He, Guangliang Li",
        "published": "2022-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlcr57210.2022.00013"
    },
    {
        "id": 29775,
        "title": "Perturbational Complexity by Distribution Mismatch: A Systematic Analysis of Reinforcement Learning in Reproducing Kernel Hilbert Space",
        "authors": "Jihao Long null, Jiequn Han",
        "published": "2022-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4208/jml.220114"
    },
    {
        "id": 29776,
        "title": "NavREn-Rl: Learning to fly in real environment via end-to-end deep reinforcement learning using monocular images",
        "authors": "Malik Aqeel Anwar, Arijit Raychowdhury",
        "published": "2018-11",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/m2vip.2018.8600838"
    },
    {
        "id": 29777,
        "title": "Structure Learning-Based Task Decomposition for Reinforcement Learning in Non-stationary Environments",
        "authors": "Honguk Woo, Gwangpyo Yoo, Minjong Yoo",
        "published": "2022-6-28",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) agents empowered by deep neural networks have been considered a feasible solution to automate control functions in a cyber-physical system. \n In this work, we consider an RL-based agent and address the issue of learning via continual interaction with a time-varying dynamic system modeled as a non-stationary Markov decision process (MDP). \n We view such a non-stationary MDP as a time series of conventional MDPs that can be parameterized by hidden variables. To infer the hidden parameters, we present a task decomposition method that exploits CycleGAN-based structure learning. \n This method enables the separation of time-variant tasks from a non-stationary MDP, establishing the task decomposition embedding specific to time-varying information. \n To mitigate the adverse effect due to inherent noises of task embedding, we also leverage continual learning on sequential tasks by adapting the orthogonal gradient descent scheme with a sliding window.\n Through various experiments, we demonstrate that our approach renders the RL agent adaptable to time-varying dynamic environment conditions, outperforming other methods including state-of-the-art non-stationary MDP algorithms.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i8.20844"
    },
    {
        "id": 29778,
        "title": "Maximum Entropy Inverse Reinforcement Learning Based on Behavior Cloning of Expert Examples",
        "authors": "Dazi Li, Jianghai Du",
        "published": "2021-5-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls52934.2021.9455476"
    },
    {
        "id": 29779,
        "title": "An online prediction algorithm for reinforcement learning with linear function approximation using cross entropy method",
        "authors": "Ajin George Joseph, Shalabh Bhatnagar",
        "published": "2018-9",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-018-5727-z"
    },
    {
        "id": 29780,
        "title": "Learning Skills to Navigate without a Master: A Sequential Multi-Policy Reinforcement Learning Algorithm",
        "authors": "Ambedkar Dukkipati, Rajarshi Banerjee, Ranga Shaarad Ayyagari, Dhaval Parmar Udaybhai",
        "published": "2022-10-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros47612.2022.9981607"
    },
    {
        "id": 29781,
        "title": "Linking Learning Fundamental Reinforcement Learning Concepts with Being Physically Active",
        "authors": "Ramakrishna Sai Annaluru, Christine Julien, Jamie Payton",
        "published": "2022-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3545947.3576316"
    },
    {
        "id": 29782,
        "title": "Adaptive Learning: A New Decentralized Reinforcement Learning Approach for Cooperative Multiagent Systems",
        "authors": "Meng-Lin Li, Shaofei Chen, Jing Chen",
        "published": "2020",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2020.2997899"
    },
    {
        "id": 29783,
        "title": "A Transfer Learning Strategy for Improving the Data Efficiency of Deep Reinforcement Learning Control in Smart Buildings",
        "authors": "Kadir Amasyali, Yan Liu, Helia Zandi",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isgt59692.2024.10454120"
    },
    {
        "id": 29784,
        "title": "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks",
        "authors": "Vahid Behzadan, Arslan Munir",
        "published": "2017",
        "citations": 93,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-62416-7_19"
    },
    {
        "id": 29785,
        "title": "Learning to Calibrate Battery Models in Real-Time with Deep Reinforcement Learning",
        "authors": "Ajaykumar Unagar, Yuan Tian, Manuel Arias Chao, Olga Fink",
        "published": "2021-3-2",
        "citations": 17,
        "abstract": "Lithium-ion (Li-I) batteries have recently become pervasive and are used in many physical assets. For the effective management of the batteries, reliable predictions of the end-of-discharge (EOD) and end-of-life (EOL) are essential. Many detailed electrochemical models have been developed for the batteries. Their parameters are calibrated before they are taken into operation and are typically not re-calibrated during operation. However, the degradation of batteries increases the reality gap between the computational models and the physical systems and leads to inaccurate predictions of EOD/EOL. The current calibration approaches are either computationally expensive (model-based calibration) or require large amounts of ground truth data for degradation parameters (supervised data-driven calibration). This is often infeasible for many practical applications. In this paper, we introduce a reinforcement learning-based framework for reliably inferring calibration parameters of battery models in real time. Most importantly, the proposed methodology does not need any labeled data samples of observations and the ground truth parameters. The experimental results demonstrate that our framework is capable of inferring the model parameters in real time with better accuracy compared to approaches based on unscented Kalman filters. Furthermore, our results show better generalizability than supervised learning approaches even though our methodology does not rely on ground truth information during training.",
        "link": "http://dx.doi.org/10.3390/en14051361"
    },
    {
        "id": 29786,
        "title": "Deep sparse representation via deep dictionary learning for reinforcement learning",
        "authors": "Jianhao Tang, Zhenni Li, Shengli Xie, Shuxue Ding, Shaolong Zheng, Xueni Chen",
        "published": "2022-7-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9902583"
    },
    {
        "id": 29787,
        "title": "Planning for potential: efficient safe reinforcement learning",
        "authors": "Floris den Hengst, Vincent François-Lavet, Mark Hoogendoorn, Frank van Harmelen",
        "published": "2022-6",
        "citations": 1,
        "abstract": "AbstractDeep reinforcement learning (DRL) has shown remarkable success in artificial domains and in some real-world applications. However, substantial challenges remain such as learning efficiently under safety constraints. Adherence to safety constraints is a hard requirement in many high-impact application domains such as healthcare and finance. These constraints are preferably represented symbolically to ensure clear semantics at a suitable level of abstraction. Existing approaches to safe DRL assume that being unsafe leads to low rewards. We show that this is a special case of symbolically constrained RL and analyze a generic setting in which total reward and being safe may or may not be correlated. We analyze the impact of symbolic constraints and identify a connection between expected future reward and distance towards a goal in an automaton representation of the constraints. We use this connection in an algorithm for learning complex behaviors safely and efficiently. This algorithm relies on symbolic reasoning over safety constraints to improve the efficiency of a subsymbolic learner with a symbolically obtained measure of progress. We measure sample efficiency on a grid world and a conversational product recommender with real-world constraints. The so-called Planning for Potential algorithm converges quickly and significantly outperforms all baselines. Specifically, we find that symbolic reasoning is necessary for safety during and after learning and can be effectively used to guide a neural learner towards promising areas of the solution space. We conclude that RL can be applied both safely and efficiently when combined with symbolic reasoning.",
        "link": "http://dx.doi.org/10.1007/s10994-022-06143-6"
    },
    {
        "id": 29788,
        "title": "RIFLING: A reinforcement learning‐based GPU scheduler for deep learning research and development platforms",
        "authors": "Zhaoyun Chen",
        "published": "2022-6",
        "citations": 4,
        "abstract": "AbstractGPU platforms have been widely adopted in both academia and industry to support deep learning (DL) research and development (R&D). Compared with giant companies who favor custom‐designed AI platforms, most small‐and‐medium‐sized enterprises, institutes and universities (EIUs) prefer to build or rent a cost‐effective GPU cluster, usually in a limited‐scale, to process diverse DL R&D workloads. Therefore, more attention has been attracted by DL scheduling with the aim of improving the system efficiency and task performance. However, prior prediction‐based schedulers are limited in terms of their prediction accuracy and profiling overhead. Accordingly, in this article, we propose a reinforcement learning (RL)‐based online GPU scheduler, RIFLING, to model the scheduling problem as an online decision‐making process. Scheduling decisions are made according to Q‐learning, which is a typical RL method. RIFLING can achieve high scheduling efficiency based on the online exploring and exploiting of diverse scheduling strategies for various DL workloads, without the need for expensive offline profiling or sophisticated prediction model. We implement RIFLING as a plugin of Tensorflow, and deploy it on a distributed GPU cluster. Experiments demonstrate that RIFLING achieves up to 47.8% reductions and 19.6% improvements in makespan and average normalized processing rate respectively compared to the best available baseline without any manual intervention.",
        "link": "http://dx.doi.org/10.1002/spe.3066"
    },
    {
        "id": 29789,
        "title": "NEAT for large-scale reinforcement learning through evolutionary feature learning and policy gradient search",
        "authors": "Yiming Peng, Gang Chen, Harman Singh, Mengjie Zhang",
        "published": "2018-7-2",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3205455.3205536"
    },
    {
        "id": 29790,
        "title": "Reinforcement Learning with the Classical Q-Learning Algorithm for Optimizing Single Intersection Performance",
        "authors": "M. Rosyidi, Sahid Bismantoko, Tri Widodo",
        "published": "2020-10-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdabi51230.2020.9325657"
    },
    {
        "id": 29791,
        "title": "Efficient exploration by switching agents according to degree of convergence of learning on Heterogeneous Multi-Agent Reinforcement Learning in Single Robot",
        "authors": "Riku Narita, Tatsufumi Matsushima, Kentarou Kurashige",
        "published": "2021-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9659982"
    },
    {
        "id": 29792,
        "title": "Learning to Reweight Imaginary Transitions for Model-Based Reinforcement Learning",
        "authors": "Wenzhen Huang, Qiyue Yin, Junge Zhang, Kaiqi Huang",
        "published": "2021-5-18",
        "citations": 0,
        "abstract": "Model-based reinforcement learning (RL) is more sample efficient than model-free RL by using imaginary trajectories generated by the learned dynamics model. When the model is inaccurate or biased, imaginary trajectories may be deleterious for training the action-value and policy functions. To alleviate such problem, this paper proposes to adaptively reweight the imaginary transitions, so as to reduce the negative effects of poorly generated trajectories. More specifically, we evaluate the effect of an imaginary transition by calculating the change of the loss computed on the real samples when we use the transition to train the action-value and policy functions. Based on this evaluation criterion, we construct the idea of reweighting each imaginary transition by a well-designed meta-gradient algorithm. Extensive experimental results demonstrate that our method outperforms state-of-the-art model-based and model-free RL algorithms on multiple tasks. Visualization of our changing weights further validates the necessity of utilizing reweight scheme.",
        "link": "http://dx.doi.org/10.1609/aaai.v35i9.16958"
    },
    {
        "id": 29793,
        "title": "LILAC: Learning a Leader for Cooperative Reinforcement Learning",
        "authors": "Yuqian Fu, Jiajun Chai, Yuanheng Zhu, Dongbin Zhao",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog51982.2022.9893619"
    },
    {
        "id": 29794,
        "title": "A Structured Online Learning Approach to Nonlinear Tracking with Unknown Dynamics",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch6"
    },
    {
        "id": 29795,
        "title": "Generative Imitation Learning using Forward and Inverse Reinforcement Learning",
        "authors": "Eiji Uchibe",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7210/jrsj.39.617"
    },
    {
        "id": 29796,
        "title": "Two Dimensional (2D) Feedback Control Scheme Based on Deep Reinforcement Learning Algorithm for Nonlinear Non-repetitive Batch Processes",
        "authors": "Jianan Liu, Wenjing Hong, Jia Shi",
        "published": "2022-8-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls55054.2022.9858421"
    },
    {
        "id": 29797,
        "title": "DeepHaul: a deep learning and reinforcement learning-based smart automation framework for dump trucks",
        "authors": "Danish Ali, Samuel Frimpong",
        "published": "2021-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s13748-021-00233-7"
    },
    {
        "id": 29798,
        "title": "Learning from other minds: an optimistic critique of reinforcement learning models of social learning",
        "authors": "Natalia Vélez, Hyowon Gweon",
        "published": "2021-4",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cobeha.2021.01.006"
    },
    {
        "id": 29799,
        "title": "Deep Reinforcement One-Shot Learning for Change Point Detection",
        "authors": "Anton Puzanov, Kobi Cohen",
        "published": "2018-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/allerton.2018.8635928"
    },
    {
        "id": 29800,
        "title": "Security Service-aware Reinforcement Learning for Efficient Network Service Provisioning",
        "authors": "Hyeonjun Jo, Kyungbaek Kim",
        "published": "2022-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/apnoms56106.2022.9919928"
    },
    {
        "id": 29801,
        "title": "Deep Reinforcement One-Shot Learning for Change Point Detection",
        "authors": "Anton Puzanov, Kobi Cohen",
        "published": "2018-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/allerton.2018.8635928"
    },
    {
        "id": 29802,
        "title": "Security Service-aware Reinforcement Learning for Efficient Network Service Provisioning",
        "authors": "Hyeonjun Jo, Kyungbaek Kim",
        "published": "2022-9-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/apnoms56106.2022.9919928"
    },
    {
        "id": 29803,
        "title": "Universal Reinforcement Learning Algorithms: Survey and Experiments",
        "authors": "John Aslanides, Jan Leike, Marcus Hutter",
        "published": "2017-8",
        "citations": 4,
        "abstract": "Many state-of-the-art reinforcement learning (RL) algorithms typically assume that the environment is an ergodic Markov Decision Process (MDP). In contrast, the field of universal reinforcement learning (URL) is concerned with algorithms that make as few assumptions as possible about the environment. The universal Bayesian agent AIXI and a family of related URL algorithms have been developed in this setting. While numerous theoretical optimality results have been proven for these agents, there has been no empirical investigation of their behavior to date. We present a short and accessible survey of these URL algorithms under a unified notation and framework, along with results of some experiments that qualitatively illustrate some properties of the resulting policies, and their relative performance on partially-observable gridworld environments. We also present an open- source reference implementation of the algorithms which we hope will facilitate further understanding of, and experimentation with, these ideas.",
        "link": "http://dx.doi.org/10.24963/ijcai.2017/194"
    },
    {
        "id": 29804,
        "title": "Scalable Deep Reinforcement Learning for Ride-Hailing",
        "authors": "Jiekun Feng, Mark Gluzman, J. G. Dai",
        "published": "2021-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9483145"
    },
    {
        "id": 29805,
        "title": "Intra-RAN Online Distributed Reinforcement Learning For Uplink Power Control in 5G Cellular Networks",
        "authors": "Majid Butt, Jian Song, Jens Steiner, Klaus Pedersen, Istvan Kovacs",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Uplink power control plays a significant role in maintaining a good signal quality at the serving cell while minimizing interference to neighboring cells, thus maximizing the system performance. Traditionally, a single value open-loop power control (OLPC) parameter, P0, is configured for all the user equipments (UEs) in a cell, and often same setting is used for similar cells. Recent studies have demonstrated that optimal P0 depends on many factors, which yields a complex multidimensional optimization problem and there are no efficient approaches known to solve it under practical system-level settings. In this paper, we propose a solution based on reinforcement learning (RL) where each BS autonomously adjusts its P0 setting to maximize its throughput performance. As compared to conventional sub-optimal approach, our solution encompasses a smart clustering of UEs, where each cluster specifies its own P0. The proposed solution is evaluated by extensive system level simulations, where our results demonstrate a potential performance enhancement as compared to the baseline proposals.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19369046"
    },
    {
        "id": 29806,
        "title": "Data-Driven Robust Control Using Reinforcement Learning",
        "authors": "Phuong D. Ngo, Miguel Tejedor, Fred Godtliebsen",
        "published": "2022-2-21",
        "citations": 0,
        "abstract": "This paper proposes a robust control design method using reinforcement learning for controlling partially-unknown dynamical systems under uncertain conditions. The method extends the optimal reinforcement learning algorithm with a new learning technique based on the robust control theory. By learning from the data, the algorithm proposes actions that guarantee the stability of the closed-loop system within the uncertainties estimated also from the data. Control policies are calculated by solving a set of linear matrix inequalities. The controller was evaluated using simulations on a blood glucose model for patients with Type 1 diabetes. Simulation results show that the proposed methodology is capable of safely regulating the blood glucose within a healthy level under the influence of measurement and process noises. The controller has also significantly reduced the post-meal fluctuation of the blood glucose. A comparison between the proposed algorithm and the existing optimal reinforcement learning algorithm shows the improved robustness of the closed-loop system using our method.",
        "link": "http://dx.doi.org/10.3390/app12042262"
    },
    {
        "id": 29807,
        "title": "Admission Control for 5G Network Slicing based on (Deep) Reinforcement Learning",
        "authors": "William Fernando Villota Jácome, Oscar Mauricio Caicedo Rendon, Nelson Luis Saldanha da Fonseca",
        "published": "No Date",
        "citations": 0,
        "abstract": "Network Slicing is a promising technology for\nproviding customized logical and virtualized networks for the\nindustry’s vertical segments.This paper proposes SARA and DSARA for the performance of admission control and resource allocation for network slice requests of eMBB, URLLC, and MIoT type in the 5G core network. SARA introduced a Q-learning based algorithm and DSARA a DQN-based algorithm to select the most profitable requests from a set that arrived in given time windows. These algorithms are model-free, meaning they do not make assumptions about the substrate network as do optimization based approaches.",
        "link": "http://dx.doi.org/10.36227/techrxiv.14498190"
    },
    {
        "id": 29808,
        "title": "Faculty Opinions recommendation of Prefrontal cortex as a meta-reinforcement learning system.",
        "authors": "Daeyeol Lee",
        "published": "2018-7-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3410/f.733230343.793547713"
    },
    {
        "id": 29809,
        "title": "Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control",
        "authors": "Jiayu Chen, Tian Lan, Vaneet Aggarwal",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10160374"
    },
    {
        "id": 29810,
        "title": "Tracking Locomotion using Reinforcement Learning",
        "authors": "Rutuja Jadhav",
        "published": "2022-7-31",
        "citations": 0,
        "abstract": "Abstract: This article presents the concept of reinforcement learning, which prepares a static direct approach for consistent control problems, and adjusts cutting-edge techniques for testing effectiveness in benchmark Mujoco locomotion tasks. This model was designed and developed to use the Mujoco Engine to track the movement of robotic structures and eliminate problems with assessment calculations using perceptron’s and random search algorithms. Here, the machine learning model is trained to make a series of decisions. The humanoid model is considered to be one of the most difficult and ongoing problems to solve by applying state-of-the-art RL technology. The field of machine learning has a great influence on the training model of the RL environment. Here we use random seed values to provide continuous input to achieve optimized results. The goal of this project is to use the Mujoco engine in a specific context to automatically determine the ideal behavior of the robot in an augmented reality environment. Enhanced random search was introduced to train linear guidelines for achieving the efficiency of Mujoco roaming tasks. The results of these models highlight the variability of the Mujoco benchmark task and lead to efficiently optimized rewards",
        "link": "http://dx.doi.org/10.22214/ijraset.2022.45509"
    },
    {
        "id": 29811,
        "title": "Deep reinforcement learning framework for controlling infectious disease outbreaks in the context of multi-jurisdictions",
        "authors": "Seyedeh Nazanin Khatami, Chaitra Gopalappa",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractIn the absence of pharmaceutical interventions, social distancing and lockdown have been key options for controlling new or reemerging respiratory infectious disease outbreaks. The timely implementation of these interventions is vital for effectively controlling and safeguarding the economy.Motivated by the COVID-19 pandemic, we evaluated whether, when, and to what level lockdowns are necessary to minimize epidemic and economic burdens of new disease outbreaks. We formulated the question as a sequential decision-making Markov Decision Process and solved it using deep Q-network algorithm. We evaluated the question under two objective functions: a 2-objective function to minimize economic burden and hospital capacity violations, suitable for diseases with severe health risks but with minimal death, and a 3-objective function that additionally minimizes the number of deaths, suitable for diseases that have high risk of mortality. A key feature of the model is that we evaluated the above questions in the context of two-geographical jurisdictions that interact through travel but make autonomous and independent decisions, evaluating under cross-jurisdictional cooperation and non-cooperation.In the 2-objective function under cross-jurisdictional cooperation, the optimal policy was to aim for shutdowns at 50% and 25% per day. Though this policy avoided hospital capacity violations, the shutdowns extended until a large proportion of the population reached herd immunity. Delays in initiating this optimal policy or non-cooperation from an outside jurisdiction required shutdowns at a higher level of 75% per day, thus adding to economic burdens. In the 3-objective function, the optimal policy under cross-jurisdictional cooperation was to aim for shutdowns of up to 75% per day to prevent deaths by reducing infected cases. This optimal policy continued for the entire duration of the simulation, suggesting that, until pharmaceutical interventions such as treatment or vaccines become available, contact reductions through physical distancing would be necessary to minimize deaths. Deviating from this policy increased the number of shutdowns and led to several deaths.In summary, we present a decision-analytic methodology for identifying optimal lockdown strategy under the context of interactions between jurisdictions that make autonomous and independent decisions. The numerical analysis outcomes are intuitive and, as expected, serve as proof of the feasibility of such a model.",
        "link": "http://dx.doi.org/10.1101/2022.10.18.22281063"
    },
    {
        "id": 29812,
        "title": "The Harm and Countermeasures of Automobile Exhaust Pollution Based on Deep Reinforcement Learning",
        "authors": "",
        "published": "2021-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.38007/ajeb.2021.020405"
    },
    {
        "id": 29813,
        "title": "Digital Campus Financial Data Sharing Based on Distributed Reinforcement Learning Algorithm",
        "authors": "Yuejun Hu",
        "published": "2022-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aiars57204.2022.00019"
    },
    {
        "id": 29814,
        "title": "Design of Transfer Reinforcement Learning Mechanisms for Autonomous Collision Avoidance",
        "authors": "Xiongqing Liu, Yan Jin",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-05363-5_17"
    },
    {
        "id": 29815,
        "title": "Morphing Aircraft Adaptive Attitude Control Based on Deep Reinforcement Learning",
        "authors": "Shaojie Ma, Junpeng Hui, Yuhang Wang, Xuan Zhang",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10241099"
    },
    {
        "id": 29816,
        "title": "Replication Package for Article",
        "authors": "He Zhu, Zikang Xiong, Stephen Magill, Suresh Jagannathan",
        "published": "2019-4-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3325983"
    },
    {
        "id": 29817,
        "title": "Interpersonal trust modelling through multi-agent Reinforcement Learning",
        "authors": "Vincent Frey, Julian Martinez",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.cogsys.2023.101157"
    },
    {
        "id": 29818,
        "title": "Optimal Operation of Integrated Energy System Based on Deep Reinforcement Learning",
        "authors": "Fangchi Zhang, Fan Zhou, Jun Zhao, Wei Wang",
        "published": "2020-11-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac51589.2020.9326839"
    },
    {
        "id": 29819,
        "title": "A Comprehensive Survey on Security Attacks to Edge Server of IoT Devices through Reinforcement Learning",
        "authors": "Anit Kumar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4253769"
    },
    {
        "id": 29820,
        "title": "Multi-step reinforcement learning for medical image super-resolution",
        "authors": "Alix Bouffard, Mihaela Pop, Mehran Ebrahimi",
        "published": "2023-4-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2653655"
    },
    {
        "id": 29821,
        "title": "Phrase-Level Action Reinforcement Learning for Neural Dialog Response Generation",
        "authors": "Takato Yamazaki, Akiko Aizawa",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.findings-acl.446"
    },
    {
        "id": 29822,
        "title": "Deep reinforcement learning and automatic target recognition: synergies and opportunities",
        "authors": "Miguel Morales",
        "published": "2021-4-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2595804"
    },
    {
        "id": 29823,
        "title": "Galvanometer Motor Control Based on Reinforcement Learning",
        "authors": "Kainan Liu, Xiaoshi Cai, Xiaojun Ban, Jian Zhang",
        "published": "2022-9-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoias56028.2022.9931291"
    },
    {
        "id": 29824,
        "title": "A Hybrid MPC for Constrained Deep Reinforcement Learning applied for Planar Robotic Arm",
        "authors": "Mostafa Al-Gabalawy",
        "published": "2021-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.isatra.2021.03.046"
    },
    {
        "id": 29825,
        "title": "Reinforcement learning for dynamic optimization problems",
        "authors": "Abdennour Boulesnane, Souham Meshoul",
        "published": "2021-7-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3449726.3459543"
    },
    {
        "id": 29826,
        "title": "Neural Network Pruning Through Constrained Reinforcement Learning",
        "authors": "Shehryar Malik, Muhammad Umair Haider, Omer Iqbal, Murtaza Taj",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956050"
    },
    {
        "id": 29827,
        "title": "Reinforcement Learning Environment for Advanced Vehicular Ad Hoc Networks Communication Systems",
        "authors": "Lincoln Herbert Teixeira, Árpád Huszák",
        "published": "2022-6-23",
        "citations": 8,
        "abstract": "Ad hoc vehicular networks have been identified as a suitable technology for intelligent communication amongst smart city stakeholders as the intelligent transportation system has progressed. However, in a highly mobile area, the growing usage of wireless technologies creates a challenging context. To increase communication reliability in this environment, it is necessary to use intelligent tools to solve the routing problem to create a more stable communication system. Reinforcement Learning (RL) is an excellent tool to solve this problem. We propose creating a complex objective space with geo-positioning information of vehicles, propagation signal strength, and environmental path loss with obstacles (city map, with buildings) to train our model and get the best route based on route stability and hop number. The obtained results show significant improvement in the routes’ strength compared with traditional communication protocols and even with other RL tools when only one parameter is used for decision making.",
        "link": "http://dx.doi.org/10.3390/s22134732"
    },
    {
        "id": 29828,
        "title": "Experiment of reinforcement learning with extremum seeking",
        "authors": "Megumi Miyashita, Ryo Hirotani, Shiro Yano, Toshiyuki Kondo",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ict-ispc.2017.8075301"
    },
    {
        "id": 29829,
        "title": "Singular violations of transitivity disrupt inferred relational knowledge in humans and reinforcement learning models",
        "authors": "Thomas Graham, Bernhard Spitzer",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1279-0"
    },
    {
        "id": 29830,
        "title": "Brief Survey of Model-Based Reinforcement Learning Techniques",
        "authors": "Constantin-Valentin Pal, Florin Leon",
        "published": "2020-10-8",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icstcc50638.2020.9259716"
    },
    {
        "id": 29831,
        "title": "Reinforcement learning-based tracking control for AUVs subject to disturbances",
        "authors": "Guangcang Wang, Dianfeng Zhang, Zhaojing Wu",
        "published": "2022-8-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc55256.2022.10033544"
    },
    {
        "id": 29832,
        "title": "Research on Intelligent Optimization Algorithm of Arranging Based on Reinforcement Learning",
        "authors": "Fang He",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/netcit57419.2022.00087"
    },
    {
        "id": 29833,
        "title": "Push Recovery Control for Humanoid Robot Using Reinforcement Learning",
        "authors": "Harin Kim, Donghyeon Seo, Donghan Kim",
        "published": "2019-2",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/irc.2019.00102"
    },
    {
        "id": 29834,
        "title": "A Precision Advertising Strategy Based on Deep Reinforcement Learning",
        "authors": "Haiqing Liang",
        "published": "2020-6-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18280/isi.250316"
    },
    {
        "id": 29835,
        "title": "Reliable Off-Policy Evaluation for Reinforcement Learning",
        "authors": "Jie Wang, Rui Gao, Hongyuan Zha",
        "published": "2022-10-11",
        "citations": 0,
        "abstract": " Off-policy evaluation is an important topic in reinforcement learning, which estimates the expected cumulative reward of a target policy using logged trajectory data generated from a different behavior policy, without execution of the target policy. It is imperative to quantify the uncertainty of the off-policy estimate before deployment of the target policy. Here we leverage methodologies from (Wasserstein) distributionally robust optimization to provide robust and optimistic cumulative reward estimates. With proper selection of the size of the distributional uncertainty set, these estimates serve as confidence bounds with nonasymptotic and asymptotic guarantees under stochastic or adversarial environments. We also generalize those results to batch reinforcement learning. ",
        "link": "http://dx.doi.org/10.1287/opre.2022.2382"
    },
    {
        "id": 29836,
        "title": "Multi-rotor UAV reinforcement learning control modeling and algorithm design",
        "authors": "X. Xu, Z. Liao",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.2541"
    },
    {
        "id": 29837,
        "title": "Vision-based deep reinforcement learning to control a manipulator",
        "authors": "Wonchul Kim, Taewan Kim, Jonggu Lee, H. Jin Kim",
        "published": "2017-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ascc.2017.8287315"
    },
    {
        "id": 29838,
        "title": "Reinforcement-Learning Based Threshold Policies for Continuous Intraday Electricity Market Trading",
        "authors": "Gilles Bertrand, Anthony Papavasiliou",
        "published": "2019-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/pesgm40551.2019.8973602"
    },
    {
        "id": 29839,
        "title": "Localisation-Safe Reinforcement Learning for Mapless Navigation",
        "authors": "Feiqiang Lin, Ze Ji, Changyun Wei, Raphael Grech",
        "published": "2022-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio55434.2022.10011937"
    },
    {
        "id": 29840,
        "title": "Graph Reinforcement Learning-based CNN Inference Offloading in Dynamic Edge Computing",
        "authors": "Nan Li, Alexandros Iosifidis, Qi Zhang",
        "published": "2022-12-4",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom48099.2022.10001067"
    },
    {
        "id": 29841,
        "title": "Action Spaces in Deep Reinforcement Learning to Mimic Human Input Devices",
        "authors": "Marco Pleines, Frank Zimmer, Vincent-Pierre Berges",
        "published": "2019-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8848080"
    },
    {
        "id": 29842,
        "title": "Reinforcement-learning-based miniature UAV identification",
        "authors": "She Xiaoyu, Guan Zhenyu, Mao Ruizhi, Li Jie, Yang Chengwei",
        "published": "2017-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus.2017.8278347"
    },
    {
        "id": 29843,
        "title": "Adaptive exploration strategies for reinforcement learning",
        "authors": "Kao-Shing Hwang, Chih-Wen Li, Wei-Cheng Jiang",
        "published": "2017-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsse.2017.8030828"
    },
    {
        "id": 29844,
        "title": "Real-Time Parameter Identification for Forging Machine Using Reinforcement Learning",
        "authors": "Dapeng Zhang, Lifeng Du, Zhiwei Gao",
        "published": "2021-10-18",
        "citations": 3,
        "abstract": "It is a challenge to identify the parameters of a mechanism model under real-time operating conditions disrupted by uncertain disturbances due to the deviation between the design requirement and the operational environment. In this paper, a novel approach based on reinforcement learning is proposed for forging machines to achieve the optimal model parameters by applying the raw data directly instead of observation window. This approach is an online parameter identification algorithm in one period without the need of the labelled samples as training database. It has an excellent ability against unknown distributed disturbances in a dynamic process, especially capable of adapting to a new process without historical data. The effectiveness of the algorithm is demonstrated and validated by a simulation of acquiring the parameter values of a forging machine.",
        "link": "http://dx.doi.org/10.3390/pr9101848"
    },
    {
        "id": 29845,
        "title": "Massive MIMO Power Allocation Using Deep Reinforcement Learning",
        "authors": "Huynh Vu Hoang Phuc, Ha Hoang Kha",
        "published": "2023-10-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isee59483.2023.10299873"
    },
    {
        "id": 29846,
        "title": "Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective",
        "authors": "Nelson Vadori",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3604237.3626837"
    },
    {
        "id": 29847,
        "title": "Integral Reinforcement Learning-Based Angular Acceleration Autopilot for High Dynamic Flight Vehicles",
        "authors": "Yingxin Liu, Yuhui Hu, Kai Shen, Jiatai Qiu, Konstantin A. Neusypin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4646396"
    },
    {
        "id": 29848,
        "title": "Leveraging Lstm and Reinforcement Learning to Enhance Adaptive Sensing in Multi-Sensing Nodes",
        "authors": "Sushmita Ghosh, Siamak Layeghy, Swades De, Shouri Chatterjee, Marius Portmann",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4666282"
    },
    {
        "id": 29849,
        "title": "Deep Reinforcement Learning Based Virtual Network Embedding for 6G Satellite Networks",
        "authors": "Ruijie Zhu, Gong Li, Peisen Wang, Junling Yuan",
        "published": "2021",
        "citations": 1,
        "abstract": "We establish a satellite network and propose a deep reinforcement learning based virtual network embedding (DRVE-SN) algorithm. Simulation results show that it performs better than the state-of-art algorithm in blocking probability and average resource utilization.",
        "link": "http://dx.doi.org/10.1364/oecc.2021.js2a.16"
    },
    {
        "id": 29850,
        "title": "Reinforcement Learning-Driven Continuous and Crashless Load Test Architecture",
        "authors": "Tolga Buyuktanir, Batiray Erbay, Mert Altun",
        "published": "2022-5-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu55565.2022.9864834"
    },
    {
        "id": 29851,
        "title": "Curriculum Offline Reinforcement Learning with Progressive Action Space in Intelligent Healthcare Decision-Making",
        "authors": "Chao Yu, Qikai Huang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4167820"
    },
    {
        "id": 29852,
        "title": "Dynamic Pricing Based on Demand Response Using Actor–Critic Agent Reinforcement Learning",
        "authors": "Ahmed Ismail, Mustafa Baysal",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "Eco-friendly technologies for sustainable energy development require the efficient utilization of energy resources. Real-time pricing (RTP), also known as dynamic pricing, offers advantages over other pricing systems by enabling demand response (DR) actions. However, existing methods for determining and controlling DR have limitations in managing an increasing demand and predicting future pricing. This paper presents a novel approach to address the limitations of existing methods for determining and controlling demand response (DR) in the context of dynamic pricing systems for sustainable energy development. By leveraging actor–critic agent reinforcement learning (RL) techniques, a dynamic pricing DR model is proposed for efficient energy management. The model’s learning framework was trained using DR and real-time pricing data extracted from the Australian Energy Market Operator (AEMO) spanning a period of 17 years. The efficacy of the RL-based dynamic pricing approach was evaluated through two predicting cases: actual-predicted demand and actual-predicted price. Initially, long short-term memory (LSTM) models were employed to predict price and demand, and the results were subsequently enhanced using the deep RL model. Remarkably, the proposed approach achieved an impressive accuracy of 99% for every 30 min future price prediction. The results demonstrated the efficiency of the proposed RL-based model in accurately predicting both demand and price for effective energy management.",
        "link": "http://dx.doi.org/10.3390/en16145469"
    },
    {
        "id": 29853,
        "title": "Velocity Regulation for Automatic Train Operation via Meta-Reinforcement Learning",
        "authors": "Feiran Zhao, Keyou You, Yunxin Fan, Gang Yan",
        "published": "2020-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc50068.2020.9188581"
    },
    {
        "id": 29854,
        "title": "Notice of Removal: Organizational Resource Scheduling using Deep Reinforcement Learning",
        "authors": "Lihi Idan",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dsaa60987.2023.10302505"
    },
    {
        "id": 29855,
        "title": "Intra-RAN Online Distributed Reinforcement Learning For Uplink Power Control in 5G Cellular Networks",
        "authors": "Majid Butt, Jian Song, Jens Steiner, Klaus Pedersen, Istvan Kovacs",
        "published": "No Date",
        "citations": 0,
        "abstract": "<div>Uplink power control plays a significant role in maintaining a good signal quality at the serving cell while minimizing interference to neighboring cells, thus maximizing the system performance. Traditionally, a single value open-loop power control (OLPC) parameter, P0, is configured for all the user equipments (UEs) in a cell, and often same setting is used for similar cells. Recent studies have demonstrated that optimal P0 depends on many factors, which yields a complex multidimensional optimization problem and there are no efficient approaches known to solve it under practical system-level settings. In this paper, we propose a solution based on reinforcement learning (RL) where each BS autonomously adjusts its P0 setting to maximize its throughput performance. As compared to conventional sub-optimal approach, our solution encompasses a smart clustering of UEs, where each cluster specifies its own P0. The proposed solution is evaluated by extensive system level simulations, where our results demonstrate a potential performance enhancement as compared to the baseline proposals.</div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19369046.v1"
    },
    {
        "id": 29856,
        "title": "Tracking control for networked control systems with DoS attacks via reinforcement learning method",
        "authors": "Jinliang Liu, Yanhui Dong, Lijuan Zha, Xiangpeng Xie, Engang Tian",
        "published": "No Date",
        "citations": 0,
        "abstract": "This paper is concerned with the tracking control problem for a class of\nnetworked systems subject to denial-of-service (DoS) attacks using\nreinforcement learning methods. Taking the effects of DoS attacks into\nconsideration, a novel value function is proposed, which considers the\ncost of the control input, external disturbance and tracking error.\nThen, using the structure of the value function, the tracking Bellman\nequation and Hamilton function are defined. By employing the Bellman\noptimality theory, the optimal control strategy and the game algebraic\nRiccati equation (GARE) are solved with the Hamilton function. Next, the\ndesired tracking performance is guaranteed as the solution of the GARE\nis found. Furthermore, an attacks-based Q-learning algorithm is\nprojected to find the solution to the optimal tracking problem without\nthe system dynamics and the convergence of the Q-learning algorithm is\ngiven. Finally, the F-404 aircraft engine system is given to verify the\neffectiveness of the proposed control strategy.",
        "link": "http://dx.doi.org/10.22541/au.168172563.35721111/v1"
    },
    {
        "id": 29857,
        "title": "Navigation of Ground Robots with Reinforcement Learning",
        "authors": "Sharanya S, Divya Radhakrishna Varma, Ajay Paul",
        "published": "2023-7-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/acirs58671.2023.10240285"
    },
    {
        "id": 29858,
        "title": "Fault Localization for Reinforcement Learning",
        "authors": "Jesús Morán, Antonia Bertolino, Claudio De La Riva, Javier Tuya",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aitest58265.2023.00016"
    },
    {
        "id": 29859,
        "title": "Leveraging Graph Networks to Model Environments in Reinforcement Learning",
        "authors": "Viswanath Chadalapaka, Volkan Ustun, Lixing Liu",
        "published": "2023-5-8",
        "citations": 1,
        "abstract": "This paper proposes leveraging graph neural networks (GNNs) to model an agent’s environment to construct superior policy networks in reinforcement learning (RL). To this end, we explore the effects of different combinations of GNNs and graph network pooling functions on policy performance. We also run experiments at different levels of problem complexity, which affect how easily we expect an agent to learn an optimal policy and therefore show whether or not graph networks are effective at various problem complexity levels. The efficacy of our approach is shown via experimentation in a partially-observable, non-stationary environment that parallels the highly-practical scenario of a military training exercise with human trainees, where the learning goal is to become the best sparring partner possible for human trainees. Our results present that our models can generate better-performing sparring partners by employing GNNs, as demonstrated by these experiments in the proof-of-concept environment. We also explore our model’s applicability in Multi-Agent RL scenarios. Our code is available online at https://github.com/Derposoft/GNNsAsEnvs.",
        "link": "http://dx.doi.org/10.32473/flairs.36.133118"
    },
    {
        "id": 29860,
        "title": "A Novel Deep Reinforcement Learning Model for Resilient Road Network Recovery from Multiple Hazards",
        "authors": "Xudong Fan, Xijin Zhang, Xiaowei Wang, Xiong Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAs the backbone and the ‘blood vessel’ of modern cities, road networks provide critical support for community activities and economic growth, with their roles even more crucial due to the dramatic progress in urbanization. The service of road networks is subjected to the increasing frequency of high-consequence natural hazards such as earthquakes, floods, hurricanes, etc. Identifying resilient restoration sequences is essential to mitigate the disruption of such important infrastructure networks. This paper investigates a novel decision-support model to optimize post-disaster road network repair sequence. The model, named as GCN-DRL model, integrates the advantages of deep reinforced learning (DRL) with graph convolutional neural network (GCN), two emerging artificial intelligence (AI) techniques to achieve efficient recovery of road network service. The model is applied to analyze two cases of community road networks in the US that are subjected to different types of hazards, i.e., earthquakes and flooding. The performance of repair sequence by the GCN-DRL model is compared with two commonly used methods, i.e., repair sequence by the genetic algorithm and by prioritization based on graph importance with betweenness centrality. The results showed the decision sequence by GCN-DRL model consistently achieved superior performance in road network restoration than the conventional methods. The AI-based decision model also features high computational efficiency since the GCN-DRL model can be trained before the hazard. With a pre-trained GCN-DRL model, a close to optimal decision-making process can be made available rapidly for different types of new hazards, which is advantageous in efficiently responding to hazards when they happen. This study demonstrates the promise of a new AI-based decision support model to improve the resilience of road networks by enabling efficient post-hazards recovery.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2052084/v1"
    },
    {
        "id": 29861,
        "title": "Bias Correction in Reinforcement Learning via the Deterministic Policy Gradient Method for MPC-Based Policies",
        "authors": "Sebastien Gros, Mario Zanon",
        "published": "2021-5-25",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9483016"
    },
    {
        "id": 29862,
        "title": "Production flow control through the use of reinforcement learning",
        "authors": "Tomé Silva, Américo Azevedo",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.promfg.2020.01.026"
    },
    {
        "id": 29863,
        "title": "On Simple Reactive Neural Networks for Behaviour-Based Reinforcement Learning",
        "authors": "Ameya Pore, Gerardo Aragon-Camarasa",
        "published": "2020-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra40945.2020.9197262"
    },
    {
        "id": 29864,
        "title": "Research on Machine Translation (MT) System Based on Deep Reinforcement Learning",
        "authors": "Junchen He",
        "published": "2022-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccsmt58129.2022.00108"
    },
    {
        "id": 29865,
        "title": "Choosing the Right Method",
        "authors": "Ken Ramirez",
        "published": "2020-1-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781118968543.ch4"
    },
    {
        "id": 29866,
        "title": "A Reinforcement Learning based End-to-End Algorithm for Confrontation Problem",
        "authors": "Siqiang WANG, Haodi YAO, Yu YAO, Fenghua HE",
        "published": "2019-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8866337"
    },
    {
        "id": 29867,
        "title": "Deep Reinforcement Learning for Wireless Resource Allocation Using Buffer State Information",
        "authors": "Eike-Manuel Bansbach, Victor Eliachevitch, Laurent Schmalen",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom46510.2021.9685702"
    },
    {
        "id": 29868,
        "title": "Reinforcement Learning Versus Model Predictive Control on Greenhouse Climate Control",
        "authors": "Bernardo Morcego, Wenjie Yin, Sjoerd Boersma, Eldert  van Henten, Vicenç Puig, congcong sun",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4525429"
    },
    {
        "id": 29869,
        "title": "CVA Hedging by Risk-Averse Stochastic-Horizon Reinforcement Learning",
        "authors": "Roberto Daluiso, Marco Pinciroli, Michele Trapletti, Edoardo Vittori",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4673150"
    },
    {
        "id": 29870,
        "title": "Deep Reinforcement Learning (DRL) for Real-Time Traffic Management in Smart Cities",
        "authors": "Dhiraj Singh",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccsai59793.2023.10421359"
    },
    {
        "id": 29871,
        "title": "Reinforcement Learning Based Joint Allocation Scheme in a TWDM-PON Based mMIMO Fronthaul Network",
        "authors": "Yuansen Cheng, Chun-Kit Chan",
        "published": "2021",
        "citations": 1,
        "abstract": "We propose a reinforcement-learning based joint allocation algorithm for TWDM-PON based mMIMO fronthaul network. By adopting the combined Pointer Network and Actor Critic algorithm, we realize superior performance in resource block and wavelength utilization efficiencies.",
        "link": "http://dx.doi.org/10.1364/oecc.2021.t2a.2"
    },
    {
        "id": 29872,
        "title": "Additive Manufacturing Process Parameter Design for Variable Component Geometries Using Reinforcement Learning",
        "authors": "Ehsan Vaghefi, Seyemehrab Hosseini, Amir Hosseini Afsharinejad, B.C. Prorok, Elham Mirkoohi",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4698273"
    },
    {
        "id": 29873,
        "title": "3D Network-On-Chip Data Acquisition System Mapping Based on Reinforcement Learning and Improved Attention Mechanism",
        "authors": "Chuanpei Xu, Xiuli Shi, Yang Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe 3D Network-on-Chip (NoC) data acquisition system utilizes NoC technology to establish a time-interleaved data acquisition system The mapping scheme determines the location of each Intellectual Property (IP) node in the NoC topology. The optimization of the mapping algorithm is one of the important means to reduce the communication delay of the acquisition system. The abundance of functional IP nodes in the 3D NoC data acquisition system creates a mapping challenge. To address this, we propose a mapping algorithm called Reinforcement Learning and an improved Attention Mechanism Mapping algorithm (RA-Map). The RA-Map mapping algorithm employs node function encoding and node position encoding to express the properties of an IP node in the task graph preprocessing. The local attention mechanism is used in the mapping network encoder, and the fusion of dynamic key node information is proposed in the decoder. The mapping result evaluation network achieves unsupervised training of the mapping network. These targeted improvements ultimately lead to an enhancement in mapping quality. Experimental results demonstrate that when compared to the discrete particle swarm algorithm and simulated annealing algorithm, the RA-Map mapping algorithm reduces the average communication cost by 6.5% and 8.5%, respectively. Furthermore, while ensuring mapping quality, it also shortens the mapping time.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3142584/v1"
    },
    {
        "id": 29874,
        "title": "Viznav: A Modular Off-Policy Deep Reinforcement Learning Framework for Vision-Based Autonomous Uav Navigation in 3d Dynamic Environments",
        "authors": "Fadi AlMahamid, Katarina Grolinger",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4573189"
    },
    {
        "id": 29875,
        "title": "Market Making with Deep Reinforcement Learning from Limit Order Books",
        "authors": "Hong Guo, Jianwu Lin, Fanlin Huang",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191123"
    },
    {
        "id": 29876,
        "title": "Multi-day Residential EV Charging Strategy Using Reinforcement Learning",
        "authors": "Dominic Goh, Peter Sokolowski, Mahdi Jalili",
        "published": "2021-6-20",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isie45552.2021.9576168"
    },
    {
        "id": 29877,
        "title": "Reinforcement-Learning designs droplet microfluidic networks",
        "authors": "Mohammad Shahab, Raghunathan Rengaswamy",
        "published": "2022-5",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.compchemeng.2022.107787"
    },
    {
        "id": 29878,
        "title": "Optimization of a Robust Reinforcement Learning Policy",
        "authors": "Bilkan Ince, Hyo-Sang Shin, Antonios Tsourdos",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2023-0967"
    },
    {
        "id": 29879,
        "title": "Spontaneous emergence of eyes in reinforcement learning agents",
        "authors": "Dianjing Liu, Boyuan Liu, Ming Zhou, Yurui Qu, Zhicheng Wu, Qingyi Zhou, Zongfu Yu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nA living animal exhibits remarkable ability to survive. It processes sensory input and takes actions to maximize the likelihood of survival. Researchers have been inspired to develop similar artificial agents powered by reinforcement learning—for instance, the Deep-Q learning agent, which learns to play Atari arcade games. In the recent development, the ability to process high-dimensional raw sensory data such as images, instead of handcrafted features, is one of the most important enablers, making it possible to train agents for different applications at scale. However, these agents are still different from fully autonomous agents such as living beings who not only process raw sensory data but also develop sensory function as part of their learning process. In this article, we show that an artificial agent powered by reinforcement learning can also spontaneously develop sensory apparatus. It can build its own bridge to connect the digital world to the physical one. This capability could be used to develop resilient agents that are adaptive in changing environments.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2391898/v1"
    },
    {
        "id": 29880,
        "title": "Reinforcement Learning based End-to-End Control of Bimanual Robotic Coordination",
        "authors": "Boran Wang, Yue Xiao",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsai61474.2023.10423358"
    },
    {
        "id": 29881,
        "title": "Deep Reinforcement Learning in M2M Communication for Resource Scheduling",
        "authors": "Yichen Che, Fei Lin, Jiemei Liu",
        "published": "2021-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wccct52091.2021.00025"
    },
    {
        "id": 29882,
        "title": "Reinforcement Learning Scheduler for Vehicle-to-Vehicle Communications Outside Coverage",
        "authors": "Taylan Sahin, Ramin Khalili, Mate Boban, Adam Wolisz",
        "published": "2018-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vnc.2018.8628366"
    },
    {
        "id": 29883,
        "title": "Distributed Online Service Coordination Using Deep Reinforcement Learning",
        "authors": "Stefan Schneider, Haydar Qarawlus, Holger Karl",
        "published": "2021-7",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdcs51616.2021.00058"
    },
    {
        "id": 29884,
        "title": "Reinforcement Learning for Energy Optimization with 5G Communications in Vehicular Social Networks",
        "authors": "Hyebin Park, Yujin Lim",
        "published": "2020-4-21",
        "citations": 23,
        "abstract": "Increased data traffic resulting from the increase in the deployment of connected vehicles has become relevant in vehicular social networks (VSNs). To provide efficient communication between connected vehicles, researchers have studied device-to-device (D2D) communication. D2D communication not only reduces the energy consumption and loads of the system but also increases the system capacity by reusing cellular resources. However, D2D communication is highly affected by interference and therefore requires interference-management techniques, such as mode selection and power control. To make an optimal mode selection and power control, it is necessary to apply reinforcement learning that considers a variety of factors. In this paper, we propose a reinforcement-learning technique for energy optimization with fifth-generation communication in VSNs. To achieve energy optimization, we use centralized Q-learning in the system and distributed Q-learning in the vehicles. The proposed algorithm learns to maximize the energy efficiency of the system by adjusting the minimum signal-to-interference plus noise ratio to guarantee the outage probability. Simulations were performed to compare the performance of the proposed algorithm with that of the existing mode-selection and power-control algorithms. The proposed algorithm performed the best in terms of system energy efficiency and achievable data rate.",
        "link": "http://dx.doi.org/10.3390/s20082361"
    },
    {
        "id": 29885,
        "title": "Within and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory",
        "authors": "Anne GE Collins, Michael J Frank",
        "published": "No Date",
        "citations": 4,
        "abstract": "AbstractLearning from rewards and punishments is essential to survival, and facilitates flexible human behavior. It is widely appreciated that multiple cognitive and reinforcement learning systems contribute to behavior, but the nature of their interactions is elusive. Here, we leverage novel methods for extracting trial-by-trial indices of reinforcement learning (RL) and working memory (WM) in human electroencephalography to reveal single trial computations beyond that afforded by behavior alone. Within-trial dynamics confirmed that increases in neural expectation were predictive of reduced neural surprise in the following feedback period, supporting central tenets of RL models. Cross-trial dynamics revealed a cooperative interplay between systems for learning, in which WM contributes expectations to guide RL, despite competition between systems during choice. Together, these results provide a deeper understanding of how multiple neural systems interact for learning and decision making, and facilitate analysis of their disruption in clinical populations.One sentence summaryDecoding of dynamical neural signals in humans reveals cooperation between cognitive and habit learning systems.",
        "link": "http://dx.doi.org/10.1101/184812"
    },
    {
        "id": 29886,
        "title": "Hippocampal pattern separation supports reinforcement learning",
        "authors": "Ian C. Ballard, Anthony D. Wagner, Samuel M. McClure",
        "published": "2019-3-6",
        "citations": 33,
        "abstract": "AbstractAnimals rely on learned associations to make decisions. Associations can be based on relationships between object features (e.g., the three leaflets of poison ivy leaves) and outcomes (e.g., rash). More often, outcomes are linked to multidimensional states (e.g., poison ivy is green in summer but red in spring). Feature-based reinforcement learning fails when the values of individual features depend on the other features present. One solution is to assign value to multi-featural conjunctive representations. Here, we test if the hippocampus forms separable conjunctive representations that enables the learning of response contingencies for stimuli of the form: AB+, B−, AC−, C+. Pattern analyses on functional MRI data show the hippocampus forms conjunctive representations that are dissociable from feature components and that these representations, along with those of cortex, influence striatal prediction errors. Our results establish a novel role for hippocampal pattern separation and conjunctive representation in reinforcement learning.",
        "link": "http://dx.doi.org/10.1038/s41467-019-08998-1"
    },
    {
        "id": 29887,
        "title": "Off-Policy Reinforcement Learning for Optimal Control of a Two Wheeled Self Balancing Robot",
        "authors": "Athira Mullachery, Shaikshavali Chitraganti",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10441833"
    },
    {
        "id": 29888,
        "title": "An Online Crowd Semantic Segmentation Method Based on Reinforcement Learning",
        "authors": "Yu Cheng, Hua Yang, Lin Chen",
        "published": "2019-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2019.8803324"
    },
    {
        "id": 29889,
        "title": "Cooperative reinforcement learning based throughput optimization in energy harvesting wireless sensor networks",
        "authors": "Yin Wu, Kun Yang",
        "published": "2018-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wocc.2018.8372691"
    },
    {
        "id": 29890,
        "title": "Deep Reinforcement Learning-Based Network Slicing Algorithm for 5G Heterogenous Services",
        "authors": "George Alkhoury, Sara Berri, Arsenia Chorti",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436893"
    },
    {
        "id": 29891,
        "title": "Towards Efficient Workflow Scheduling Over Yarn Cluster Using Deep Reinforcement Learning",
        "authors": "Jianguo Xue, Ting Wang, Puyu Cai",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436820"
    },
    {
        "id": 29892,
        "title": "DQN Reinforcement Learning-based Steering Control Strategy for Autonomous Driving",
        "authors": "",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3901/jme.2023.16.315"
    },
    {
        "id": 29893,
        "title": "Deep reinforcement learning identifies personalized intermittent androgen deprivation therapy for prostate cancer",
        "authors": "Yitao Lu, Qian Chu, Zhen Li, Mengdi Wang, Qingpeng Zhang",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nThe evolution of drug resistance leads to treatment failure and tumor progression. Intermittent androgen deprivation therapy (IADT) helps responsive cancer cells compete with resistant cancer cells in intratumoral competition. However, conventional IADT is population-based and ignores the heterogeneous phenotypes of individual patients. To address this challenge, we developed a time-varied, mixed-effect, and generative Lotka-Volterra (tM-GLV) model to account for the heterogeneity of the evolution mechanism and the pharmacokinetics of individual patients. Then, we proposed a reinforcement learningenabled individualized IADT framework, namely, I2ADT, to learn the patient-specific tumor dynamics and derive the optimal drug administration policy. Experiments with clinical trial data demonstrated that the proposed I2ADT can significantly prolong the time to progression of prostate cancer patients with reduced cumulative drug dosage. This research elucidates the application of reinforcement learning techniques to identify personalized adaptive cancer therapy.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1573462/v1"
    },
    {
        "id": 29894,
        "title": "A Novel Deep Reinforcement Learning Based Dynamic Load Balancing Mechanism in Datacenter Networks",
        "authors": "Jin Wang, Wangqing Luo, Yi He, Shuying Rao, Jinbin Hu",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4327073"
    },
    {
        "id": 29895,
        "title": "Controlling the Risk of Conversational Search via Reinforcement Learning",
        "authors": "Zhenduo Wang, Qingyao Ai",
        "published": "2021-4-19",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3442381.3449893"
    },
    {
        "id": 29896,
        "title": "Deep Reinforcement Learning Based Coalition Formation for Energy Trading in Smart Grid",
        "authors": "Mohammad Sadeghi, Melike Erol-Kantarci",
        "published": "2021-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/5gwf52925.2021.00042"
    },
    {
        "id": 29897,
        "title": "Finding Effective Security Strategies through Reinforcement Learning and Self-Play",
        "authors": "Kim Hammar, Rolf Stadler",
        "published": "2020-11-2",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/cnsm50824.2020.9269092"
    },
    {
        "id": 29898,
        "title": "Human-Aware Reinforcement Learning for Adaptive Human Robot Teaming",
        "authors": "Saurav Singh, Jamison Heard",
        "published": "2022-3-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/hri53351.2022.9889530"
    },
    {
        "id": 29899,
        "title": "Shielded Planning Guided Data-Efficient and Safe Reinforcement Learning",
        "authors": "Hao Wang, Jiahu Qin, Zhen Kan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2024.3359031"
    },
    {
        "id": 29900,
        "title": "The Importance of Interpretability in AI Systems and Its Implications for Deep Learning",
        "authors": "Muhammad Adnan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Particularly inside the context of deep learning, the concept of interpretability in artificial intelligence systems is crucial for boosting the degree of trust and self-belief that human beings have in machine-learning fashions. Deep learning models have many parameters and complex architectures that make them function like mysterious “black boxes,” making it difficult for users to apprehend how they function. This opacity increases questions about those models' ethics, dependability, and viable biases. In the field of deep learning, achieving interpretability is crucial for several reasons. First off, interpretable models enhance transparency by making the model's judgments and forecasts simpler for customers to understand. This is particularly essential in complicated fields like banking and healthcare, wherein knowledge and self-assurance are vital. Moreover, interpretability facilitates the identification and correction of biases in the model or the training statistics, performing as a car for fairness and duty.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch003"
    },
    {
        "id": 29901,
        "title": "The Importance of Interpretability in AI Systems and Its Implications for Deep Learning",
        "authors": "Muhammad Adnan",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Particularly inside the context of deep learning, the concept of interpretability in artificial intelligence systems is crucial for boosting the degree of trust and self-belief that human beings have in machine-learning fashions. Deep learning models have many parameters and complex architectures that make them function like mysterious “black boxes,” making it difficult for users to apprehend how they function. This opacity increases questions about those models' ethics, dependability, and viable biases. In the field of deep learning, achieving interpretability is crucial for several reasons. First off, interpretable models enhance transparency by making the model's judgments and forecasts simpler for customers to understand. This is particularly essential in complicated fields like banking and healthcare, wherein knowledge and self-assurance are vital. Moreover, interpretability facilitates the identification and correction of biases in the model or the training statistics, performing as a car for fairness and duty.",
        "link": "http://dx.doi.org/10.4018/979-8-3693-1738-9.ch003"
    },
    {
        "id": 29902,
        "title": "Special Issue on Deep Reinforcement Learning and Adaptive Dynamic Programming",
        "authors": "Dongbin Zhao, Derong Liu, F. L. Lewis, Jose C. Principe, Stefano Squartini",
        "published": "2018-6",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2018.2818878"
    },
    {
        "id": 29903,
        "title": "Plug-and-Play Model-Agnostic Counterfactual Policy Synthesis for Deep Reinforcement Learning-Based Recommendation",
        "authors": "Siyu Wang, Xiaocong Chen, Julian McAuley, Sally Cripps, Lina Yao",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3329808"
    },
    {
        "id": 29904,
        "title": "Hierarchical Deep Reinforcement Learning for Continuous Action Control",
        "authors": "Zhaoyang Yang, Kathryn Merrick, Lianwen Jin, Hussein A. Abbass",
        "published": "2018-11",
        "citations": 114,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2018.2805379"
    },
    {
        "id": 29905,
        "title": "Deep sparse representation via deep dictionary learning for reinforcement learning",
        "authors": "Jianhao Tang, Zhenni Li, Shengli Xie, Shuxue Ding, Shaolong Zheng, Xueni Chen",
        "published": "2022-7-25",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ccc55666.2022.9902583"
    },
    {
        "id": 29906,
        "title": "Planning for potential: efficient safe reinforcement learning",
        "authors": "Floris den Hengst, Vincent François-Lavet, Mark Hoogendoorn, Frank van Harmelen",
        "published": "2022-6",
        "citations": 1,
        "abstract": "AbstractDeep reinforcement learning (DRL) has shown remarkable success in artificial domains and in some real-world applications. However, substantial challenges remain such as learning efficiently under safety constraints. Adherence to safety constraints is a hard requirement in many high-impact application domains such as healthcare and finance. These constraints are preferably represented symbolically to ensure clear semantics at a suitable level of abstraction. Existing approaches to safe DRL assume that being unsafe leads to low rewards. We show that this is a special case of symbolically constrained RL and analyze a generic setting in which total reward and being safe may or may not be correlated. We analyze the impact of symbolic constraints and identify a connection between expected future reward and distance towards a goal in an automaton representation of the constraints. We use this connection in an algorithm for learning complex behaviors safely and efficiently. This algorithm relies on symbolic reasoning over safety constraints to improve the efficiency of a subsymbolic learner with a symbolically obtained measure of progress. We measure sample efficiency on a grid world and a conversational product recommender with real-world constraints. The so-called Planning for Potential algorithm converges quickly and significantly outperforms all baselines. Specifically, we find that symbolic reasoning is necessary for safety during and after learning and can be effectively used to guide a neural learner towards promising areas of the solution space. We conclude that RL can be applied both safely and efficiently when combined with symbolic reasoning.",
        "link": "http://dx.doi.org/10.1007/s10994-022-06143-6"
    },
    {
        "id": 29907,
        "title": "Two Dimensional (2D) Feedback Control Scheme Based on Deep Reinforcement Learning Algorithm for Nonlinear Non-repetitive Batch Processes",
        "authors": "Jianan Liu, Wenjing Hong, Jia Shi",
        "published": "2022-8-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls55054.2022.9858421"
    },
    {
        "id": 29908,
        "title": "RIFLING: A reinforcement learning‐based GPU scheduler for deep learning research and development platforms",
        "authors": "Zhaoyun Chen",
        "published": "2022-6",
        "citations": 4,
        "abstract": "AbstractGPU platforms have been widely adopted in both academia and industry to support deep learning (DL) research and development (R&D). Compared with giant companies who favor custom‐designed AI platforms, most small‐and‐medium‐sized enterprises, institutes and universities (EIUs) prefer to build or rent a cost‐effective GPU cluster, usually in a limited‐scale, to process diverse DL R&D workloads. Therefore, more attention has been attracted by DL scheduling with the aim of improving the system efficiency and task performance. However, prior prediction‐based schedulers are limited in terms of their prediction accuracy and profiling overhead. Accordingly, in this article, we propose a reinforcement learning (RL)‐based online GPU scheduler, RIFLING, to model the scheduling problem as an online decision‐making process. Scheduling decisions are made according to Q‐learning, which is a typical RL method. RIFLING can achieve high scheduling efficiency based on the online exploring and exploiting of diverse scheduling strategies for various DL workloads, without the need for expensive offline profiling or sophisticated prediction model. We implement RIFLING as a plugin of Tensorflow, and deploy it on a distributed GPU cluster. Experiments demonstrate that RIFLING achieves up to 47.8% reductions and 19.6% improvements in makespan and average normalized processing rate respectively compared to the best available baseline without any manual intervention.",
        "link": "http://dx.doi.org/10.1002/spe.3066"
    },
    {
        "id": 29909,
        "title": "LILAC: Learning a Leader for Cooperative Reinforcement Learning",
        "authors": "Yuqian Fu, Jiajun Chai, Yuanheng Zhu, Dongbin Zhao",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog51982.2022.9893619"
    },
    {
        "id": 29910,
        "title": "Reinforcement Learning with the Classical Q-Learning Algorithm for Optimizing Single Intersection Performance",
        "authors": "M. Rosyidi, Sahid Bismantoko, Tri Widodo",
        "published": "2020-10-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdabi51230.2020.9325657"
    },
    {
        "id": 29911,
        "title": "Efficient exploration by switching agents according to degree of convergence of learning on Heterogeneous Multi-Agent Reinforcement Learning in Single Robot",
        "authors": "Riku Narita, Tatsufumi Matsushima, Kentarou Kurashige",
        "published": "2021-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9659982"
    },
    {
        "id": 29912,
        "title": "Learning to Reweight Imaginary Transitions for Model-Based Reinforcement Learning",
        "authors": "Wenzhen Huang, Qiyue Yin, Junge Zhang, Kaiqi Huang",
        "published": "2021-5-18",
        "citations": 0,
        "abstract": "Model-based reinforcement learning (RL) is more sample efficient than model-free RL by using imaginary trajectories generated by the learned dynamics model. When the model is inaccurate or biased, imaginary trajectories may be deleterious for training the action-value and policy functions. To alleviate such problem, this paper proposes to adaptively reweight the imaginary transitions, so as to reduce the negative effects of poorly generated trajectories. More specifically, we evaluate the effect of an imaginary transition by calculating the change of the loss computed on the real samples when we use the transition to train the action-value and policy functions. Based on this evaluation criterion, we construct the idea of reweighting each imaginary transition by a well-designed meta-gradient algorithm. Extensive experimental results demonstrate that our method outperforms state-of-the-art model-based and model-free RL algorithms on multiple tasks. Visualization of our changing weights further validates the necessity of utilizing reweight scheme.",
        "link": "http://dx.doi.org/10.1609/aaai.v35i9.16958"
    },
    {
        "id": 29913,
        "title": "A Structured Online Learning Approach to Nonlinear Tracking with Unknown Dynamics",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.ch6"
    },
    {
        "id": 29914,
        "title": "Generative Imitation Learning using Forward and Inverse Reinforcement Learning",
        "authors": "Eiji Uchibe",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7210/jrsj.39.617"
    },
    {
        "id": 29915,
        "title": "NEAT for large-scale reinforcement learning through evolutionary feature learning and policy gradient search",
        "authors": "Yiming Peng, Gang Chen, Harman Singh, Mengjie Zhang",
        "published": "2018-7-2",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3205455.3205536"
    },
    {
        "id": 29916,
        "title": "DeepHaul: a deep learning and reinforcement learning-based smart automation framework for dump trucks",
        "authors": "Danish Ali, Samuel Frimpong",
        "published": "2021-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s13748-021-00233-7"
    },
    {
        "id": 29917,
        "title": "Research on Safe Reinforcement Controller Using Deep Reinforcement Learning with Control Barrier Function",
        "authors": "Yoon-Ha Ryu, Doukhi Oualid, Deok-Jin Lee",
        "published": "2022-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5302/j.icros.2022.22.0187"
    },
    {
        "id": 29918,
        "title": "A Reinforcement Learning Approach to Price Cloud Resources With Provable Convergence Guarantees",
        "authors": "Hong Xie, John C. S. Lui",
        "published": "2022-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3085088"
    },
    {
        "id": 29919,
        "title": "VASE: Variational Assorted Surprise Exploration for Reinforcement Learning",
        "authors": "Haitao Xu, Lech Szymanski, Brendan McCane",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3105140"
    },
    {
        "id": 29920,
        "title": "A Survey on Reinforcement Learning for Recommender Systems",
        "authors": "Yuanguo Lin, Yong Liu, Fan Lin, Lixin Zou, Pengcheng Wu, Wenhua Zeng, Huanhuan Chen, Chunyan Miao",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3280161"
    },
    {
        "id": 29921,
        "title": "Optimizing Digital Coupon Assignment Using Constrained Reinforcement Learning",
        "authors": "Xinlin Yao, Xianghua Lu",
        "published": "2019-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3310986.3311004"
    },
    {
        "id": 29922,
        "title": "Classic Hebbian learning endows feed-forward networks with sufficient adaptability in challenging reinforcement learning tasks",
        "authors": "Thomas F. Burns",
        "published": "2021-6-1",
        "citations": 1,
        "abstract": " A common pitfall of current reinforcement learning agents implemented in computational models is in their inadaptability postoptimization. Najarro and Risi [Najarro E, Risi S. Proc 33rd Conf Neural Inf Process Systems (NeurIPS 2020). 2020: 20719–20731, 2020] demonstrate how such adaptability may be salvaged in artificial feed-forward networks by optimizing coefficients of classic Hebbian rules to dynamically control the networks’ weights instead of optimizing the weights directly. Although such models fail to capture many important neurophysiological details, allying the fields of neuroscience and artificial intelligence in this way bears many fruits for both fields, especially when computational models engage with topics with a rich history in neuroscience such as Hebbian plasticity. ",
        "link": "http://dx.doi.org/10.1152/jn.00712.2020"
    },
    {
        "id": 29923,
        "title": "A 2D UAV Path Planning Method Based on Reinforcement Learning in the Presence of Dense Obstacles and Kinematic Constraints",
        "authors": "Xinming Tang, Yi Chai, Qie Liu",
        "published": "2022-8-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ddcls55054.2022.9858514"
    },
    {
        "id": 29924,
        "title": "A Categorization of Reinforcement Learning Exploration Techniques which Facilitates Combination of Different Methods",
        "authors": "Bjorn Ivar Teigen, Kai Olav Ellefsen, Jim Torresen",
        "published": "2019-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/devlrn.2019.8850685"
    },
    {
        "id": 29925,
        "title": "Deep Reinforcement Learning for Resource Constrained Multiclass Scheduling in Wireless Networks",
        "authors": "Apostolos Avranas, Philippe Ciblat, Marios Kountouris",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tmlcn.2023.3314705"
    },
    {
        "id": 29926,
        "title": "Correction to: Multi-agent reinforcement learning for fast-timescale demand response of residential loads",
        "authors": "Vincent Mai, Philippe Maisonneuve, Tianyu Zhang, Hadi Nekoei, Liam Paull, Antoine Lesage-Landry",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10994-024-06514-1"
    },
    {
        "id": 29927,
        "title": "Reinforcement Learning and Stochastic Optimization with Deep Learning-Based Forecasting on Power Grid Scheduling",
        "authors": "Cheng Yang, Jihai Zhang, Wei Jiang, Li Wang, Hanwei Zhang, Zhongkai Yi, Fangquan Lin",
        "published": "2023-11-8",
        "citations": 0,
        "abstract": "The emission of greenhouse gases is a major contributor to global warming. Carbon emissions from the electricity industry account for over 40% of the total carbon emissions. Researchers in the field of electric power are making efforts to mitigate this situation. Operating and maintaining the power grid in an economic, low-carbon, and stable environment is challenging. To address the issue, we propose a grid dispatching technique that combines deep learning-based forecasting technology, reinforcement learning, and optimization technology. Deep learning-based forecasting can forecast future power demand and solar power generation, while reinforcement learning and optimization technology can make charging and discharging decisions for energy storage devices based on current and future grid conditions. In the optimization method, we simplify the complex electricity environment to speed up the solution. The combination of proposed deep learning-based forecasting and stochastic optimization with online data augmentation is used to address the uncertainty of the dispatch system. A multi-agent reinforcement learning method is proposed to utilize team reward among energy storage devices. At last, we achieved the best results by combining reinforcement and optimization strategies. Comprehensive experiments demonstrate the effectiveness of our proposed framework.",
        "link": "http://dx.doi.org/10.3390/pr11113188"
    },
    {
        "id": 29928,
        "title": "Delving into Macro Placement with Reinforcement Learning",
        "authors": "Zixuan Jiang, Ebrahim Songhori, Shen Wang, Anna Goldie, Azalia Mirhoseini, Joe Jiang, Young-Joon Lee, David Z. Pan",
        "published": "2021-8-30",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlcad52597.2021.9531313"
    },
    {
        "id": 29929,
        "title": "Learning to Identify Critical States for Reinforcement Learning from Videos",
        "authors": "Haozhe Liu, Mingchen Zhuge, Bing Li, Yuhui Wang, Francesco Faccio, Bernard Ghanem, Jürgen Schmidhuber",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00187"
    },
    {
        "id": 29930,
        "title": "Hierarchical Reinforcement Learning Approach Towards Autonomous Cross-Country Soaring",
        "authors": "",
        "published": "2020-1-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2021-2010.vid"
    },
    {
        "id": 29931,
        "title": "Towards an Adaptive e-Learning System Based on Deep Learner Profile, Machine Learning Approach, and Reinforcement Learning",
        "authors": "Riad Mustapha, Gouraguine Soukaina, Qbadou Mohammed, Aoula Es-Sâadia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140528"
    },
    {
        "id": 29932,
        "title": "Integration of imitation learning using GAIL and reinforcement learning using task-achievement rewards via probabilistic graphical model",
        "authors": "Akira Kinose, Tadahiro Taniguchi",
        "published": "2020-8-17",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1080/01691864.2020.1778521"
    },
    {
        "id": 29933,
        "title": "Outperformed by a Computer? - Comparing Human Decisions to Reinforcement Learning Agents, Assigning Lot Sizes in a Learning Factory",
        "authors": "Thomas Voß, Alexander Rokoss, Janine Tatjana Maier, Matthias Schmidt, Jens Heger",
        "published": "No Date",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3859196"
    },
    {
        "id": 29934,
        "title": "Simultaneous Learning and Planning using Rapidly Exploring Random Tree* and Reinforcement Learning",
        "authors": "Arup Kumar Sadhu, Shubham Shukla, Sarvesh Sortee, Mohit Ludhiyani, Ranjan Dasgupta",
        "published": "2021-6-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icuas51884.2021.9476861"
    },
    {
        "id": 29935,
        "title": "Learning to Solve 3-D Bin Packing Problem via Deep Reinforcement Learning and Constraint Programming",
        "authors": "Yuan Jiang, Zhiguang Cao, Jie Zhang",
        "published": "2023-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tcyb.2021.3121542"
    },
    {
        "id": 29936,
        "title": "Multi-granularity fusion resource allocation algorithm based on dual-attention deep reinforcement learning and lifelong learning architecture in heterogeneous IIoT",
        "authors": "Ying Wang, Fengjun Shang, Jianjun Lei",
        "published": "2023-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.inffus.2023.101871"
    },
    {
        "id": 29937,
        "title": "Estimation of reward function maximizing learning efficiency in inverse reinforcement learning",
        "authors": "Yuki Kitazato, Sachiyo Arai",
        "published": "2018-6-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1541/ieejeiss.138.720"
    },
    {
        "id": 29938,
        "title": "Data-Driven Robust Multi-Agent Reinforcement Learning",
        "authors": "Yudan Wang, Yue Wang, Yi Zhou, Alvaro Velasquez, Shaofeng Zou",
        "published": "2022-8-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlsp55214.2022.9943500"
    },
    {
        "id": 29939,
        "title": "Category learning in a recurrent neural network with reinforcement learning",
        "authors": "Ying Zhang, Xiaochuan Pan, Yihong Wang",
        "published": "2022-10-25",
        "citations": 0,
        "abstract": "It is known that humans and animals can learn and utilize category information quickly and efficiently to adapt to changing environments, and several brain areas are involved in learning and encoding category information. However, it is unclear that how the brain system learns and forms categorical representations from the view of neural circuits. In order to investigate this issue from the network level, we combine a recurrent neural network with reinforcement learning to construct a deep reinforcement learning model to demonstrate how the category is learned and represented in the network. The model consists of a policy network and a value network. The policy network is responsible for updating the policy to choose actions, while the value network is responsible for evaluating the action to predict rewards. The agent learns dynamically through the information interaction between the policy network and the value network. This model was trained to learn six stimulus-stimulus associative chains in a sequential paired-association task that was learned by the monkey. The simulated results demonstrated that our model was able to learn the stimulus-stimulus associative chains, and successfully reproduced the similar behavior of the monkey performing the same task. Two types of neurons were found in this model: one type primarily encoded identity information about individual stimuli; the other type mainly encoded category information of associated stimuli in one chain. The two types of activity-patterns were also observed in the primate prefrontal cortex after the monkey learned the same task. Furthermore, the ability of these two types of neurons to encode stimulus or category information was enhanced during this model was learning the task. Our results suggest that the neurons in the recurrent neural network have the ability to form categorical representations through deep reinforcement learning during learning stimulus-stimulus associations. It might provide a new approach for understanding neuronal mechanisms underlying how the prefrontal cortex learns and encodes category information.",
        "link": "http://dx.doi.org/10.3389/fpsyt.2022.1008011"
    },
    {
        "id": 29940,
        "title": "The Method of Learning Personal Preference with Reinforcement Learning",
        "authors": "Yong Hee Park, Won Seok Choi, Seong Gon Choi",
        "published": "2022-2-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/icact53585.2022.9728964"
    },
    {
        "id": 29941,
        "title": "Improving coordination in small-scale multi-agent deep reinforcement learning through memory-driven communication",
        "authors": "Emanuele Pesce, Giovanni Montana",
        "published": "2020-9",
        "citations": 24,
        "abstract": "AbstractDeep reinforcement learning algorithms have recently been used to train multiple interacting agents in a centralised manner whilst keeping their execution decentralised. When the agents can only acquire partial observations and are faced with tasks requiring coordination and synchronisation skills, inter-agent communication plays an essential role. In this work, we propose a framework for multi-agent training using deep deterministic policy gradients that enables concurrent, end-to-end learning of an explicit communication protocol through a memory device. During training, the agents learn to perform read and write operations enabling them to infer a shared representation of the world. We empirically demonstrate that concurrent learning of the communication device and individual policies can improve inter-agent coordination and performance in small-scale systems. Our experimental results show that the proposed method achieves superior performance in scenarios with up to six agents. We illustrate how different communication patterns can emerge on six different tasks of increasing complexity. Furthermore, we study the effects of corrupting the communication channel, provide a visualisation of the time-varying memory content as the underlying task is being solved and validate the building blocks of the proposed memory device through ablation studies.",
        "link": "http://dx.doi.org/10.1007/s10994-019-05864-5"
    },
    {
        "id": 29942,
        "title": "Structured-policy Q-learning: an LMI-based Design Strategy for Distributed Reinforcement Learning",
        "authors": "Lorenzo Sforni, Andrea Camisa, Giuseppe Notarstefano",
        "published": "2022-12-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc51059.2022.9992584"
    },
    {
        "id": 29943,
        "title": "UAV Relay in VANETs Against Smart Jamming with Reinforcement Learning",
        "authors": "Liang Xiao, Weihua Zhuang, Sheng Zhou, Cailian Chen",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-01731-6_5"
    },
    {
        "id": 29944,
        "title": "An Adversarial Reinforcement Learning Framework for Robust Machine Learning-based Malware Detection",
        "authors": "Mohammadreza Reza Ebrahimi, Weifeng Li, Yidong Chai, Jason Pacheco, Hsinchun Chen",
        "published": "2022-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdmw58026.2022.00079"
    },
    {
        "id": 29945,
        "title": "Resource Allocation in Mobility-Aware Federated Learning Networks: A Deep Reinforcement Learning Approach",
        "authors": "Huy T. Nguyen, Nguyen Cong Luong, Jun Zhao, Chau Yuen, Dusit Niyato",
        "published": "2020-6",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wf-iot48130.2020.9221089"
    },
    {
        "id": 29946,
        "title": "Learning to Herd Agents Amongst Obstacles: Training Robust Shepherding Behaviors Using Deep Reinforcement Learning",
        "authors": "Jixuan Zhi, Jyh-Ming Lien",
        "published": "2021-4",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/lra.2021.3068955"
    },
    {
        "id": 29947,
        "title": "Web Service Classification Based on Reinforcement Learning and Structured Representation Learning",
        "authors": "Hankang Sheng, Zhangbing Li, Jianxun Liu, Xiao Zhang",
        "published": "2021-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aibt53261.2021.00011"
    },
    {
        "id": 29948,
        "title": "Classification without gradients: multi-agent reinforcement learning approach to optimization (Conference Presentation)",
        "authors": "Amir Morcos, Hong Man, Brian Maguire, Aaron West",
        "published": "2023-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2664025"
    },
    {
        "id": 29949,
        "title": "A Counting Method based on Deep Reinforcement Learning Combined with Generative Adversarial Network",
        "authors": "Zhoubao Sun, Yi Zhu",
        "published": "2022-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlccim55934.2022.00079"
    },
    {
        "id": 29950,
        "title": "Meta-Reinforcement Learning for Mastering Multiple Skills and Generalizing across Environments in Text-based Games",
        "authors": "Zhenjie Zhao, Mingfei Sun, Xiaojuan Ma",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.metanlp-1.1"
    },
    {
        "id": 29951,
        "title": "Intelligent Adapted e-Learning System based on Deep Reinforcement Learning",
        "authors": "Mohammed El Fouki, Noura Aknin, K. Ed El. Kadiri",
        "published": "2017-11-14",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3167486.3167574"
    },
    {
        "id": 29952,
        "title": "Random Access Protocol Learning in LEO Satellite Networks via Reinforcement Learning",
        "authors": "Ju-Hyung Lee, Hyowoon Seo, Jihong Park, Mehdi Bennis, Young-Chai Ko, Joongheon Kim",
        "published": "2022-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vtc2022-spring54318.2022.9860594"
    },
    {
        "id": 29953,
        "title": "Value-Consistent Representation Learning for Data-Efficient Reinforcement Learning",
        "authors": "Yang Yue, Bingyi Kang, Zhongwen Xu, Gao Huang, Shuicheng Yan",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Deep reinforcement learning (RL) algorithms suffer severe performance degradation when the interaction data is scarce, which limits their real-world application. Recently, visual representation learning has been shown to be effective and promising for boosting sample efficiency in RL. These methods usually rely on contrastive learning and data augmentation to train a transition model, which is different from how the model is used in RL---performing value-based planning.  Accordingly, the learned representation by these visual methods may be good for recognition but not optimal for estimating state value and solving the decision problem. To address this issue, we propose a novel method, called value-consistent representation learning (VCR), to learn representations that are directly related to decision-making. More specifically, VCR trains a model to predict the future state (also referred to as the \"imagined state'') based on the current one and a sequence of actions. Instead of aligning this imagined state with a real state returned by the environment, VCR applies a Q value head on both of the states and obtains two distributions of action values. Then a distance is computed and minimized to force the imagined state to produce a similar action value prediction as that by the real state. We develop two implementations of the above idea for the discrete and continuous action spaces  respectively. We conduct experiments on Atari 100k and DeepMind Control Suite benchmarks to validate their effectiveness for improving sample efficiency. It has been demonstrated that our methods achieve new state-of-the-art performance for search-free RL algorithms.",
        "link": "http://dx.doi.org/10.1609/aaai.v37i9.26311"
    },
    {
        "id": 29954,
        "title": "End-to-End Binocular Active Visual Tracking Based on Reinforcement Learning",
        "authors": "Biao Zhang, Songchang Jin, Shaowu Yang, Qianying Ouyang, Yuxi Zheng, Dianxi Shi",
        "published": "2023-7-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mlccim60412.2023.00054"
    },
    {
        "id": 29955,
        "title": "Deep Reinforcement Learning Framework with Q Learning For Optimal Scheduling in Cloud Computing",
        "authors": "Et al. Nihar Ranjan Sabat",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "Cloud computing is an emerging technology that is increasingly being appreciated for its diverse uses, encompassing data processing, The Internet of Things (IoT) and the storing of data. The continuous growth in the number of cloud users and the widespread use of IoT devices have resulted in a significant increase in the volume of data being generated by these users and the integration of IoT devices with cloud platforms. The process of managing data stored in the cloud has become more challenging to complete. There are numerous significant challenges that must be overcome in the process of migrating all data to cloud-hosted data centers. High bandwidth consumption, longer wait times, greater costs, and greater energy consumption are only some of the difficulties that must be overcome. Cloud computing, as a result, is able to allot resources in line with the specific actions made by users, which is a result of the conclusion that was mentioned earlier. This phenomenon can be attributed to the provision of a superior Quality of Service (QoS) to clients or users, with an optimal response time. Additionally, adherence to the established Service Level Agreement further contributes to this outcome. Due to this circumstance, it is of utmost need to effectively use the computational resources at hand, hence requiring the formulation of an optimal approach for task scheduling. The goal of this proposed study is to find ways to allocate and schedule cloud-based virtual machines (VMs) and tasks in such a way as to reduce completion times and associated costs. This study presents a new method of scheduling that makes use of Q-Learning to optimize the utilization of resources.The algorithm's primary goals include optimizing its objective function, building the ideal network, and utilizing experience replay techniques.",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i10.8464"
    },
    {
        "id": 29956,
        "title": "Market Making Strategy Optimization via Deep Reinforcement Learning",
        "authors": "Tianyuan Sun, Dechun Huang, Jie Yu",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2022.3143653"
    },
    {
        "id": 29957,
        "title": "Generative Learning Plan Recommendation for Employees: A Performance-aware Reinforcement Learning Approach",
        "authors": "Zhi Zheng, Ying Sun, Xin Song, Hengshu Zhu, Hui Xiong",
        "published": "2023-9-14",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3604915.3608795"
    },
    {
        "id": 29958,
        "title": "Learning style detection based on cognitive skills to support adaptive learning environment – A reinforcement approach",
        "authors": "V. Balasubramanian, S. Margret Anouncia",
        "published": "2018-12",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.asej.2016.04.012"
    },
    {
        "id": 29959,
        "title": "Learning a Faster Locomotion Gait for a Quadruped Robot with Model-Free Deep Reinforcement Learning",
        "authors": "Biao Hu, Shibo Shao, Zhengcai Cao, Qing Xiao, Qunzhi Li, Chao Ma",
        "published": "2019-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/robio49542.2019.8961651"
    },
    {
        "id": 29960,
        "title": "Combining Evolution and Deep Reinforcement Learning for Policy Search: A Survey",
        "authors": "Olivier Sigaud",
        "published": "2023-9-30",
        "citations": 5,
        "abstract": "Deep neuroevolution and deep Reinforcement Learning have received a lot of attention over the past few years. Some works have compared them, highlighting their pros and cons, but an emerging trend combines them so as to benefit from the best of both worlds. In this article, we provide a survey of this emerging trend by organizing the literature into related groups of works and casting all the existing combinations in each group into a generic framework. We systematically cover all easily available papers irrespective of their publication status, focusing on the combination mechanisms rather than on the experimental results. In total, we cover 45 algorithms more recent than 2017. We hope this effort will favor the growth of the domain by facilitating the understanding of the relationships between the methods, leading to deeper analyses, outlining missing useful comparisons and suggesting new combinations of mechanisms.",
        "link": "http://dx.doi.org/10.1145/3569096"
    },
    {
        "id": 29961,
        "title": "Learning Assisted Agent-based Energy Optimization: A Reinforcement Learning Based Consensus + Innovations Approach",
        "authors": "Yuhan Du, Meiyi Li, Javad Mohammadi, Erik Blasch, Alex Aved, David Ferris, Philip Morrone, Erika Ardiles Cruz",
        "published": "2022-10-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/naps56150.2022.10012166"
    },
    {
        "id": 29962,
        "title": "Learning to Operate Distribution Networks With Safe Deep Reinforcement Learning",
        "authors": "Hepeng Li, Haibo He",
        "published": "2022-5",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tsg.2022.3142961"
    },
    {
        "id": 29963,
        "title": "Deep Learning and Hierarchical Reinforcement Learning for modeling a Conversational Recommender System",
        "authors": "Pierpaolo Basile, Claudio Greco, Alessandro Suglia, Giovanni Semeraro",
        "published": "2019-1-29",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3233/ia-170031"
    },
    {
        "id": 29964,
        "title": "Learning Locally, Communicating Globally: Reinforcement Learning of Multi-robot Task Allocation for Cooperative Transport",
        "authors": "Kazuki Shibata, Tomohiko Jimbo, Tadashi Odashima, Keisuke Takeshita, Takamitsu Matsubara",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.10.431"
    },
    {
        "id": 29965,
        "title": "An interactive user groups recommender system based on reinforcement learning",
        "authors": "Hediyeh Naderi Allaf, Mohsen Kahani",
        "published": "2022-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccke57176.2022.9960042"
    },
    {
        "id": 29966,
        "title": "Human-level reinforcement learning performance of recurrent neural networks is linked to hyperperseveration, not directed exploration",
        "authors": "D. Tuzsus, A. Brands, I. Pappas, J. Peters",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractA key feature of animal and human decision-making is to balance the exploration of unknown options for information gain (directed exploration) versus selecting known options for immediate reward (exploitation), which is often examined using restless bandit tasks. Recurrent neural network models (RNNs) have recently gained traction in both human and systems neuroscience work on reinforcement learning, due to their ability to show meta-learning of task domains. Here we comprehensively compared the performance of a range of RNN architectures as well as human learners on restless four-armed bandit problems. The best-performing architecture (LSTM network with computation noise) exhibited human-level performance. Cognitive modeling first revealed that both human and RNN behavioral data contain signatures of higher-order perseveration, i.e., perseveration beyond the last trial, but this effect was more pronounced in RNNs. In contrast, human learners, but not RNNs, exhibited a positive effect of uncertainty on choice probability (directed exploration). RNN hidden unit dynamics revealed that exploratory choices were associated with a disruption of choice predictive signals during states of low state value, resembling a win-stay-loose-shift strategy, and resonating with previous single unit recording findings in monkey prefrontal cortex. Our results highlight both similarities and differences between exploration behavior as it emerges in meta-learning RNNs, and computational mechanisms identified in cognitive and systems neuroscience work.",
        "link": "http://dx.doi.org/10.1101/2023.04.27.538570"
    },
    {
        "id": 29967,
        "title": "Reinforcement Learning based Orchestration for Elastic Services",
        "authors": "Mauricio Fadel Argerich, Bin Cheng, Jonathan Furst",
        "published": "2019-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wf-iot.2019.8767180"
    },
    {
        "id": 29968,
        "title": "Deep Reinforcement Learning for Long Term Hydropower Production Scheduling",
        "authors": "Signe Riemer-Sorensen, Gjert H. Rosenlund",
        "published": "2020-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sest48500.2020.9203208"
    },
    {
        "id": 29969,
        "title": "Deep Reinforcement Learning for Inventory Control: A Roadmap",
        "authors": "Robert N. Boute, Joren Gijsbrechts, Willem van Jaarsveld, Nathalie Vanvuchelen",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3861821"
    },
    {
        "id": 29970,
        "title": "Reinforcement Learning to Rank",
        "authors": "Maarten de Rijke",
        "published": "2019-1-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3289600.3291605"
    },
    {
        "id": 29971,
        "title": "Regional Attention Reinforcement Learning for Rapid Object Detection",
        "authors": "Hongge Yao, peng dong, Siyi Cheng, Jun Yu, Hong Jiang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4005925"
    },
    {
        "id": 29972,
        "title": "Meta-Adversarial Inverse Reinforcement Learning for Decision-making Tasks",
        "authors": "Pin Wang, Hanhan Li, Ching-Yao Chan",
        "published": "2021-5-30",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9561330"
    },
    {
        "id": 29973,
        "title": "Explainable Multi-Agent Reinforcement Learning for Temporal Queries",
        "authors": "Kayla Boggess, Sarit Kraus, Lu Feng",
        "published": "2023-8",
        "citations": 0,
        "abstract": "As multi-agent reinforcement learning (MARL) systems are increasingly deployed throughout society, it is imperative yet challenging for users to understand the emergent behaviors of MARL agents in complex environments. This work presents an approach for generating policy-level contrastive explanations for MARL to answer a temporal user query, which specifies a sequence of tasks completed by agents with possible cooperation. The proposed approach encodes the temporal query as a PCTL* logic formula and checks if the query is feasible under a given MARL policy via probabilistic model checking. Such explanations can help reconcile discrepancies between the actual and anticipated multi-agent behaviors. The proposed approach also generates correct and complete explanations to pinpoint reasons that make a user query infeasible. We have successfully applied the proposed approach to four benchmark MARL domains (up to 9 agents in one domain). Moreover, the results of a user study show that the generated explanations significantly improve user performance and satisfaction.",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/7"
    },
    {
        "id": 29974,
        "title": "Reinforcement Learning Based Adaptive Video Streaming on Named Data Networking",
        "authors": "Suphakit Awiphan, Jakramate Bootkrajang, Jiro Katto",
        "published": "2020-10-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce50665.2020.9292037"
    },
    {
        "id": 29975,
        "title": "Enhancing machine translation with quality estimation and reinforcement learning",
        "authors": "Zijian Győző Yang, László János Laki",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.33039/ami.2023.08.008"
    },
    {
        "id": 29976,
        "title": "Adaptive Duty Cycle Control for Optimal Battery Energy Storage System Charging by Reinforcement Learning",
        "authors": "Richard Wiencek, Sagnika Ghosh",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00013"
    },
    {
        "id": 29977,
        "title": "A Deep Reinforcement Learning Approach for Dynamic Contents Caching in HetNets",
        "authors": "Manyou Ma, Vincent W.S. Wong",
        "published": "2020-6",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icc40277.2020.9148764"
    },
    {
        "id": 29978,
        "title": "Vector Quantization for Adaptive State Aggregation in Reinforcement Learning",
        "authors": "Christos N. Mavridis, John S. Baras",
        "published": "2021-5-25",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc50511.2021.9483052"
    },
    {
        "id": 29979,
        "title": "Deep Reinforcement Learning for Tehran Stock Trading",
        "authors": "Neda Yousefi",
        "published": "2022-11-16",
        "citations": 0,
        "abstract": "One of the most interesting topics for research, as well as for making a profit, is stock trading. It is known that artificial intelligence has had a great influence on this path. A lot of research has been done to investigate the application of machine learning and deep learning methods in stock trading. Despite the large amount of research done in the field of prediction and automation trading, stock trading as a deep reinforcement-learning problem remains an open research area. The progress of reinforcement learning, as well as the intrinsic properties of reinforcement learning, make it a suitable method for market trading in theory. In this paper, single stock trading models are presented based on the fine-tuned state-of-the-art deep reinforcement learning algorithms (Deep Deterministic Policy Gradient (DDPG) and Advantage Actor Critic (A2C)). These algorithms are able to interact with the trading market and capture the financial market dynamics. The proposed models are compared, evaluated, and verified on historical stock trading data. Annualized return and Sharpe ratio have been used to evaluate the performance of proposed models. The results show that the agent designed based on both algorithms is able to make intelligent decisions on historical data. The DDPG strategy performs better than the A2C and achieves better results in terms of convergence, stability, and evaluation criteria.",
        "link": "http://dx.doi.org/10.56741/jnest.v1i02.171"
    },
    {
        "id": 29980,
        "title": "Deep Reinforcement Learning with Importance Sampling for QoE Enhancement in Edge-Driven Video Delivery Services",
        "authors": "Mandan Naresh, Vikramjeet Das, Manik Gupta, Paresh Saxena",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAdaptive bitrate (ABR) algorithms are used to adapt the video bitrate based on the network conditions to improve the overall video quality of experience (QoE). Further, with the rise of multi-access edge computing (MEC), a higher QoE can be guaranteed for video services by performing computations over the edge servers rather than the cloud servers. Recently, reinforcement learning (RL) and asynchronous advantage actor-critic (A3C) methods have been used to improve adaptive bit rate algorithms and they have been shown to enhance the overall QoE as compared to fixed-rule ABR algorithms. However, a common issue in the A3C methods is the lag between behavior policy and target policy. As a result, the behavior and the target policies are no longer synchronized with one another which results in suboptimal updates. In this work, we present the deep reinforcement learning with an importance sampling based approach focused on edge-driven video delivery services to achieve an overall better user experience. We refer to our proposed approach as ALISA: Actor-Learner Architecture with Importance Sampling for efficient learning in ABR algorithms. ALISA incorporates importance sampling weights to give higher weightage to relevant experience to address the lag issues incurred in the existing A3C methods. We present the design and implementation of ALISA, and compare its performance to state-of-the-art video rate adaptation algorithms including vanilla A3C and other fixed-rule schedulers. Our results show that ALISA provides up to 25%-48% higher average QoE than vanilla A3C, whereas the gains are even higher when compared to fixed-rule schedulers.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-865937/v1"
    },
    {
        "id": 29981,
        "title": "Distributed Reinforcement Learning Framework for Resource Allocation in Disaster Response",
        "authors": "Cesar Lopez Castellanos, Jose R. Marti, Sarbjit Sarkaria",
        "published": "2018-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ghtc.2018.8601911"
    },
    {
        "id": 29982,
        "title": "Maximum Entropy Reinforcement Learning in Two-Player Perfect Information Games",
        "authors": "Taichi Nakayashiki, Tomoyuki Kaneko",
        "published": "2021-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ssci50451.2021.9659991"
    },
    {
        "id": 29983,
        "title": "Reinforcement Learning-Based Safe Path Planning for a 3R Planar Robot",
        "authors": "Mustafa Can BİNGOL",
        "published": "2022-2-28",
        "citations": 0,
        "abstract": "Path planning is an essential topic of robotics studies. Robotic researchers have suggested some methods such as particle swarm optimization, A*, and reinforcement learning (RL) to obtain a path. In the current study, it was aimed to generate RL-based safe path planning for a 3R planar robot. For this purpose, firstly, the environment was performed. Later, state, action, reward, and terminate functions were determined. Lastly, actor and critic artificial neural networks (ANN), which are basic components of deep deterministic policy gradients (DDPG), were formed in order to generate a safe path. Another aim of the current study was to obtain an optimum actor ANN. Different ANN structures that have 2, 4, and 8-layers and 512, 1024, 2048, and 4096-units were formed to get an optimum actor ANN. These formed ANN structures were trained during 5000 episodes and 200 steps and the best results were obtained by 4-layer, 1024, and 2048-units structures. Owing to this reason, 4 different ANN structures were performed utilizing 4-layer, 1024, and 2048-units. The proposed structures were trained. The NET-M2U-4L structure generated the best result among 4 different proposed structures. The NET-M2U-4L structure was tested by using 1000 different scenarios. As a result of the tests, the rate of generating a safe path was calculated as 93.80% and the rate of colliding to the obstacle was computed as 1.70%. As a consequence, a safe path was planned and an optimum actor ANN was obtained for a 3R planar robot.",
        "link": "http://dx.doi.org/10.16984/saufenbilder.911942"
    },
    {
        "id": 29984,
        "title": "Electric Vehicle Battery Thermal Management Under Extreme Fast Charging with Deep Reinforcement Learning",
        "authors": "Ziba Arjmandzadeh, Mohammad Hossein Abbasi, Hanchen Wang, jiangfeng zhang, Bin Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4499570"
    },
    {
        "id": 29985,
        "title": "B-spline curve fitting based on reinforcement learning",
        "authors": "Masahito TAKEZAWA, Kohei MATSUO",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1299/jsmedsd.2020.30.2305"
    },
    {
        "id": 29986,
        "title": "Systematic choice of video game benchmarks in Deep Reinforcement Learning",
        "authors": "Elvio Gomes, Marlo Souza",
        "published": "2021-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sbgames54170.2021.00028"
    },
    {
        "id": 29987,
        "title": "Incident Duration Sequential Predictions with Reinforcement Learning for Advanced Traveler Information System Applications",
        "authors": "Zirui (Raymond) Huang, Xiaofeng Li, Yi-Chang Chiu, Yao-Jan Wu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4532420"
    },
    {
        "id": 29988,
        "title": "A Versatile Door Opening System with Mobile Manipulator Through Adaptive Position-Force Control and Reinforcement Learning",
        "authors": "Gyuree Kang, Hyunki Seong, Daegyu Lee, Hyunchul Shim",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4634811"
    },
    {
        "id": 29989,
        "title": "DeepDefrag: a deep reinforcement learning framework for spectrum defragmentation",
        "authors": "Ehsan Etezadi, Carlos Natalino, Renzo Diaz, Anders Lindgren, Stefan Melin, Lena Wosinska, Paolo Monti, Marija Furdek",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>An exponential growth of bandwidth demand, spurred by emerging network services, often with diverse characteristics and stringent performance requirements, drive the need for more dynamic operation of optical networks, efficient use of spectral resources, and automation. Spectrum fragmentation is one of the main challenges of dynamic, resource-efficient Elastic Optical Networks (EONs). Fragmented, stranded spectrum slots lead to poor resource utilization and increase the blocking probability of incoming service requests. Conventional approaches for Spectrum Defragmentation (SD) apply various criteria to decide when, and which portion of the spectrum to defragment. However, these polices often address only a subset of tasks related to defragmentation, are not adaptable, and have limited automation potential. To address these issues, we propose DeepDefrag, a novel framework based on reinforcement learning that addresses the main aspects of the SD process: determining when to perform defragmentation, which connections to reconfigure, and which part of the spectrum to reallocate them to. DeepDefrag outperforms the well-known Oldest-First FirstFit (OF-FF) defragmentation heuristic, substantially reducing blocking probability and defragmentation overhead.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.20013458"
    },
    {
        "id": 29990,
        "title": "Creating Interactive Crowds with Reinforcement Learning",
        "authors": "Ariel Kwiatkowski",
        "published": "2022-6-28",
        "citations": 0,
        "abstract": "The entertainment industry, as well as the field of Computer Graphics, frequently faces the issue of creating large virtual crowds that would populate a scene. One of the ways to achieve that, particularly with modern rendering techniques, is by using simulation -- this, however, is nontrivial to design and control. The main goal of my PhD work is working towards the creation of a tool enabling the creation of virtual crowds that one can interact with, and we believe the best way to that is through Multiagent Reinforcement Learning techniques. These animated crowds can then be used both in movies and video games. Especially for the latter, it is highly desirable that both the crowd as a whole, as well as the individual characters, can react to the user's input in real time.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i11.21580"
    },
    {
        "id": 29991,
        "title": "Power Control in Energy Harvesting Multiple Access System with Reinforcement Learning",
        "authors": "Man Chu, Xuewen Liao, Hang Li, Shuguang Cui",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/globecom38437.2019.9014132"
    },
    {
        "id": 29992,
        "title": "Policy Reuse in Reinforcement Learning for Modular Agents",
        "authors": "Sayyed Jaffar Ali Raza, Mingjie Lin",
        "published": "2019-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/infoct.2019.8710861"
    },
    {
        "id": 29993,
        "title": "Deep Reinforcement Learning algorithms for Low Latency Edge Computing Systems",
        "authors": "K. Kumaran, E. Sasikala",
        "published": "2023-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aisp57993.2023.10134928"
    },
    {
        "id": 29994,
        "title": "Reinforcement Learning-Based Bonobo Optimizer for Efficient Load Balancing in Cloud Computing",
        "authors": "Adarsh M. G, Bhargavi K",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/conit59222.2023.10205866"
    },
    {
        "id": 29995,
        "title": "Appendix",
        "authors": "",
        "published": "2022-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/9781119808602.app1"
    },
    {
        "id": 29996,
        "title": "Boosting Deep Reinforcement Learning Agents with Generative Data Augmentation",
        "authors": "Tasos Papagiannis, Georgios Alexandridis, Andreas Stafylopatis",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "Data augmentation is a promising technique in improving exploration and convergence speed in deep reinforcement learning methodologies. In this work, we propose a data augmentation framework based on generative models for creating completely novel states and increasing diversity. For this purpose, a diffusion model is used to generate artificial states (learning the distribution of original, collected states), while an additional model is trained to predict the action executed between two consecutive states. These models are combined to create synthetic data for cases of high and low immediate rewards, which are encountered less frequently during the agent’s interaction with the environment. During the training process, the synthetic samples are mixed with the actually observed data in order to speed up agent learning. The proposed methodology is tested on the Atari 2600 framework, producing realistic and diverse synthetic data which improve training in most cases. Specifically, the agent is evaluated on three heterogeneous games, achieving a reward increase of up to 31%, although the results indicate performance variance among the different environments. The augmentation models are independent of the learning process and can be integrated to different algorithms, as well as different environments, with slight adaptations.",
        "link": "http://dx.doi.org/10.3390/app14010330"
    },
    {
        "id": 29997,
        "title": "Combining Cognitive Modeling and Reinforcement Learning for Clarification in Dialogue",
        "authors": "Baber Khalid, Malihe Alikhani, Matthew Stone",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.391"
    },
    {
        "id": 29998,
        "title": "Deep Reinforcement Learning based Control for two-dimensional Coherent Combining",
        "authors": "Bashir Mohammed, Mariam Kiran, Dan Wang, Qiang Du, Russell Wilcox",
        "published": "2020",
        "citations": 0,
        "abstract": "We demonstrate deep reinforcement learning based phase stabilization control for two-dimensional filled-aperture diffractive coherent combining. Various approaches were tested on simulation and experimental data for combining 3 × 3 beams using pattern recognition.",
        "link": "http://dx.doi.org/10.1364/assl.2020.jtu5a.7"
    },
    {
        "id": 29999,
        "title": "Cooperative and competitive multi-agent deep reinforcement learning",
        "authors": "Ruizhi Chen",
        "published": "2022-11-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2641830"
    },
    {
        "id": 30000,
        "title": "On the Value of Operational Flexibility in the Trailer Shipment and Assignment Problem: Data-Driven Approaches and Reinforcement Learning",
        "authors": "Seung Hwan Jung, Yunsi Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4364020"
    }
]