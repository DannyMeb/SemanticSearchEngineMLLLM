[
    {
        "id": 18105,
        "title": "Explainable AI: Interpretable Models for Transparent Decision-Making",
        "authors": "",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48047/ijfans/09/03/30"
    },
    {
        "id": 18106,
        "title": "Transparency in AI Decision Making: A Survey of Explainable AI Methods and Applications",
        "authors": "Jain R",
        "published": "2024-1-19",
        "citations": 0,
        "abstract": "Artificial Intelligence (AI) systems have become pervasive in numerous facets of modern life, wielding considerable influence in critical decision-making realms such as healthcare, finance, criminal justice, and beyond. Yet, the inherent opacity of many AI models presents significant hurdles concerning trust, accountability, and fairness. To address these challenges, Explainable AI (XAI) has emerged as a pivotal area of research, striving to augment the transparency and interpretability of AI systems. This survey paper serves as a comprehensive exploration of the state-of-the-art in XAI methods and their practical applications. We delve into a spectrum of techniques, spanning from model-agnostic approaches to interpretable machine learning models, meticulously scrutinizing their respective strengths, limitations, and real-world implications. The landscape of XAI is rich and varied, with diverse methodologies tailored to address different facets of interpretability. Model-agnostic approaches offer versatility by providing insights into model behavior across various AI architectures. In contrast, interpretable machine learning models prioritize transparency by design, offering inherent understandability at the expense of some predictive performance. Layer-wise Relevance Propagation (LRP) and attention mechanisms delve into the inner workings of neural networks, shedding light on feature importance and decision processes. Additionally, counterfactual explanations open avenues for exploring what-if scenarios, elucidating the causal relationships between input features and model outcomes. In tandem with methodological exploration, this survey scrutinizes the deployment and impact of XAI across multifarious domains. Successful case studies showcase the practical utility of transparent AI in healthcare diagnostics, financial risk assessment, criminal justice systems, and more. By elucidating these use cases, we illuminate the transformative potential of XAI in enhancing decision-making processes while fostering accountability and fairness. Nevertheless, the journey towards fully transparent AI systems is fraught with challenges and opportunities. As we traverse the current landscape of XAI, we identify pressing areas for further research and development. These include refining interpretability metrics, addressing the scalability of XAI techniques to complex models, and navigating the ethical dimensions of transparency in AI decision-making.Through this survey, we endeavor to cultivate a deeper understanding of transparency in AI decision-making, empowering stakeholders to navigate the intricate interplay between accuracy, interpretability, and ethical considerations. By fostering interdisciplinary dialogue and inspiring collaborative innovation, we aspire to catalyze future advancements in Explainable AI, ultimately paving the way towards more accountable and trustworthy AI systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.23880/art-16000110"
    },
    {
        "id": 18107,
        "title": "Explainable AI in Credit Card Fraud Detection: Interpretable Models and Transparent Decision-making for Enhanced Trust and Compliance in the USA",
        "authors": " Md Rokibul Hasan,  Md Sumon Gazi,  Nisha Gurung",
        "published": "2024-4-6",
        "citations": 0,
        "abstract": "Credit Card Fraud presents significant challenges across various domains, comprising, healthcare, insurance, finance, and e-commerce.  The principal objective of this research was to examine the efficacy of Machine Learning techniques in detecting credit card fraud. Four key Machine Learning techniques were employed, notably, Support Vector Machine, Logistic Regression, Random Forest, and Artificial Neural Network. Subsequently, model performance was evaluated using Precision, Recall, Accuracy, and F-measure metrics. While all models demonstrated high accuracy rates (99%), this was largely due to the dataset's size, with 284,807 attributes and only 492 fraudulent transactions. Nevertheless, accuracy solely did not provide a comprehensive comparison metric. Support Vector Machine showed the highest recall (89.5), correctly identifying the most positive instances, highlighting its efficacy in detecting true positives. On the other hand, the Artificial Neural Network model exhibited the highest precision (79.4, indicating its capability to make accurate identifications, making it proficient in optimistic predictions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.32996/jcsts.2024.6.2.1"
    },
    {
        "id": 18108,
        "title": "Errors in Physician-AI Collaboration: Insights From a Mixed-methods Study of Explainable AI and Trust in Clinical Decision-making",
        "authors": "Rikard Rosenbacke",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4773350"
    },
    {
        "id": 18109,
        "title": "Explainable AI (XAI) and its Applications in Building Trust and Understanding in AI Decision Making",
        "authors": "Rudra Tiwari",
        "published": "2023-1-27",
        "citations": 1,
        "abstract": "In recent years, there has been a growing need for Explainable AI (XAI) to build trust and understanding in AI decision making. XAI is a field of AI research that focuses on developing algorithms and models that can be easily understood and interpreted by humans. The goal of XAI is to make the inner workings of AI systems transparent and explainable, which can help people to understand the reasoning behind the decisions made by AI and make better decisions. In this paper, we will explore the various applications of XAI in different domains such as healthcare, finance, autonomous vehicles, and legal and government decisions. We will also discuss the different techniques used in XAI such as feature importance analysis, model interpretability, and natural language explanations. Finally, we will examine the challenges and future directions of XAI research. This paper aims to provide an overview of the current state of XAI research and its potential impact on building trust and understanding in AI decision making.",
        "keywords": "",
        "link": "http://dx.doi.org/10.55041/ijsrem17592"
    },
    {
        "id": 18110,
        "title": "Transparent AI in Auditing through Explainable AI",
        "authors": "Chen Zhong, Sunita Goel",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "SUMMARY\nThe scope and complexity of artificial intelligence (AI) applications in auditing have grown beyond automating tasks to performing decision-making tasks. Consequently, understanding how AI-based models arrive at their decisions has become crucial, particularly for auditing tasks that demand greater accountability and that involve complex decision-making processes. In this paper, we explore the implementation of explainable AI (XAI) through a fraud detection use case and demonstrate how integrating an explainability layer using XAI can improve the interpretability of AI models, enabling stakeholders to understand the models’ decision-making process. We also present emerging AI regulations in this context.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2308/ciia-2023-009"
    },
    {
        "id": 18111,
        "title": "Explainable AI for Medical Imaging: Advancing Transparency and Trust in Diagnostic Decision-Making",
        "authors": "Rajaraman PV, Udhayakumar Shanmugam",
        "published": "2023-12-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/i-pact58649.2023.10434658"
    },
    {
        "id": 18112,
        "title": "Explainable AI is Dead, Long Live Explainable AI!",
        "authors": "Tim Miller",
        "published": "2023-6-12",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3593013.3594001"
    },
    {
        "id": 18113,
        "title": "Analyzing Employee Attrition Using Explainable AI for Strategic HR Decision-Making",
        "authors": "Gabriel Marín Díaz, José Javier Galán Hernández, José Luis Galdón Salvador",
        "published": "2023-11-17",
        "citations": 1,
        "abstract": "Employee attrition and high turnover have become critical challenges faced by various sectors in today’s competitive job market. In response to these pressing issues, organizations are increasingly turning to artificial intelligence (AI) to predict employee attrition and implement effective retention strategies. This paper delves into the application of explainable AI (XAI) in identifying potential employee turnover and devising data-driven solutions to address this complex problem. The first part of the paper examines the escalating problem of employee attrition in specific industries, analyzing the detrimental impact on organizational productivity, morale, and financial stability. The second section focuses on the utilization of AI techniques to predict employee attrition. AI can analyze historical data, employee behavior, and various external factors to forecast the likelihood of an employee leaving an organization. By identifying early warning signs, businesses can intervene proactively and implement personalized retention efforts. The third part introduces explainable AI techniques which enhance the transparency and interpretability of AI models. By incorporating these methods into AI-based predictive systems, organizations gain deeper insights into the factors driving employee turnover. This interpretability enables human resources (HR) professionals and decision-makers to understand the model’s predictions and facilitates the development of targeted retention and recruitment strategies that align with individual employee needs.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11224677"
    },
    {
        "id": 18114,
        "title": "EXPLAINABLE AI AND ITS IMPORTANCE IN DECISION MAKING",
        "authors": "Himanshu Arora, Minika Lamba, Lokesh kumar, Sameer Kumar",
        "published": "2023",
        "citations": 0,
        "abstract": "AI is the technology that builds intelligent machines able to perform tasks that generally need human intelligence in machines that are programmed to think and act like humans. AI has penetrated many organization processes, resulting in a growing fear that intelligent machines will soon replace many humans in decision-making. To provide a more proactive and pragmatic perspective, this article highlights the complementarities of humans and AI. It examines how each can strengthen organizational decision-making processes typically characterized by uncertain, complex and equal vocalists. With excellent computation information processing capacity and an analytical approach, AI can extend human cognition when addressing complexity. In contrast, humans can still offer a more holistic, intuitive approach in dealing with uncertainly and equal vocalist in organizational decisionmaking.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36893/jnao.2023.v14i1.0019-0027"
    },
    {
        "id": 18115,
        "title": "From Black Boxes to Transparent Machines: The Quest for Explainable AI",
        "authors": "Shalom Akhai",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4390887"
    },
    {
        "id": 18116,
        "title": "An explainable AI framework for robust and transparent data-driven wind turbine power curve models",
        "authors": "Simon Letzgus, Klaus-Robert Müller",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.egyai.2023.100328"
    },
    {
        "id": 18117,
        "title": "Cracking the Code: Self-Explaining AI Models for Transparent Decision Making in Complex Algorithms.",
        "authors": "Aatmaj Amol Salunke -",
        "published": "2023-8-16",
        "citations": 0,
        "abstract": "This research paper explores self-explaining AI models that bridge the gap between complex black-box algorithms and human interpretability. The study focuses on techniques like LIME, SHAP, attention mechanisms, and rule-based systems to create locally interpretable models. By providing transparent and understandable explanations for AI predictions, these models enhance user trust and comprehension. Real-world applications in healthcare, finance, and autonomous systems are evaluated to demonstrate the effectiveness of self-explaining AI models. Ethical considerations regarding fairness, bias, and accountability in AI decision-making are also addressed. The findings underscore the potential of such models to unlock the mysteries of complex algorithms, making AI more accessible and interpretable for diverse applications.",
        "keywords": "",
        "link": "http://dx.doi.org/10.36948/ijfmr.2023.v05i04.5395"
    },
    {
        "id": 18118,
        "title": "Multimodal classification of Alzheimer's disease and mild cognitive impairment using custom MKSCDDL kernel over CNN with transparent decision-making for explainable diagnosis",
        "authors": "V. Adarsh, G. R. Gangadharan, Ugo Fiore, Paolo Zanetti",
        "published": "2024-1-20",
        "citations": 1,
        "abstract": "AbstractThe study presents an innovative diagnostic framework that synergises Convolutional Neural Networks (CNNs) with a Multi-feature Kernel Supervised within-class-similar Discriminative Dictionary Learning (MKSCDDL). This integrative methodology is designed to facilitate the precise classification of individuals into categories of Alzheimer's Disease, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN) statuses while also discerning the nuanced phases within the MCI spectrum. Our approach is distinguished by its robustness and interpretability, offering clinicians an exceptionally transparent tool for diagnosis and therapeutic strategy formulation. We use scandent decision trees to deal with the unpredictability and complexity of neuroimaging data. Considering that different people's brain scans are different, this enables the model to make more detailed individualised assessments and explains how the algorithm illuminates the specific neuroanatomical regions that are indicative of cognitive impairment. This explanation is beneficial for clinicians because it gives them concrete ideas for early intervention and targeted care. The empirical review of our model shows that it makes diagnoses with a level of accuracy that is unmatched, with a classification efficacy of 98.27%. This shows that the model is good at finding important parts of the brain that may be damaged by cognitive diseases.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-024-52185-2"
    },
    {
        "id": 18119,
        "title": "Improving Trust in AI with Mitigating Confirmation Bias: Effects of Explanation Type and Debiasing Strategy for Decision-Making with Explainable AI",
        "authors": "Taehyun Ha, Sangyeon Kim",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/10447318.2023.2285640"
    },
    {
        "id": 18120,
        "title": "Effects of Explanation Strategy and Autonomy of Explainable AI on Human–AI Collaborative Decision-making",
        "authors": "Bingcheng Wang, Tianyi Yuan, Pei-Luen Patrick Rau",
        "published": "2024-4-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s12369-024-01132-2"
    },
    {
        "id": 18121,
        "title": "Mitigating bias in artificial intelligence: Fair data generation via causal models for transparent and explainable decision-making",
        "authors": "Rubén González-Sendino, Emilio Serrano, Javier Bajo",
        "published": "2024-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.future.2024.02.023"
    },
    {
        "id": 18122,
        "title": "Mind the gap: Making explainable AI better understood in contexts",
        "authors": "",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.36334/modsim.2023.huston380"
    },
    {
        "id": 18123,
        "title": "Blockchain and explainable AI for enhanced decision making in cyber threat detection",
        "authors": "Prabhat Kumar, Danish Javeed, Randhir Kumar, A.K.M Najmul Islam",
        "published": "2024-2-19",
        "citations": 0,
        "abstract": "SummaryArtificial Intelligence (AI) based cyber threat detection tools are widely used to process and analyze a large amount of data for improved intrusion detection performance. However, these models are often considered as black box by the cybersecurity experts due to their inability to comprehend or interpret the reasoning behind the decisions. Moreover, AI‐based threat hunting is data‐driven and is usually modeled using the data provided by multiple cloud vendors. This is another critical challenge, as a malicious cloud can provide false information (i.e., insider attacks) and can degrade the threat‐hunting capability. In this paper, we present a blockchain‐enabled eXplainable AI (XAI) for enhancing the decision‐making capability of cyber threat detection in the context of Smart Healthcare Systems. Specifically, first, we use blockchain to validate and store data between multiple cloud vendors by implementing a Clique Proof‐of‐Authority (C‐PoA) consensus. Second, a novel deep learning‐based threat‐hunting model is built by combining Parallel Stacked Long Short Term Memory (PSLSTM) networks with a multi‐head attention mechanism for improved attack detection. The extensive experiment confirms its potential to be used as an enhanced decision support system by cybersecurity analysts.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/spe.3319"
    },
    {
        "id": 18124,
        "title": "Exploring Explainable Artificial Intelligence for Transparent Decision Making",
        "authors": "D. David Winster Praveenraj, Melvin Victor, C. Vennila, Ahmed Hussein Alawadi, Pardaeva Diyora, N. Vasudevan, T. Avudaiappan",
        "published": "2023",
        "citations": 0,
        "abstract": "Artificial intelligence (AI) has become a potent tool in many fields, allowing complicated tasks to be completed with astounding effectiveness. However, as AI systems get more complex, worries about their interpretability and transparency have become increasingly prominent. It is now more important than ever to use Explainable Artificial Intelligence (XAI) methodologies in decision-making processes, where the capacity to comprehend and trust AI-based judgments is crucial. This abstract explores the idea of XAI and how important it is for promoting transparent decision-making. Finally, the development of Explainable Artificial Intelligence (XAI) has shown to be crucial for promoting clear decision-making in AI systems. XAI approaches close the cognitive gap between complicated algorithms and human comprehension by empowering users to comprehend and analyze the inner workings of AI models. XAI equips stakeholders to evaluate and trust AI systems, assuring fairness, accountability, and ethical standards in fields like healthcare and finance where AI-based choices have substantial ramifications. The development of XAI is essential for attaining AI's full potential while retaining transparency and human-centric decision making, despite ongoing hurdles.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1051/e3sconf/202339904030"
    },
    {
        "id": 18125,
        "title": "Judicial Decision-Making and Explainable AI (XAI) – Insights from the Japanese Judicial System",
        "authors": "Yachiko Yamada",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "The recent development of artificial intelligence (AI) in information technology (IT) is remarkable. These developments have led to claims that AI can be used in courts to replace judges. In the article, the author addresses a matrix of these issues using the concept of explainable AI (XAI). The article examines how regulation can ensure that AI is ethical, and how this ethicality is closely related to (XAI). It concludes that, in the current context, the contribution of AI to the decision-making process is limited by the lack of sufficient explainability and interpretability of AI, although these aspects are adequately addressed and discussed. In addition, it is crucial to consider the impact of AI’s contribution on the legal authority that forms the foundation of the justice system, and a possible approach is suggested to consider conducting an experimental study as AI arbitration.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17951/sil.2023.32.4.157-173"
    },
    {
        "id": 18126,
        "title": "Advancing Educational Insights: Explainable AI Models for Informed Decision-Making",
        "authors": "Venkata Lakshmi Namburi, Karamvir Singh, Trisha Reddygari, Niteesh Kumar S, Venkata Satya Sai Varun Dudipala",
        "published": "2024-3-31",
        "citations": 0,
        "abstract": "Abstract: Over the past two decades, the integration of machine learning (ML) techniques within educational frameworks has garnered significant attention. However, despite its widespread adoption, there remains a dearth of research focusing on developing AI systems with a core emphasis on interpretability and explainability. This paper seeks to bridge this gap by proposing an advanced framework that not only predicts students' performance accurately but also offers reliable and interpretable results tailored for career counseling. The framework merges the concepts of ML and Explainable AI (XAI) to address the complexities of career counseling in educational settings. Drawing inspiration from educational data mining, the framework aims to provide insights conducive to students' career growth and decision-making processes. By incorporating MLbased White and Black Box models, the approach analyzes a comprehensive educational dataset comprising academic and employability attributes crucial for job placements and skill development. To enhance interpretability, the framework leverages the NGBoost algorithm, known for its efficiency in prediction modeling. Additionally, it integrates Local Interpretable Modelagnostic Explanations (LIME) and Shapley Additive Explanations (SHAP) methods for providing both local and global explanations of model predictions, ensuring transparency and comprehensibility. Through a series of use cases, the paper showcases the applicability and effectiveness of the framework in providing actionable insights to educators and students alike. In conclusion, this research contributes to the advancement of career counseling in educational contexts by offering a robust and interpretable ML-based framework. By providing transparent insights into students' academic performance and career prospects, the approach facilitates informed decision-making and supports personalized guidance for optimal career trajectories.",
        "keywords": "",
        "link": "http://dx.doi.org/10.22214/ijraset.2024.59026"
    },
    {
        "id": 18127,
        "title": "Developing interpretable models and techniques for explainable AI in decision-making",
        "authors": " Jayaganesh Jagannathan,  Dr. Agrawal Rajesh K,  Dr. Neelam Labhade-Kumar,  Ravi Rastogi,  Manu Vasudevan Unni,  K. K. Baseer",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "The rapid proliferation of artificial intelligence (AI) technologies across various industries and decision-making processes has undeniably transformed the way of approaching complex problems and tasks. AI systems have proven their prowess in areas such as healthcare, finance, and autonomous systems, revolutionizing how decisions are made. Nevertheless, this proliferation of AI has raised critical concerns regarding the transparency, accountability, and fairness of these systems, as many of the state-of-the-art AI models often resemble complex black boxes. These intricate models, particularly deep learning neural networks, harbor non-linear relationships that are difficult for human users to decipher, thereby raising concerns about bias, fairness, and overall trustworthiness in AI-driven decisions. The urgency of this issue is underscored by the realization that AI should not merely be accurate; it should also be interpretable. Explainable AI (XAI) has emerged as a vital field of research, emphasizing the development of models and techniques that render AI systems comprehensible and transparent in their decision-making processes. This paper investigates into the relevance and significance of XAI across various domains, including healthcare, finance, and autonomous systems, where the ability to understand the rationale behind AI decisions is paramount. In healthcare, where AI assists in diagnosis and treatment, the interpretability of AI models is crucial for clinicians to make informed decisions. In finance, applications like credit scoring and investment analysis demand transparent AI to ensure fairness and accountability. In the realm of autonomous systems, transparency is indispensable to guarantee safety and compliance with regulations. Moreover, government agencies in areas such as law enforcement and social services require interpretable AI to maintain ethical standards and accountability. This paper also highlights the diverse array of research efforts in the XAI domain, spanning from model-specific interpretability methods to more general approaches aimed at unveiling complex AI models. Interpretable models like decision trees and rule-based systems have gained attention for their inherent transparency, while integrating explanation layers into deep neural networks strives to balance accuracy with interpretability. The study emphasizes the significance of this burgeoning field in bridging the gap between AI's advanced capabilities and human users' need for comprehensible AI systems. It seeks to contribute to this field by exploring the design, development, and practical applications of interpretable AI models and techniques, with the ultimate goal of enhancing the trust and understanding of AI-driven decisions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.58414/scientifictemper.2023.14.4.39"
    },
    {
        "id": 18128,
        "title": "Leveraging explainable AI for enhanced decision making in humanitarian logistics: An Adversarial CoevoluTION (ACTION) framework",
        "authors": "Su Nguyen, Greg O’Keefe, Sobhan Arisian, Kerry Trentelman, Damminda Alahakoon",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ijdrr.2023.104004"
    },
    {
        "id": 18129,
        "title": "Enhancing Supplier Selection through Explainable AI: A Transparent and Interpretable Approach",
        "authors": "Kumar P, Vinodh Kumar S",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/rmkmate59243.2023.10369820"
    },
    {
        "id": 18130,
        "title": "Empowering Glioma Prognosis With Transparent Machine Learning and Interpretative Insights Using Explainable AI",
        "authors": "Anisha Palkar, Cifha Crecil Dias, Krishnaraj Chadaga, Niranjana Sampathila",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3370238"
    },
    {
        "id": 18131,
        "title": "Enhancing Intrusion Detection with Explainable AI: A Transparent Approach to Network Security",
        "authors": "Seshu Bhavani Mallampati, Hari Seetha",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "Abstract\nAn Intrusion Detection System (IDS) is essential to identify cyber-attacks and implement appropriate measures for each risk. The efficiency of the Machine Learning (ML) techniques is compromised in the presence of irrelevant features and class imbalance. In this research, an efficient data pre-processing strategy was proposed to enhance the model’s generalizability. The class dissimilarity is addressed using k-Means SMOTE. After this, we furnish a hybrid feature selection method that combines filters and wrappers. Further, a hyperparameter-tuned Light Gradient Boosting Machine (LGBM) is analyzed by varying the optimal feature subsets. The experiments used the datasets – UNSW-NB15 and CICIDS-2017, yielding an accuracy of 90.71% and 99.98%, respectively. As the transparency and generalizability of the model depend significantly on understanding each component of the prediction, we employed the eXplainable Artificial Intelligence (XAI) method, SHapley Additive exPlanation (SHAP), to improve the comprehension of forecasted results.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2478/cait-2024-0006"
    },
    {
        "id": 18132,
        "title": "Investigating Responsible Nudge Design for Informed Decision-Making Enabling Transparent and Reflective Decision-Making",
        "authors": "David Leimstädtner, Peter Sörries, Claudia Müller-Birn",
        "published": "2023-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3603555.3603567"
    },
    {
        "id": 18133,
        "title": "Measuring service quality based on customer emotion: An explainable AI approach",
        "authors": "Yiting Guo, Yilin Li, De Liu, Sean Xin Xu",
        "published": "2024-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dss.2023.114051"
    },
    {
        "id": 18134,
        "title": "Suggestions for Ethical Decision-Making Model through Collaboration between Human and AI",
        "authors": "Hyunsoo Kim,  ",
        "published": "2023-8-31",
        "citations": 0,
        "abstract": "Purpose: The purpose of this study is to explore and propose a model that allows humans and AI to collaborate in the process of making decisions about ethical issues. Due to AI's autonomy and mission performance capabilities, AI is sometimes viewed as an agent competing with humans. However, since the autonomy and mission performance capabilities of AI are applied at very diverse levels and areas, it is necessary to set certain categories and review their application. This study sought to reveal that more valid decisions can be made by collaborating between humans and AI in the category of ethical decision-making.\r\nMethod: This study uses methods of literature research and development research. First, using literature research to review various previous studies to understand the autonomy of AI in the relationship between humans and AI. Next, analyzing the meaning and characteristics of ethical judgment. Next, looking at a series of models that explain decision making. Second, using development research methods, for design and propose a model in which humans and AI appropriately collaborate in the process of making ethical decisions.\r\nResults: The results of this study reveal the following points. First, the results of ethical decision-making by humans and AI involve greater responsibility and related issues than the results of general decision-making. Second, in order to solve these problems, it is necessary to utilize collective intelligence through collective decision-making and at the same time distribute responsibility. Third, as a public and collective entity functioning as a committee, humans become the subjects of final judgment and responsibility, and AI must play a role in actively and functionally assisting such judgment. Fourth, this decision-making process needs to be presented in the form of a model as a principle that can be applied to various specific cases.\r\nConclusion: The conclusion of this study suggests that effective and valid ethical decisions can be made through collaboration between humans and AI in the ethical communication process. And based on this, we present a collaboration model between humans and AI. This model consists of the following steps: First, AI should be actively involved in the process of exploring data sources, collecting data, storing data, and refining and analyzing data for ethical decisions. Second, ethical decisions based on this are made by a human community in the form of a committee as a group thinking process. Third, allow humans and AI to evaluate and exchange opinions on the results of these ethical judgments through mutual feedback and collaboration.",
        "keywords": "",
        "link": "http://dx.doi.org/10.22471/ai.2023.8.12"
    },
    {
        "id": 18135,
        "title": "Explainable artificial intelligence and agile decision-making in supply chain cyber resilience",
        "authors": "Kiarash Sadeghi R., Divesh Ojha, Puneet Kaur, Raj V. Mahto, Amandeep Dhir",
        "published": "2024-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dss.2024.114194"
    },
    {
        "id": 18136,
        "title": "Enhancing accuracy and interpretability in EEG-based medical decision making using an explainable ensemble learning framework application for stroke prediction",
        "authors": "Samar Bouazizi, Hela Ltifi",
        "published": "2024-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dss.2023.114126"
    },
    {
        "id": 18137,
        "title": "A Taxonomy for AI Hazard Analysis",
        "authors": "Mary L. Cummings",
        "published": "2024-1-9",
        "citations": 0,
        "abstract": " With the rise of artificial intelligence in safety-critical systems like surface transportation, there is a commensurate need for new hazard analysis approaches to determine if and how AI contributes to accidents, which are also increasing in number and severity. The original Swiss Cheese model widely used for hazard analyses focuses uniquely on human activities that lead to accidents, but cannot address accidents where AI is a possible causal factor. To this end, the Taxonomy for AI Hazard Analysis (TAIHA) is proposed that introduces layers focusing on the oversight, design, maintenance, and testing of AI. TAIHA is illustrated with real-world accidents. TAIHA does not replace the traditional Swiss Cheese model, which should be used in concert when a joint human-AI system exists, such as when people are driving a car with AI-based advanced driving assist features. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/15553434231224096"
    },
    {
        "id": 18138,
        "title": "Automated decision-making and the problem of evil",
        "authors": "Andrea Berber",
        "published": "2023-11-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00146-023-01814-x"
    },
    {
        "id": 18139,
        "title": "Explainable Artificial Intelligence (XAI) Approaches for Transparency and Accountability in Financial Decision-Making",
        "authors": "Nitin Rane, Saurabh Choudhary, Jayesh Rane",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4640316"
    },
    {
        "id": 18140,
        "title": "Enhancing Human-AI Trust by Describing AI Decision-Making Behavior",
        "authors": "Zhouwei Lou, Haikun Wei",
        "published": "2023-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icus58632.2023.10318430"
    },
    {
        "id": 18141,
        "title": "Exploring explainable AI in pharmaceutical decision-making : Bridging the gap between black box models and clinical insights",
        "authors": "V. S. Ghorpade, Pradnya A. Jadhav, R. S. Jadhav, Satish N. Gujar, Wankhede Vishal Ashok, Shraddha V. Pandit",
        "published": "2024",
        "citations": 0,
        "abstract": "This study explores the crucial nexus between artificial intelligence (AI) and clinical decision-making in pharmaceuticals, highlighting the need to close the growing gap between black box models and clinical insights. The opaqueness of black box models creates questions about regulatory compliance, interpretability, and transparency as AI becomes more and more integrated into clinical applications, drug development, and discovery processes. Acknowledging the importance of Explainable AI (XAI) in this regard, we thoroughly examine XAI methods, focusing on their use in medical environments. The review highlights the need for responsible AI solutions by taking ethical and regulatory factors into account. We highlight effective XAI implementations in clinical decision support systems and drug development through case studies and evaluations. The study highlights adoption and technical barriers and makes suggestions to improve model interpretability without sacrificing effectiveness. By providing insights into the complex world of XAI in the pharmaceutical sector, this research opens the door for ethically sound, transparent, and well-informed AI applications.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47974/jsms-1249"
    },
    {
        "id": 18142,
        "title": "xAI: An Explainable AI Model for the Diagnosis of COPD from CXR Images",
        "authors": "Agughasi Victor Ikechukwu, S. Murali",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdds59137.2023.10434619"
    },
    {
        "id": 18143,
        "title": "A hybrid framework using explainable AI (XAI) in cyber-risk management for defence and recovery against phishing attacks",
        "authors": "Baidyanath Biswas, Arunabha Mukhopadhyay, Ajay Kumar, Dursun Delen",
        "published": "2024-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dss.2023.114102"
    },
    {
        "id": 18144,
        "title": "Decision Tree-Based Explainable AI for Diagnosis of Chronic Kidney Disease",
        "authors": "Manju V N, Aparna N, Krishna Sowjanya K",
        "published": "2023-8-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icirca57980.2023.10220774"
    },
    {
        "id": 18145,
        "title": "Explainable Artificial Intelligence: A Study of Current State-of-the-Art Techniques for Making ML Models Interpretable and Transparent",
        "authors": "Ayush Thakur, Rashmi Vashisth, Sudhanshu Tripathi",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ictacs59847.2023.10390035"
    },
    {
        "id": 18146,
        "title": "Explainable AI-Based Clinical Decision Support System for Obesity Comorbidity Analysis",
        "authors": "Grazia Veronica Aiosa, Maurizio Palesi, Francesca Sapuppo, Maria Gabriella Xibilia",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/e-science58273.2023.10254948"
    },
    {
        "id": 18147,
        "title": "Towards Modelling and Verification of Social Explainable AI",
        "authors": "Damian Kurpiewski, Wojciech Jamroga, Teofil Sidoruk",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011799900003393"
    },
    {
        "id": 18148,
        "title": "Which product description phrases affect sales forecasting? An explainable AI framework by integrating WaveNet neural network models with multiple regression",
        "authors": "Shan Chen, Shengjie Ke, Shuihua Han, Shivam Gupta, Uthayasankar Sivarajah",
        "published": "2024-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.dss.2023.114065"
    },
    {
        "id": 18149,
        "title": "An AI decision‐making framework for business value maximization",
        "authors": "Naveen Gudigantala, Sreedhar Madhavaram, Pelin Bicen",
        "published": "2023-3",
        "citations": 1,
        "abstract": "AbstractThis article addresses a key question of why businesses are failing to maximize business value from their artificial intelligence (AI) investments and proposes a strategic decision‐making framework for AI decision‐making to address this problem. We suggest that a firm's business strategy must drive AI‐driven business outcomes and measurements, which in turn should drive the AI implementation decisions. Very often, we find that businesses fail to successfully cast business problems into AI problems. To bridge this gap, we propose that firms use a performance management system such as objectives and key results (OKRs) to ensure that the business and AI goals & objectives are well defined, tightly aligned, and made transparent across the company, and the AI efforts are approached in an integrated manner by the different parts of a firm. We use McDonald's use of AI initiatives as a business use case to demonstrate support for our AI decision‐making framework. We argue that using the business strategy as a primary driver will enable firms to solve the right problems using AI, turning it to be a source of technology innovation and competitive advantage.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/aaai.12076"
    },
    {
        "id": 18150,
        "title": "ThyExp: An explainable AI-assisted Decision Making Toolkit for Thyroid Nodule Diagnosis based on Ultra-sound Images",
        "authors": "Jamie Morris, Zehao Liu, Huizhi Liang, Sidhartha Nagala, Xia Hong",
        "published": "2023-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583780.3615131"
    },
    {
        "id": 18151,
        "title": "Transparent and Accurate COVID-19 Diagnosis: Integrating Explainable AI with Advanced Deep Learning in CT Imaging",
        "authors": "Mohammad Mehedi Hassan, Salman A. AlQahtani, Mabrook S. AlRakhami, Ahmed Zohier Elhendi",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmes.2024.047940"
    },
    {
        "id": 18152,
        "title": "Towards Transparent Load Disaggregation – A Framework for Quantitative Evaluation of Explainability Using Explainable AI",
        "authors": "Djordje Batic, Vladimir Stankovic, Lina Stankovic",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tce.2023.3300530"
    },
    {
        "id": 18153,
        "title": "Integrating Automated Knowledge Extraction with Large Language Models for Explainable Medical Decision-Making",
        "authors": "Haodi Zhang, Jiahong Li, Yichi Wang, Yuanfeng Song",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385557"
    },
    {
        "id": 18154,
        "title": "Tracking PII ex-filtration: Exploring decision tree and neural network with explainable AI",
        "authors": "Rishika Kohli, Shreyas Chatterjee, Shaifu Gupta, Manoj Singh Gaur",
        "published": "2023-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ants59832.2023.10469568"
    },
    {
        "id": 18155,
        "title": "Scalable Verification of Social Explainable AI by Variable Abstraction",
        "authors": "Wojciech Jamroga, Yan Kim, Damian Kurpiewski",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012474800003636"
    },
    {
        "id": 18156,
        "title": "Social context of the issue of discriminatory algorithmic decision-making systems",
        "authors": "Daniel Varona, Juan Luis Suarez",
        "published": "2023-9-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00146-023-01741-x"
    },
    {
        "id": 18157,
        "title": "Editorial: Artificial intelligence solutions for decision making in robotics",
        "authors": "Qasem Abu Al-Haija",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/frobt.2024.1389191"
    },
    {
        "id": 18158,
        "title": "A scientific definition of explainable artificial intelligence for decision making",
        "authors": "Fabian Wahler, Michael Neubert",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijtcs.2023.131664"
    },
    {
        "id": 18159,
        "title": "A scientific definition of explainable artificial intelligence for decision making",
        "authors": "Michael Neubert, Fabian Wahler",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijtcs.2023.10057082"
    },
    {
        "id": 18160,
        "title": "Enhancing personalized learning with explainable AI: A chaotic particle swarm optimization based decision support system",
        "authors": "R. Parkavi, P. Karthikeyan, A. Sheik Abdullah",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.asoc.2024.111451"
    },
    {
        "id": 18161,
        "title": "A Comparative Analysis of AI Models in Complex Medical Decision-Making Scenarios: Evaluating ChatGPT, Claude AI, Bard, and Perplexity",
        "authors": "Vamsi Krishna Uppalapati, Deb Sanjay Nag",
        "published": "2024-1-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.7759/cureus.52485"
    },
    {
        "id": 18162,
        "title": "When something goes wrong: Who is responsible for errors in ML decision-making?",
        "authors": "Andrea Berber, Sanja Srećković",
        "published": "2023-2-13",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00146-023-01640-1"
    },
    {
        "id": 18163,
        "title": "Making AI Policies Transparent to Humans through Demonstrations",
        "authors": "Michael S. Lee",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Demonstrations are a powerful way of increasing the transparency of AI policies to humans. Though we can approximately model human learning from demonstrations as inverse reinforcement learning, we note that human learning can differ from algorithmic learning in key ways, e.g. humans are computationally limited and may sometimes struggle to understand all of the nuances of a demonstration. Unlike related work that provide demonstrations to humans that simply maximize information gain, I leverage concepts from the human education literature, such as the zone of proximal development and scaffolding, to show demonstrations that balance informativeness and difficulty of understanding to maximize human learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i21.30399"
    },
    {
        "id": 18164,
        "title": "Explainable Artificial Intelligence (XAI): Concepts and Challenges in Healthcare",
        "authors": "Tim Hulsen",
        "published": "2023-8-10",
        "citations": 19,
        "abstract": "Artificial Intelligence (AI) describes computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. Examples of AI techniques are machine learning, neural networks, and deep learning. AI can be applied in many different areas, such as econometrics, biometry, e-commerce, and the automotive industry. In recent years, AI has found its way into healthcare as well, helping doctors make better decisions (“clinical decision support”), localizing tumors in magnetic resonance images, reading and analyzing reports written by radiologists and pathologists, and much more. However, AI has one big risk: it can be perceived as a “black box”, limiting trust in its reliability, which is a very big issue in an area in which a decision can mean life or death. As a result, the term Explainable Artificial Intelligence (XAI) has been gaining momentum. XAI tries to ensure that AI algorithms (and the resulting decisions) can be understood by humans. In this narrative review, we will have a look at some central concepts in XAI, describe several challenges around XAI in healthcare, and discuss whether it can really help healthcare to advance, for example, by increasing understanding and trust. Finally, alternatives to increase trust in AI are discussed, as well as future research possibilities in the area of XAI.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/ai4030034"
    },
    {
        "id": 18165,
        "title": "AI-Driven Predictive Analytics for Financial Decision Making and Fraud Detection in Financial Institutions",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/jier.v4i1.697"
    },
    {
        "id": 18166,
        "title": "Exploring The Use of AI In Legal Decision Making: Benefits and Ethical Implications",
        "authors": "Sreelatha A, Gyandeep Choudhary",
        "published": "2023-9",
        "citations": 0,
        "abstract": "There has been a lot of discussion about how to incorporate AI into legal decision-making. The purpose of this research is to investigate the potential positive outcomes and potential negative consequences of using artificial intelligence in the legal system. A thorough understanding of the potential benefits and ethical considerations tied to the use of AI in legal decision-making can be attained through a thorough review of relevant literature, the formulation of research inquiries, and the establishment of research objectives.",
        "keywords": "",
        "link": "http://dx.doi.org/10.57029/scheel4"
    },
    {
        "id": 18167,
        "title": "Melanoma Classification Through Deep Ensemble Learning and Explainable AI",
        "authors": "Wadduwage Perera, Abm Islam, Van Pham, Min An",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012575400003657"
    },
    {
        "id": 18168,
        "title": "A context-specific analysis of ethical principles relevant for AI-assisted decision-making in health care",
        "authors": "Larissa Schlicht, Miriam Räker",
        "published": "2023-7-24",
        "citations": 1,
        "abstract": "AbstractArtificial intelligence (AI)-assisted technologies may exert a profound impact on social structures and practices in care contexts. Our study aimed to complement ethical principles considered relevant for the design of AI-assisted technology in health care with a context-specific conceptualization of the principles from the perspectives of individuals potentially affected by the implementation of AI technologies in nursing care. We conducted scenario-based semistructured interviews focusing on situations involving moral decision-making occurring in everyday nursing practice with nurses (N = 15) and care recipients (N = 13) working, respectively, living in long-term care facilities in Germany. First, we analyzed participants’ concepts of the ethical principles beneficence, respect for autonomy and justice. Second, we investigated participants’ expectations regarding the actualization of these concepts within the context of AI-assisted decision-making. The results underscore the importance of a context-specific conceptualization of ethical principles for overcoming epistemic uncertainty regarding the risks and opportunities associated with the (non)fulfillment of these ethical principles. Moreover, our findings provide indications regarding which concepts of the investigated ethical principles ought to receive extra attention when designing AI technologies to ensure that these technologies incorporate the moral interests of stakeholders in the care sector.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s43681-023-00324-2"
    },
    {
        "id": 18169,
        "title": "AI-driven decision support systems and epistemic reliance: a qualitative study on obstetricians’ and midwives’ perspectives on integrating AI-driven CTG into clinical decision making",
        "authors": "Rachel Dlugatch, Antoniya Georgieva, Angeliki Kerasidou",
        "published": "2024-1-6",
        "citations": 0,
        "abstract": "Abstract\nBackground\nGiven that AI-driven decision support systems (AI-DSS) are intended to assist in medical decision making, it is essential that clinicians are willing to incorporate AI-DSS into their practice. This study takes as a case study the use of AI-driven cardiotography (CTG), a type of AI-DSS, in the context of intrapartum care. Focusing on the perspectives of obstetricians and midwives regarding the ethical and trust-related issues of incorporating AI-driven tools in their practice, this paper explores the conditions that AI-driven CTG must fulfill for clinicians to feel justified in incorporating this assistive technology into their decision-making processes regarding interventions in labor.\n\nMethods\nThis study is based on semi-structured interviews conducted online with eight obstetricians and five midwives based in England. Participants were asked about their current decision-making processes about when to intervene in labor, how AI-driven CTG might enhance or disrupt this process, and what it would take for them to trust this kind of technology. Interviews were transcribed verbatim and analyzed with thematic analysis. NVivo software was used to organize thematic codes that recurred in interviews to identify the issues that mattered most to participants. Topics and themes that were repeated across interviews were identified to form the basis of the analysis and conclusions of this paper.\n\nResults\nThere were four major themes that emerged from our interviews with obstetricians and midwives regarding the conditions that AI-driven CTG must fulfill: (1) the importance of accurate and efficient risk assessments; (2) the capacity for personalization and individualized medicine; (3) the lack of significance regarding the type of institution that develops technology; and (4) the need for transparency in the development process.\n\nConclusions\nAccuracy, efficiency, personalization abilities, transparency, and clear evidence that it can improve outcomes are conditions that clinicians deem necessary for AI-DSS to meet in order to be considered reliable and therefore worthy of being incorporated into the decision-making process. Importantly, healthcare professionals considered themselves as the epistemic authorities in the clinical context and the bearers of responsibility for delivering appropriate care. Therefore, what mattered to them was being able to evaluate the reliability of AI-DSS on their own terms, and have confidence in implementing them in their practice.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s12910-023-00990-1"
    },
    {
        "id": 18170,
        "title": "Efficient FDP Optimization for AI Enhanced Decision Making",
        "authors": "Giorgio De Paola, Richar Villarroel Danger",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "Abstract\nThe present work introduces an efficient workflow for AI-enhanced decision-making in Field Development Planning Optimization. Despite the clear importance of uncertainty quantification in decision-making, we find that constraints in time, hardware, and costs are often limiting factors during field evaluation, with the drawback of having a biased uncertainty description or a wrong risk perception. The proposed work encompasses history matching, solution analysis, and production optimization with special emphasis on reducing both simulation and processing time, maximizing what we can call the result per core hour.\nAt the center of our work is an AI-guided optimizer suited to avoid excessive convergence bias and maintain an optimal exploration vs. exploitation performance. The optimizer allows the integration of a multi-objective (MO) formulation in standard history matching and optimization workflows. Despite the flexibility of MO optimization and the vast literature in the energy industry, its usage in real-field cases has always been quite limited due to its formulation availability in commercial software and the increased computation time. This work will show improvement in solution accuracy and formulation flexibility compared to Single Objective (SO) formulations at no increase in runtime.\nMO is based on the iterative convergence of an efficient frontier from the results generated by the simulation. This same concept has been brought to a user analysis step to allow the identification of best solutions across multiple evaluation workflows, lowering the expertise level for a solution.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2118/214345-ms"
    },
    {
        "id": 18171,
        "title": "AI IN AUTONOMOUS VEHICLE: SAFETY AND DECISION-MAKING",
        "authors": "",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.56726/irjmets49861"
    },
    {
        "id": 18172,
        "title": "Decision-making AI for customer worthiness and viability",
        "authors": "Elmin Marevac, Selman Patković, Emir Žunić",
        "published": "2023-3-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/infoteh57020.2023.10094207"
    },
    {
        "id": 18173,
        "title": "Theory Is All You Need: AI, Human Cognition, and Decision Making",
        "authors": "Teppo Felin, Matthias Holweg",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4737265"
    },
    {
        "id": 18174,
        "title": "A data-driven decision support framework for DEA target setting: an explainable AI approach",
        "authors": "Mustafa Jahangoshai Rezaee, Mohsen Abbaspour Onari, Morteza Saberi",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107222"
    },
    {
        "id": 18175,
        "title": "A Dynamic Framework: Advancing Machine Learning in Commodity Import Decision-Making",
        "authors": "Ziyang Bryan Zhong",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/transai60598.2023.00031"
    },
    {
        "id": 18176,
        "title": "The Impact of AI Transparency and Reliability on Human-AI Collaborative Decision-Making",
        "authors": "Xujinfeng Wang, Yicheng Yang, Da Tao, Tingru Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "Human-AI collaborative decision-making has become a prevalent interaction paradigm, but the lack of transparency in AI algorithms presents challenges for humans to understand the decision-making process. Such lack of comprehension can lead to issues of over-reliance or under-reliance on AI recommendations. In this study, we focused on a human-AI collaborative income predicting task and investigated the influence of AI transparency and reliability on task performance. The results revealed that when AI reliability was high (75% and 90%), transparency had no significant effects on human decision-making. However, at a lower level of reliability (60%), higher transparency levels led to increased compliance with AI suggestions, thereby demonstrating a persuasive effect. Further analysis indicated that compliance rates only improved when AI made correct decisions, rather than when AI made incorrect ones. However, transparency did not significantly impact humans' ability to correctly reject erroneous recommendations from AI, suggesting that increasing transparency alone did not enhance humans’ error detecting ability. In conclusion, when the reliability of AI is low, heightening transparency can promote appropriate dependence on AI without elevating the risk of over-reliance. Nevertheless, further research is necessary to explore effective strategies that can assist humans in identifying AI errors effectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54941/ahfe1004203"
    },
    {
        "id": 18177,
        "title": "Effects of AI Speaker\"s Cognitive and Emotional Decision-Making on AI Trust",
        "authors": "Jeongmin Ko, Byeng-Hee Chang",
        "published": "2023-6-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5392/ijoc.2023.19.2.028"
    },
    {
        "id": 18178,
        "title": "AI for Equity: Unpacking Potential Human Bias in Decision Making in Higher Education",
        "authors": "Tasha Austin, Bharat S. Rawal, Alexandra Diehl, Jonathan Cosme",
        "published": "2023-5-24",
        "citations": 0,
        "abstract": "The purpose of this study is to show how AI can serve as an assessment tool to detect potential human bias in decision making for students in higher education. Using student application data, we conduct a small study and apply a set of algorithms to perform deep learning analyses and assess human behaviors when identifying scholarship recipients. We conduct an interview with the organization’s leaders using this data to understand their criteria and expectations for identifying scholarship recipients and collectively explore the insights uncovered using these algorithms. Upon comparison to those recipients awarded the scholarships, we identify opportunities for the organization to implement a quantitative framework—a repeatable set of algorithms to help identify potential bias before awarding future scholarship recipients. ﻿",
        "keywords": "",
        "link": "http://dx.doi.org/10.5772/acrt.20"
    },
    {
        "id": 18179,
        "title": "Enabling explainable artificial intelligence capabilities in supply chain decision support making",
        "authors": "Femi Olan, Konstantina Spanaki, Wasim Ahmed, Guoqing Zhao",
        "published": "2024-2-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/09537287.2024.2313514"
    },
    {
        "id": 18180,
        "title": "Who Should I Trust: Human-AI Trust Model in AI Assisted Decision-Making",
        "authors": "Yechen Yang",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "AI technology, relying on its extraordinary data searching and calculating capability, has been widely applied in assisting human decision-makers in various industries: healthcare, business management, public policies, etc. As a crucial factor influencing the performance of human and AI interaction, trust has come to be valued more in the research area in recent years. Previous studies have emphasized multiple factors that have significant impacts on the trust between human decision-makers and AI assistants. Yet, more attention needs to be paid to building up a systematic model for trust in the human-AI collaboration context. Therefore, to construct a systematic model of trust for the AI decision-making area, this paper reviews the recently conducted research, analyzes and synthesizes the significant factors of trust in the AI-assisted decision-making process and establishes a theoretical ternary interaction model from three major aspects: human decision-maker-related, AI-related, and scenario-related. Factors from the three aspects construct the three major elements of trust, which can eventually evaluate trust in the assisted decision-making process. This systematic trust model fills the theoretical gap in the current studies of trust in human-AI interaction and provides implications for further research studies in studying AI trust-related topics.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2753-7048/41/20240805"
    },
    {
        "id": 18181,
        "title": "Explainable AI In Education : Current Trends, Challenges, And Opportunities",
        "authors": "Ashwin Rachha, Mohammed Seyam",
        "published": "2023-4-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/southeastcon51012.2023.10115140"
    },
    {
        "id": 18182,
        "title": "Cracking the Code: Enhancing Trust in AI through Explainable Models",
        "authors": "",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48047/resmil.v10i1.20"
    },
    {
        "id": 18183,
        "title": "An Explainable AI driven Decision Support System for COVID-19 Diagnosis using Fused Classification and Segmentation",
        "authors": "K Niranjan, S Shankar Kumar, S Vedanth, Dr. S. Chitrakala",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.procs.2023.01.168"
    },
    {
        "id": 18184,
        "title": "The Opaque Nature of Intelligence and the Pursuit of Explainable AI",
        "authors": "Sarah Thomson, Niki van Stein, Daan van den Berg, Cees van Leeuwen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012249500003595"
    },
    {
        "id": 18185,
        "title": "AI and clinical decision making",
        "authors": "M. Dorri",
        "published": "2023-5-26",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41415-023-5928-0"
    },
    {
        "id": 18186,
        "title": "An explainable AI model for power plant NOx emission control",
        "authors": "Yuanye Zhou, Ioanna Aslanidou, Mikael Karlsson, Konstantinos Kyprianidis",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.egyai.2023.100326"
    },
    {
        "id": 18187,
        "title": "‘Explainable’ AI identifies a new class of antibiotics",
        "authors": "",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/d41586-023-03668-1"
    },
    {
        "id": 18188,
        "title": "Employing Nudge Theory and Persuasive Principles with Explainable AI in Clinical Decision Support",
        "authors": "Seda Polat Erdeniz, Thi Ngoc Trang Tran, Alexander Felfernig, Sebastian Lubos, Michael Schrempf, Diether Kramer, Peter P. Rainer",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385315"
    },
    {
        "id": 18189,
        "title": "Trust and robotics: a multi-staged decision-making approach to robots in community",
        "authors": "Wenxi Zhang, Willow Wong, Mark Findlay",
        "published": "2023-6-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00146-023-01705-1"
    },
    {
        "id": 18190,
        "title": "Impact of AI on Human Decision-Making: Analysis of Human, AI, and Environment of Interaction",
        "authors": "Liwen, Zhang",
        "published": "2023-12-7",
        "citations": 0,
        "abstract": "In recent years, the application of AI in the decision-making field is increasing, and AI is frequently and deeply involved in every aspect of decision-making. As AI-assisted decision-making is becoming an important intersection of computer science and psychology, there is an emerging need for understanding how would AI affect peoples decision-making. Due to the rapid development of AI technology, a gap of literature reviews considering this field has arisen. In response, this article systematically reviews previous related researches from the classic perspectives of human-computer interaction: human, AI, and the environment of interaction, and each section contains several sub aspects. In detail, explainability and presentation of AIs decision, preference and demographic variables of human, and three specific circumstances of interaction are included. In addition, some principled issues such as research ethics have also been discussed. In sum, this article organizes structured empirical evidence for ongoing related researches, summarizes the shortcomings of previous researches, and proposes possible directions for more in-depth researches.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2753-7048/28/20231348"
    },
    {
        "id": 18191,
        "title": "Explainable Image Classification: The Journey So Far and the Road Ahead",
        "authors": "Vidhya Kamakshi, Narayanan C. Krishnan",
        "published": "2023-8-1",
        "citations": 1,
        "abstract": "Explainable Artificial Intelligence (XAI) has emerged as a crucial research area to address the interpretability challenges posed by complex machine learning models. In this survey paper, we provide a comprehensive analysis of existing approaches in the field of XAI, focusing on the tradeoff between model accuracy and interpretability. Motivated by the need to address this tradeoff, we conduct an extensive review of the literature, presenting a multi-view taxonomy that offers a new perspective on XAI methodologies. We analyze various sub-categories of XAI methods, considering their strengths, weaknesses, and practical challenges. Moreover, we explore causal relationships in model explanations and discuss approaches dedicated to explaining cross-domain classifiers. The latter is particularly important in scenarios where training and test data are sampled from different distributions. Drawing insights from our analysis, we propose future research directions, including exploring explainable allied learning paradigms, developing evaluation metrics for both traditionally trained and allied learning-based classifiers, and applying neural architectural search techniques to minimize the accuracy–interpretability tradeoff. This survey paper provides a comprehensive overview of the state-of-the-art in XAI, serving as a valuable resource for researchers and practitioners interested in understanding and advancing the field.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/ai4030033"
    },
    {
        "id": 18192,
        "title": "Three Challenges for AI-Assisted Decision-Making",
        "authors": "Mark Steyvers, Aakriti Kumar",
        "published": "2023-7-13",
        "citations": 3,
        "abstract": " Artificial intelligence (AI) has the potential to improve human decision-making by providing decision recommendations and problem-relevant information to assist human decision-makers. However, the full realization of the potential of human–AI collaboration continues to face several challenges. First, the conditions that support complementarity (i.e., situations in which the performance of a human with AI assistance exceeds the performance of an unassisted human or the AI in isolation) must be understood. This task requires humans to be able to recognize situations in which the AI should be leveraged and to develop new AI systems that can learn to complement the human decision-maker. Second, human mental models of the AI, which contain both expectations of the AI and reliance strategies, must be accurately assessed. Third, the effects of different design choices for human-AI interaction must be understood, including both the timing of AI assistance and the amount of model information that should be presented to the human decision-maker to avoid cognitive overload and ineffective reliance strategies. In response to each of these three challenges, we present an interdisciplinary perspective based on recent empirical and theoretical findings and discuss new research directions. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/17456916231181102"
    },
    {
        "id": 18193,
        "title": "AI, Unethical Decision Making and Moral Disengagement: The Devil is in the Detail",
        "authors": "Derek Thean Aik Ho",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4719700"
    },
    {
        "id": 18194,
        "title": "AI-Enabled Offloading Decision-Making and Resource Allocation Optimization Under Emergency Scenarios",
        "authors": "Mengqian Cheng, Xiaoqin Song, Lei Lei",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gcwkshps58843.2023.10464428"
    },
    {
        "id": 18195,
        "title": "Exploring differences in ethical decision-making processes between humans and ChatGPT-3 model: a study of trade-offs",
        "authors": "Umair Rehman, Farkhund Iqbal, Muhammad Umair Shah",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s43681-023-00335-z"
    },
    {
        "id": 18196,
        "title": "PAHD: perception-action based human decision making using explainable graph neural networks on SAR images",
        "authors": "Sasindu Wijeratne, Bingyi Zhang, Rajgopal Kannan, Viktor K. Prasanna, Carl Busart",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2663699"
    },
    {
        "id": 18197,
        "title": "Explainable AI based Maternal Health Risk Prediction using Machine Learning and Deep Learning",
        "authors": "Anika Rahman, Md. Golam Rabiul Alam",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiiot58121.2023.10174540"
    },
    {
        "id": 18198,
        "title": "A Study on Modeling of Activated Sludge Process in Wastewater Treatment System Utilizing XAI(eXplainable AI)",
        "authors": "Eui-Seok Nahm",
        "published": "2023-2-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5370/kiee.2023.72.2.263"
    },
    {
        "id": 18199,
        "title": "A decision-making framework for landfill site selection in Saudi Arabia using explainable artificial intelligence and multi-criteria analysis",
        "authors": "Mohammed Al Awadh, Javed Mallick",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eti.2023.103464"
    },
    {
        "id": 18200,
        "title": "Power, knowledge, and AI in political decision-making",
        "authors": "Dina Babushkina",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "n/a",
        "keywords": "",
        "link": "http://dx.doi.org/10.59490/jhtr.2023.1.7017"
    },
    {
        "id": 18201,
        "title": "EXplainable AI for Decision Support to Obesity Comorbidities Diagnosis",
        "authors": "Grazia V. Aiosa, Maurizio Palesi, Francesca Sapuppo",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3320057"
    },
    {
        "id": 18202,
        "title": "Explainable AI (XAI) for AI-Acceptability: The Coming Age of Digital Management 5.0",
        "authors": "Samia Chehbi Gamoura",
        "published": "2023-10-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icnsc58704.2023.10319030"
    },
    {
        "id": 18203,
        "title": "The ABC of algorithmic aversion: not agent, but benefits and control determine the acceptance of automated decision-making",
        "authors": "Gabi Schaap, Tibor Bosse, Paul Hendriks Vettehen",
        "published": "2023-3-29",
        "citations": 0,
        "abstract": "AbstractWhile algorithmic decision-making (ADM) is projected to increase exponentially in the coming decades, the academic debate on whether people are ready to accept, trust, and use ADM as opposed to human decision-making is ongoing. The current research aims at reconciling conflicting findings on ‘algorithmic aversion’ in the literature. It does so by investigating algorithmic aversion while controlling for two important characteristics that are often associated with ADM: increased benefits (monetary and accuracy) and decreased user control. Across three high-powered (Ntotal = 1192), preregistered 2 (agent: algorithm/human) × 2 (benefits: high/low) × 2 (control: user control/no control) between-subjects experiments, and two domains (finance and dating), the results were quite consistent: there is little evidence for a default aversion against algorithms and in favor of human decision makers. Instead, users accept or reject decisions and decisional agents based on their predicted benefits and the ability to exercise control over the decision.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00146-023-01649-6"
    },
    {
        "id": 18204,
        "title": "Collaborative Decision-Making Assistant for Healthcare Professionals: A Human-Centered AI Prototype Powered by Azure Open AI",
        "authors": "Kenneth Yamikani Fukizi",
        "published": "2023-8-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3588001.3609370"
    },
    {
        "id": 18205,
        "title": "Towards Explainable AI: Interpretable Models and Feature Attribution",
        "authors": "Tapomoy Adhikari",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4376176"
    },
    {
        "id": 18206,
        "title": "Review of “AI assurance: towards trustworthy, explainable, safe, and ethical AI” by Feras A. Batarseh and Laura J. Freeman, Academic Press, 2023",
        "authors": "Jialei Wang, Li Fu",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00146-023-01802-1"
    },
    {
        "id": 18207,
        "title": "Experiential AI: Between Arts and Explainable AI",
        "authors": "Drew Hemment, Dave Murray-Rust, Vaishak Belle, Ruth Aylett, Matjaz Vidmar, Frank Broz",
        "published": "2024-4-2",
        "citations": 0,
        "abstract": "Abstract\nExperiential artificial intelligence (AI) is an approach to the design, use, and evaluation of AI in cultural or other real-world settings that foregrounds human experience and context. It combines arts and engineering to support rich and intuitive modes of model interpretation and interaction, making AI tangible and explicit. The ambition is to enable significant cultural works and make AI systems more understandable to nonexperts, thereby strengthening the basis for responsible deployment. This paper discusses limitations and promising directions in explainable AI, contributions the arts offer to enhance and go beyond explainability and methodology to support, deepen, and extend those contributions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1162/leon_a_02524"
    },
    {
        "id": 18208,
        "title": "Integrating explainable artificial intelligence and blockchain to smart agriculture: Research prospects for decision making and improved security",
        "authors": "Hsin-Yuan Chen, Komal Sharma, Chetan Sharma, Shamneesh Sharma",
        "published": "2023-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.atech.2023.100350"
    },
    {
        "id": 18209,
        "title": "From Explainable AI to Explainable Simulation: Using Machine Learning and XAI to understand System Robustness",
        "authors": "Niclas Feldkamp, Steffen Strassburger",
        "published": "2023-6-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3573900.3591114"
    },
    {
        "id": 18210,
        "title": "Explainable AI (XAI): Bridging the Gap between Machine Learning and Human Understanding",
        "authors": "",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48047/resmil.v10i1.19"
    },
    {
        "id": 18211,
        "title": "Marketing Factors Affecting to Decision Making on Buying Cannabis-Ingredients Beverages",
        "authors": "Manoon Rojnarong, Arthitaya Kawee-AI, Supakiat Supasin",
        "published": "2024-1-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.55248/gengpi.5.0124.0129"
    },
    {
        "id": 18212,
        "title": "Infosphere, Datafication, and Decision-Making Processes in the AI Era",
        "authors": "Andrea Lavazza, Mirko Farina",
        "published": "2023-7",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11245-023-09919-0"
    },
    {
        "id": 18213,
        "title": "AI Technologies in the Judiciary: Critical Appraisal of Large Language Models in Judicial Decision-making",
        "authors": "Juan David Gutiérrez Rodríguez",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4667572"
    },
    {
        "id": 18214,
        "title": "Artificial Intelligence and Agency: Tie-breaking in AI Decision-Making",
        "authors": "Danielle Swanepoel, Daniel Corks",
        "published": "2024-3-29",
        "citations": 0,
        "abstract": "AbstractDetermining the agency-status of machines and AI has never been more pressing. As we progress into a future where humans and machines more closely co-exist, understanding hallmark features of agency affords us the ability to develop policy and narratives which cater to both humans and machines. This paper maintains that decision-making processes largely underpin agential action, and that in most instances, these processes yield good results in terms of making good choices. However, in some instances, when faced with two (or more) choices, an agent may find themselves with equal reasons to choose either - thus being presented with a tie. This paper argues that in the event of a tie, the ability to create a voluntarist reason is a hallmark feature of agency, and second, that AI, through current tie-breaking mechanisms does not have this ability, and thus fails at this particular feature of agency.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11948-024-00476-2"
    },
    {
        "id": 18215,
        "title": "Ownership and Trust - A corporate law framework for board decision-making in the age of AI",
        "authors": "Katja C. Langenbucher",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4746241"
    },
    {
        "id": 18216,
        "title": "Automated machine learning: AI-driven decision making in business analytics",
        "authors": "Marc Schmitt",
        "published": "2023-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.iswa.2023.200188"
    },
    {
        "id": 18217,
        "title": "Luck of the Draw III: Using AI to Examine Decision-Making in Federal Court Stays of Removal",
        "authors": "Sean Rehaag",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4322881"
    },
    {
        "id": 18218,
        "title": "Prompt-engineering testing ChatGPT4 and Bard for assessing Generative-AI efficacy to support decision-making",
        "authors": "Adam Svendsen, Bruce Garvey",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4495320"
    },
    {
        "id": 18219,
        "title": "Logic for Explainable AI",
        "authors": "Adnan Darwiche",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lics56636.2023.10175757"
    },
    {
        "id": 18220,
        "title": "Prediction of hydrogen uptake of metal organic frameworks using explainable machine learning",
        "authors": "Sitaram Meduri, Jalaiah Nandanavanam",
        "published": "2023-4",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.egyai.2023.100230"
    },
    {
        "id": 18221,
        "title": "Human favoritism, not AI aversion: People’s perceptions (and bias) toward generative AI, human experts, and human–GAI collaboration in persuasive content generation",
        "authors": "Yunhao Zhang, Renée Gosline",
        "published": "2023",
        "citations": 2,
        "abstract": "Abstract\nWith the wide availability of large language models and generative AI, there are four primary paradigms for human–AI collaboration: human-only, AI-only (ChatGPT-4), augmented human (where a human makes the final decision with AI output as a reference), or augmented AI (where the AI makes the final decision with human output as a reference). In partnership with one of the world’s leading consulting firms, we enlisted professional content creators and ChatGPT-4 to create advertising content for products and persuasive content for campaigns following the aforementioned paradigms. First, we find that, contrary to the expectations of some of the existing algorithm aversion literature on conventional predictive AI, the content generated by generative AI and augmented AI is perceived as of higher quality than that produced by human experts and augmented human experts. Second, revealing the source of content production reduces—but does not reverse—the perceived quality gap between human- and AI-generated content. This bias in evaluation is predominantly driven by human favoritism rather than AI aversion: Knowing that the same content is created by a human expert increases its (reported) perceived quality, but knowing that AI is involved in the creation process does not affect its perceived quality. Further analysis suggests this bias is not due to a ‘quality prime’ as knowing the content they are about to evaluate comes from competent creators (e.g., industry professionals and state-of-the-art AI) without knowing exactly that the creator of each piece of content does not increase participants’ perceived quality.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1017/jdm.2023.37"
    },
    {
        "id": 18222,
        "title": "Decision making in open agent systems",
        "authors": "Adam Eck, Leen‐Kiat Soh, Prashant Doshi",
        "published": "2023-12",
        "citations": 2,
        "abstract": "AbstractIn many real‐world applications of AI, the set of actors and tasks are not constant, but instead change over time. Robots tasked with suppressing wildfires eventually run out of limited suppressant resources and need to temporarily disengage from the collaborative work in order to recharge, or they might become damaged and leave the environment permanently. In a large business organization, objectives and goals change with the market, requiring workers to adapt to perform different sets of tasks across time. We call these multiagent systems (MAS) open agent systems (OASYS), and the openness of the sets of agents and tasks necessitates new capabilities and modeling for decision making compared to planning and learning in closed environments. In this article, we discuss three notions of openness: agent openness, task openness, and type openness. We also review the past and current research on addressing the novel challenges brought about by openness in OASYS. We share lessons learned from these efforts and suggest directions for promising future work in this area. We also encourage the community to engage and participate in this area of MAS research to address critical real‐world problems in the application of AI to enhance our daily lives.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/aaai.12131"
    },
    {
        "id": 18223,
        "title": "Neobjašnjiv objašnjiv AI",
        "authors": "Hyeongjoo Kim",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "Ovaj rad kritički istražuje projekt objašnjive umjetne inteligencije (XAI). Analiziram riječ »objasniti« u XAI-ju i teoriji objašnjenja i identificiram neslaganje između značenja za »objašnjenje« za koje se tvrdi da je potrebno i onoga što je stvarno predočeno. Nakon sažetka povijesti umjetne inteligencije vezane uz objašnjivost, tvrdim da je američka filozofija 1900-ih djelovala u pozadini navedene povijesti. Zatim izdvajam značenje objašnjenja s obzirom na XAI, da bih razjasnio odnos među umjetnom inteligencijom, logikom i teorijom objašnjenja. Čineći to, nastojim otkriti DARPA-ino prikriveno definicijsko povlačenje u smislu njegovog sadržaja i formalne pogreške sophisma figurae dictionis, polazeći od Kantova paralogizma. Zaključujem da ova namjerna pogreška postoji prije projekta XAI-a i da mu je u podlozi preuzetna uporaba uma koju Kant kritizira.",
        "keywords": "",
        "link": "http://dx.doi.org/10.21464/sp38203"
    },
    {
        "id": 18224,
        "title": "Foundations for Human-AI teaming for self-regulated learning with explainable AI (XAI)",
        "authors": "Judy Kay",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.chb.2023.107848"
    },
    {
        "id": 18225,
        "title": "Statistical Decision Making of Spending and Savings Habits among Students of Delhi NCR Region",
        "authors": "Nisha Jain, Aarti Sehgal",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4108/eai.16-12-2022.2326192"
    },
    {
        "id": 18226,
        "title": "Explaining the Uncertainty in AI-Assisted Decision Making",
        "authors": "Thao Le",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "The aim of this project is to improve human decision-making using explainability; specifically, how to explain the (un)certainty of machine learning models. Prior research has used uncertainty measures to promote trust and decision-making. However, the direction of explaining why the AI prediction is confident (or not confident) in its prediction needs to be addressed. By explaining the model uncertainty, we can promote trust, improve understanding and improve decision-making for users.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i13.26920"
    },
    {
        "id": 18227,
        "title": "COPE Discussion Document: Artificial intelligence (AI) in decision making",
        "authors": "Council COPE",
        "published": "2024-2-25",
        "citations": 0,
        "abstract": ".",
        "keywords": "",
        "link": "http://dx.doi.org/10.24069/sep-23-22"
    },
    {
        "id": 18228,
        "title": "CRACKING THE CODE: ENHANCING TRUST IN AI THROUGH EXPLAINABLE MODELS (2020)",
        "authors": "",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48047/jcr.07.03.372"
    },
    {
        "id": 18229,
        "title": "AI-Driven Competitive Intelligence: Enhancing Business Strategy and Decision Making",
        "authors": "Andrejs Cekuls",
        "published": "2023-3-9",
        "citations": 0,
        "abstract": "In the world of business, the importance of competitive intelligence cannot be overdone. As companies compete for market share and seek to gain an edge over their competitors, understanding the market and their competition becomes increasingly critical. As artificial intelligence continues to evolve, its potential to impact competitive intelligence grows.",
        "keywords": "",
        "link": "http://dx.doi.org/10.37380/jisib.v12i3.961"
    },
    {
        "id": 18230,
        "title": "AI-assisted diplomatic decision-making during crises—Challenges and opportunities",
        "authors": "Neeti Pokhriyal, Till Koebe",
        "published": "2023-5-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fdata.2023.1183313"
    },
    {
        "id": 18231,
        "title": "XAIVIER the Savior: A Web Application for Interactive Explainable AI in Time Series Data",
        "authors": "Ilija Šimić, Christian Partl, Vedran Sabol",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011684800003417"
    },
    {
        "id": 18232,
        "title": "Explainable AI and Taxation: A Real-Life Application",
        "authors": "Benjamin Alarie, Anthony Niblett",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4541441"
    },
    {
        "id": 18233,
        "title": "Explainability in Process Mining: A Framework for Improved Decision-Making",
        "authors": "Luca Nannini",
        "published": "2023-8-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3600211.3604729"
    },
    {
        "id": 18234,
        "title": "AI assessment tools for decision-making on telemedicine: liability in case of mistakes",
        "authors": "Sandra Camacho Clavijo",
        "published": "2024-3-19",
        "citations": 0,
        "abstract": "AbstractThe implementation of artificial intelligence as a medical decision support tool for triage, such as the SmED system in Germany, raises its potential application as a medical decision support for the use of telemedicine. The use of this self-learning artificial intelligence system (machine learning) raises the question of who is liable for damages in the event of an erroneous prediction by the system. This paper explores the answer to this question in line with the proposed new regulatory framework for AI in the European Union: the Proposal for a Directive of the European Parliament and of the Council on adapting non-contractual civil liability rules to artificial intelligence [AI Liability Directive-COM (2022) 496 final] and the Proposal for a Directive on liability for defective products [COM (2022) 493 final].",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s44163-024-00117-4"
    },
    {
        "id": 18235,
        "title": "Explainable ensemble technique for enhancing credit risk prediction",
        "authors": "Pavitha Nooji, Shounak Sugave",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "<span>Credit risk prediction is a critical task in financial institutions that can impact lending decisions and financial stability. While machine learning (ML) models have shown promise in accurately predicting credit risk, the complexity of these models often makes them difficult to interpret and explain. The paper proposes the explainable ensemble method to improve credit risk prediction while maintaining interpretability. In this study, an ensemble model is built by combining multiple base models that uses different ML algorithms. In addition, the model interpretation techniques to identify the most important features and visualize the model's decision-making process. Experimental results demonstrate that the proposed explainable ensemble model outperforms individual base models and achieves high accuracy with low loss. Additionally, the proposed model provides insights into the factors that contribute to credit risk, which can help financial institutions make more informed lending decisions. Overall, the study highlights the potential of explainable ensemble methods in enhancing credit risk prediction and promoting transparency and trust in financial decision-making.</span>",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v13.i1.pp917-924"
    },
    {
        "id": 18236,
        "title": "The role of explainable Artificial Intelligence in high-stakes decision-making systems: a systematic review",
        "authors": "Bukhoree Sahoh, Anant Choksuriwong",
        "published": "2023-6",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s12652-023-04594-w"
    },
    {
        "id": 18237,
        "title": "Human Centered Explainable AI Framework for Military Cyber Operations",
        "authors": "Clara Maathuis",
        "published": "2023-10-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/milcom58377.2023.10356338"
    },
    {
        "id": 18238,
        "title": "Ethical decision-making in human-automation collaboration: a case study of the nurse rostering problem",
        "authors": "Vincent Bebien, Odile Bellenguez, Gilles Coppin, Anna Ma-Wyatt, Rachel Stephens",
        "published": "2024-4-3",
        "citations": 0,
        "abstract": "AbstractAs artificial intelligence (AI) is increasingly present in different aspects of society and its harmful impacts are more visible, concrete methods to help design ethical AI systems and limit currently encountered risks must be developed. Taking the example of a well-known Operations Research problem, the Nurse Rostering Problem (NRP), this paper presents a way to help close the gap between abstract principles and on-the-ground applications with two different steps. We first propose a normative step that uses dedicated scientific knowledge to provide new rules for an NRP model, with the aim of improving nurses’ well-being. However, this step alone may be insufficient to comprehensively deal with all key ethical issues, particularly autonomy and explicability. Therefore, as a complementary second step, we introduce an interactive process that integrates a human decision-maker in the loop and allows practical ethics to be applied. Using input from stakeholders to enrich a mathematical model may help compensate for flaws in automated tools.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s43681-024-00459-w"
    },
    {
        "id": 18239,
        "title": "Phishing Website Detection Model for User Decision Making Based on XAI",
        "authors": "Daeyeob Kim",
        "published": "2023-8-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.9717/kmms.2023.26.8.1013"
    },
    {
        "id": 18240,
        "title": "Harnessing the power of blockchain technology to support decision-making in e-commerce processes",
        "authors": "Khaldun G. Al-Moghrabi, Ali M. Al-Ghonmein",
        "published": "2024-6-1",
        "citations": 0,
        "abstract": "Technology, such as blockchain, has emerged as a promising solution for addressing the challenges of e-commerce decision-making. In this study, we explore the potential benefits of integrating blockchain technology into e-commerce and its role in supporting decision-making in e-commerce. We also examine blockchain’s benefits in terms of enhanced security, transparency, and efficiency for e-commerce platforms. Furthermore, the study discusses the challenges of implementing blockchain for e-commerce, including scalability, integration, regulatory frameworks, user experience, privacy, interoperability, and sustainability. By analyzing these challenges, the study provides valuable insights for future research and development efforts to facilitate a seamless adoption of blockchain technology in e-commerce decisions. Blockchain technology holds the potential to transform an e-commerce ecosystem by overcoming these challenges and unlocking its transformative potential.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v13.i2.pp1380-1387"
    },
    {
        "id": 18241,
        "title": "Exploration of Explainable AI for Trust Development on Human-AI Interaction",
        "authors": "Ezekiel L. Bernardo, Rosemary R. Seva",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3639592.3639625"
    },
    {
        "id": 18242,
        "title": "Optimizing human-AI collaboration: Effects of motivation and accuracy information in AI-supported decision-making",
        "authors": "Simon Eisbach, Markus Langer, Guido Hertel",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.chbah.2023.100015"
    },
    {
        "id": 18243,
        "title": "Development of transparent high-frequency soft sensor of total nitrogen and total phosphorus concentrations in rivers using stacked convolutional auto-encoder and explainable AI",
        "authors": "Abdulrahman H. Ba-Alawi, SungKu Heo, Hanaa Aamer, Roberto Chang, TaeYong Woo, MinHan Kim, ChangKyoo Yoo",
        "published": "2023-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jwpe.2023.103661"
    },
    {
        "id": 18244,
        "title": "Organizing Structures and Information for Developing AI-enabled Military Decision-Making Systems",
        "authors": "Salvatore Alessandro Sarcia’, Giordano Colo’",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/techdefense59795.2023.10380925"
    },
    {
        "id": 18245,
        "title": "Prescriptive Analytics-based Robust Decision-Making Model for Cyber Disaster Risk Reduction",
        "authors": "Joseph Ponnoly, John Puthenveetil, Patricia D’Urso",
        "published": "2024-2-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaic60265.2024.10433837"
    },
    {
        "id": 18246,
        "title": "AI Renaissance, artificial intelligence, information overload, human-computer interaction, decision-making",
        "authors": "Ishita Goyal",
        "published": "2023-8-17",
        "citations": 3,
        "abstract": "Objective: This paper aims to explore the concept of AI as a modern-day Renaissance movement, triggered by the proliferation of the internet and advancements in artificial intelligence technologies. It delves into the transformative impact of AI on human-computer interactions and decision-making processes.\r\nResults: O’Leary's (1997) early notion of a Renaissance movement sparked by the internet's ubiquity finds resonance in the emergence of the AI renaissance. AI technologies such as natural language processing, machine learning, heuristic language processing, and neural networks have integrated into intricate networked computing environments. These technologies facilitate the handling, retrieval, and analysis of vast amounts of data available on the World Wide Web. Given the overwhelming volume of data, direct human analysis has become impractical, necessitating AI-driven support for efficient data utilization. In today's competitive and tech-driven landscape, the time available for decision-making has diminished, prompting reliance on intelligent agents and delegating decision-making tasks to these digital surrogates.\r\nConclusions: The contemporary AI renaissance signifies a paradigm shift in human-computer dynamics. The convergence of AI technologies with the internet's vast information landscape has created a symbiotic relationship, redefining traditional computer roles. AI-enabled tools not only manage the deluge of data but also extend decision-making capabilities, optimizing efficiency in an increasingly fast-paced world. This transformative movement transcends conventional computing boundaries and has paved the way for a new era of human-machine interaction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.37497/rev.artif.intell.educ.v4i00.12"
    },
    {
        "id": 18247,
        "title": "Deep Reinforcement Learning Unleashing the Power of AI in Decision-Making",
        "authors": "Jeff Shuford",
        "published": "2024-2-2",
        "citations": 0,
        "abstract": "Deep Reinforcement Learning (DRL) has emerged as a transformative paradigm in the field of artificial intelligence (AI), offering unprecedented capabilities in decision-making across diverse domains. This article explores the profound impact of DRL on enhancing the decision-making capabilities of AI systems, elucidating its underlying principles, applications, and implications.DRL represents a fusion of deep learning and reinforcement learning, enabling machines to learn complex behaviors and make decisions by interacting with their environment. The utilization of neural networks allows DRL algorithms to handle high-dimensional input spaces, making it well-suited for tasks that involve intricate decision-making processes.One of the key strengths of DRL lies in its ability to address problems with sparse and delayed rewards, common challenges in traditional reinforcement learning. Through a process of trial and error, DRL algorithms can learn optimal decision strategies by navigating through a vast decision space, adapting to dynamic environments, and maximizing cumulative rewards over time.The applications of DRL span various domains, including robotics, finance, healthcare, gaming, and autonomous systems. In robotics, DRL facilitates the development of intelligent agents capable of autonomously navigating complex environments, performing intricate tasks, and adapting to unforeseen circumstances. In finance, DRL is leveraged for portfolio optimization, algorithmic trading, and risk management, demonstrating its potential to revolutionize traditional financial strategies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.60087/jaigs.v1i1.36"
    },
    {
        "id": 18248,
        "title": "Improve Decision Making Efficiency in Ridesharing Systems through a Hybrid Firefly-PSO algorithm",
        "authors": "Fu-Shiung Hsieh",
        "published": "2023-6-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aiiot58121.2023.10174375"
    },
    {
        "id": 18249,
        "title": "AI Integration for Decision Making",
        "authors": "Vaishnavi Raturi, Anita Yadav, Shravan Kumar, Shaik Vaseem Akram, Yerrolla Chanti",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aece59614.2023.10428584"
    },
    {
        "id": 18250,
        "title": "Data Science and AI in Cricket: Revolutionizing Performance Analysis and Decision-Making",
        "authors": "Pranav Srinivasan",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "This paper sheds light on the impact of Data Science and ArtificialIntelligence in cricket, both on and off the field. It gives information on how DS&AI benefits the teams on matters like player performance analysis, game strategy optimization, prevention of injuries and so on.",
        "keywords": "",
        "link": "http://dx.doi.org/10.55041/ijsrem25556"
    },
    {
        "id": 18251,
        "title": "Ethical Considerations in AI-assisted Decision- Making for End-Of-Life Care in Healthcare.",
        "authors": "Et. al Sivasubramanian Balasubramanian",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "This paper delves into the ethical implications of deploying artificial intelligence (AI) in decision-making processes related to end-of-life care within healthcare settings. As AI continues to advance, its integration in healthcare introduces both opportunities and challenges, particularly in navigating the sensitive realm of end-of-life care. The paper explores this intersection, seeking to contribute valuable insights to the ongoing discourse on responsible AI implementation in the healthcare sector. Central to ethical considerations in end-of-life care is the principle of autonomy, emphasizing the importance of respecting patients' ability to make informed decisions about their care preferences. The paper argues for the need to design AI systems that augment rather than diminish patient autonomy, ensuring that individuals facing end-of-life decisions remain active participants in the process. Furthermore, the principles of beneficence and non-maleficence are highlighted, emphasizing the imperative for AI systems to enhance patient well-being while minimizing the risk of harm, both physical and psychological. Justice in the distribution of healthcare resources, including AI technologies, is crucial, and the paper emphasizes the need to address potential disparities in access. Transparent and explainable AI systems are advocated to foster trust among patients, families, and healthcare providers, enabling a better understanding of the rationale behind AI-driven recommendations. [1] The concept of accountability is explored, emphasizing the continued responsibility of healthcare professionals in overseeing and validating AI recommendations to maintain ethical standards. Cultural sensitivity is identified as a key consideration, recognizing and respecting diverse perspectives on end-of-life care. The paper underscores the significance of designing AI systems that accommodate cultural nuances and avoid imposing values that may conflict with patients' beliefs and preferences. Additionally, the emotional and psychological impact of AI-assisted decision-making is addressed, emphasizing the importance of maintaining the human touch in end-of-life care and acknowledging the roles of empathy, compassion, and human connection. The paper provides a comprehensive examination of the ethical dimensions surrounding AI-assisted decision-making in end-of-life care. By addressing autonomy, beneficence, justice, transparency, accountability, cultural sensitivity, and emotional impact, it offers a framework for responsible AI integration that aligns with ethical principles in healthcare, ultimately contributing to the enhancement of end-of-life care practices.",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/pst.168"
    },
    {
        "id": 18252,
        "title": "An explainable forecasting system for humanitarian needs assessment",
        "authors": "Rahul Nair, Bo Madsen, Alexander Kjærum",
        "published": "2023-12",
        "citations": 0,
        "abstract": "AbstractWe present a machine learning system for forecasting forced displacement populations deployed at the Danish Refugee Council (DRC). The system, named Foresight, supports long‐term forecasts aimed at humanitarian response planning. It is explainable, providing evidence and context supporting the forecast. Additionally, it supports scenarios, whereby analysts are able to generate forecasts under alternative conditions. The system has been in deployment since early 2020 and powers several downstream business functions within DRC. It is central to our annual Global Displacement Report, which informs our response planning. We describe the system, key outcomes, lessons learnt, along with technical limitations and challenges in deploying machine learning systems in the humanitarian sector.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/aaai.12133"
    },
    {
        "id": 18253,
        "title": "Creating meaningful work in the age of AI: explainable AI, explainability, and why it matters to organizational designers",
        "authors": "Kristin Wulff, Hanne Finnestrand",
        "published": "2023-1-26",
        "citations": 5,
        "abstract": "AbstractIn this paper, we contribute to research on enterprise artificial intelligence (AI), specifically to organizations improving the customer experiences and their internal processes through using the type of AI called machine learning (ML). Many organizations are struggling to get enough value from their AI efforts, and part of this is related to the area of explainability. The need for explainability is especially high in what is called black-box ML models, where decisions are made without anyone understanding how an AI reached a particular decision. This opaqueness creates a user need for explanations. Therefore, researchers and designers create different versions of so-called eXplainable AI (XAI). However, the demands for XAI can reduce the accuracy of the predictions the AI makes, which can reduce the perceived usefulness of the AI solution, which, in turn, reduces the interest in designing the organizational task structure to benefit from the AI solution. Therefore, it is important to ensure that the need for XAI is as low as possible. In this paper, we demonstrate how to achieve this by optimizing the task structure according to sociotechnical systems design principles. Our theoretical contribution is to the underexplored field of the intersection of AI design and organizational design. We find that explainability goals can be divided into two groups, pattern goals and experience goals, and that this division is helpful when defining the design process and the task structure that the AI solution will be used in. Our practical contribution is for AI designers who include organizational designers in their teams, and for organizational designers who answer that challenge.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00146-023-01633-0"
    },
    {
        "id": 18254,
        "title": "Explainable AI and Pricing Algorithms: A Case For Accountability in Pricing",
        "authors": "Brahm sareen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4509797"
    },
    {
        "id": 18255,
        "title": "Comparative analysis of explainable AI models for predicting lung cancer using diverse datasets",
        "authors": "Shahin Makubhai, Ganesh R Pathak, Pankaj R Chandre",
        "published": "2024-6-1",
        "citations": 0,
        "abstract": "Lung cancer prediction is crucial for early detec-tion and treatment, and explainable AI models have gained attention for their interpretability. This study aims to compare various explainable AI models using diverse datasets for lung cancer prediction. Clinical, genomic, and imaging data from multiple sources were collected, prepro-cessed, and used to train models such as Logistic Regression, SVC-Linear, SVC-rbf, Decision Tree, Random Forest, AdaBoost Classifier, and XGBoost Classifier. Preliminary results indicate that Random Forest achieved the highest accuracy of 98.9% across multiple datasets. Evaluation metrics such as accuracy, precision, recall, and F1 score were utilized, along with interpretability techniques like feature importance rankings and rule extraction methods. The study's findings will aid in identifying effective and interpretable AI models, facilitating early detection and treatment decisions for lung cancer.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v13.i2.pp1980-1991"
    },
    {
        "id": 18256,
        "title": "AI輔助企業管理──AI決策之法律責任探討",
        "authors": "張禎庭 張禎庭",
        "published": "2023-12",
        "citations": 0,
        "abstract": "",
        "keywords": "",
        "link": "http://dx.doi.org/10.53106/181646412023120074003"
    },
    {
        "id": 18257,
        "title": "Invited Perspective: Why Systematic Reviews, Scoping Reviews, and Evidence-to-Decision Frameworks Are Critical for Transparent, Consistent, Equitable, and Science-Based Decision-Making in Environmental Health",
        "authors": "Nicholas Chartres, Rashmi Joglekar",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1289/ehp14346"
    },
    {
        "id": 18258,
        "title": "Harnessing ChatGPT-4 and Explainable AI for Financial Nowcasting",
        "authors": "Thomas Yue, David Au",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4633383"
    },
    {
        "id": 18259,
        "title": "Enhancing Hate Speech Detection through Explainable AI",
        "authors": "Dipti Mittal, Harmeet Singh",
        "published": "2023-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsmdi57622.2023.00028"
    },
    {
        "id": 18260,
        "title": "Explainable AI as evidence of fair decisions",
        "authors": "Derek Leben",
        "published": "2023-2-14",
        "citations": 1,
        "abstract": "This paper will propose that explanations are valuable to those impacted by a model's decisions (model patients) to the extent that they provide evidence that a past adverse decision was unfair. Under this proposal, we should favor models and explainability methods which generate counterfactuals of two types. The first type of counterfactual is positive evidence of fairness: a set of states under the control of the patient which (if changed) would have led to a beneficial decision. The second type of counterfactual is negative evidence of fairness: a set of irrelevant group or behavioral attributes which (if changed) would not have led to a beneficial decision. Each of these counterfactual statements is related to fairness, under the Liberal Egalitarian idea that treating one person differently than another is justified only on the basis of features which were plausibly under each person's control. Other aspects of an explanation, such as feature importance and actionable recourse, are not essential under this view, and need not be a goal of explainable AI.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fpsyg.2023.1069426"
    },
    {
        "id": 18261,
        "title": "Evaluating text classification with explainable artificial intelligence",
        "authors": "Kanwal Zahoor, Narmeen Zakaria Bawany, Tehreem Qamar",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "<span lang=\"EN-US\">Nowadays, artificial intelligence (AI) in general and machine learning techniques in particular has been widely employed in automated systems. Increasing complexity of these machine learning based systems have consequently given rise to blackbox models that are typically not understandable or explainable by humans. There is a need to understand the logic and reason behind these automated decision-making black box models as they are involved in our day-to-day activities such as driving, facial recognition identity systems, online recruitment. Explainable artificial intelligence (XAI) is an evolving field that makes it possible for humans to evaluate machine learning models for their correctness, fairness, and reliability. We extend our previous research work and perform a detailed analysis of the model created for text classification and sentiment analysis using a popular Explainable AI tool named local interpretable model agnostic explanations (LIME). The results verify that it is essential to evaluate machine learning models using explainable AI tools as accuracy and other related metrics does not ensure the correctness, fairness, and reliability of the model. We also present the comparison of explainability and interpretability of various machine learning algorithms using LIME. </span>",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v13.i1.pp278-286"
    },
    {
        "id": 18262,
        "title": "AI and Bureaucratic Decision-Making: A New Layer of Opacity",
        "authors": "Antonio Cordella, Francesco Gualdi",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5465/amproc.2023.14863abstract"
    },
    {
        "id": 18263,
        "title": "AI Shall Have No Dominion: on How to Measure Technology Dominance in AI-supported Human decision-making",
        "authors": "Federico Cabitza, Andrea Campagner, Riccardo Angius, Chiara Natali, Carlo Reverberi",
        "published": "2023-4-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3544548.3581095"
    },
    {
        "id": 18264,
        "title": "The Use of AI and Algorithms for Decision-making in Workplace Recruitment Practices",
        "authors": "Kelvin Mariani, Frederick Vega Lozada",
        "published": "2023-2-28",
        "citations": 0,
        "abstract": "The use of artificial intelligence (AI) and algorithms in human resource management systems has increased in recent years significantly due to the COVID-19 pandemic and the rise of remote work. Research has shown that implementing AI systems in different workplace scenarios increases efficiency, reduces worktime, production and labor costs and the need for human workers to perform the tasks that AI systems can perform. Within human resource management practices, AI systems have been employed to do a multiple array of tasks such as: screening candidates, performance evaluations, facial and voice analysis during candidate interviews, training, and development. Other recruitment practices that employ AI systems are considered to have implications that make employers liable for possible discrimination which could occur as an adverse effect of having these systems partake in the decision-making process for employee selection. Certain authors have recognized the lack of the human factor in this process as detrimental and increases the possibility of error and bias in selection of candidates.\r\nThis study explored the use of these AI systems in the workplace and what regulations have been created to oversee how they can be employed for recruitment practices. It also aims to highlight the positive and negative impact of these AI supported practices. The results highlight the importance of regulating these practices as an effort to protect minority rights and privacy rights of people that are seeking employment. Identifying international and national legislation is necessary for adopting better regulated practices and guaranteeing worker’s rights.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47611/jsr.v12i1.1855"
    },
    {
        "id": 18265,
        "title": "MSR94 Harnessing AI in Health Economics: Enhancing Efficiency, Accuracy, and Decision-Making",
        "authors": "A. Bentley, R. Bradford",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jval.2023.09.2153"
    },
    {
        "id": 18266,
        "title": "Investigating the Relative Strengths of Humans and Machine Learning in Decision-Making",
        "authors": "Charvi Rastogi",
        "published": "2023-8-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3600211.3604738"
    },
    {
        "id": 18267,
        "title": "Decision-making in organizations: should managers use AI?",
        "authors": "Anniek Brink, Louis-David Benyayer, Martin Kupp",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "\nPurpose\nPrior research has revealed that a large share of managers is reluctant towards the use of artificial intelligence (AI) in decision-making. This aversion can be caused by several factors, including individual drivers. The purpose of this paper is to better understand the extent to which individual factors influence managers’ attitudes towards the use of AI and, based on these findings, to propose solutions for increasing AI adoption.\n\n\nDesign/methodology/approach\nThe paper builds on prior research, especially on the factors driving the adoption of AI in companies. In addition, data was collected by means of 16 expert interviews using a semi-structured interview guideline.\n\n\nFindings\nThe study concludes on four groups of individual factors ranked according to their importance: demographics, familiarity, psychology and personality. Moreover, the findings emphasized the importance of communication and training, explainability and transparency and participation in the process to foster the adoption of AI in decision-making.\n\n\nResearch limitations/implications\nThe paper identifies four ways to foster AI integration for organizational decision-making as areas for further empirical analysis by business researchers.\n\n\nPractical implications\nThis paper offers four ways to foster AI adoption for organizational decision-making: explaining the benefits and training the more adverse categories, explaining how the algorithms work and being transparent about the shortcomings, striking a good balance between automated and human-made decisions, and involving users in the design process.\n\n\nSocial implications\nThe study concludes on four groups of individual factors ranked according to their importance: demographics, familiarity, psychology and personality. Moreover, the findings emphasized the importance of communication and training, explainability and transparency and participation in the process to foster the adoption of AI in decision-making.\n\n\nOriginality/value\nThis study is one of few to conduct qualitative research into the individual factors driving usage intention among managers; hence, providing more in-depth insights about managers’ attitudes towards algorithmic decision-making. This research could serve as guidance for developers developing algorithms and for managers implementing and using algorithms in organizational decision-making.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1108/jbs-04-2023-0068"
    },
    {
        "id": 18268,
        "title": "Comparative analysis of fuzzy multi-criteria decision-making methods for quality of service-based web service selection",
        "authors": "Paul Aazagreyir, Peter Appiahene, Obed Appiah, Samuel Boateng",
        "published": "2024-6-1",
        "citations": 0,
        "abstract": "This research aims to compare and analyze the effectiveness of four popular fuzzy multi-criteria decision-making methods (FMCDMMs) for quality of service (QoS)-based web service selection. These methods are fuzzy DEMATEL (FD), fuzzy TOPSIS (FT), fuzzy VIKOR (FV), and fuzzy PROMETHEE (FP), including three ranking versions of FV. We assess the ranking similarities among these methods using Spearman's relationship figure. We describe the algorithms of these six FMCDMs in the methods section. In a case study, we collected primary data from five experts who rated nine QoS factors of nine web services. We used modified online software for analysis. The results showed that S6 ranked first in all FMCDMs, except for FD and FP, where it was ranked 2nd and 8th, respectively. The highest association coefficient (Rs) was found between FT and FV ranking in S techniques (0.983), FV ranking in S and FV ranking in Q (0.883), and FT and FV ranking Q (0.833) when comparing the similarity measure of the FMCDMMs. This analysis helps decision-makers and researchers choose the most suitable methods for integrated FMCDMs studies and real-world problem-solving.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v13.i2.pp1408-1419"
    },
    {
        "id": 18269,
        "title": "Multi-objective load balancing in cloud infrastructure through fuzzy based decision making and genetic algorithm based optimization",
        "authors": "Neema George, Anoop Balakrishnan Kadan, Vinodh P. Vijayan",
        "published": "2023-6-1",
        "citations": 1,
        "abstract": "Cloud computing became a popular technology which influence not only product development but also made technology business easy. The services like infrastructure, platform and software can reduce the complexity of technology requirement for any ecosystem. As the users of cloud-based services increases the complexity of back-end technologies also increased. The heterogeneous requirement of users in terms for various configurations creates different unbalancing issues related to load. Hence effective load balancing in a cloud system with reference to time and space become crucial as it adversely affect system performance. Since the user requirement and expected performance is multi-objective use of decision-making tools like fuzzy logic will yield good results as it uses human procedure knowledge in decision making. The overall system performance can be further improved by dynamic resource scheduling using optimization technique like genetic algorithm.",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v12.i2.pp678-685"
    },
    {
        "id": 18270,
        "title": "Ai-Powered Customer Experience: Personalization, Engagement, and Intelligent Decision-Making in Crm",
        "authors": "Tran Minh Tung, Duong Hoai Lan",
        "published": "2024-4-13",
        "citations": 0,
        "abstract": "There has been a recent increase in interest regarding the remarkable potential of artificial intelligence (AI) to profoundly transform online advertising. The purpose of this research is to critically assess how AI can enhance customer experience (CX) in various business applications. We aim to identify important concepts, evaluate the impact of AI-powered CX initiatives, and offer suggestions for future research. By conducting a thorough analysis of academic publications, industry reports, and case studies, this study extracts theoretical frameworks, empirical findings, and practical insights. The results highlight the significant changes that occur with the integration of AI into Customer Relationship Management (CRM). AI enables personalized interactions, strengthens customer engagement through interactive agents, provides data-driven insights, and empowers informed decision-making throughout the customer journey. Four key themes emerge from research findings: personalized service, improved engagement, data-driven strategy, and intelligent decision-making. However, challenges such as data privacy concerns, ethical considerations, and potential negative experiences with poorly implemented AI persist. This article makes a valuable contribution to the AI in CRM discourse by summarizing the current state, exploring key themes, and suggesting future research opportunities. It is strongly advocated for responsible AI implementation, emphasizing ethical considerations and providing guidance to organizations as they navigate the opportunities and challenges presented by AI.",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/jes.1832"
    },
    {
        "id": 18271,
        "title": "Editorial: Feature issue on fair and explainable decision support systems",
        "authors": "Luis Galárraga, Miguel Couceiro",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ejdp.2024.100046"
    },
    {
        "id": 18272,
        "title": "EXPLAINABLE AI (XAI): BRIDGING THE GAP BETWEEN MACHINE LEARNING AND HUMAN UNDERSTANDING (2020)",
        "authors": "",
        "published": "2024-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.48047/jcr.07.03.374"
    },
    {
        "id": 18273,
        "title": "Explainable AI-driven model for gastrointestinal cancer classification",
        "authors": "Faisal Binzagr",
        "published": "2024-4-15",
        "citations": 0,
        "abstract": "Although the detection procedure has been shown to be highly effective, there are several obstacles to overcome in the usage of AI-assisted cancer cell detection in clinical settings. These issues stem mostly from the failure to identify the underlying processes. Because AI-assisted diagnosis does not offer a clear decision-making process, doctors are dubious about it. In this instance, the advent of Explainable Artificial Intelligence (XAI), which offers explanations for prediction models, solves the AI black box issue. The SHapley Additive exPlanations (SHAP) approach, which results in the interpretation of model predictions, is the main emphasis of this work. The intermediate layer in this study was a hybrid model made up of three Convolutional Neural Networks (CNNs) (InceptionV3, InceptionResNetV2, and VGG16) that combined their predictions. The KvasirV2 dataset, which comprises pathological symptoms associated to cancer, was used to train the model. Our combined model yielded an accuracy of 93.17% and an F1 score of 97%. After training the combined model, we use SHAP to analyze images from these three groups to provide an explanation of the decision that affects the model prediction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fmed.2024.1349373"
    },
    {
        "id": 18274,
        "title": "Exploring Evaluation Methodologies for Explainable AI: Guidelines for Objective and Subjective Assessment",
        "authors": "Sule Tekkesinoglu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4667052"
    },
    {
        "id": 18275,
        "title": "Trust's Significance in Human-AI Communication and Decision-Making",
        "authors": "Sameer Kumar, Dr. S.K.Manju Bargavi",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "With artificial intelligence (AI) continuing to pervade many aspects of society, it is critical to comprehend the dynamics of trust in AI decision-making and human-AI interaction. This study explores the many facets of trust and looks at how important it is in influencing user attitudes, actions, and the general effectiveness of AI systems. In order to understand the complex interactions between intelligent machines and people, the research incorporates multidisciplinary viewpoints from the fields of psychology, human-computer interaction, and ethics. The first area of inquiry is what influences the creation of first faith in AI. [1]We investigate how consumers' desire to trust AI-driven technology is influenced by system transparency, explain ability, and user experience through empirical study. The creation of design concepts intended to build a foundation of trust in AI systems is informed by insights gained at this stage. The second aspect of the study focuses on how trust changes over time in extended encounters between humans and artificial intelligence. We study the dynamics of trust-building and erosion by monitoring user experiences and system performance. This helps to clarify the critical points and factors that affect the trust's trajectory. This long-term viewpoint aids in the creation of adaptable artificial intelligence systems that can adapt to changing user demands and address issues with trust. The third line of investigation concerns the function of trust in AI-influenced decision-making processes. We evaluate the extent to which users depend on AI-generated insights and the influence of trust on decision outcomes using experimental scenarios and real-world case studies. This stage clarifies the fine balance needed to maximise the collaboration between AI and humans and emphasises the significance of matching AI suggestions with user values.  The research concludes with an examination of the consequences of trust in AI for wider societal contexts, with a focus on ethical issues. We look at accountability frameworks, the potential fallout from blind trust, and the moral obligations of AI engineers in creating reliable systems. In order to foster a symbiotic relationship between humans and intelligent systems in a world increasingly driven by AI, this thorough investigation of the role of trust in human-AI interaction and decision-making ultimately aims to provide actionable insights for the design, implementation, and governance of AI technologies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.55041/ijsrem28468"
    },
    {
        "id": 18276,
        "title": "Explainable AI for Reliable Detection of Cyberbullying",
        "authors": "Vaishali U. Gongane, Mousami V. Munot, Alwin Anuse",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/punecon58714.2023.10450132"
    },
    {
        "id": 18277,
        "title": "Confirmation bias in AI-assisted decision-making: AI triage recommendations congruent with expert judgments increase psychologist trust and recommendation acceptance",
        "authors": "Anna Bashkirova, Dario Krpan",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.chbah.2024.100066"
    },
    {
        "id": 18278,
        "title": "AI-powered decision-making in facilitating insurance claim dispute resolution",
        "authors": "Wen Zhang, Jingwen Shi, Xiaojun Wang, Henry Wynn",
        "published": "2023-10-23",
        "citations": 1,
        "abstract": "AbstractLeveraging Artificial Intelligence (AI) techniques to empower decision-making can promote social welfare by generating significant cost savings and promoting efficient utilization of public resources, besides revolutionizing commercial operations. This study investigates how AI can expedite dispute resolution in road traffic accident (RTA) insurance claims, benefiting all parties involved. Specifically, we devise and implement a disciplined AI-driven approach to derive the cost estimates and inform negotiation decision-making, compared to conventional practices that draw upon official guidance and lawyer experience. We build the investigation on 88 real-life RTA cases and detect an asymptotic relationship between the final judicial cost and the duration of the most severe injury, marked by a notable predicted $${R}^{2}$$\n\n\nR\n\n2\n\n value of 0.527. Further, we illustrate how various AI-powered toolkits can facilitate information processing and outcome prediction: (1) how regular expression (RegEx) collates precise injury information for subsequent predictive analysis; (2) how alternative natural language processing (NLP) techniques construct predictions directly from narratives. Our proposed RegEx framework enables automated information extraction that accommodates diverse report formats; different NLP methods deliver comparable plausible performance. This research unleashes AI’s untapped potential for social good to reinvent legal-related decision-making processes, support litigation efforts, and aid in the optimization of legal resource consumption.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10479-023-05631-9"
    },
    {
        "id": 18279,
        "title": "AI in Public Governance: An Expert Survey on the Impact of Data Driven Decision Making in Politics",
        "authors": "Patrick Helmholz, Martin Nolte, Marc Schmitt",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4787049"
    },
    {
        "id": 18280,
        "title": "Explainable AI (XAI): Explained",
        "authors": "G. Pradeep Reddy, Y. V. Pavan Kumar",
        "published": "2023-4-27",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/estream59056.2023.10134984"
    },
    {
        "id": 18281,
        "title": "The Explanation One Needs for the Explanation One Gives. The Necessity of Explainable AI (XAI) for Causal Explanations of AI-related harm - Deconstructing the ‘Refuge of Ignorance’ in the EU’s AI liability Regulation",
        "authors": "Ljupcho Grozdanovski",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4740419"
    },
    {
        "id": 18282,
        "title": "Just accountability structures – a way to promote the safe use of automated decision-making in the public sector",
        "authors": "Hanne Hirvonen",
        "published": "2024-2",
        "citations": 3,
        "abstract": "AbstractThe growing use of automated decision-making (ADM) systems in the public sector and the need to control these has raised many legal questions in academic research and in policymaking. One of the timely means of legal control is accountability, which traditionally includes the ability to impose sanctions on the violator as one dimension. Even though many risks regarding the use of ADM have been noted and there is a common will to promote the safety of these systems, the relevance of the safety research has been discussed little in this context. In this article, I evaluate regulating accountability over the use of ADM in the public sector in relation to the findings of safety research. I conducted the study by focusing on ongoing regulatory projects regarding ADM, the Finnish ADM legislation draft and the EU proposal for the AI Act. The critical question raised in the article is what the role of sanctions is. I ask if official accountability could mean more of an opportunity to learn from mistakes, share knowledge and compensate for harm instead of control via sanctions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00146-023-01731-z"
    },
    {
        "id": 18283,
        "title": "Tutorial 1 - Explainable AI in the Problems of Image Clarification",
        "authors": "",
        "published": "2023-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ipta59101.2023.10320026"
    },
    {
        "id": 18284,
        "title": "Balancing the Scales of Explainable and Transparent AI Agents within Human-Agent Teams",
        "authors": "Sarvesh Sawant, Rohit Mallick, Camden Brady, Kapil Chalil Madathil, Nathan McNeese, Jeff Bertrand, Nikhil Rangaraju",
        "published": "2023-9",
        "citations": 0,
        "abstract": " With the progressive nature of Human-Agent Teams becoming more and more useful for high-quality work output, there is a proportional need for bi-directional communication between teammates to increase efficient collaboration. This need is centered around the well-known issue of innate mistrust between humans and artificial intelligence, resulting in sub-optimal work. To combat this, computer scientists and humancomputer interaction researchers alike have presented and refined specific solutions to this issue through different methods of AI interpretability. These different methods include explicit AI explanations as well as implicit manipulations of the AI interface, otherwise known as AI transparency. Individually these solutions hold considerable merit in repairing the relationship of trust between teammates, but also have individual flaws. We posit that the combination of different interpretable mechanisms mitigates each other’s flaws and extenuates their strengths within human-agent teams. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/21695067231192250"
    },
    {
        "id": 18285,
        "title": "Explainable Crowd Decision Making methodology guided by expert natural language opinions based on Sentiment Analysis with Attention-based Deep Learning and Subgroup Discovery",
        "authors": "Cristina Zuheros, Eugenio Martínez-Cámara, Enrique Herrera-Viedma, Iyad A. Katib, Francisco Herrera",
        "published": "2023-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.inffus.2023.101821"
    },
    {
        "id": 18286,
        "title": "From Decision-Making to Soldier Augmentation: How AI is Enhancing Military Capabilities",
        "authors": "Jody Holeton",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "Abstract: The integration of Artificial Intelligence (AI) into military operations signifies a revolutionary shift in the conduct and strategy of modern warfare. This paper, authored by MAJ Jody Holeton, provides an in-depth analysis of the multifaceted role of AI in enhancing the operational capabilities of the U.S. Army. The study begins with an introduction to the evolving landscape of AI in military domains, emphasizing the need for advanced technologies in the face of rapidly changing combat environments and the pursuit of strategic superiority. The paper delves into the critical role of AI in enhancing decision-making processes, highlighting projects like Maven which utilize AI for real-time intelligence analysis and predictive analytics. The focus then shifts to predictive maintenance, using examples like the UH-60 Black Hawk helicopter to demonstrate how AI improves equipment reliability and operational readiness",
        "keywords": "",
        "link": "http://dx.doi.org/10.22214/ijraset.2024.57617"
    },
    {
        "id": 18287,
        "title": "Understanding Uncertainty: How Lay Decision-makers Perceive and Interpret Uncertainty in Human-AI Decision Making",
        "authors": "Snehal Prabhudesai, Leyao Yang, Sumit Asthana, Xun Huan, Q. Vera Liao, Nikola Banovic",
        "published": "2023-3-27",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3581641.3584033"
    },
    {
        "id": 18288,
        "title": "Organ donation: Key factors influencing the younger generation's decision-making in China",
        "authors": "Xiulan Chen, Wei Wei, Weili Ai",
        "published": "2023-2-6",
        "citations": 1,
        "abstract": "BackgroundThe organ transplantation sector in China is facing a severe shortage of donors, and the organ donation rate needs to be increased. Since 2015, voluntary donation by citizens has become the only source of organs for transplantation in China. In recent years, there has been a relatively positive change in young people's attitudes toward organ donation after death. The aim of the study was to understand young people's perceptions and attitudes toward organ donation and the factors that influence them and can positively impact the promotion of organ donation.MethodsBy analyzing relevant literature and legal texts, we developed a questionnaire. Information was obtained through questionnaires and interviews, and 501 valid questionnaires were returned from the target group. A chi-square test was used to examine whether there were significant differences in the willingness to organ donation among young people with different characteristics. A factor analysis was used to investigate the main factors influencing the different attitudes of young people toward organ donation, and a one-way ANOVA was used to examine whether young people with different characteristics were affected differently by different factors.ResultsIn our survey of young people aged 18–30 years, 99.2% of respondents knew about organ donation, 47.1% were willing to donate organs, and 15.2% understood that there were corresponding laws and regulations for organ donation. The study's findings showed that urban residents are more willing to be organ donators than rural residents; people with higher education levels have better awareness and are more willing to donate an organ; and people with religious beliefs are more likely to donate organs. The main factors that support the willingness of young people to donate are the social environment that provides support, their optimism in dealing with death, and their desire to realize their final value after death. The main factors for those unwilling to donate were low awareness or misconceptions about organ donation among individuals and their families and their attitudes toward death. As the people who took the questionnaire are probably interested in organ donation, the sample results will show a higher percentage of people who know about organ donation. We hope to discuss further with a larger and broader sample coverage to improve the estimates' validity and reflect the overall picture more accurately in a future study.ConclusionYoung people knew about organ donation but had a low depth of awareness. Household registration type, education level, and religious affiliation significantly correlate with people's willingness to donate. The supportive environment for organ donation in society and the correct understanding of the organ donation process and laws and regulations can influence people's willingness to donate.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fpubh.2023.1052875"
    },
    {
        "id": 18289,
        "title": "Generative AI for Business Decision-Making: A Case of ChatGPT",
        "authors": "Euclides Lourenco Chuma, Gabriel Gomes De Oliveira",
        "published": "2023-7-1",
        "citations": 4,
        "abstract": "ChatGPT (Generative Pretrained Transformer) is a chatbot using artificial intelligence (AI) launched by OpenAI, which is an AI research and deployment company. The ChatGPT has taken the technology world by storm. The ChatGPT is a trained AI model that can chat almost like a human. The dialog format allows the ChatGPT to answer follow-up questions, admit mistakes, challenge incorrect premises, and reject inappropriate requests. The ChatGPT can be utilized for compiling research, drafting marketing content, brainstorming ideas, delivering aftercare services, increasing customer engagement, and many others. The ChatGPT can provide enormous opportunities for companies leveraging this breakthrough technology strategically. Thus, we evaluate ChatGPT as a tool in common business decision-making cases in the current study. For example, the ChatGPT was asked about the impacts of a hypothetical merging of two supermarket chains in Sweden. In another example, the ChatGPT was asked about recommendations for investment in a Brazilian oil company. Finally, it was asked about the factors that influence online shopping behavior. The results are significant and demonstrate the tremendous potential of the ChatGPT in revolutionizing the corporate world.\n ",
        "keywords": "",
        "link": "http://dx.doi.org/10.52812/msbd.63"
    },
    {
        "id": 18290,
        "title": "Human-Centered Evaluation of Explanations in AI-Assisted Decision-Making",
        "authors": "Xinru Wang",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3640544.3645239"
    },
    {
        "id": 18291,
        "title": "“Exploring the Cognitive Framework: How Students Perceive AI in Financial Decision-Making”",
        "authors": "Shagun Tyagi, Himanshu Kargeti, Neha Rastogi, Rajesh Tiwari,  Anuj",
        "published": "2023-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icacctech61146.2023.00102"
    },
    {
        "id": 18292,
        "title": "Decision-making in the era of AI support—How decision environment and individual decision preferences affect advice-taking in forecasts.",
        "authors": "Bernadette Mayer, Florian Fuchs, Volker Lingnau",
        "published": "2023-3",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1037/npe0000170"
    },
    {
        "id": 18293,
        "title": "An Object-Oriented Neural Representation and its Implication Towards Explainable Ai",
        "authors": "Enoch Arulprakash, Martin Aruldoss",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4394380"
    },
    {
        "id": 18294,
        "title": "Towards Trustworthy Telemedicine: Applying Explainable AI for Remote Healthcare Recommendations",
        "authors": "",
        "published": "2023-11-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.58257/ijprems32270"
    },
    {
        "id": 18295,
        "title": "Quantitative Explainable AI For Face Recognition",
        "authors": "Shu Peng, Naipeng Dong, Guangdong Bai",
        "published": "2023-6-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceccs59891.2023.00014"
    },
    {
        "id": 18296,
        "title": "Human-Centered Explainable AI at the Edge for eHealth",
        "authors": "Joy Dutta, Deepak Puthal",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/edge60047.2023.00044"
    },
    {
        "id": 18297,
        "title": "Enhancing crop recommendation systems with explainable artificial intelligence: a study on agricultural decision-making",
        "authors": "Mahmoud Y. Shams, Samah A. Gamel, Fatma M. Talaat",
        "published": "2024-4",
        "citations": 0,
        "abstract": "AbstractCrop Recommendation Systems are invaluable tools for farmers, assisting them in making informed decisions about crop selection to optimize yields. These systems leverage a wealth of data, including soil characteristics, historical crop performance, and prevailing weather patterns, to provide personalized recommendations. In response to the growing demand for transparency and interpretability in agricultural decision-making, this study introduces XAI-CROP an innovative algorithm that harnesses eXplainable artificial intelligence (XAI) principles. The fundamental objective of XAI-CROP is to empower farmers with comprehensible insights into the recommendation process, surpassing the opaque nature of conventional machine learning models. The study rigorously compares XAI-CROP with prominent machine learning models, including Gradient Boosting (GB), Decision Tree (DT), Random Forest (RF), Gaussian Naïve Bayes (GNB), and Multimodal Naïve Bayes (MNB). Performance evaluation employs three essential metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared (R2). The empirical results unequivocally establish the superior performance of XAI-CROP. It achieves an impressively low MSE of 0.9412, indicating highly accurate crop yield predictions. Moreover, with an MAE of 0.9874, XAI-CROP consistently maintains errors below the critical threshold of 1, reinforcing its reliability. The robust R2 value of 0.94152 underscores XAI-CROP's ability to explain 94.15% of the data's variability, highlighting its interpretability and explanatory power.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-09391-2"
    },
    {
        "id": 18298,
        "title": "System Identification-informed Transparent and Explainable Machine Learning with Application to Power Consumption Forecasting",
        "authors": "Hua-Liang Wei",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceccme57830.2023.10252535"
    },
    {
        "id": 18299,
        "title": "Explainable AI for Bearing Fault Detection Systems: Gaining Human Trust",
        "authors": "Aparna Sinha, Shaik Fayaz Ahmed, Debanjan Das",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gcon58516.2023.10183502"
    },
    {
        "id": 18300,
        "title": "Hybrid cyber threats detection using explainable AI in Industrial IoT",
        "authors": "Yifan Liu, Shancang Li",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/hccs59561.2023.10452621"
    },
    {
        "id": 18301,
        "title": "Explaining Optimal Trajectories Using Indirect Methods and Explainable AI",
        "authors": "Kshitij Mall, Sean Nolan, Daniel DeLaurentis",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2024-2120"
    },
    {
        "id": 18302,
        "title": "A framework of distributionally robust possibilistic optimization",
        "authors": "Romain Guillaume, Adam Kasperski, Paweł Zieliński",
        "published": "2024-2-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10700-024-09420-2"
    },
    {
        "id": 18303,
        "title": "Consumer preference towards buying AI-enabled devices: A systematic Review",
        "authors": "Aniruddh Saxena, Akanksha Upadhyaya",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4108/eai.16-12-2022.2326198"
    },
    {
        "id": 18304,
        "title": "Understanding the Causes and Solutions of AI Induced Misinformation Impacting the Decision Making Behavior of Students",
        "authors": "Vo Quoc Huy Huy",
        "published": "2023-12-22",
        "citations": 0,
        "abstract": "This research aims to investigate the causes and potential solutions for AI induced misinformation that impacts the decision making behavior of students. With the increasing prevalence of artificial intelligence (AI) technologies in our daily lives, thereis growing concern about the spread of misinformation and its influence on individuals' decision making processes. This study seeks to explore the underlying factors that contribute to the dissemination of AI induced misinformation, including algorithm biases, echo chambers, and the lack of critical thinking skills among students. Additionally, the research aims to identify effective strategies and interventions to mitigate the negative effects of AI induced misinformation on students' decision making behavior. By understanding the causes and developing potential solutions, this study intends to contribute to the development of informed decision making practices in the context of AI technologies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.59670/ml.v21is1.6452"
    }
]