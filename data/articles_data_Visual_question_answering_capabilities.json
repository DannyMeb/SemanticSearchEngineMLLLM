[
    {
        "id": 29705,
        "title": "Visual Question Answering Analysis: Datasets, Methods, and Image Featurization Techniques",
        "authors": "Vijay Kumari, Abhimanyu Sethi, Yashvardhan Sharma, Lavika Goel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011655900003411"
    },
    {
        "id": 29706,
        "title": "Extracting Visual Semantic Information for Solving Visual Question Answering",
        "authors": "Yuxiang Huang",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdsca59871.2023.10393030"
    },
    {
        "id": 29707,
        "title": "A visual question answering method based on question intention",
        "authors": "Kai Wang, Yun Pan, Xiang Yao",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icsp58490.2023.10248540"
    },
    {
        "id": 29708,
        "title": "Question-conditioned debiasing with focal visual context fusion for visual question answering",
        "authors": "Jin Liu, GuoXiang Wang, ChongFeng Fan, Fengyu Zhou, HuiJuan Xu",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.110879"
    },
    {
        "id": 29709,
        "title": "Question-Guided Graph Convolutional Network for Visual Question Answering Based on Object-Difference",
        "authors": "Minchang Huangfu, Yushui Geng",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/swc57546.2023.10449195"
    },
    {
        "id": 29710,
        "title": "Retrieving Multimodal Prompts for Generative Visual Question Answering",
        "authors": "Timothy Ossowski, Junjie Hu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.158"
    },
    {
        "id": 29711,
        "title": "Outside-Knowledge Visual Question Answering for Visual Impaired People",
        "authors": "Yeqi Sun, Qingyi Si, Zheng Lin, Weiping Wang, Ting Mei",
        "published": "2023-11-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/medai59581.2023.00014"
    },
    {
        "id": 29712,
        "title": "Toward Unsupervised Realistic Visual Question Answering",
        "authors": "Yuwei Zhang, Chih-Hui Ho, Nuno Vasconcelos",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01431"
    },
    {
        "id": 29713,
        "title": "Enhancing Visual Question Answering via Deconstructing Questions and Explicating Answers",
        "authors": "Feilong Chen, Minglun Han, Jing Shi, Shuang Xu, Bo Xu",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-539"
    },
    {
        "id": 29714,
        "title": "An Visual Question Answering System using Cross-Attention Network",
        "authors": "R Dhana Lakshmi, S Abirami",
        "published": "2023-8-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icoac59537.2023.10249985"
    },
    {
        "id": 29715,
        "title": "Confidence-based interactable neural-symbolic visual question answering",
        "authors": "Yajie Bao, Tianwei Xing, Xun Chen",
        "published": "2024-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126991"
    },
    {
        "id": 29716,
        "title": "Visual Question Answering Bahasa Indonesia Berbasis Deep Learning untuk Pembelajaran Visual Anak TK",
        "authors": "Asiyah Hanifah, Rizka Wakhidatus Sholikah, R.V. Hari Ginardi",
        "published": "2024-2-21",
        "citations": 0,
        "abstract": "Indonesia semakin gencar melakukan persiapan transformasi digital dalam berbagai sektor, termasuk dalam bidang pendidikan. Salah satu upaya yang dilakukan pemerintah adalah dengan mengimplementasikan platform e-learning dalam kegiatan belajar mengajar hingga ke jenjang taman kanak-kanak. Metode pembelajaran visual pada taman kanak-kanak dapat diimplementasikan ke dalam e-learning yang lebih interaktif dan menarik dengan sistem Visual Question Answering (VQA). Sistem VQA dapat memberikan pertanyaan terkait dengan gambar yang ditampilkan dan mengecek kesesuaian jawaban dari siswa secara otomatis. Pada penelitian ini dibangun sistem VQA yang dapat menerima pertanyaan berbahasa Indonesia dan mengoreksi jawaban dalam bahasa Indonesia. Sistem dibangun dengan menggunakan model Bootstrapping Language-Image Pre-training (BLIP) untuk VQA dan model No Language Left Behind (NLLB) untuk penerjemahan. Uji coba dilakukan pada enam jenis jawaban yaitu ya/tidak, kata benda, kata kerja, kata sifat, kata keterangan, dan numeral. Hasil pengujian menunjukkan bahwa sistem dapat menjawab dengan nilai ketepatan 100 untuk jawaban ya/tidak, kata benda, kata kerja, dan numeral. Sementara untuk kata sifat dan kata keterangan masing-masing memiliki nilai ketepatan 62,5 dan 87,5.",
        "keywords": "",
        "link": "http://dx.doi.org/10.62411/tc.v23i1.9694"
    },
    {
        "id": 29717,
        "title": "Incorporation of Question Segregation Procedure in Visual Question Answering Models",
        "authors": "Doli Phukan, Badal Soni, Souvik Chowdhury",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1504/ijcsm.2023.10058564"
    },
    {
        "id": 29718,
        "title": "A Symbolic-Neural Reasoning Model for Visual Question Answering",
        "authors": "Jingying Gao, Alan Blair, Maurice Pagnucco",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191538"
    },
    {
        "id": 29719,
        "title": "Interpretable Visual Question Answering Via Reasoning Supervision",
        "authors": "Maria Parelli, Dimitrios Mallis, Markos Diomataris, Vassilis Pitsikalis",
        "published": "2023-10-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10223156"
    },
    {
        "id": 29720,
        "title": "Multimodal Visual Question Answering Model Enhanced with Image Emotional Information",
        "authors": "Jin Cai, Guoyong Cai",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icnlp58431.2023.00056"
    },
    {
        "id": 29721,
        "title": "A Retriever-Reader Framework with Visual Entity Linking for Knowledge-Based Visual Question Answering",
        "authors": "Jiuxiang You, Zhenguo Yang, Qing Li, Wenyin Liu",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00011"
    },
    {
        "id": 29722,
        "title": "Visual question answering on blood smear images using convolutional block attention module powered object detection",
        "authors": "A. Lubna, Saidalavi Kalady, A. Lijiya",
        "published": "2024-4-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00371-024-03359-6"
    },
    {
        "id": 29723,
        "title": "RS-MCQA: Multi-class Question Aware Visual Question Answering for Optical Remote Sensing Datasets",
        "authors": "Hitul Desai, Debabrata Pal, Ankit Jha, Avik Hati, Biplab Banerjee",
        "published": "2023-7-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isscs58449.2023.10190896"
    },
    {
        "id": 29724,
        "title": "Knowledge-based Visual Question Answering about Named Entities",
        "authors": "Paul Lerner",
        "published": "2023-12",
        "citations": 0,
        "abstract": "This thesis is positioned at the intersection of several research fields, Natural Language Processing, Information Retrieval (IR) and Computer Vision, which have unified around representation learning and pre-training methods. We have defined and studied a new multimodal task: Knowledge-based Visual Question Answering about Named Entities (KVQAE). We were particularly interested in cross-modal interactions and different ways of representing named entities. We also focused on data used to train and, more importantly, evaluate Question Answering systems through different metrics. We annotated a dataset for this purpose, the first in KVQAE comprising various types of entities. We also defined an experimental framework for dealing with KVQAE in two stages through an unstructured knowledge base and identified IR as the main bottleneck of KVQAE, especially for questions about non-person entities. To improve the IR stage, we studied different multimodal fusion methods, which are pre-trained through an original task: the Multimodal Inverse Cloze Task. We found that these models leveraged a cross-modal interaction that we had not originally considered, and which may address the heterogeneity of visual representations of named entities. These results were strengthened by a study of the CLIP model, which allows this cross-modal interaction to be modeled directly.\n\nAwarded by\n            : Université Paris-Saclay, Orsay, France on 8 November 2023.\n          \n\nSupervised by\n            : Olivier Ferret and Camille Guinaudeau.\n          \n\nAvailable at\n            : https://www.theses.fr/s247993.\n          ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3642979.3643009"
    },
    {
        "id": 29725,
        "title": "Visual Question Answering in the Medical Domain",
        "authors": "Louisa Canepa, Sonit Singh, Arcot Sowmya",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dicta60407.2023.00059"
    },
    {
        "id": 29726,
        "title": "Benchmarking Out-of-Distribution Detection in Visual Question Answering",
        "authors": "Xiangxi Shi, Stefan Lee",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv57701.2024.00540"
    },
    {
        "id": 29727,
        "title": "Cross-Attention Based Text-Image Transformer for Visual Question\nAnswering",
        "authors": "Mahdi Rezapour",
        "published": "2024-1-30",
        "citations": 0,
        "abstract": "\nBackground:\nVisual question answering (VQA) is a challenging task that requires\nmultimodal reasoning and knowledge. The objective of VQA is to answer natural language\nquestions based on corresponding present information in a given image. The challenge of VQA\nis to extract visual and textual features and pass them into a common space. However, the\nmethod faces the challenge of object detection being present in an image and finding the relationship between objects.\n\n\nMethods:\nIn this study, we explored different methods of feature fusion for VQA, using pretrained models to encode the text and image features and then applying different attention\nmechanisms to fuse them. We evaluated our methods on the DAQUAR dataset.\n\n\nResults:\nWe used three metrics to measure the performance of our methods: WUPS, Acc, and\nF1. We found that concatenating raw text and image features performs slightly better than selfattention for VQA. We also found that using text as query and image as key and value performs worse than other methods of cross-attention or self-attention for VQA because it might\nnot capture the bidirectional interactions between the text and image modalities\n\n\nConclusion:\nIn this paper, we presented a comparative study of different feature fusion methods for VQA, using pre-trained models to encode the text and image features and then applying\ndifferent attention mechanisms to fuse them. We showed that concatenating raw text and image\nfeatures is a simple but effective method for VQA while using text as query and image as key\nand value is a suboptimal method for VQA. We also discussed the limitations and future directions of our work.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.2174/0126662558291150240102111855"
    },
    {
        "id": 29728,
        "title": "Interpretable Visual Question Answering Referring to Outside Knowledge",
        "authors": "He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama",
        "published": "2023-10-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222423"
    },
    {
        "id": 29729,
        "title": "Expert Evaluation of Export Control-Related Question Answering Capabilities of LLMs",
        "authors": "Rafal Rzepka, Shinji Muraji, Akihiko Obayashi",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csde59766.2023.10487735"
    },
    {
        "id": 29730,
        "title": "Object-Assisted Question Featurization and Multi-CNN Image Feature Fusion for Visual Question Answering",
        "authors": "Sruthy Manmadhan, Binsu C. Kovoor",
        "published": "2023-3-3",
        "citations": 0,
        "abstract": "Visual question answering (VQA) demands a meticulous and concurrent proficiency in image interpretation and natural language understanding to correctly answer the question about an image. The existing VQA solutions either focus only on improving the joint multi-modal embedding or on the fine-tuning of visual understanding through attention. This research, in contrast to the current trend, investigates the feasibility of an object-assisted language understanding strategy titled semantic object ranking (SOR) framework for VQA. The proposed system refines the natural language question representation with the help of detected visual objects. For multi-CNN image representation, the system employs canonical correlation analysis (CCA). The suggested model is assessed using accuracy and WUPS measures on the DAQUAR dataset. On the DAQUAR dataset, the analytical outcomes reveal that the presented system outperforms the prior state-of-the-art by a significant factor. In addition to the quantitative analysis, proper illustrations are supplied to observe the reasons for performance improvement.",
        "keywords": "",
        "link": "http://dx.doi.org/10.4018/ijiit.318671"
    },
    {
        "id": 29731,
        "title": "Question-guided feature pyramid network for medical visual question answering",
        "authors": "Yonglin Yu, Haifeng Li, Hanrong Shi, Lin Li, Jun Xiao",
        "published": "2023-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2022.119148"
    },
    {
        "id": 29732,
        "title": "Multilingual Augmentation for Robust Visual Question Answering in Remote Sensing Images",
        "authors": "Zhenghang Yuan, Lichao Mou, Xiao Xiang Zhu",
        "published": "2023-5-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jurse57346.2023.10144189"
    },
    {
        "id": 29733,
        "title": "Counting-based visual question answering with serial cascaded attention deep learning",
        "authors": "Tesfayee MeshuWelde, Lejian Liao",
        "published": "2023-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patcog.2023.109850"
    },
    {
        "id": 29734,
        "title": "IMCN: Improved modular co-attention networks for visual question answering",
        "authors": "Cheng Liu, Chao Wang, Yan Peng",
        "published": "2024-4-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10489-024-05456-4"
    },
    {
        "id": 29735,
        "title": "Caption based Co-attention Architecture for Open-Ended Visual Question Answering",
        "authors": "Chandra Churh Chatterjee, C. Chandra Sekhar",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/indicon59947.2023.10440681"
    },
    {
        "id": 29736,
        "title": "Cross-Modal Dense Passage Retrieval for Outside Knowledge Visual Question Answering",
        "authors": "Benjamin Reichman, Larry Heck",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00304"
    },
    {
        "id": 29737,
        "title": "Improving Visual Question Answering by Multimodal Gate Fusion Network",
        "authors": "Shenxiang Xiang, Qiaohong Chen, Xian Fang, Menghao Guo",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191453"
    },
    {
        "id": 29738,
        "title": "From Pixels to Explanations: Uncovering the Reasoning Process in Visual Question Answering",
        "authors": "Siqi Zhang, Jing Liu, Zhihua Wei",
        "published": "2023-12-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3595916.3626376"
    },
    {
        "id": 29739,
        "title": "Advanced Visual and Textual Co-context Aware Attention Network with Dependent Multimodal Fusion Block for Visual Question Answering",
        "authors": "Hesam Shokri Asri, Reza Safabakhsh",
        "published": "2024-3-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-024-18871-z"
    },
    {
        "id": 29740,
        "title": "Symmetric Graph-Based Visual Question Answering Using Neuro-Symbolic Approach",
        "authors": "Jiyoun Moon",
        "published": "2023-9-7",
        "citations": 0,
        "abstract": "As the applications of robots expand across a wide variety of areas, high-level task planning considering human–robot interactions is emerging as a critical issue. Various elements that facilitate flexible responses to humans in an ever-changing environment, such as scene understanding, natural language processing, and task planning, are thus being researched extensively. In this study, a visual question answering (VQA) task was examined in detail from among an array of technologies. By further developing conventional neuro-symbolic approaches, environmental information is stored and utilized in a symmetric graph format, which enables more flexible and complex high-level task planning. We construct a symmetric graph composed of information such as color, size, and position for the objects constituting the environmental scene. VQA, using graphs, largely consists of a part expressing a scene as a graph, a part converting a question into SPARQL, and a part reasoning the answer. The proposed method was verified using a public dataset, CLEVR, with which it successfully performed VQA. We were able to directly confirm the process of inferring answers using SPARQL queries converted from the original queries and environmental symmetric graph information, which is distinct from existing methods that make it difficult to trace the path to finding answers.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/sym15091713"
    },
    {
        "id": 29741,
        "title": "Investigation of Available Datasets and Techniques for Visual Question Answering",
        "authors": "Lata Bhavnani, Dr. Narendra Patel",
        "published": "2023-8-3",
        "citations": 0,
        "abstract": "Visual Question Answering (VQA) is an emerging AI research problem that combines computer vision, natural language processing, knowledge representation & reasoning (KR). Given image and question related to the image as input, it requires analysis of visual components of the image, type of question, and common sense or general knowledge to predict the right answer. VQA is useful in different real-time applications like blind person assistance, autonomous driving, solving trivial tasks like spotting empty tables in hotels, parks, or picnic places, etc. Since its introduction in 2014, many researchers have worked and applied different techniques for Visual question answering. Also, different datasets have been introduced. This paper presents an overview of available datasets and evaluation metrices used in the VQA area. Further paper presents different techniques used in the VQA domain. Techniques are categorized based on the mechanism used. Based on the detailed discussion and performance comparison we discuss various challenges in the VQA domain and provide directions for future work.",
        "keywords": "",
        "link": "http://dx.doi.org/10.47164/ijngc.v14i3.767"
    },
    {
        "id": 29742,
        "title": "Knowledge Blended Open Domain Visual Question Answering using Transformer",
        "authors": "Dipali Koshti, Ashutosh Gupta, Mukesh Kalla",
        "published": "2023-2-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icais56108.2023.10073911"
    },
    {
        "id": 29743,
        "title": "Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering",
        "authors": "Nandita Naik, Christopher Potts, Elisa Kreiss",
        "published": "2023-10-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00301"
    },
    {
        "id": 29744,
        "title": "FVQA 2.0: Introducing Adversarial Samples into Fact-based Visual Question Answering",
        "authors": "Weizhe Lin, Zhilin Wang, Bill Byrne",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.11"
    },
    {
        "id": 29745,
        "title": "RESCUENet-VQA: A Large-Scale Visual Question Answering Benchmark for Damage Assessment",
        "authors": "Argho Sarkar, Maryam Rahnemoonfar",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10281747"
    },
    {
        "id": 29746,
        "title": "FashionVQA: A Domain-Specific Visual Question Answering System",
        "authors": "Min Wang, Ata Mahjoubfar, Anupama Joshi",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00356"
    },
    {
        "id": 29747,
        "title": "PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese",
        "authors": "Nghia Hieu Nguyen, Kiet Van Nguyen",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mapr59823.2023.10288833"
    },
    {
        "id": 29748,
        "title": "Parallel multi-head attention and term-weighted question embedding for medical visual question answering",
        "authors": "Sruthy Manmadhan, Binsu C Kovoor",
        "published": "2023-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-14981-2"
    },
    {
        "id": 29749,
        "title": "Delving Deeper into Cross-lingual Visual Question Answering",
        "authors": "Chen Liu, Jonas Pfeiffer, Anna Korhonen, Ivan Vulić, Iryna Gurevych",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.186"
    },
    {
        "id": 29750,
        "title": "Multi-modal Domain Adaptation for Text Visual Question Answering Tasks",
        "authors": "Zhiyuan Li, Dongnan Liu, Weidong Cai",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dicta60407.2023.00058"
    },
    {
        "id": 29751,
        "title": "Additive Attention for Medical Visual Question Answering",
        "authors": "Ya Liu, Nan Yang, Shuanglong Yao, Xing Wang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icftic59930.2023.10456252"
    },
    {
        "id": 29752,
        "title": "Visual Question Answering Optimized Framework using Mixed Precision Training",
        "authors": "Souvik Chowdhury, Badal Soni",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaia57370.2023.10169318"
    },
    {
        "id": 29753,
        "title": "Barlow constrained optimization for Visual Question Answering",
        "authors": "Abhishek Jha, Badri Patro, Luc Van Gool, Tinne Tuytelaars",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00114"
    },
    {
        "id": 29754,
        "title": "Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering",
        "authors": "Chenxi Whitehouse, Tillman Weyde, Pranava Madhyastha",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.126"
    },
    {
        "id": 29755,
        "title": "Research on visual question answering system based on deep neural network model",
        "authors": "Xushuo Tang",
        "published": "2023-3-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2669805"
    },
    {
        "id": 29756,
        "title": "Scene text visual question answering by using YOLO and STN",
        "authors": "Kimiya Nourali, Elham Dolkhani",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10772-023-10081-6"
    },
    {
        "id": 29757,
        "title": "Visual Question Generation Answering (VQG-VQA) using Machine Learning Models",
        "authors": "Atul Kachare, Mukesh Kalla, Ashutosh Gupta",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "Presented automated visual question-answer system generates graphics-based question-answer pairs. The system consists of the Visual Query Generation (VQG) and Visual Question Answer (VQA) modules. VQG generates questions based on visual cues, and VQA provides matching answers to the VQG modules. VQG system generates questions using LSTM and VGG19 model, training parameters, and predicting words with the highest probability for output. VQA uses VGG-19 convolutional neural network for image encoding, embedding, and multilayer perceptron for high-quality responses. The proposed system reduces the need for human annotation and thus supports the traditional education sector by significantly reducing the human intervention required to generate text queries. The system can be used in interactive interfaces to help young children learn.",
        "keywords": "",
        "link": "http://dx.doi.org/10.37394/23202.2023.22.67"
    },
    {
        "id": 29758,
        "title": "Balancing and Contrasting Biased Samples for Debiased Visual Question Answering",
        "authors": "Runlin Cao, Zhixin Li",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdm58522.2023.00108"
    },
    {
        "id": 29759,
        "title": "Visual AI for Satellite Imagery Perspective: A Visual Question Answering Framework in the Geospatial Domain",
        "authors": "Siddharth Bhorge, Milind Rane, Nikhil Rane, Mandar Patil, Prathamesh Saraf, Jyotika Nilgar",
        "published": "2023-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/i2ct57861.2023.10126467"
    },
    {
        "id": 29760,
        "title": "A Simple Baseline for Knowledge-Based Visual Question Answering",
        "authors": "Alexandros Xenos, Themos Stafylakis, Ioannis Patras, Georgios Tzimiropoulos",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.919"
    },
    {
        "id": 29761,
        "title": "Progress in Combining Image Captioning and Visual Question Answering Tasks",
        "authors": "Prathiksha Kamath, Pratibha Jamkhandi, Prateek Ghanti, Priyanshu Gupta, M. Lakshmi Neelima",
        "published": "2023-5-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/incet57972.2023.10170744"
    },
    {
        "id": 29762,
        "title": "Guiding Visual Question Answering with Attention Priors",
        "authors": "Thao Minh Le, Vuong Le, Sunil Gupta, Svetha Venkatesh, Truyen Tran",
        "published": "2023-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00436"
    },
    {
        "id": 29763,
        "title": "Aggregated Co-attention based Visual Question Answering",
        "authors": "Aakansha Mishra, Ashish Anand, Prithwijit Guha",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3627631.3627659"
    },
    {
        "id": 29764,
        "title": "Self-Attention Based Image Feature Representation for Medical Visual Question Answering",
        "authors": "Sushmita Upadhyay, Sanjaya Shankar Tripathy",
        "published": "2024-1-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciec59440.2024.10468242"
    },
    {
        "id": 29765,
        "title": "Overcoming Language Bias in Remote Sensing Visual Question Answering Via Adversarial Training",
        "authors": "Zhenghang Yuan, Lichao Mou, Xiao Xiang Zhu",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10282946"
    },
    {
        "id": 29766,
        "title": "Bidirectional Contrastive Split Learning for Visual Question Answering",
        "authors": "Yuwei Sun, Hideya Ochiai",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Visual Question Answering (VQA) based on multi-modal data facilitates real-life applications such as home robots and medical diagnoses. One significant challenge is to devise a robust decentralized learning framework for various client models where centralized data collection is refrained due to confidentiality concerns. This work aims to tackle privacy-preserving VQA by decoupling a multi-modal model into representation modules and a contrastive module, leveraging inter-module gradients sharing and inter-client weight sharing. To this end, we propose Bidirectional Contrastive Split Learning (BiCSL) to train a global multi-modal model on the entire data distribution of decentralized clients. We employ the contrastive loss that enables a more efficient self-supervised learning of decentralized modules. Comprehensive experiments are conducted on the VQA-v2 dataset based on five SOTA VQA models, demonstrating the effectiveness of the proposed method. Furthermore, we inspect BiCSL's robustness against a dual-key backdoor attack on VQA. Consequently, BiCSL shows significantly enhanced resilience when exposed to the multi-modal adversarial attack compared to the centralized learning method, which provides a promising approach to decentralized multi-modal learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i19.30158"
    },
    {
        "id": 29767,
        "title": "Causal Reasoning through Two Cognition Layers for Improving Generalization in Visual Question Answering",
        "authors": "Trang Nguyen, Naoaki Okazaki",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.573"
    },
    {
        "id": 29768,
        "title": "A Review of Recent Advances in Visual Question Answering: Capsule Networks and Vision Transformers in Focus",
        "authors": "Miranda Surya Prakash,  , S N Devananda",
        "published": "2024-1-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.17485/ijst/v16i47.2643"
    },
    {
        "id": 29769,
        "title": "PubMedCLIP: How Much Does CLIP Benefit Visual Question Answering in the Medical Domain?",
        "authors": "Sedigheh Eslami, Christoph Meinel, Gerard de Melo",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.88"
    },
    {
        "id": 29770,
        "title": "FAQ-Based Question Answering Systems with Query-Question and Query-Answer Similarity",
        "authors": "Vijay Kumari, Miloni Mittal, Yashvardhan Sharma, Lavika Goel",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012454800003636"
    },
    {
        "id": 29771,
        "title": "Improving the Cross-Lingual Generalisation in Visual Question Answering",
        "authors": "Farhad Nooralahzadeh, Rico Sennrich",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "While several benefits were realized for multilingual vision-language pretrained models, recent benchmarks across various tasks and languages showed poor cross-lingual generalisation when multilingually pre-trained vision-language models are applied to non-English data, with a large gap between (supervised) English performance and (zero-shot) cross-lingual transfer. In this work, we explore the poor performance of these models on a zero-shot cross-lingual visual question answering (VQA) task, where models are fine-tuned on English visual-question data and evaluated on 7 typologically diverse languages. We improve cross-lingual transfer with three strategies: (1) we introduce a linguistic prior objective to augment the cross-entropy loss with a similarity-based loss to guide the model during training, (2) we learn a task-specific subnetwork that improves cross-lingual generalisation and reduces variance without model modification, (3) we augment training examples using synthetic code-mixing to promote alignment of embeddings between source and target languages. Our experiments on xGQA using the pretrained multilingual multimodal transformers UC2 and M3P demonstrates the consistent effectiveness of the proposed fine-tuning strategy for 7 languages, outperforming existing transfer methods with sparse models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i11.26574"
    },
    {
        "id": 29772,
        "title": "Encoder–decoder cycle for visual question answering based on perception-action cycle",
        "authors": "Safaa Abdullahi Moallim Mohamud, Amin Jalali, Minho Lee",
        "published": "2023-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patcog.2023.109848"
    },
    {
        "id": 29773,
        "title": "Knowledge-Based Visual Question Answering Using Multi-Modal Semantic Graph",
        "authors": "Lei Jiang, Zuqiang Meng",
        "published": "2023-3-14",
        "citations": 5,
        "abstract": "The field of visual question answering (VQA) has seen a growing trend of integrating external knowledge sources to improve performance. However, owing to the potential incompleteness of external knowledge sources and the inherent mismatch between different forms of data, current knowledge-based visual question answering (KBVQA) techniques are still confronted with the challenge of effectively integrating and utilizing multiple heterogeneous data. To address this issue, a novel approach centered on a multi-modal semantic graph (MSG) is proposed. The MSG serves as a mechanism for effectively unifying the representation of heterogeneous data and diverse types of knowledge. Additionally, a multi-modal semantic graph knowledge reasoning model (MSG-KRM) is introduced to perform reasoning and deep fusion of image–text information and external knowledge sources. The development of the semantic graph involves extracting keywords from the image object detection information, question text, and external knowledge texts, which are then represented as symbol nodes. Three types of semantic graphs are then constructed based on the knowledge graph, including vision, question, and the external knowledge text, with non-symbol nodes added to connect these three independent graphs and marked with respective node and edge types. During the inference stage, the multi-modal semantic graph and image–text information are embedded into the feature semantic graph through three embedding methods, and a type-aware graph attention module is employed for deep reasoning. The final answer prediction is a blend of the output from the pre-trained model, graph pooling results, and the characteristics of non-symbolic nodes. The experimental results on the OK-VQA dataset show that the MSG-KRM model is superior to existing methods in terms of overall accuracy score, achieving a score of 43.58, and with improved accuracy for most subclass questions, proving the effectiveness of the proposed method.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12061390"
    },
    {
        "id": 29774,
        "title": "Integrating multimodal features by a two-way co-attention mechanism for visual question answering",
        "authors": "Himanshu Sharma, Swati Srivastava",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17945-8"
    },
    {
        "id": 29775,
        "title": "Transformer-based visual question answering model comparison",
        "authors": "Zhicheng He, Yuanzhi Li, Dingming Zhang",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "Abstract\nThe potential use of visual question and answer (VQA) models in fields including robotics, autonomous driving, and health care has recently attracted a lot of attention. However, the development of the multi-modular learning VL (Vision-Language Learning) model itself in the “pre-training + fine-tuning” mode is challenged under the rapid development of large-scale language models. Therefore, optimizing the value and feasibility of the single-task model itself is also worth thinking more cautiously. With both model principles, structures, and experimental contrast analysed in this paper, the two excellent models, LXMERT and UNITER, are evaluated to demonstrate the reasons for performance differences on specific downstream tasks VQA deeply. In addition, this paper provides insight into the possibility of further model optimization in the future for multi-modular tasks, especially VQA. In comparison to UNITER, the findings reveal greater accuracy in the training set and verification set from beginning for LXMERT, which indicates that the better model architecture and pre-training method is where LXMERT’s higher adaptability probably comes from. However, the UNITER, which has fewer parameters and smaller architecture, can also achieve fine results after fine-tuning. This paper highlights the differences in accuracy shown by existing Transformers-based VQA models in local test sets and explores the possibility of optimizing VQA systems through fine-tuning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1742-6596/2646/1/012031"
    },
    {
        "id": 29776,
        "title": "Multi-modal adaptive gated mechanism for visual question answering",
        "authors": "Yangshuyi Xu, Lin Zhang, Xiang Shen",
        "published": "2023-6-28",
        "citations": 2,
        "abstract": "Visual Question Answering (VQA) is a multimodal task that uses natural language to ask and answer questions based on image content. For multimodal tasks, obtaining accurate modality feature information is crucial. The existing researches on the visual question answering model mainly start from the perspective of attention mechanism and multimodal fusion, which will tend to ignore the impact of modal interaction learning and the introduction of noise information in the process of modal fusion on the overall performance of the model. This paper proposes a novel and efficient multimodal adaptive gated mechanism model, MAGM. The model adds an adaptive gate mechanism to the intra- and inter-modality learning and the modal fusion process. This model can effectively filter irrelevant noise information, obtain fine-grained modal features, and improve the ability of the model to adaptively control the contribution of the two modal features to the predicted answer. In intra- and inter-modality learning modules, the self-attention gated and self-guided-attention gated units are designed to filter text and image features’ noise information effectively. In modal fusion module, the adaptive gated modal feature fusion structure is designed to obtain fine-grained modal features and improve the accuracy of the model in answering questions. Quantitative and qualitative experiments on the two VQA task benchmark datasets, VQA 2.0 and GQA, proved that the method in this paper is superior to the existing methods. The MAGM model has an overall accuracy of 71.30% on the VQA 2.0 dataset and an overall accuracy of 57.57% on the GQA dataset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1371/journal.pone.0287557"
    },
    {
        "id": 29777,
        "title": "Zero-shot Visual Question Answering with Language Model Feedback",
        "authors": "Yifan Du, Junyi Li, Tianyi Tang, Wayne Xin Zhao, Ji-Rong Wen",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.590"
    },
    {
        "id": 29778,
        "title": "A Survey of Language Priors for Visual Question Answering",
        "authors": "Hantao Xu, Xia Ye, Zhangping Yang, Pujie Zhao",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "In recent years, with the development of deep learning technology, visual question answering tasks have gradually attracted the attention of scientific researchers. Due to the continuous improvement of relevant large-scale standard data sets, a large number of visual questions answering research results have been released one after another, and the accuracy rate of the visual question answering model based on deep learning on the data set has been continuously improved. Recent studies have found that the previously proposed visual question answering model has different degrees of data set language prior problems, that is, the model is overly dependent on the strong phase between the question and the answer in the training process. Many articles briefly describe various research methods, and look forward to the future development direction of alleviating the prior problem of visual question answering based on the existing research.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/fcis.v4i2.9941"
    },
    {
        "id": 29779,
        "title": "Enhancing yes/no question answering with weak supervision via extractive question answering",
        "authors": "Dimitris Dimitriadis, Grigorios Tsoumakas",
        "published": "2023-11",
        "citations": 0,
        "abstract": "AbstractThe effectiveness of natural language processing models relies on various factors, including the architecture, number of parameters, data used during training, and the tasks they were trained on. Recent studies indicate that models pre-trained on large corpora and fine-tuned on task-specific datasets, covering multiple tasks, can generate remarkable results across various benchmarks. We propose a new approach based on a straightforward hypothesis: improving model performance on a target task by considering other artificial tasks defined on the same training dataset. By doing so, the model can gain further insights into the training dataset and attain a greater understanding, improving efficiency on the target task. This approach differs from others that consider multiple pre-existing tasks on different datasets. We validate this hypothesis by focusing on the problem of answering yes/no questions and introducing a multi-task model that outputs a span of the reference text, serving as evidence for answering the question. The task of span extraction is an artificial one, designed to benefit the performance of the model answering yes/no questions. We acquire weak supervision for these spans, by using a pre-trained extractive question answering model, dispensing the need for costly human annotation. Our experiments, using modern transformer-based language models, demonstrate that this method outperforms the standard approach of training models to answer yes/no questions. Although the primary objective was to enhance the performance of the model in answering yes/no questions, it was discovered that span texts are a significant source of information. These spans, derived from the question reference texts, provided valuable insights for the users to better comprehend the answers to the questions. The model’s improved accuracy in answering yes/no questions, coupled with the supplementary information provided by the span texts, led to a more comprehensive and informative user experience.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10489-023-04751-w"
    },
    {
        "id": 29780,
        "title": "Question-Aware Global-Local Video Understanding Network for Audio-Visual Question Answering",
        "authors": "Zailong Chen, Lei Wang, Peng Wang, Peng Gao",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tcsvt.2023.3318220"
    },
    {
        "id": 29781,
        "title": "Dual Attention and Question Categorization-Based Visual Question Answering",
        "authors": "Aakansha Mishra, Ashish Anand, Prithwijit Guha",
        "published": "2023-2",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2022.3160418"
    },
    {
        "id": 29782,
        "title": "Context-aware Multi-level Question Embedding Fusion for visual question answering",
        "authors": "Shengdong Li, Chen Gong, Yuqing Zhu, Chuanwen Luo, Yi Hong, Xueqiang Lv",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.inffus.2023.102000"
    },
    {
        "id": 29783,
        "title": "MoQA: Benchmarking Multi-Type Open-Domain Question Answering",
        "authors": "Howard Yen, Tianyu Gao, Jinhyuk Lee, Danqi Chen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.dialdoc-1.2"
    },
    {
        "id": 29784,
        "title": "Emotion-Cause Pair Extraction as Question Answering",
        "authors": "Huu-Hiep Nguyen, Minh-Tien Nguyen",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011883100003393"
    },
    {
        "id": 29785,
        "title": "Logical Implications for Visual Question Answering Consistency",
        "authors": "Sergio Tascon-Morales, Pablo Márquez-Neila, Raphael Sznitman",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.00650"
    },
    {
        "id": 29786,
        "title": "S-VQA: Sentence-Based Visual Question Answering",
        "authors": "Sanchit Pathak, Garima Singh, Ashish Anand, Prithwijit Guha",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3627631.3627670"
    },
    {
        "id": 29787,
        "title": "Learning visual question answering on controlled semantic noisy labels",
        "authors": "Haonan Zhang, Pengpeng Zeng, Yuxuan Hu, Jin Qian, Jingkuan Song, Lianli Gao",
        "published": "2023-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patcog.2023.109339"
    },
    {
        "id": 29788,
        "title": "An Answer FeedBack Network for Visual Question Answering",
        "authors": "Weidong Tian, Ruihua Tian, Zhongqiu Zhao, Quan Ren",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191079"
    },
    {
        "id": 29789,
        "title": "Prompt-Based Personalized Federated Learning for Medical Visual Question Answering",
        "authors": "He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10445933"
    },
    {
        "id": 29790,
        "title": "A Logic-based Approach to Contrastive Explainability for Neurosymbolic Visual Question Answering",
        "authors": "Thomas Eiter, Tobias Geibinger, Nelson Higuera, Johannes Oetsch",
        "published": "2023-8",
        "citations": 2,
        "abstract": "Visual Question Answering (VQA) is a well-known problem for which deep-learning is key. This poses a challenge for explaining answers to questions, the more if advanced notions like contrastive explanations (CEs) should be provided. The latter explain why an answer has been reached in contrast to a different one and are attractive as they focus on reasons necessary to flip a query answer. We present a CE framework for VQA that uses a neurosymbolic VQA architecture which disentangles perception from reasoning. Once the reasoning part is provided as logical theory, we use answer-set programming, in which CE generation can be framed as an abduction problem. We validate our approach on the CLEVR dataset, which we extend by more sophisticated questions to further demonstrate the robustness of the modular architecture. While we achieve top performance compared to related approaches, we can also produce CEs for explanation, model debugging, and validation tasks, showing the versatility of the declarative approach to reasoning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/408"
    },
    {
        "id": 29791,
        "title": "MM-Reasoner: A Multi-Modal Knowledge-Aware Framework for Knowledge-Based Visual Question Answering",
        "authors": "Mahmoud Khademi, Ziyi Yang, Felipe Frujeri, Chenguang Zhu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.437"
    },
    {
        "id": 29792,
        "title": "MaXM: Towards Multilingual Visual Question Answering",
        "authors": "Soravit Changpinyo, Linting Xue, Michal Yarom, Ashish Thapliyal, Idan Szpektor, Julien Amelot, Xi Chen, Radu Soricut",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.176"
    },
    {
        "id": 29793,
        "title": "Digging out Discrimination Information from Generated Samples for Robust Visual Question Answering",
        "authors": "Zhiquan Wen, Yaowei Wang, Mingkui Tan, Qingyao Wu, Qi Wu",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.432"
    },
    {
        "id": 29794,
        "title": "Self-Supervised Learning based 3D Visual Question answering for Scene Understanding",
        "authors": "Xiang Li, Zhiguang Fan, Xuexiang Li, Nan Lin",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icftic59930.2023.10455775"
    },
    {
        "id": 29795,
        "title": "Using Markov Random Field (MRF) Hypergraph Transformer Method for Visual Question Answering (VQA) Application",
        "authors": "Jiawei Lin, Sei-Ichiro Kamata",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/prai59366.2023.10332038"
    },
    {
        "id": 29796,
        "title": "Application of a Neural Network-based Visual Question Answering System in Preschool Language Education",
        "authors": "Ying Cheng",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5573/ieiespc.2023.12.5.419"
    },
    {
        "id": 29797,
        "title": "Visual question answering model based on fusing global-local feature",
        "authors": "Guan Yang, Ziming Zhang, Xiaoming Liu",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2684385"
    },
    {
        "id": 29798,
        "title": "Feasibility of Visual Question Answering (VQA) for Post-Disaster Damage Detection Using Aerial Footage",
        "authors": "Rafael De Sa Lowande, Hakki Erhan Sevil",
        "published": "2023-4-19",
        "citations": 1,
        "abstract": "Natural disasters are a major source of significant damage and costly repairs around the world. After a natural disaster occurs, there is usually a significant amount of damage, and with that, there are also a lot of costs involved with repairing and aiding all the people involved. In addition, the occurrence of natural phenomena has increased significantly in the past decade. With that in mind, post-disaster damage detection is usually performed manually by human operators. Taking into consideration all the areas one has to closely look into, as well as the difficult terrain and places with hard access, it becomes easy to understand how incredibly difficult it is for a surveyor to identify and annotate every single possible damage out there. Because of that, it has become essential to find new creative solutions for damage detection and classification in the case of natural disasters, especially hurricanes. This study focuses on the feasibility of using a Visual Question Answering (VQA) method for post-disaster damage detection, using aerial footage taken from an Unmanned Aerial Vehicle (UAV). Two other approaches are also utilized to provide comparison and to evaluate the performance of VQA. Our case study on our custom dataset collected after Hurricane Sally shows successful results using VQA for post-disaster damage detection applications.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13085079"
    },
    {
        "id": 29799,
        "title": "Visual Question Answering reasoning with external knowledge based on bimodal graph neural network",
        "authors": "Zhenyu Yang, Lei Wu, Peian Wen, Peng Chen",
        "published": "2023",
        "citations": 0,
        "abstract": "<abstract><p>Visual Question Answering (VQA) with external knowledge requires external knowledge and visual content to answer questions about images. The defect of existing VQA solutions is that they need to identify task-related information in the obtained pictures, questions, and knowledge graphs. It is necessary to properly fuse and embed the information between different modes identified, to reduce the noise and difficulty in cross-modality reasoning of VQA models. However, this process of rationally integrating information between different modes and joint reasoning to find relevant evidence to correctly predict the answer to the question still deserves further study. This paper proposes a bimodal Graph Neural Network model combining pre-trained Language Models and Knowledge Graphs (BIGNN-LM-KG). Researchers built the concepts graph by the images and questions concepts separately. In constructing the concept graph, we used the combined reasoning advantages of LM+KG. Specifically, use KG to jointly infer the images and question entity concepts to build a concept graph. Use LM to calculate the correlation score to screen the nodes and paths of the concept graph. Then, we form a visual graph from the visual and spatial features of the filtered image entities. We use the improved GNN to learn the representation of the two graphs and to predict the most likely answer by fusing the information of two different modality graphs using a modality fusion GNN. On the common dataset of VQA, the model we proposed obtains good experiment results. It also verifies the validity of each component in the model and the interpretability of the model.</p></abstract>",
        "keywords": "",
        "link": "http://dx.doi.org/10.3934/era.2023100"
    },
    {
        "id": 29800,
        "title": "An Empirical Study of Multilingual Scene-Text Visual Question Answering",
        "authors": "Lin Li, Haohan Zhang, Zeqin Fang",
        "published": "2023-10-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3607540.3617140"
    },
    {
        "id": 29801,
        "title": "Prompting Large Language Models with Fine-Grained Visual Relations from Scene Graph for Visual Question Answering",
        "authors": "Jiapeng Liu, Chengyang Fang, Liang Li, Bing Li, Dayong Hu, Can Ma",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10448321"
    },
    {
        "id": 29802,
        "title": "Learning Representations from Explainable and Connectionist Approaches for Visual Question Answering",
        "authors": "Aakansha Mishra, Miriyala Srinivas Soumitri, Vikram N Rajendiran",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447493"
    },
    {
        "id": 29803,
        "title": "Design and Development of Visual Question Answering model for the Visually Impaired",
        "authors": "Shruthi G, Pradyumna Patil, Krishna Raj P M",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icraset59632.2023.10419884"
    },
    {
        "id": 29804,
        "title": "Aesthetic Visual Question Answering of Photographs",
        "authors": "Xin Jin, Yuchen Li, Wu Zhou, Xinghui Zhou, Hongtao Yang",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmew59549.2023.00068"
    },
    {
        "id": 29805,
        "title": "Visual Analysis of Scene-Graph-Based Visual Question Answering",
        "authors": "Noel Schäfer, Sebastian Künzel, Tanja Munz-Körner, Pascal Tilli, Sandeep Vidyapu, Ngoc Thang Vu, Daniel Weiskopf",
        "published": "2023-9-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3615522.3615547"
    },
    {
        "id": 29806,
        "title": "Visual Question Answering in Remote Sensing with Cross-Attention and Multimodal Information Bottleneck",
        "authors": "Jayesh Songara, Shivam Pande, Shabnam Choudhury, Biplab Banerjee, Rajbabu Velmurugan",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10281414"
    },
    {
        "id": 29807,
        "title": "MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering",
        "authors": "Jingjing Jiang, Nanning Zheng",
        "published": "2023-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.02318"
    },
    {
        "id": 29808,
        "title": "Language-Guided Visual Aggregation Network for Video Question Answering",
        "authors": "Xiao Liang, Di Wang, Quan Wang, Bo Wan, Lingling An, Lihuo He",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3581783.3613909"
    },
    {
        "id": 29809,
        "title": "Attention-based Image Captioning Assisted Visual Question Answering",
        "authors": "Hantao Xu, Xia Ye, Zhangping Yang, Pujie Zhao",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "Abstract\nVisual question answering has received increasing attention as a multimodal task whose goal is to be able to answer natural language-related questions by reasoning about a given picture. The SOTA model in the current mainstream visual question answering usually uses language bias in the training data to predict the question, resulting in the VQA model often lacking an image foundation. Based on the attention mechanism, this paper designs a method to use image description to assist the training of a visual question-answering model. The model is problem-oriented and encodes images and image descriptions based on a collaborative attention mechanism, which enhances the feature representation of the model and the ability to learn image information, making the model more robust and generalizable. The experimental results show that the accuracy of this method on the dataset VQA v2 has increased by 3.89% compared with the baseline, which proves that the method is effective.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1742-6596/2577/1/012001"
    },
    {
        "id": 29810,
        "title": "Hierarchical Attention Networks for Fact-based Visual Question Answering",
        "authors": "Haibo Yao, Yongkang Luo, Zhi Zhang, Jianhang Yang, Chengtao Cai",
        "published": "2023-7-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-16151-w"
    },
    {
        "id": 29811,
        "title": "A Visual Question and Answering System with Support for Compound Emotions using Facial Landmark Identification with MediaPipe and CNN classifier",
        "authors": "Lavika Goel, Nilarnab Debnath, Sanskar Mundaniya",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2024.127623"
    },
    {
        "id": 29812,
        "title": "Enhancing Multimodal Understanding With LIUS",
        "authors": "Chunlai Song",
        "published": "2024-1-12",
        "citations": 0,
        "abstract": "VQA (visual question and answer) is the task of enabling a computer to generate accurate textual answers based on given images and related questions. It integrates computer vision and natural language processing and requires a model that is able to understand not only the image content but also the question in order to generate appropriate linguistic answers. However, current limitations in cross-modal understanding often result in models that struggle to accurately capture the complex relationships between images and questions, leading to inaccurate or ambiguous answers. This research aims to address this challenge through a multifaceted approach that combines the strengths of vision and language processing. By introducing the innovative LIUS framework, a specialized vision module was built to process image information and fuse features using multiple scales. The insights gained from this module are integrated with a “reasoning module” (LLM) to generate answers.",
        "keywords": "",
        "link": "http://dx.doi.org/10.4018/joeuc.336276"
    },
    {
        "id": 29813,
        "title": "MVCE-Net: Multi-View Region Feature and Caption Enhancement Co-Attention Network for Visual Question Answering",
        "authors": "Feng Yan, Wushouer Silamu, Yanbing Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/cmc.2023.038177"
    },
    {
        "id": 29814,
        "title": "A Survey on Visual Question Answering Methodologies",
        "authors": "Aya Al-Zoghby, Aya Saleh, wael awad",
        "published": "2024-1-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21608/ejle.2024.244720.1058"
    },
    {
        "id": 29815,
        "title": "SceneGATE: Scene-Graph Based Co-Attention Networks for Text Visual Question Answering",
        "authors": "Feiqi Cao, Siwen Luo, Felipe Nunez, Zean Wen, Josiah Poon, Soyeon Caren Han",
        "published": "2023-8-7",
        "citations": 1,
        "abstract": "Visual Question Answering (VQA) models fail catastrophically on questions related to the reading of text-carrying images. However, TextVQA aims to answer questions by understanding the scene texts in an image–question context, such as the brand name of a product or the time on a clock from an image. Most TextVQA approaches focus on objects and scene text detection, which are then integrated with the words in a question by a simple transformer encoder. The focus of these approaches is to use shared weights during the training of a multi-modal dataset, but it fails to capture the semantic relations between an image and a question. In this paper, we proposed a Scene Graph-Based Co-Attention Network (SceneGATE) for TextVQA, which reveals the semantic relations among the objects, the Optical Character Recognition (OCR) tokens and the question words. It is achieved by a TextVQA-based scene graph that discovers the underlying semantics of an image. We create a guided-attention module to capture the intra-modal interplay between the language and the vision as a guidance for inter-modal interactions. To permit explicit teaching of the relations between the two modalities, we propose and integrate two attention modules, namely a scene graph-based semantic relation-aware attention and a positional relation-aware attention. We conduct extensive experiments on two widely used benchmark datasets, Text-VQA and ST-VQA. It is shown that our SceneGATE method outperforms existing ones because of the scene graph and its attention modules.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/robotics12040114"
    },
    {
        "id": 29816,
        "title": "Hierarchical reasoning based on perception action cycle for visual question answering",
        "authors": "Safaa Abdullahi Moallim Mohamud, Amin Jalali, Minho Lee",
        "published": "2024-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.122698"
    },
    {
        "id": 29817,
        "title": "Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts",
        "authors": "Deepanway Ghosal, Navonil Majumder, Roy Lee, Rada Mihalcea, Soujanya Poria",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.809"
    },
    {
        "id": 29818,
        "title": "An Empirical Study on the Language Modal in Visual Question Answering",
        "authors": "Daowan Peng, Wei Wei, Xian-Ling Mao, Yuanyuan Fu, Dangyang Chen",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Generalization beyond in-domain experience to out-of-distribution data is of paramount significance in the AI domain. Of late, state-of-the-art Visual Question Answering (VQA) models have shown impressive performance on in-domain data, partially due to the language prior bias which, however, hinders the generalization ability in practice. This paper attempts to provide new insights into the influence of language modality on VQA performance from an empirical study perspective. To achieve this, we conducted a series of experiments on six models. The results of these experiments revealed that, 1) apart from prior bias caused by question types, there is a notable influence of postfix-related bias in inducing biases, and 2) training VQA models with word-sequence-related variant questions demonstrated improved performance on the out-of-distribution benchmark, and the LXMERT even achieved a 10-point gain without adopting any debiasing methods. We delved into the underlying reasons behind these experimental results and put forward some simple proposals to reduce the models' dependency on language priors. The experimental results demonstrated the effectiveness of our proposed method in improving performance on the out-of-distribution benchmark, VQA-CPv2.  We hope this study can inspire novel insights for future research on designing bias-reduction approaches.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/457"
    },
    {
        "id": 29819,
        "title": "Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs",
        "authors": "Yuxin Zuo, Bei Li, Chuanhao Lv, Tong Zheng, Tong Xiao, JingBo Zhu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.978"
    },
    {
        "id": 29820,
        "title": "A multi-modal model based on transformers for medical visual question answering",
        "authors": "Mingchun Huang, Ming Xu, Fuhuang Liu, Liyan Chen",
        "published": "2023-4-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2671434"
    },
    {
        "id": 29821,
        "title": "Video Question Answering Using Clip-Guided Visual-Text Attention",
        "authors": "Shuhong Ye, Weikai Kong, Chenglin Yao, Jianfeng Ren, Xudong Jiang",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222286"
    },
    {
        "id": 29822,
        "title": "SQT: Debiased Visual Question Answering via Shuffling Question Types",
        "authors": "Tianyu Huai, Shuwen Yang, Junhang Zhang, Guoan Wang, Xinru Yu, Tianlong Ma, Liang He",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00109"
    },
    {
        "id": 29823,
        "title": "Improved Blending Attention Mechanism in Visual Question Answering",
        "authors": "Siyu Lu, Yueming Ding, Zhengtong Yin, Mingzhe Liu, Xuan Liu, Wenfeng Zheng, Lirong Yin",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/csse.2023.038598"
    },
    {
        "id": 29824,
        "title": "Separate and Locate: Rethink the Text in Text-based Visual Question Answering",
        "authors": "Chengyang Fang, Jiangnan Li, Liang Li, Can Ma, Dayong Hu",
        "published": "2023-10-26",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3581783.3611753"
    },
    {
        "id": 29825,
        "title": "Complex visual question answering based on uniform form and content",
        "authors": "Deguang Chen, Jianrui Chen, Chaowei Fang, Zhichao Zhang",
        "published": "2024-4-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10489-024-05383-4"
    },
    {
        "id": 29826,
        "title": "Scene Understanding for Autonomous Driving Using Visual Question Answering",
        "authors": "Adrien Wantiez, Tianming Qiu, Stefan Matthes, Hao Shen",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191714"
    },
    {
        "id": 29827,
        "title": "Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering",
        "authors": "Abhirama Subramanyam Penamakuri, Manish Gupta, Mithun Das Gupta, Anand Mishra",
        "published": "2023-8",
        "citations": 0,
        "abstract": "We study visual question answering in a setting where the answer has to be mined from a pool of relevant and irrelevant images given as a context. For such a setting, a model must first retrieve relevant images from the pool and answer the question from these retrieved images. We refer to this problem as retrieval-based visual question answering (or RETVQA in short). The RETVQA is distinctively different and more challenging than the traditionally-studied Visual Question Answering (VQA), where a given question has to be answered with a single relevant image in context. Towards solving the RETVQA task, we propose a unified Multi Image BART (MI-BART) that takes a question and retrieved images using our relevance encoder for free-form fluent answer generation. Further, we introduce the largest dataset in this space, namely RETVQA, which has the following salient features: multi-image and retrieval requirement for VQA, metadata-independent questions over a pool of heterogeneous images, expecting a mix of classification-oriented and open-ended generative answers. Our proposed framework achieves an accuracy of 76.5% and a fluency of 79.3% on the proposed dataset, namely RETVQA and also outperforms state-of-the-art methods by 4.9% and 11.8% on the image segment of the publicly available WebQA dataset on the accuracy and fluency metrics, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/146"
    },
    {
        "id": 29828,
        "title": "Development of Counting Based Visual Question Answering System by Using Transformer and Pyramid Networks with Hybrid Deep Learning Model",
        "authors": "Nigisha S., Anugirba K.",
        "published": "2023-8-5",
        "citations": 0,
        "abstract": "Visual Question Answering (VQA) merges images and natural language processing, that enables machines to respond to queries about visual content with prowess by comprehending visual features and contextual cues in text. VQA aims to bridge the gap between human-like understanding and visual comprehension. Counting-based VQA is a specific subfield within VQA that focuses on answering questions related to counting objects or quantities in images. The objective of counting-based VQA is to develop algorithms and models capable of accurately answering questions that involve counting specific objects or quantities in visual data. Our Model consists of Bidirectional Encoder Representations from Transformers (BERT) to extract the texture features from the Question part and for the visual part, Feature Pyramid Network (FPN) is used to extract the deep features from images. Both the textual and visual features are integrated to form a combined set of features. These fused features are fed in to a hybrid model for answer prediction. This hybrid model is an integration of Gated Recurrent Unit (GRU) and One-Dimensional Convolutional Neural Network (1DCNN).",
        "keywords": "",
        "link": "http://dx.doi.org/10.59544/fswn5535/ngcesi23p38"
    },
    {
        "id": 29829,
        "title": "Multimodal Natural Language Explanation Generation for Visual Question Answering Based on Multiple Reference Data",
        "authors": "He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama",
        "published": "2023-5-10",
        "citations": 2,
        "abstract": "As deep learning research continues to advance, interpretability is becoming as important as model performance. Conducting interpretability studies to understand the decision-making processes of deep learning models can improve performance and provide valuable insights for humans. The interpretability of visual question answering (VQA), a crucial task for human–computer interaction, has garnered the attention of researchers due to its wide range of applications. The generation of natural language explanations for VQA that humans can better understand has gradually supplanted heatmap representations as the mainstream focus in the field. Humans typically answer questions by first identifying the primary objects in an image and then referring to various information sources, both within and beyond the image, including prior knowledge. However, previous studies have only considered input images, resulting in insufficient information that can lead to incorrect answers and implausible explanations. To address this issue, we introduce multiple references in addition to the input image. Specifically, we propose a multimodal model that generates natural language explanations for VQA. We introduce outside knowledge using the input image and question and incorporate object information into the model through an object detection module. By increasing the information available during the model generation process, we significantly improve VQA accuracy and the reliability of the generated explanations. Moreover, we employ a simple and effective feature fusion joint vector to combine information from multiple modalities while maximizing information preservation. Qualitative and quantitative evaluation experiments demonstrate that the proposed method can generate more reliable explanations than state-of-the-art methods while maintaining answering accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12102183"
    },
    {
        "id": 29830,
        "title": "RMLVQA: A Margin Loss Approach For Visual Question Answering with Language Biases",
        "authors": "Abhipsa Basu, Sravanti Addepalli, R. Venkatesh Babu",
        "published": "2023-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.01123"
    },
    {
        "id": 29831,
        "title": "NS-IL: Neuro-Symbolic Visual Question Answering Using Incrementally Learnt, Independent Probabilistic Models for Small Sample Sizes",
        "authors": "Penny Johnston, Keiller Nogueira, Kevin Swingler",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3341007"
    },
    {
        "id": 29832,
        "title": "Object Attribute Matters in Visual Question Answering",
        "authors": "Peize Li, Qingyi Si, Peng Fu, Zheng Lin, Yan Wang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Visual question answering is a multimodal task that requires the joint comprehension of visual and textual information. However, integrating visual and textual semantics solely through attention layers is insufficient to comprehensively understand and align information from both modalities.  Intuitively, object attributes can naturally serve as a bridge to unify them,  which has been overlooked in previous research. In this paper, we propose a novel VQA approach from the perspective of utilizing object attribute, aiming to achieve better object-level visual-language alignment and multimodal scene understanding. Specifically, we design an attribute fusion module and a contrastive knowledge distillation module. The attribute fusion module constructs a multimodal graph neural network to fuse attributes and visual features through message passing. The enhanced object-level visual features contribute to solving fine-grained problem like counting-question. The better object-level visual-language alignment aids in understanding multimodal scenes, thereby improving the model's robustness. Furthermore, to augment scene understanding and the out-of-distribution performance, the contrastive knowledge distillation module introduces a series of implicit knowledge. We distill knowledge into attributes through contrastive loss, which further strengthens the representation learning of attribute features and facilitates visual-linguistic alignment. Intensive experiments on six datasets, COCO-QA, VQAv2, VQA-CPv2, VQA-CPv1, VQAvs and TDIUC, show the superiority of the proposed method.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i17.29816"
    },
    {
        "id": 29833,
        "title": "Learning neighbor-enhanced region representations and question-guided visual representations for visual question answering",
        "authors": "Ling Gao, Hongda Zhang, Nan Sheng, Lida Shi, Hao Xu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.122239"
    },
    {
        "id": 29834,
        "title": "Adapting Grounded Visual Question Answering Models to Low Resource Languages",
        "authors": "Ying Wang, Jonas Pfeiffer, Nicolas Carion, Yann LeCun, Aishwarya Kamath",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvprw59228.2023.00258"
    },
    {
        "id": 29835,
        "title": "CausalME: Balancing bi-modalities in Visual Question Answering",
        "authors": "Chenji Lu, Ge Bai, Shilong Li, Ying Liu, Xiyan Liu, Zerong Zeng, Ruifang Liu",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447342"
    },
    {
        "id": 29836,
        "title": "Multi-Lang Question Answering Framework for Decision Support in Educational Institutes",
        "authors": "Walaa Elnozahy, Ghada El Khayat",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012059700003470"
    },
    {
        "id": 29837,
        "title": "Unified Transformer with Cross-Modal Mixture Experts for Remote-Sensing Visual Question Answering",
        "authors": "Gang Liu, Jinlong He, Pengfei Li, Shenjun Zhong, Hongyang Li, Genrong He",
        "published": "2023-9-24",
        "citations": 1,
        "abstract": "Remote-sensing visual question answering (RSVQA) aims to provide accurate answers to remote sensing images and their associated questions by leveraging both visual and textual information during the inference process. However, most existing methods ignore the significance of the interaction between visual and language features, which typically adopt simple feature fusion strategies and fail to adequately model cross-modal attention, struggling to capture the complex semantic relationships between questions and images. In this study, we introduce a unified transformer with cross-modal mixture expert (TCMME) model to address the RSVQA problem. Specifically, we utilize the vision transformer (VIT) and BERT to extract visual and language features, respectively. Furthermore, we incorporate cross-modal mixture experts (CMMEs) to facilitate cross-modal representation learning. By leveraging the shared self-attention and cross-modal attention within CMMEs, as well as the modality experts, we effectively capture the intricate interactions between visual and language features and better focus on their complex semantic relationships. Finally, we conduct qualitative and quantitative experiments on two benchmark datasets: RSVQA-LR and RSVQA-HR. The results demonstrate that our proposed method surpasses the current state-of-the-art (SOTA) techniques. Additionally, we perform an extensive analysis to validate the effectiveness of different components in our framework.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs15194682"
    },
    {
        "id": 29838,
        "title": "Mix-tower: Light visual question answering framework based on exclusive self-attention mechanism",
        "authors": "Deguang Chen, Jianrui Chen, Luheng Yang, Fanhua Shang",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2024.127686"
    },
    {
        "id": 29839,
        "title": "VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge",
        "authors": "Sahithya Ravi, Aditya Chinchure, Leonid Sigal, Renjie Liao, Vered Shwartz",
        "published": "2023-1",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00121"
    },
    {
        "id": 29840,
        "title": "EVJVQA CHALLENGE: MULTILINGUAL VISUAL QUESTION ANSWERING",
        "authors": "Ngan Luu-Thuy Nguyen,  Nghia Hieu Nguyen, Duong T.D. Vo, Khanh Quoc Tran, Kiet Van Nguyen",
        "published": "2023-9-26",
        "citations": 2,
        "abstract": "Visual Question Answering (VQA) is a challenging task of natural language processing (NLP) and computer vision (CV), attracting significant attention from researchers. English is a resource-rich language that has witnessed various developments in datasets and models for visual question answering. Visual question answering in other languages also would be developed for resources and models. In addition, there is no multilingual dataset targeting the visual content of a particular country with its own objects and cultural characteristics. To address the weakness, we provide the research community with a benchmark dataset named EVJVQA, including 33,000+ pairs of question-answer over three languages: Vietnamese, English, and Japanese, on approximately 5,000 images taken from Vietnam for evaluating multilingual VQA systems or models. EVJVQA is used as a benchmark dataset for the challenge of multilingual visual question answering at the 9th Workshop on Vietnamese Language and Speech Processing (VLSP 2022). This task attracted 62 participant teams from various universities and organizations. In this article, we present details of the organization of the challenge, an overview of the methods employed by shared-task participants, and the results. The highest performances are 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The multilingual QA systems proposed by the top 2 teams use ViT for the pre-trained vision model and mT5 for the pre-trained language model, a powerful pre-trained language model based on the transformer architecture. EVJVQA is a challenging dataset that motivates NLP and CV researchers to further explore the multilingual models or systems for visual question answering systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.15625/1813-9663/18157"
    },
    {
        "id": 29841,
        "title": "Explainable Knowledge reasoning via thought chains for knowledge-based visual question answering",
        "authors": "Chen Qiu, Zhiqiang Xie, Maofu Liu, Huijun Hu",
        "published": "2024-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ipm.2024.103726"
    },
    {
        "id": 29842,
        "title": "VAQA: Visual Arabic Question Answering",
        "authors": "Sarah M. kamel, Shimaa I. Hassan, Lamiaa Elrefaei",
        "published": "2023-8",
        "citations": 1,
        "abstract": "AbstractVisual Question Answering (VQA) is the problem of automatically answering a natural language question about a given image or video. Standard Arabic is the sixth most spoken language around the world. However, to the best of our knowledge, there are neither research attempts nor datasets for VQA in Arabic. In this paper, we generate the first Visual Arabic Question Answering (VAQA) dataset, which is fully automatically generated. The dataset consists of almost 138k Image-Question-Answer (IQA) triplets and is specialized in yes/no questions about real-world images. A novel database schema and an IQA ground-truth generation algorithm are specially designed to facilitate automatic VAQA dataset creation. We propose the first Arabic-VQA system, where the VQA task is formulated as a binary classification problem. The proposed system consists of five modules, namely visual features extraction, question pre-processing, textual features extraction, feature fusion, and answer prediction. Since it is the first research for VQA in Arabic, we investigate several approaches in the question channel, to identify the most effective approaches for Arabic question pre-processing and representation. For this purpose, 24 Arabic-VQA models are developed, where two question-tokenization approaches, three word-embedding algorithms, and four LSTM networks with different architectures are investigated. A comprehensive performance comparison is conducted between all these Arabic-VQA models on the VAQA dataset. Experiments indicate that the performance of all Arabic-VQA models ranges from 80.8 to 84.9%, while utilizing Arabic-specified question pre-processing approaches of considering the special case of separating the question tool \"Image missing\" and embedding the question words using fine-tuned Word2Vec models from AraVec2.0 have significantly improved the performance. The best-performing model is which treats the question tool \"Image missing\" as a separate token, embeds the question words using AraVec2.0 Skip-Gram model, and extracts the textual feature using one-layer unidirectional LSTM. Further, our best Arabic-VQA model is compared with related VQA models developed on other popular VQA datasets in a different natural language, considering their performance only on yes/no questions according to the scope of this paper, showing a very comparable performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13369-023-07687-y"
    },
    {
        "id": 29843,
        "title": "MF<sup>2</sup>-MVQA: A Multi-Stage Feature Fusion Method for Medical Visual Question Answering",
        "authors": "Shanshan Song, Jiangyun Li, Jing Wang, Yuanxiu Cai, Wenkai Dong",
        "published": "2023-4-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230530"
    },
    {
        "id": 29844,
        "title": "POP-VQA – Privacy preserving, On-device, Personalized Visual Question Answering",
        "authors": "Pragya Paramita Sahu, Abhishek Raut, Jagdish Singh Samant, Mahesh Gorijala, Vignesh Lakshminarayanan, Pinaki Bhaskar",
        "published": "2024-1-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv57701.2024.00828"
    },
    {
        "id": 29845,
        "title": "Self-Supervised Vision-Language Pretraining for Medial Visual Question Answering",
        "authors": "Pengfei Li, Gang Liu, Lin Tan, Jinying Liao, Shenjun Zhong",
        "published": "2023-4-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230743"
    },
    {
        "id": 29846,
        "title": "Robust visual question answering via semantic cross modal augmentation",
        "authors": "Akib Mashrur, Wei Luo, Nayyar A. Zaidi, Antonio Robles-Kelly",
        "published": "2024-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103862"
    },
    {
        "id": 29847,
        "title": "BARTPhoBEiT: Pre-trained Sequence-to-Sequence and Image Transformers Models for Vietnamese Visual Question Answering",
        "authors": "Khiem Vinh Tran, Kiet Van Nguyen, Ngan Luu Thuy Nguyen",
        "published": "2023-10-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mapr59823.2023.10288874"
    },
    {
        "id": 29848,
        "title": "Counting in Visual Question Answering: Methods, Datasets, and Future Work",
        "authors": "Tesfayee Meshu Welde, Lejian Liao",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": " Visual Question Answering (VQA) is a language-based method for analyzing images, which is highly helpful in assisting people with visual impairment. The VQA system requires a demonstrated holistic image understanding and conducts basic reasoning tasks concerning the image in contrast to the specific task-oriented models that simply classifies object into categories. Thus, VQA systems contribute to the growth of Artificial Intelligence (AI) technology by answering open-ended, arbitrary questions about a given image. In addition, VQA is also used to assess the system’s ability by conducting Visual Turing Test (VTT). However, because of the inability to generate the essential datasets and being incapable of evaluating the systems due to flawlessness and bias, the VQA system is incapable of assessing the system’s overall efficiency. This is seen as a possible and significant limitation of the VQA system. This, in turn, has a negative impact on the progress of performance observed in VQA algorithms. Currently, the research on the VQA system is dealing with more specific sub-problems, which include counting in VQA systems. The counting sub-problem of VQA is a more sophisticated one, riddling with several challenging questions, especially when it comes to complex counting questions such as those that demand object identifications along with detection of objects attributes and positional reasoning. The pooling operation that is considered to perform an attention mechanism in VQA is found to degrade the counting performance. A number of algorithms have been developed to address this issue. In this paper, we provide a comprehensive survey of counting techniques in the VQA system that is developed especially for answering questions such as “How many?”. However, the performance progress achieved by this system is still not satisfactory due to bias that occurs in the datasets from the way we phrase the questions and because of weak evaluation metrics. In the future, fully-fledged architecture, wide-size datasets with complex counting questions and a detailed breakdown in categories, and strong evaluation metrics for evaluating the ability of the system to answer complex counting questions, such as positional and comparative reasoning will be executed. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s0219467825500445"
    },
    {
        "id": 29849,
        "title": "Visual Explanation for Open-domain Question Answering with BERT",
        "authors": "Zekai Shao, Shuran Sun, Yuheng Zhao, Siyuan Wang, Zhongyu Wei, Tao Gui, Cagatay Turkay, Siming Chen",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvcg.2023.3243676"
    },
    {
        "id": 29850,
        "title": "Comprehensive Visual Question Answering on Point Clouds through Compositional Scene Manipulation",
        "authors": "Xu Yan, Zhihao Yuan, Yuhao Du, Yinghong Liao, Yao Guo, Shuguang Cui, Zhen Li",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvcg.2023.3340679"
    },
    {
        "id": 29851,
        "title": "(QA)2: Question Answering with Questionable Assumptions: Question Answering with Questionable Assumptions",
        "authors": "Najoung Kim, Phu Mon Htut, Samuel R. Bowman, Jackson Petty",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.472"
    },
    {
        "id": 29852,
        "title": "Vision–Language Model for Visual Question Answering in Medical Imagery",
        "authors": "Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Laila Bashmal, Mansour Zuair",
        "published": "2023-3-20",
        "citations": 4,
        "abstract": "In the clinical and healthcare domains, medical images play a critical role. A mature medical visual question answering system (VQA) can improve diagnosis by answering clinical questions presented with a medical image. Despite its enormous potential in the healthcare industry and services, this technology is still in its infancy and is far from practical use. This paper introduces an approach based on a transformer encoder–decoder architecture. Specifically, we extract image features using the vision transformer (ViT) model, and we embed the question using a textual encoder transformer. Then, we concatenate the resulting visual and textual representations and feed them into a multi-modal decoder for generating the answer in an autoregressive way. In the experiments, we validate the proposed model on two VQA datasets for radiology images termed VQA-RAD and PathVQA. The model shows promising results compared to existing solutions. It yields closed and open accuracies of 84.99% and 72.97%, respectively, for VQA-RAD, and 83.86% and 62.37%, respectively, for PathVQA. Other metrics such as the BLUE score showing the alignment between the predicted and true answer sentences are also reported.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/bioengineering10030380"
    },
    {
        "id": 29853,
        "title": "Research on the Teaching Method of College Students’ Education Based on Visual Question Answering Technology",
        "authors": "Fang Lin",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "With the changes of the times, visual question answering (VQA) technology has gradually been widely used in many fields, such as intelligent bionic robots, student learning and education, and visual assistance for the disabled. In view of this, the study proposes a method of college student education and teaching system based on VQA technology. First, a multiscale fusion feature method is proposed for the representation of image features, and then an improved mixed attention mechanism is proposed based on this. Solve the network noise problem; finally, introduce the VQA system. The results show that the research method tends to a stable state when the iteration reaches the 28th time, and the corresponding loss function converges to 2.38, which is 0.5% lower than the traditional model; in the application effect analysis, on the data set test-dev, the research model The accuracies of the four types of questions about “total, whether, count, and others” are 69.94%, 86.21%, 50.16%, and 59.61%, respectively; among the example outputs of the VQA dataset, the research model can accurately infer the categories of each target object. Analyzing the change of students’ comprehensive literacy, when the teaching times of the constructed model reach six times, taking students’ enthusiasm as an example, the score ratios of innovation literacy in subjects such as English, mathematics, and Chinese are 96.32, 93.14, and 88.56. The above results all show that the accuracy of the research method is better, and it has better feasibility and effectiveness in the field of education and the teaching of students. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.3991/ijet.v18i22.44103"
    },
    {
        "id": 29854,
        "title": "A Multi-modal Debiasing Model with Dynamical Constraint for Robust Visual Question Answering",
        "authors": "Yu Li, Bojie Hu, Fengshuo Zhang, Yahan Yu, Jian Liu, Yufeng Chen, Jinan Xu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.311"
    },
    {
        "id": 29855,
        "title": "Multi-granularity Text Representation and Transformer-Based Fusion Method for Visual Question Answering",
        "authors": "Xingang Wang, Xiaoyu Liu, Xiaomin Li, Jinan Cui",
        "published": "2023-5-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cscwd57460.2023.10152629"
    },
    {
        "id": 29856,
        "title": "SkillCLIP: Skill Aware Modality Fusion Visual Question Answering (Student Abstract)",
        "authors": "Atharva Naik, Yash Parag Butala, Navaneethan Vaikunthan, Raghav Kapoor",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "When humans are posed with a difficult problem, they often approach it by identifying key skills, honing them, and finally effectively combining them. We propose a novel method and apply it for the VizWiz VQA task to predict the visual skills needed to answer a question, and leverage expert modules to produce intermediary outputs and fuse them in a skill-aware manner. Unlike prior works in visual question-answering (VQA) that use intermediate outputs such as detected objects and Optical Character Recognition (OCR), our approach explicitly guides the model with a skill embedding on what to focus on. While our results show that using skill-aware fusion outperforms skill-unaware models for only a subset of questions, we believe our results provide interesting directions for future work. We also release our code, model, and illustrative demonstrations for future research purposes.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i21.30486"
    },
    {
        "id": 29857,
        "title": "Cross-Modal Feature Distribution Calibration for Few-Shot Visual Question Answering",
        "authors": "Jing Zhang, Xiaoqiang Liu, Mingzhe Chen, Zhe Wang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Few-shot Visual Question Answering (VQA) realizes few-shot cross-modal learning,  which is an emerging and challenging task in computer vision. Currently, most of the few-shot VQA methods are confined to simply extending few-shot classification methods to cross-modal tasks while ignoring the spatial distribution properties of multimodal features and cross-modal information interaction. To address this problem, we propose a novel Cross-modal feature Distribution Calibration Inference Network (CDCIN) in this paper, where a new concept named visual information entropy is proposed to realize multimodal features distribution calibration by cross-modal information interaction for more effective few-shot VQA. Visual information entropy is a statistical variable that represents the spatial distribution of visual features guided by the question, which is aligned before and after the reasoning process to mitigate redundant information and improve multi-modal features by our proposed visual information entropy calibration module. To further enhance the inference ability of cross-modal features, we additionally propose a novel pre-training method, where the reasoning sub-network of CDCIN is pretrained on the base class in a VQA classification paradigm and fine-tuned on the few-shot VQA datasets. Extensive experiments demonstrate that our proposed CDCIN achieves excellent performance on few-shot VQA and outperforms state-of-the-art methods on three widely used benchmark datasets.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i7.28543"
    },
    {
        "id": 29858,
        "title": "EDUVQA – Visual Question Answering: An Educational Perspective",
        "authors": " Dipali Koshti,  Ashutosh Gupta,  Mukesh Kalla,  Pramit Kanjilal,  Sushant Shanbhag,  Nirmit Karkera",
        "published": "2024-3-26",
        "citations": 0,
        "abstract": "Increasing applications of artificial intelligence in the field of education have changed the way school children learn various concepts. Educational Visual Question Answering or EDUVQA is one such application that allows students to interact directly with images, ask educational questions, and get the correct answer. Two major challenges faced by educational VQA are the lack of availability of domain-specific datasets and often it requires referring to the external knowledge bases to answer open-domain questions. We propose a novel EDUVQA model developed especially for educational purposes and introduce our own EDUVQA dataset. The dataset consists of four categories of images - animals, plants, fruits, and vegetables. The majority of the currently used techniques focus on the extraction of picture and question characteristics in order to discover the joint feature embeddings via multimodal fusion or attention mechanisms. We propose a different method that aims to better utilize the semantic knowledge present in images. Our approach entails building an EDUVQA dataset using educational images, where each data point is made up of an image, a question that corresponds to it, a valid response, and a fact that supports it. The fact is created in the form of <S,V,O> triplet where ‘s’ denotes a subject, ‘v’ a verb, and ‘o’ an object. First, an SVO detector model is trained on EDUVQA dataset capable of predicting the Subject, Verb, and Object present in the image-question pair. Using this <S,V,O> triplet, the most relevant facts from our fact base are extracted. The final answer is predicted using these extracted facts, image, and question attributes. The image features are extricated using pretrained ResNet and question features using a pre-trained BERT model. We have optimized and improved on the current methodologies that use a Relation-based approach and built our SVO-detector model that outperforms current models by 10%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.37934/araset.42.1.144157"
    },
    {
        "id": 29859,
        "title": "Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual Question Answering",
        "authors": "Alireza Salemi, Mahta Rafiee, Hamed Zamani",
        "published": "2023-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3578337.3605137"
    },
    {
        "id": 29860,
        "title": "Compressing and Debiasing Vision-Language Pre-Trained Models for Visual Question Answering",
        "authors": "Qingyi Si, Yuanxin Liu, Zheng Lin, Peng Fu, Yanan Cao, Weiping Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.34"
    },
    {
        "id": 29861,
        "title": "Enhancing visual question answering with a two‐way co‐attention mechanism and integrated multimodal features",
        "authors": "Mayank Agrawal, Anand Singh Jalal, Himanshu Sharma",
        "published": "2024-2",
        "citations": 0,
        "abstract": "AbstractIn Visual question answering (VQA), a natural language answer is generated for a given image and a question related to that image. There is a significant growth in the VQA task by applying an efficient attention mechanism. However, current VQA models use region features or object features that are not adequate to improve the accuracy of generated answers. To deal with this issue, we have used a Two‐way Co‐Attention Mechanism (TCAM), which is capable enough to fuse different visual features (region, object, and concept) from diverse perspectives. These diverse features lead to different sets of answers, and also, there is an inherent relationship between these visual features. We have developed a powerful attention mechanism that uses these two critical aspects by using both bottom‐up and top‐down TCAM to extract discriminative feature information. We have proposed a Collective Feature Integration Module (CFIM) to combine multimodal attention features and thus capture the valuable information from these visual features by employing a TCAM. Further, we have formulated a Vertical CFIM for fusing the features belonging to the same class and a Horizontal CFIM for combining the features belonging to different types, thus balancing the influence of top‐down and bottom‐up co‐attention. The experiments are conducted on two significant datasets, VQA 1.0 and VQA 2.0. On VQA 1.0, the overall accuracy of our proposed method is 71.23 on the test‐dev set and 71.94 on the test‐std set. On VQA 2.0, the overall accuracy of our proposed method is 75.89 on the test‐dev set and 76.32 on the test‐std set. The above overall accuracy clearly reflecting the superiority of our proposed TCAM based approach over the existing methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1111/coin.12624"
    },
    {
        "id": 29862,
        "title": "OpenViVQA: Task, dataset, and multimodal fusion models for visual question answering in Vietnamese",
        "authors": "Nghia Hieu Nguyen, Duong T.D. Vo, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen",
        "published": "2023-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.inffus.2023.101868"
    },
    {
        "id": 29863,
        "title": "Medical visual question answering: A survey",
        "authors": "Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi Wu, Mingguang He, Zongyuan Ge",
        "published": "2023-9",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artmed.2023.102611"
    },
    {
        "id": 29864,
        "title": "Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts",
        "authors": "Yunshi Lan, Xiang Li, Xin Liu, Yang Li, Wei Qin, Weining Qian",
        "published": "2023-10-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3581783.3612389"
    },
    {
        "id": 29865,
        "title": "ConvRGX: Recognition, Generation, and Extraction for Self-trained Conversational Question Answering",
        "authors": "Tianhua Zhang, Liping Tang, Wei Fang, Hongyin Luo, Xixin Wu, Helen Meng, James Glass",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.dialdoc-1.10"
    },
    {
        "id": 29866,
        "title": "Open-Vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
        "authors": "Dohwan Ko, Ji Soo Lee, Miso Choi, Jaewon Chu, Jihwan Park, Hyunwoo J. Kim",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00288"
    },
    {
        "id": 29867,
        "title": "From image to language: A critical analysis of Visual Question Answering (VQA) approaches, challenges, and opportunities",
        "authors": "Md. Farhan Ishmam, Md. Sakib Hossain Shovon, M.F. Mridha, Nilanjan Dey",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.inffus.2024.102270"
    },
    {
        "id": 29868,
        "title": "ASCL: Adaptive self-supervised counterfactual learning for robust visual question answering",
        "authors": "Xinyao Shu, Shiyang Yan, Xu Yang, Ziheng Wu, Zhongfeng Chen, Zhenyu Lu",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.123125"
    },
    {
        "id": 29869,
        "title": "Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA",
        "authors": "Wentao Mo, Yang Liu",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combines 2D and 3D modalities and captures fine-grained correlations between modalities, allowing them mutually augmenting each other. Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh perspective on multi-modal transformer-based architectures for 3D-VQA. Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets and significantly outperforms existing solutions. Code is available at https://github.com/matthewdm0816/BridgeQA.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i5.28222"
    },
    {
        "id": 29870,
        "title": "Knowledge-aware image understanding with multi-level visual representation enhancement for visual question answering",
        "authors": "Feng Yan, Zhe Li, Wushour Silamu, Yanbing Li",
        "published": "2023-12-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06426-6"
    },
    {
        "id": 29871,
        "title": "A Weak Supervision-based Robust Pretraining Method for Medical Visual Question Answering",
        "authors": "Shuning He, Haiwei Pan, Kejia Zhang, Cheng Gong, Zhe Li",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bibm58861.2023.10385332"
    },
    {
        "id": 29872,
        "title": "A Study of Visual Question Answering Techniques Based on Collaborative Multi-Head Attention",
        "authors": "Yingli Yang, Jingxuan Jin, De Li",
        "published": "2023-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acctcs58815.2023.00037"
    },
    {
        "id": 29873,
        "title": "LIT-4-RSVQA: Lightweight Transformer-Based Visual Question Answering in Remote Sensing",
        "authors": "Leonard Hackel, Kai Norman Clasen, Mahdyar Ravanbakhsh, Begüm Demir",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10281674"
    },
    {
        "id": 29874,
        "title": "Enhancing <scp>scene‐text</scp> visual question answering with relational reasoning, attention and dynamic vocabulary integration",
        "authors": "Mayank Agrawal, Anand Singh Jalal, Himanshu Sharma",
        "published": "2024-2",
        "citations": 0,
        "abstract": "AbstractVisual question answering (VQA) is a challenging task in computer vision. Recently, there has been a growing interest in text‐based VQA tasks, emphasizing the important role of textual information for better understanding of images. Effectively utilizing text information within the image is crucial for achieving success in this task. However, existing approaches often overlook the contextual information and neglect to utilize the relationships between scene‐text tokens and image objects. They simply incorporate the scene‐text tokens mined from the image into the VQA model without considering these important factors. In this paper, the proposed model initially analyzes the image to extract text and identify scene objects. It then comprehends the question and mines relationships among the question, OCRed text, and scene objects, ultimately generating an answer through relational reasoning by conducting semantic and positional attention. Our decoder with attention map loss enables prediction of complex answers and handles dynamic vocabularies, reducing decoding space. It outperforms softmax‐based cross entropy loss in accuracy and efficiency by accommodating varying vocabulary sizes. We evaluated our model's performance on the TextVQA dataset and achieved an accuracy of 53.91% on the validation set and 53.98% on the test set. Moreover, on the ST‐VQA dataset, our model obtained ANLS scores of 0.699 on the validation set and 0.692 on the test set.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1111/coin.12635"
    },
    {
        "id": 29875,
        "title": "Detection-Based Intermediate Supervision for Visual Question Answering",
        "authors": "Yuhang Liu, Daowan Peng, Wei Wei, Yuanyuan Fu, Wenfeng Xie, Dangyang Chen",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Recently, neural module networks (NMNs) have yielded ongoing success in answering compositional visual questions, especially those involving multi-hop visual and logical reasoning. NMNs decompose the complex question into several sub-tasks using instance-modules from the reasoning paths of that question and then exploit intermediate supervisions to guide answer prediction, thereby improving inference interpretability. However, their performance may be hindered due to sketchy modeling of intermediate supervisions. For instance, (1) a prior assumption that each instance-module refers to only one grounded object yet overlooks other potentially associated grounded objects, impeding full cross-modal alignment learning; (2) IoU-based intermediate supervisions may introduce noise signals as the bounding box overlap issue might guide the model's focus towards irrelevant objects. To address these issues, a novel method, Detection-based Intermediate Supervision (DIS), is proposed, which adopts a generative detection framework to facilitate multiple grounding supervisions via sequence generation. As such, DIS offers more comprehensive and accurate intermediate supervisions, thereby boosting answer prediction performance. Furthermore, by considering intermediate results, DIS enhances the consistency in answering compositional questions and their sub-questions. Extensive experiments demonstrate the superiority of our proposed DIS, showcasing both improved accuracy and state-of-the-art reasoning consistency compared to prior approaches.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i12.29315"
    },
    {
        "id": 29876,
        "title": "Vision-Language Transformer for Interpretable Pathology Visual Question Answering",
        "authors": "Usman Naseem, Matloob Khushi, Jinman Kim",
        "published": "2023-4",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jbhi.2022.3163751"
    },
    {
        "id": 29877,
        "title": "Visual Question Answering in Bangla to assist individuals with visual impairments extract information about objects and their spatial relationships in images",
        "authors": "Sheikh Ayatur Rahman, Albert Boateng, Sabiha Tahseen, Sabbir Hossain, Annajiat Alim Rasel",
        "published": "2023-5-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3605423.3605427"
    },
    {
        "id": 29878,
        "title": "OHYEAH AT VLSP2022-EVJVQA CHALLENGE:  A JOINTLY LANGUAGE-IMAGE MODEL FOR MULTILINGUAL  VISUAL QUESTION ANSWERING",
        "authors": "Luan Ngo Dinh, Hieu Le Ngoc, Long Quoc Phan",
        "published": "2023-12-25",
        "citations": 0,
        "abstract": "Multilingual Visual Question Answering (mVQA) is an extremely challenging task which needs to answer a question given in different languages and take the context in an image. This problem can only be addressed by the combination of Natural Language Processing and Computer Vision. In this paper, we propose applying a jointly developed model to the task of multilingual visual question answering. Specifically, we conduct experiments on a multimodal sequence-to-sequence transformer model derived from the T5 encoder-decoder architecture. Text tokens and Vision Transformer (ViT) dense image embeddings are inputs to an encoder then we used a decoder to automatically anticipate discrete text tokens. We achieved the F1-score of 0.4349 on the private test set and ranked 2nd in the EVJVQA task at the VLSP shared task 2022. For reproducing the result, the code can be found at https://github.com/DinhLuan14/VLSP2022-VQA-OhYeah",
        "keywords": "",
        "link": "http://dx.doi.org/10.15625/1813-9663/18122"
    },
    {
        "id": 29879,
        "title": "Be flexible! learn to debias by sampling and prompting for robust visual question answering",
        "authors": "Jin Liu, ChongFeng Fan, Fengyu Zhou, Huijuan Xu",
        "published": "2023-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ipm.2023.103296"
    },
    {
        "id": 29880,
        "title": "Improving Selective Visual Question Answering by Learning from Your Peers",
        "authors": "Corentin Dancette, Spencer Whitehead, Rishabh Maheshwary, Ramakrishna Vedantam, Stefan Scherer, Xinlei Chen, Matthieu Cord, Marcus Rohrbach",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.02303"
    },
    {
        "id": 29881,
        "title": "Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs",
        "authors": "Jialou Wang, Manli Zhu, Yulei Li, Honglei Li, Longzhi Yang, Wai Lok Woo",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/mis.2024.3384513"
    },
    {
        "id": 29882,
        "title": "Object-Aware Adaptive-Positivity Learning for Audio-Visual Question Answering",
        "authors": "Zhangbin Li, Dan Guo, Jinxing Zhou, Jing Zhang, Meng Wang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "This paper focuses on the Audio-Visual Question Answering (AVQA) task that aims to answer questions derived from untrimmed audible videos. To generate accurate answers, an AVQA model is expected to find the most informative audio-visual clues relevant to the given questions. In this paper, we propose to explicitly consider fine-grained visual objects in video frames (object-level clues) and explore the multi-modal relations (\\textit{i.e.}, the object, audio, and question) in terms of feature interaction and model optimization. For the former, we present an end-to-end object-oriented network that adopts a question-conditioned clue discovery module to concentrate audio/visual modalities on respective keywords of the question and designs a modality-conditioned clue collection module to highlight closely associated audio segments or visual objects. For model optimization, we propose an object-aware adaptive-positivity learning strategy that selects the highly semantic-matched multi-modal pair as \\textit{positivity}. Specifically, we design two object-aware contrastive loss functions to identify the highly relevant question-object pairs and audio-object pairs, respectively. These selected pairs are constrained to have larger similarity values than the mismatched pairs. The positivity-selecting process is adaptive as the positivity pairs selected in each video frame may be different. These two object-aware objectives help the model understand \\textit{which objects are exactly relevant to the question} and \\textit{which are making sounds}. Extensive experiments on the MUSIC-AVQA dataset demonstrate the proposed method is effective in finding favorable audio-visual clues and also achieves new state-of-the-art question-answering performance. The code is available at https://github.com/zhangbin-ai/APL.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i4.28116"
    },
    {
        "id": 29883,
        "title": "Design Principles and a Software Reference Architecture for Big Data Question Answering Systems",
        "authors": "Leonardo Moraes, Pedro Jardim, Cristina Aguiar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011842700003467"
    },
    {
        "id": 29884,
        "title": "Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization",
        "authors": "Aishwarya Agrawal, Ivana Kajic, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, Aida Nematzadeh",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.90"
    },
    {
        "id": 29885,
        "title": "RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training",
        "authors": "Zheng Yuan, Qiao Jin, Chuanqi Tan, Zhengyun Zhao, Hongyi Yuan, Fei Huang, Songfang Huang",
        "published": "2023-10-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3581783.3611830"
    },
    {
        "id": 29886,
        "title": "Human Guided Cross-Modal Reasoning with Semantic Attention Learning for Visual Question Answering",
        "authors": "Lei Liao, Mao Feng, Meng Yang",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10448302"
    },
    {
        "id": 29887,
        "title": "FedVQA: Personalized Federated Visual Question Answering over Heterogeneous Scenes",
        "authors": "Mingrui Lao, Nan Pu, Zhun Zhong, Nicu Sebe, Michael S. Lew",
        "published": "2023-10-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3581783.3611958"
    },
    {
        "id": 29888,
        "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
        "authors": "Alireza Salemi, Juan Altmayer Pizzorno, Hamed Zamani",
        "published": "2023-7-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3539618.3591629"
    },
    {
        "id": 29889,
        "title": "Variational Causal Inference Network for Explanatory Visual Question Answering",
        "authors": "Dizhan Xue, Shengsheng Qian, Changsheng Xu",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.00238"
    },
    {
        "id": 29890,
        "title": "Design and development of counting-based visual question answering model using heuristic-based feature selection with deep learning",
        "authors": "Tesfayee Meshu Welde, Lejian Liao",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10462-022-10385-0"
    },
    {
        "id": 29891,
        "title": "Enhancing Audio-Visual Question Answering with Missing Modality via Trans-Modal Associative Learning",
        "authors": "Kyu Ri Park, Youngmin Oh, Jung Uk Kim",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446292"
    },
    {
        "id": 29892,
        "title": "Empirical study on using adapters for debiased Visual Question Answering",
        "authors": "Jae Won Cho, Dawit Mureja Argaw, Youngtaek Oh, Dong-Jin Kim, In So Kweon",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103842"
    },
    {
        "id": 29893,
        "title": "Nested Attention Network with Graph Filtering for Visual Question and Answering",
        "authors": "Jing Lu, Chunlei Wu, Leiquan Wang, Shaozu Yuan, Jie Wu",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096849"
    },
    {
        "id": 29894,
        "title": "Cross Modality Bias in Visual Question Answering: A Causal View with Possible Worlds VQA",
        "authors": "Ali Vosoughi, Shijian Deng, Songyang Zhang, Yapeng Tian, Chenliang Xu, Jiebo Luo",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tmm.2024.3380259"
    },
    {
        "id": 29895,
        "title": "HybridPrompt: Bridging Language Models and Human Priors in Prompt Tuning for Visual Question Answering",
        "authors": "Zhiyuan Ma, Zhihuan Yu, Jianjun Li, Guohui Li",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "Visual Question Answering (VQA) aims to answer the natural language question about a given image by understanding multimodal content. However, the answer quality of most existing visual-language pre-training (VLP) methods is still limited, mainly due to: (1) Incompatibility. Upstream pre-training tasks are generally incompatible with downstream question answering tasks, which makes the knowledge from the language model not well transferable to downstream tasks, and greatly limits their performance in few-shot scenarios; (2) Under-fitting. They generally do not integrate human priors to compensate for universal knowledge from language models, so as to fit the challenging VQA problem and generate reliable answers. To address these issues, we propose HybridPrompt, a cloze- and verify-style hybrid prompt framework with bridging language models and human priors in prompt tuning for VQA. Specifically, we first modify the input questions into the cloze-style prompts to narrow the gap between upstream pre-training tasks and downstream VQA task, which ensures that the universal knowledge in the language model can be better transferred to subsequent human prior-guided prompt tuning. Then, we imitate the cognitive process of human brain to introduce topic and sample related priors to construct a dynamic learnable prompt template for human prior-guided prompt learning. Finally, we add fixed-length learnable free-parameters to further enhance the generalizability and scalability of prompt learning in the VQA model. Experimental results verify the effectiveness of HybridPrompt, showing that it achieves competitive performance against previous methods on widely-used VQAv2 dataset and obtains new state-of-the-art results. Our code is released at: https://github.com/zhizhi111/hybrid.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i11.26569"
    },
    {
        "id": 29896,
        "title": "Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering",
        "authors": "Yang Liu, Guanbin Li, Liang Lin",
        "published": "2023",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tpami.2023.3284038"
    },
    {
        "id": 29897,
        "title": "Collaborative Modality Fusion for Mitigating Language Bias in Visual Question Answering",
        "authors": "Qiwen Lu, Shengbo Chen, Xiaoke Zhu",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Language bias stands as a noteworthy concern in visual question answering (VQA), wherein models tend to rely on spurious correlations between questions and answers for prediction. This prevents the models from effectively generalizing, leading to a decrease in performance. In order to address this bias, we propose a novel modality fusion collaborative de-biasing algorithm (CoD). In our approach, bias is considered as the model’s neglect of information from a particular modality during prediction. We employ a collaborative training approach to facilitate mutual modeling between different modalities, achieving efficient feature fusion and enabling the model to fully leverage multimodal knowledge for prediction. Our experiments on various datasets, including VQA-CP v2, VQA v2, and VQA-VS, using different validation strategies, demonstrate the effectiveness of our approach. Notably, employing a basic baseline model resulted in an accuracy of 60.14% on VQA-CP v2.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/jimaging10030056"
    },
    {
        "id": 29898,
        "title": "BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via Graph Representation Pretraining",
        "authors": "MinJun Kim, SeungWoo Song, YouHan Lee, Haneol Jang, KyungTae Lim",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "The current research direction in generative models, such as the recently developed GPT4, aims to find relevant knowledge information for multimodal and multilingual inputs to provide answers. Under these research circumstances, the demand for multilingual evaluation of visual question answering (VQA) tasks, a representative task of multimodal systems, has increased. Accordingly, we propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that can be extended to multilingualism. The proposed data include 17K images, 17K question-answer pairs for both Korean and English and 280K instances of knowledge information related to question-answer content. We also present a framework that can effectively inject knowledge information into a VQA system by pretraining the knowledge information of BOK-VQA data in the form of graph embeddings. Finally, through in-depth analysis, we demonstrated the actual effect of the knowledge information contained in the constructed training data on VQA.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i16.29798"
    },
    {
        "id": 29899,
        "title": "LiVLR: A Lightweight Visual-Linguistic Reasoning Framework for Video Question Answering",
        "authors": "Jingjing Jiang, Ziyi Liu, Nanning Zheng",
        "published": "2023",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tmm.2022.3185900"
    },
    {
        "id": 29900,
        "title": "An Ontology-Based Question-Answering, from Natural Language to SPARQL Query",
        "authors": "Davide Varagnolo, Dora Melo, Irene Rodrigues",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012180000003598"
    },
    {
        "id": 29901,
        "title": "VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering",
        "authors": "Yanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, Jure Leskovec",
        "published": "2023-10-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01973"
    },
    {
        "id": 29902,
        "title": "3D Question Answering",
        "authors": "Shuquan Ye, Dongdong Chen, Songfang Han, Jing Liao",
        "published": "2024-3",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvcg.2022.3225327"
    },
    {
        "id": 29903,
        "title": "Generative Bias for Robust Visual Question Answering",
        "authors": "Jae Won Cho, Dong-Jin Kim, Hyeonggon Ryu, In So Kweon",
        "published": "2023-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cvpr52729.2023.01124"
    },
    {
        "id": 29904,
        "title": "SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images",
        "authors": "Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, Kuniko Saito",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "Visual question answering on document images that contain textual, visual, and layout information, called document VQA, has received much attention recently. Although many datasets have been proposed for developing document VQA systems, most of the existing datasets focus on understanding the content relationships within a single image and not across multiple images. In this study, we propose a new multi-image document VQA dataset, SlideVQA, containing 2.6k+ slide decks composed of 52k+ slide images and 14.5k questions about a slide deck. SlideVQA requires complex reasoning, including single-hop, multi-hop, and numerical reasoning, and also provides annotated arithmetic expressions of numerical answers for enhancing the ability of numerical reasoning. Moreover, we developed a new end-to-end document VQA model that treats evidence selection and question answering as a unified sequence-to-sequence format. Experiments on SlideVQA show that our model outperformed existing state-of-the-art QA models, but that it still has a large gap behind human performance. We believe that our dataset will facilitate research on document VQA.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i11.26598"
    }
]