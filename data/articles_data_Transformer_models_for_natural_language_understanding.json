[
    {
        "id": 6905,
        "title": "Transformer-Based Language Models as Psycholinguistic Subjects: Focusing on Understanding Metaphor",
        "authors": "Wonil Chung,  ",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "Metaphor is a fundamental aspect of human language and cognition, playing a crucial role in communication, comprehension, and creative expression. In light of the recent advancements demonstrated by prominent language models, a pivotal question arises: Can these expansive language models effectively discern metaphorical knowledge? The primary objective involves comparing the surprisal values estimated from neural network language models like autoregressive and bidirectional language models to the reaction times of human when exposed to both metaphorical and literal sentences. Our secondary objective involves assessing the AI's comprehension of metaphors by utilizing the sensicality ratings generated by sophisticated ChatGPT. To achieve this, we used psycholinguistic methods, and adopted the experimental materials from Lai, Currana, and Menna (2009). We found the surprisal values estimated from the autoregressive language model demonstrate metaphor processing that closely resembles that of native speakers. Furthermore, ChatGPT's processing of conventional metaphorical sentences closely resembles its approach to literal sentences, mirroring the convergence observed in native speakers' ERP response to conventional metaphorical sentences and their alignment with that of literal sentences.",
        "keywords": "",
        "link": "http://dx.doi.org/10.14342/smog.2023.119.87"
    },
    {
        "id": 6906,
        "title": "Not all quantifiers are equal: Probing Transformer-based language models’ understanding of generalised quantifiers",
        "authors": "Tharindu Madusanka, Iqra Zahid, Hao Li, Ian Pratt-Hartmann, Riza Batista-Navarro",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.536"
    },
    {
        "id": 6907,
        "title": "Disentangling Transformer Language Models as Superposed Topic Models",
        "authors": "Jia Lim, Hady Lauw",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.534"
    },
    {
        "id": 6908,
        "title": "When Language Models Fall in Love: Animacy Processing in Transformer Language Models",
        "authors": "Michael Hanna, Yonatan Belinkov, Sandro Pezzelle",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.744"
    },
    {
        "id": 6909,
        "title": "Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?",
        "authors": "Luke Gessler, Nathan Schneider",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-1.17"
    },
    {
        "id": 6910,
        "title": "Pushdown Layers: Encoding Recursive Structure in Transformer Language Models",
        "authors": "Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher Manning",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.195"
    },
    {
        "id": 6911,
        "title": "Context is not key: Detecting Alzheimer’s disease with both classical and transformer-based neural language models",
        "authors": "Behrad TaghiBeyglou, Frank Rudzicz",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100046"
    },
    {
        "id": 6912,
        "title": "Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models",
        "authors": "Lina Conti, Guillaume Wisniewski",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.641"
    },
    {
        "id": 6913,
        "title": "Effects of sub-word segmentation on performance of transformer language models",
        "authors": "Jue Hou, Anisia Katinskaia, Anh-Duc Vu, Roman Yangarber",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.459"
    },
    {
        "id": 6914,
        "title": "Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding",
        "authors": "Shuwen Deng, Paul Prasse, David Reich, Tobias Scheffer, Lena Jäger",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.400"
    },
    {
        "id": 6915,
        "title": "Transformer-Based Language Models for Bulgarian",
        "authors": "Iva Marinova,  , Kiril Simov, Petya Osenova,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_077"
    },
    {
        "id": 6916,
        "title": "CTRAN: CNN-Transformer-based network for natural language understanding",
        "authors": "Mehrdad Rafiepour, Javad Salimi Sartakhti",
        "published": "2023-11",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107013"
    },
    {
        "id": 6917,
        "title": "GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding",
        "authors": "Zekun Li, Wenxuan Zhou, Yao-Yi Chiang, Muhao Chen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.317"
    },
    {
        "id": 6918,
        "title": "A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models",
        "authors": "Takuma Udagawa, Aashka Trivedi, Michele Merler, Bishwaranjan Bhattacharjee",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-industry.3"
    },
    {
        "id": 6919,
        "title": "The evolution of transformer models from unidirectional to  bidirectional in Natural Language Processing",
        "authors": "Yihang Sun",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "Transformer models have revolutionized Natural Language Processing (NLP), transitioning from traditional sequential models to innovative architectures based on attention mechanisms. The shift from unidirectional to bidirectional models has been a remarkable development in NLP. This paper mainly focuses on the evolution of NLP caused by Transformer models, with the transition from unidirectional to bidirectional modeling. This paper explores how the transformer model has revolutionized NLP, and the evolution from traditional sequential models to innovative attention-driven architectures. In this paper, it mainly discusses the limitations of traditional NLP models like RNNs, LSTMs and CNN when handling lengthy text sequences and complex dependencies, highlighting how transformer models, employing self-attention mechanisms and bidirectional modeling (e.g., BERT and GPT), have significantly improved NLP tasks. It provides a thorough review of the shift from unidirectional to bidirectional transformer models, offering insights into their utilization and development. Finally, this paper concludes with a summary and outlook for the entire study.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/42/20230794"
    },
    {
        "id": 6920,
        "title": "How Much do Knowledge Graphs Impact Transformer Models for Extracting Biomedical Events?",
        "authors": "Laura Zanella, Yannick Toussaint",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bionlp-1.12"
    },
    {
        "id": 6921,
        "title": "HealthMavericks@MEDIQA-Chat 2023: Benchmarking different Transformer based models for Clinical Dialogue Summarization",
        "authors": "Kunal Suri, Saumajit Saha, Atul Singh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.clinicalnlp-1.50"
    },
    {
        "id": 6922,
        "title": "Zelda Rose: a tool for hassle-free training of transformer models",
        "authors": "Loïc Grobol",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.nlposs-1.6"
    },
    {
        "id": 6923,
        "title": "Analyzing Human and ChatGPT Responses: A Comparative Study of Transformer Models in Natural Language Processing",
        "authors": "Tunahan Gökçimen, Bihter Das",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391042"
    },
    {
        "id": 6924,
        "title": "A Survey of Transformer-Based Natural Language Processing Models",
        "authors": "鸣姝 赖",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.12677/airr.2023.123025"
    },
    {
        "id": 6925,
        "title": "Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models",
        "authors": "Alejo Lopez-Avila, Víctor Suárez-Paniagua",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.124"
    },
    {
        "id": 6926,
        "title": "The Evolution of Large Language Models in Natural Language Understanding",
        "authors": "Chinmay Shripad Kulkarni",
        "published": "2023-3-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.51219/jaimld/chinmay-shripad-kulkarni/28"
    },
    {
        "id": 6927,
        "title": "Medical Visual Textual Entailment for Numerical Understanding of Vision-and-Language Models",
        "authors": "Hitomi Yanaka, Yuta Nakamura, Yuki Chida, Tomoya Kurosawa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.clinicalnlp-1.2"
    },
    {
        "id": 6928,
        "title": "Improving Language Models’ Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary",
        "authors": "Myeongjun Jang, Thomas Lukasiewicz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.527"
    },
    {
        "id": 6929,
        "title": "KoBigBird-large: Transformation of Transformer for Korean Language Understanding",
        "authors": "Kisu Yang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.ijcnlp-main.68"
    },
    {
        "id": 6930,
        "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
        "authors": "Gustavo Gonçalves, Emma Strubell",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.161"
    },
    {
        "id": 6931,
        "title": "Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding",
        "authors": "Caoyun Fan, Jidong Tian, Yitian Li, Wenqing Chen, Hao He, Yaohui Jin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.913"
    },
    {
        "id": 6932,
        "title": "Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors",
        "authors": "Marek Kubis, Paweł Skórzewski, Marcin Sowańnski, Tomasz Zietkiewicz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.724"
    },
    {
        "id": 6933,
        "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
        "authors": "Mohamed Elgaar, Hadi Amiri",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.834"
    },
    {
        "id": 6934,
        "title": "Understanding the Inner-workings of Language Models Through Representation Dissimilarity",
        "authors": "Davis Brown, Charles Godfrey, Nicholas Konz, Jonathan Tu, Henry Kvinge",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.403"
    },
    {
        "id": 6935,
        "title": "Exploring the Impact of Syntactic Structure Information on Unknown Entity Recognition in Transformer-based Natural Language Understanding",
        "authors": "Rui Yang, Farnoosh Javar, Kei Wakabayashi, Johane Takeuchi",
        "published": "2023-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iiai-aai59060.2023.00072"
    },
    {
        "id": 6936,
        "title": "Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models",
        "authors": "Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, Yoonsik Kim, Sangdoo Yun, Taeho Kil, Bado Lee, Seunghyun Park",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.735"
    },
    {
        "id": 6937,
        "title": "Whisper-Slu: Extending a Pretrained Speech-to-Text Transformer for Low Resource Spoken Language Understanding",
        "authors": "Quentin Meeus, Marie-Francine Moens, Hugo Van Hamme",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asru57964.2023.10389786"
    },
    {
        "id": 6938,
        "title": "Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?",
        "authors": "Ahmed Alajrami, Katerina Margatina, Nikolaos Aletras",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.563"
    },
    {
        "id": 6939,
        "title": "CultureBERT: Measuring Corporate Culture With Transformer-Based Language Models",
        "authors": "Sebastian Koch, Stefan Pasch",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386765"
    },
    {
        "id": 6940,
        "title": "BioNART: A Biomedical Non-AutoRegressive Transformer for Natural Language Generation",
        "authors": "Masaki Asada, Makoto Miwa",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bionlp-1.34"
    },
    {
        "id": 6941,
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
        "authors": "Yue Wang, Hung Le, Akhilesh Gotmare, Nghi Bui, Junnan Li, Steven Hoi",
        "published": "2023",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.68"
    },
    {
        "id": 6942,
        "title": "Large Language Models versus Natural Language Understanding and Generation",
        "authors": "Nikitas Karanikolas, Eirini Manga, Nikoletta Samaridi, Eleni Tousidou, Michael Vassilakopoulos",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3635059.3635104"
    },
    {
        "id": 6943,
        "title": "Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding",
        "authors": "Mutian He, Philip N. Garner",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-1799"
    },
    {
        "id": 6944,
        "title": "Attacking a Transformer-Based Models for Arabic Language as Low Resources Language (LRL) Using Word-Substitution Methods",
        "authors": "Hanin Alshalan, Banafsheh Rekabdar",
        "published": "2023-9-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/transai60598.2023.00025"
    },
    {
        "id": 6945,
        "title": "Enhancing Text Summarization: Evaluating Transformer-Based Models and the Role of Large Language Models like ChatGPT",
        "authors": "Pınar Savcı, Bihter Das",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391040"
    },
    {
        "id": 6946,
        "title": "Analysis of transformer-based models for time series data, natural language processing, and computer vision",
        "authors": "Huanzhang Chen, Yunfan Hou, Tianhao Miao, Jiayu Xue",
        "published": "2023-10-23",
        "citations": 0,
        "abstract": "The birth of the Transformer revolutionarily signalled the start of a new epic chapter in the deep learning era. Through an encoder-decoder architecture, including residual connection, multi-head self-attention, etc., it completely reformed the deep models and unified the models used in traditional computer vision (CV) and natural language processing (NLP) problems. In recent years, many papers published have adapted the original Transformer model to better complete tasks in time series analysis, CV, and NLP. In the area of natural language processing, Bidirectional Encoder Representations from Transformers (BERT) employs a two-way transformer structure to learn context-based language representation, whereas Generative Pre-trained Transformer (GPT) employs a one-way transformer but enhances corpus training to enhance the model effect. The Vision Transformer model is the cornerstone of computer vision. It separates the input image into various patches, projects each patch into vectorized features, and then passes the them to Transformer. Based on the idea of the Vision Transformer, Swin Transformer and Biformer further optimized the Transformer and achieved better results. Time series combines the ideas embodied in CV and NLP, and in doing so, improves the specificity and various difficulties of time series problems to lower algorithm complexity and increase prediction accuracy. This article summarizes the uses and improvements of the Transformer in NLP, CV and time series, explores the development history and ideas on algorithm optimization, and predicts the potential developments of Transformer in these three fields.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/20/20231084"
    },
    {
        "id": 6947,
        "title": "Classical Machine Learning and Transformer Models for Offensive and Abusive Language Classification on Dziri Language",
        "authors": "Mohammed Mehdi Bouchene, Kheireddine Abainia",
        "published": "2023-9-16",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dasa59624.2023.10286654"
    },
    {
        "id": 6948,
        "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
        "authors": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, Sumit Sanghai",
        "published": "2023",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.298"
    },
    {
        "id": 6949,
        "title": "Accurate, interpretable predictions of materials properties within transformer language models",
        "authors": "Vadim Korolev, Pavel Protsenko",
        "published": "2023-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patter.2023.100803"
    },
    {
        "id": 6950,
        "title": "Shortcut Learning of Large Language Models in Natural Language Understanding",
        "authors": "Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, Xia Hu",
        "published": "2024-1",
        "citations": 3,
        "abstract": "Shortcuts often hinder the robustness of large language models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3596490"
    },
    {
        "id": 6951,
        "title": "Reliable Natural Language Understanding with Large Language Models and Answer Set Programming",
        "authors": "Abhiramon Rajasekharan, Yankai Zeng, Parth Padalkar, Gopal Gupta",
        "published": "2023-9-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4204/eptcs.385.27"
    },
    {
        "id": 6952,
        "title": "Evaluating the Enhanced Performance of Tree-Based Models for Natural Language Understanding",
        "authors": "Adlin Jebakumari S, Aasheesh Raizada, Rahul Vishnoi",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icocwc60930.2024.10470931"
    },
    {
        "id": 6953,
        "title": "20.5 C-Transformer: A 2.6-18.1μJ/Token Homogeneous DNN-Transformer/Spiking-Transformer Processor with Big-Little Network and Implicit Weight Generation for Large Language Models",
        "authors": "Sangyeob Kim, Sangjin Kim, Wooyoung Jo, Soyeon Kim, Seongyon Hong, Hoi-Jun Yoo",
        "published": "2024-2-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isscc49657.2024.10454330"
    },
    {
        "id": 6954,
        "title": "Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding",
        "authors": "Bram van Dijk, Tom Kouwenhoven, Marco Spruit, Max Johannes van Duijn",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.779"
    },
    {
        "id": 6955,
        "title": "Understanding latent affective bias in large pre-trained neural language models",
        "authors": "Anoop Kadan, Deepak P., Sahely Bhadra, Manjary P. Gangan, Lajish V.L.",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2024.100062"
    },
    {
        "id": 6956,
        "title": "On the effect of dropping layers of pre-trained transformer models",
        "authors": "Hassan Sajjad, Fahim Dalvi, Nadir Durrani, Preslav Nakov",
        "published": "2023-1",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.csl.2022.101429"
    },
    {
        "id": 6957,
        "title": "Understanding Computational Models of Semantic Change: New Insights from the Speech Community",
        "authors": "Filip Miletić, Anne Przewozny-Desriaux, Ludovic Tanguy",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.572"
    },
    {
        "id": 6958,
        "title": "Triple-Hybrid Energy-based Model Makes Better Calibrated Natural Language Understanding Models",
        "authors": "Haotian Xu, Yingying Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.eacl-main.21"
    },
    {
        "id": 6959,
        "title": "Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT",
        "authors": "Soo Ryu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.27"
    },
    {
        "id": 6960,
        "title": "Transformer Language Models Handle Word Frequency in Prediction Head",
        "authors": "Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.276"
    },
    {
        "id": 6961,
        "title": "Bidirectional Transformer Reranker for Grammatical Error Correction",
        "authors": "Ying Zhang, Hidetaka Kamigaito, Manabu Okumura",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5715/jnlp.31.3"
    },
    {
        "id": 6962,
        "title": "A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning",
        "authors": "Evans Kotei, Ramkumar Thirunavukarasu",
        "published": "2023-3-16",
        "citations": 13,
        "abstract": "Transfer learning is a technique utilized in deep learning applications to transmit learned inference to a different target domain. The approach is mainly to solve the problem of a few training datasets resulting in model overfitting, which affects model performance. The study was carried out on publications retrieved from various digital libraries such as SCOPUS, ScienceDirect, IEEE Xplore, ACM Digital Library, and Google Scholar, which formed the Primary studies. Secondary studies were retrieved from Primary articles using the backward and forward snowballing approach. Based on set inclusion and exclusion parameters, relevant publications were selected for review. The study focused on transfer learning pretrained NLP models based on the deep transformer network. BERT and GPT were the two elite pretrained models trained to classify global and local representations based on larger unlabeled text datasets through self-supervised learning. Pretrained transformer models offer numerous advantages to natural language processing models, such as knowledge transfer to downstream tasks that deal with drawbacks associated with training a model from scratch. This review gives a comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks. Finally, we present future directions to further improvement in pretrained transformer-based language models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info14030187"
    },
    {
        "id": 6963,
        "title": "Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization",
        "authors": "Chong Yu, Tao Chen, Zhongxue Gan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.15"
    },
    {
        "id": 6964,
        "title": "Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness",
        "authors": "Xiaoyu Tan, Shaojie Shi, Xihe Qiu, Chao Qu, Zhenting Qi, Yinghui Xu, Yuan Qi",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-industry.62"
    },
    {
        "id": 6965,
        "title": "How Are Idioms Processed Inside Transformer Language Models?",
        "authors": "Ye Tian, Isobel James, Hye Son",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.starsem-1.16"
    },
    {
        "id": 6966,
        "title": "MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models",
        "authors": "Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, Xu Tan, Wei Ye, Shikun Zhang, Jiang Bian",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-demo.21"
    },
    {
        "id": 6967,
        "title": "Understanding Telecom Language Through Large Language Models",
        "authors": "Lina Bariah, Hang Zou, Qiyang Zhao, Belkacem Mouhouche, Faouzi Bader, Merouane Debbah",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437725"
    },
    {
        "id": 6968,
        "title": "Improving Natural Language Inference in Arabic Using Transformer Models and Linguistically Informed Pre-Training",
        "authors": "Mohammad Majd Saad Al Deen, Maren Pielka, Jörn Hees, Bouthaina Soulef Abdou, Rafet Sifa",
        "published": "2023-12-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10371891"
    },
    {
        "id": 6969,
        "title": "Leveraging Transformer-based Language Models for Enhanced Service Insight in Tourism",
        "authors": "Aleyna Er, Şuayb Talha Özçelik, Meltem Turhan Yöndem",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iisec59749.2023.10391041"
    },
    {
        "id": 6970,
        "title": "Transformer-Based Conditional Language Models to Generate Filipino News Articles",
        "authors": "Kenrick Lance T. Buñag, Rosanna A. Esquivel",
        "published": "2023-3-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.46254/an13.20230595"
    },
    {
        "id": 6971,
        "title": "Understanding the Attention Mechanism in Neural Network Transformer Models in Image Restoration Tasks",
        "authors": "Nikita Berezhnov, Alexander Sirota",
        "published": "2023-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/summa60232.2023.10349626"
    },
    {
        "id": 6972,
        "title": "UsingWikidata for Enhancing Compositionality in Pre-trained Language Models",
        "authors": "Meriem Beloucif,  , Mihir Bansal, Chris Biemann,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_019"
    },
    {
        "id": 6973,
        "title": "Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks",
        "authors": "Sunit Bhattacharya, Ondřej Bojar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.blackboxnlp-1.9"
    },
    {
        "id": 6974,
        "title": "Pipeline Signed Japanese Translation Using PBSMT and Transformer in a Low-Resource Setting",
        "authors": "Ken Yano, Akira Utsumi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5715/jnlp.30.30"
    },
    {
        "id": 6975,
        "title": "Compact Transformer-based Language Models for the Moroccan Darija",
        "authors": "Mohamed Aghzal, Mohamed Amine El Bouni, Saad Driouech, Asmaa Mourhir",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cist56084.2023.10409912"
    },
    {
        "id": 6976,
        "title": "Few-Shot Spoken Language Understanding Via Joint Speech-Text Models",
        "authors": "Chung-Ming Chien, Mingjiamei Zhang, Ju-Chieh Chou, Karen Livescu",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asru57964.2023.10389660"
    },
    {
        "id": 6977,
        "title": "Exploring Transformer Model in Longitudinal Pharmacokinetic/Pharmacodynamic Analyses and Comparing with Alternative Natural Language Processing Models",
        "authors": "Yiming Cheng, Hongxiang Hu, Xin Dong, Xiaoran Hao, Yan Li",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.xphs.2024.02.008"
    },
    {
        "id": 6978,
        "title": "A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models",
        "authors": "Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song",
        "published": "2024-3-31",
        "citations": 24,
        "abstract": "Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3617680"
    },
    {
        "id": 6979,
        "title": "Evaluating Unsupervised Hierarchical Topic Models Using a Labeled Dataset",
        "authors": "Judicael Poumay,  , Ashwin Ittoo,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_091"
    },
    {
        "id": 6980,
        "title": "Transformer-based text similarity and second language proficiency: A case of written production by learners of Korean",
        "authors": "Gyu-Ho Shin, Boo Kyung Jung, Seongmin Mun",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2024.100060"
    },
    {
        "id": 6981,
        "title": "Using of Transformer-Based Language Models to Separate Traffic Packets of Different Protocols",
        "authors": "Zalina Rusinova, Yury Chernyshov",
        "published": "2023-10-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/redundancy59964.2023.10330184"
    },
    {
        "id": 6982,
        "title": "Leveraging Multilingual Self-Supervised Pretrained Models for Sequence-to-Sequence End-to-End Spoken Language Understanding",
        "authors": "Pavel Denisov, Ngoc Thang Vu",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asru57964.2023.10389655"
    },
    {
        "id": 6983,
        "title": "JGLUE: Japanese General Language Understanding Evaluation",
        "authors": "Kentaro Kurihara, Daisuke Kawahara, Tomohide Shibata",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5715/jnlp.30.63"
    },
    {
        "id": 6984,
        "title": "Stellenwert von Natural Language Processing und chatbasierten Generative Language Models",
        "authors": "Markus Haar, Michael Sonntagbauer, Stefan Kluge",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00063-023-01098-5"
    },
    {
        "id": 6985,
        "title": "Transformer Working Memory Enables Regular Language Reasoning And Natural Language Length Extrapolation",
        "authors": "Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, Peter Ramadge",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.397"
    },
    {
        "id": 6986,
        "title": "Evaluating Large Language Models’ Understanding of Financial Terminology via Definition Modeling",
        "authors": "James Jhirad, Edison Marrese-Taylor, Yutaka Matsuo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.ijcnlp-srw.12"
    },
    {
        "id": 6987,
        "title": "Towards Evaluation and Understanding of Large Language Models for Cyber Operation Automation",
        "authors": "Madeena Sultana, Adrian Taylor, Li Li, Suryadipta Majumdar",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cns59707.2023.10288677"
    },
    {
        "id": 6988,
        "title": "Leveraging Transformer Models in the Cyberbullying Text Classification System for the Low-resource Bengali Language",
        "authors": "Md. Nesarul Hoque, Md. Hanif Seddiqui",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccit60459.2023.10441412"
    },
    {
        "id": 6989,
        "title": "Dynamic Low-rank Estimation for Transformer-based Language Models",
        "authors": "Ting Hua, Xiao Li, Shangqian Gao, Yen-Chang Hsu, Yilin Shen, Hongxia Jin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.621"
    },
    {
        "id": 6990,
        "title": "Online Aggression Identification Using Ensembled Transformer-based Language Models",
        "authors": "Sneha Chinivar, Roopa M S, Arunalatha J S, Venugopal K R",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10307921"
    },
    {
        "id": 6991,
        "title": "The Generalization and Robustness of Transformer-Based Language Models on Commonsense Reasoning",
        "authors": "Ke Shen",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "The advent of powerful transformer-based discriminative language models and, more recently, generative GPT-family models, has led to notable advancements in natural language processing (NLP), particularly in commonsense reasoning tasks. One such task is commonsense reasoning, where performance is usually evaluated through multiple-choice question-answering benchmarks. Till date, many such benchmarks have been proposed and `leaderboards' tracking state-of-the-art performance on those benchmarks suggest that transformer-based models are approaching human-like performance. However, due to documented problems such as hallucination and bias, the research focus is shifting from merely quantifying accuracy on the task to an in-depth, context-sensitive probing of LLMs' generalization and robustness. To gain deeper insight into diagnosing these models' performance in commonsense reasoning scenarios, this thesis addresses three main studies: the generalization ability of transformer-based language models on commonsense reasoning, the trend in confidence distribution of these language models confronted with ambiguous inference tasks, and a proposed risk-centric evaluation framework for both discriminative and generative language models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i21.30410"
    },
    {
        "id": 6992,
        "title": "Comparative Analysis of LSTM, GRU and Transformer Models for German to English Language Translation",
        "authors": "Premanand Ghadekar, Neel Malwatkar, Nikhil Sontakke, Nirvisha Soni",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asiancon58793.2023.10270018"
    },
    {
        "id": 6993,
        "title": "DTT: An Example-Driven Tabular Transformer for Joinability by Leveraging Large Language Models",
        "authors": "Arash Dargahi Nobari, Davood Rafiei",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "Many organizations rely on data from government and third-party sources, and those sources rarely follow the same data formatting. This introduces challenges in integrating data from multiple sources or aligning external sources with internal databases. Commercial database systems do not offer adequate support for integrating data from heterogeneous sources, and manual integration is both time-consuming and inefficient. State-of-the-art data integration approaches that rely on similarity functions and textual transformations often fail to handle challenging cases where multiple mappings are required, or the mappings go beyond simple textual transformations.\nIn this paper, we study the potentials of deep neural models for transforming tables for joinability. In particular, we cast the problem as a prediction task and develop a framework that leverages large deep-learning language models to transform tabular data from a source formatting to a desired target representation. Our framework can efficiently learn the patterns for mapping a source formatting into an expected target using just a few examples, which can then be used for tasks such as table joining, filling in missing values, and error detection. Compared to state-of-the-art mapping and joining approaches, our framework delivers noticeably more accurate and scalable performance on both real-world and synthetic datasets. Our experimental evaluation also shows that the performance of the proposed framework using our fine-tuned model is at par or better than large language models such as GPT-3, despite the significant difference in size, and that using large language models within our framework improves their performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3639279"
    },
    {
        "id": 6994,
        "title": "Applications of transformer-based language models in bioinformatics: a survey",
        "authors": "Shuang Zhang, Rui Fan, Yuti Liu, Shuang Chen, Qiao Liu, Wanwen Zeng",
        "published": "2023-1-5",
        "citations": 24,
        "abstract": "AbstractSummaryThe transformer-based language models, including vanilla transformer, BERT and GPT-3, have achieved revolutionary breakthroughs in the field of natural language processing (NLP). Since there are inherent similarities between various biological sequences and natural languages, the remarkable interpretability and adaptability of these models have prompted a new wave of their application in bioinformatics research. To provide a timely and comprehensive review, we introduce key developments of transformer-based language models by describing the detailed structure of transformers and summarize their contribution to a wide range of bioinformatics research from basic sequence analysis to drug discovery. While transformer-based applications in bioinformatics are diverse and multifaceted, we identify and discuss the common challenges, including heterogeneity of training data, computational expense and model interpretability, and opportunities in the context of bioinformatics research. We hope that the broader community of NLP researchers, bioinformaticians and biologists will be brought together to foster future research and development in transformer-based language models, and inspire novel bioinformatics applications that are unattainable by traditional methods.Supplementary informationSupplementary data are available at Bioinformatics Advances online.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/bioadv/vbad001"
    },
    {
        "id": 6995,
        "title": "Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",
        "authors": "Xinsong Zhang, Yan Zeng, Jipeng Zhang, Hang Li",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.40"
    },
    {
        "id": 6996,
        "title": "ShellGPT: Generative Pre-trained Transformer Model for Shell Language Understanding",
        "authors": "Jie Shi, Sihang Jiang, Bo Xu, Jiaqing Liang, Yanghua Xiao, Wei Wang",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/issre59848.2023.00082"
    },
    {
        "id": 6997,
        "title": "NLP Transformers: Enhanced Text Summarization and Language Understanding",
        "authors": "Yunus Emre IŞIKDEMİR",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "As the amount of the available information continues to grow, finding the relevant information has become increasingly challenging. As a solution, text summarization has emerged as a vital method for extracting essential information from lengthy documents. There are various techniques available for filtering documents and extracting the pertinent information. In this study, a comparative analysis is conducted to evaluate traditional approaches and state-of-the-art methods on the BBC News and CNN/DailyMail datasets. This study offers valuable insights for researchers to advance their research and helps practitioners in selecting the most suitable techniques for their specific use cases.",
        "keywords": "",
        "link": "http://dx.doi.org/10.31796/ogummf.1303569"
    },
    {
        "id": 6998,
        "title": "Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models",
        "authors": "James O’Neill, Sourav Dutta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-short.114"
    },
    {
        "id": 6999,
        "title": "Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization",
        "authors": "Ningyu Xu, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-emnlp.931"
    },
    {
        "id": 7000,
        "title": "BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bangla",
        "authors": "Saumajit Saha, Albert Nanda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.17"
    },
    {
        "id": 7001,
        "title": "Multimodal Transformer for Risk Classification: Analyzing the Impact of Different Data Modalities",
        "authors": "Niklas Holtz, Jorge Marx Gomez",
        "published": "2023-5-20",
        "citations": 1,
        "abstract": "Risk classification plays a critical role in domains such as finance, insurance, and healthcare. However, identifying risks can be a challenging task when dealing with different types of data. In this paper, we present a novel approach using the Multimodal Transformer for risk classification, and we investigate the use of data augmentation for risk data through automated retrieval of news articles. We achieved this through keyword extraction based on the title and descriptions of risks and using various selection metrics. We evaluate our approach using a real-world dataset containing numerical, categorical, and textual data. Our results demonstrate that the use of the Multimodal Transformer for risk classification outperforms other models that only utilize textual data. We show that the inclusion of numerical and categorical data improves the performance of the model, particularly for risks that are difficult to classify based on textual data alone. Additionally, our research indicates that the utilization of data augmentation techniques yields enhanced performance outcomes in models. This methodology presents a promising avenue for enterprises to effectively mitigate risks and make well-informed decisions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.130803"
    },
    {
        "id": 7002,
        "title": "StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding",
        "authors": "Cheng Jiayang, Lin Qiu, Tsz Chan, Tianqing Fang, Weiqi Wang, Chunkit Chan, Dongyu Ru, Qipeng Guo, Hongming Zhang, Yangqiu Song, Yue Zhang, Zheng Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.706"
    },
    {
        "id": 7003,
        "title": "Transformer-based language models for mental health issues: A survey",
        "authors": "Candida M. Greco, Andrea Simeri, Andrea Tagarelli, Ester Zumpano",
        "published": "2023-3",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.patrec.2023.02.016"
    },
    {
        "id": 7004,
        "title": "Transformer Models for Recognizing Abusive Language An investigation and review on Tweeteval and SOLID dataset",
        "authors": "Fabeela Ali Rawther, Geevarghese Titus",
        "published": "2023-4-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceeict56924.2023.10157848"
    },
    {
        "id": 7005,
        "title": "Employing large language models in survey research",
        "authors": "Bernard J. Jansen, Soon-gyo Jung, Joni Salminen",
        "published": "2023-9",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100020"
    },
    {
        "id": 7006,
        "title": "Evaluating Generative Models for Graph-to-Text Generation",
        "authors": "Shuzhou Yuan,  , Michael Färber,  ",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_133"
    },
    {
        "id": 7007,
        "title": "Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models",
        "authors": "James Fiacco, David Adamson, Carolyn Ros",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.bea-1.20"
    },
    {
        "id": 7008,
        "title": "Automatic text summarization using transformer-based language models",
        "authors": "Ritika Rao, Sourabh Sharma, Nitin Malik",
        "published": "2024-3-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13198-024-02280-4"
    },
    {
        "id": 7009,
        "title": "An Attention-Based Backend Allowing Efficient Fine-Tuning of Transformer Models for Speaker Verification",
        "authors": "Junyi Peng, Oldrich Plchot, Themos Stafylakis, Ladislav Mosner, Lukas Burget, Jan Cernocky",
        "published": "2023-1-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/slt54892.2023.10022775"
    },
    {
        "id": 7010,
        "title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts",
        "authors": "Saumajit Saha, Albert Nanda",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.34"
    },
    {
        "id": 7011,
        "title": "Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models",
        "authors": "Mohammadreza Banaei, Klaudia Bałazy, Artur Kasymov, Rémi Lebret, Jacek Tabor, Karl Aberer",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.133"
    },
    {
        "id": 7012,
        "title": "Sparse Universal Transformer",
        "authors": "Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron Courville, Chuang Gan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.12"
    },
    {
        "id": 7013,
        "title": "Microsyntactic Unit Detection usingWord Embedding Models: Experiments on Slavic Languages",
        "authors": "Iuliia Zaitova,  , Irina Stenger, Tania Avgustinova,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_134"
    },
    {
        "id": 7014,
        "title": "Challenges in natural language processing and natural language understanding by considering both technical and natural domains",
        "authors": "Pouya Ardehkhani, Amir Vahedi, Hossein Aghababa",
        "published": "2023-2-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ipria59240.2023.10147185"
    },
    {
        "id": 7015,
        "title": "Security Challenges in Natural Language Processing Models",
        "authors": "Qiongkai Xu, Xuanli He",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-tutorial.2"
    },
    {
        "id": 7016,
        "title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
        "authors": "Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny Manchanda, Tanmoy Chakraborty",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.225"
    },
    {
        "id": 7017,
        "title": "A Comprehensive Understanding of Code-Mixed Language Semantics Using Hierarchical Transformer",
        "authors": "Tharun Suresh, Ayan Sengupta, Md Shad Akhtar, Tanmoy Chakraborty",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tcss.2024.3360378"
    },
    {
        "id": 7018,
        "title": "A survey of GPT-3 family large language models including ChatGPT and GPT-4",
        "authors": "Katikapalli Subramanyam Kalyan",
        "published": "2024-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100048"
    },
    {
        "id": 7019,
        "title": "CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models",
        "authors": "Aitor Ormazabal, Mikel Artetxe, Eneko Agirre",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.180"
    },
    {
        "id": 7020,
        "title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation",
        "authors": "Jiwan Chung, Youngjae Yu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.46"
    },
    {
        "id": 7021,
        "title": "Statistical Depth for Ranking and Characterizing Transformer-Based Text Embeddings",
        "authors": "Parker Seegmiller, Sarah Preum",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.596"
    },
    {
        "id": 7022,
        "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
        "authors": "Lujia Shen, Yuwen Pu, Shouling Ji, Changjiang Li, Xuhong Zhang, Chunpeng Ge, Ting Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14722/ndss.2024.24115"
    },
    {
        "id": 7023,
        "title": "Online Handwriting Gesture Recognition Using Transformer and Natural Language Processing",
        "authors": "Guénolé C.M. Silvestre, Félix Balado, Olumide Akinremi, Mirco Ramo",
        "published": "2023-9-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10289873"
    },
    {
        "id": 7024,
        "title": "Multi-Feature Fusion Transformer for Natural Language Inference",
        "authors": "Lei Sun, Xiangyang Li, Hengxin Yan",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ricai60863.2023.10489156"
    },
    {
        "id": 7025,
        "title": "Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models",
        "authors": "Shun Kiyono,  , Sho Takase, Shengzhe Li, Toshinori Sato,  ,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_062"
    },
    {
        "id": 7026,
        "title": "Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction",
        "authors": "V.S.D.S.Mahesh Akavarapu, Arnab Bhattacharya",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.423"
    },
    {
        "id": 7027,
        "title": "CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models",
        "authors": "Xingbo Wang, Renfei Huang, Zhihua Jin, Tianqing Fang, Huamin Qu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tvcg.2023.3327153"
    },
    {
        "id": 7028,
        "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective",
        "authors": "Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, Yue Zhang",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.806"
    },
    {
        "id": 7029,
        "title": "Generative Models For Indic Languages: Evaluating Content Generation Capabilities",
        "authors": "Savita Bhat,  , Vasudeva Varma, Niranjan Pedanekar,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_021"
    },
    {
        "id": 7030,
        "title": "Transformer-Based Composite Language Models for Text Evaluation and Classification",
        "authors": "Mihailo Škorić, Miloš Utvić, Ranka Stanković",
        "published": "2023-11-16",
        "citations": 0,
        "abstract": "Parallel natural language processing systems were previously successfully tested on the tasks of part-of-speech tagging and authorship attribution through mini-language modeling, for which they achieved significantly better results than independent methods in the cases of seven European languages. The aim of this paper is to present the advantages of using composite language models in the processing and evaluation of texts written in arbitrary highly inflective and morphology-rich natural language, particularly Serbian. A perplexity-based dataset, the main asset for the methodology assessment, was created using a series of generative pre-trained transformers trained on different representations of the Serbian language corpus and a set of sentences classified into three groups (expert translations, corrupted translations, and machine translations). The paper describes a comparative analysis of calculated perplexities in order to measure the classification capability of different models on two binary classification tasks. In the course of the experiment, we tested three standalone language models (baseline) and two composite language models (which are based on perplexities outputted by all three standalone models). The presented results single out a complex stacked classifier using a multitude of features extracted from perplexity vectors as the optimal architecture of composite language models for both tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11224660"
    },
    {
        "id": 7031,
        "title": "Bringing order into the realm of Transformer-based language models for artificial intelligence and law",
        "authors": "Candida M. Greco, Andrea Tagarelli",
        "published": "2023-11-20",
        "citations": 1,
        "abstract": "AbstractTransformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10506-023-09374-7"
    },
    {
        "id": 7032,
        "title": "Using Transformer Language Models to Validate Peer-Assigned Essay Scores in Massive Open Online Courses (MOOCs)",
        "authors": "Wesley Morris, Scott Crossley, Langdon Holmes, Anne Trumbore",
        "published": "2023-3-13",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3576050.3576098"
    },
    {
        "id": 7033,
        "title": "Masked transformer through knowledge distillation for unsupervised text style transfer",
        "authors": "Arthur Scalercio, Aline Paes",
        "published": "2023-7-25",
        "citations": 0,
        "abstract": "Abstract\nText style transfer (TST) aims at automatically changing a text’s stylistic features, such as formality, sentiment, authorial style, humor, and complexity, while still trying to preserve its content. Although the scientific community has investigated TST since the 1980s, it has recently regained attention by adopting deep unsupervised strategies to address the challenge of training without parallel data. In this manuscript, we investigate how relying on sequence-to-sequence pretraining models affects the performance of TST when the pretraining step leverages pairs of paraphrase data. Furthermore, we propose a new technique to enhance the sequence-to-sequence model by distilling knowledge from masked language models. We evaluate our proposals on three unsupervised style transfer tasks with widely used benchmarks: author imitation, formality transfer, and polarity swap. The evaluation relies on quantitative and qualitative analyses and comparisons with the results of state-of-the-art models. For the author imitation and the formality transfer task, we show that using the proposed techniques improves all measured metrics and leads to state-of-the-art (SOTA) results in content preservation and an overall score in the author imitation domain. In the formality transfer domain, we paired with the SOTA method in the style control metric. Regarding the polarity swap domain, we show that the knowledge distillation component improves all measured metrics. The paraphrase pretraining increases content preservation at the expense of harming style control. Based on the results reached in these domains, we also discuss in the manuscript if the tasks we address have the same nature and should be equally treated as TST tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1017/s1351324923000323"
    },
    {
        "id": 7034,
        "title": "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?",
        "authors": "Byung-Doh Oh, William Schuler",
        "published": "2023-3-27",
        "citations": 14,
        "abstract": "AbstractThis work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to ‘memorize’ sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1162/tacl_a_00548"
    },
    {
        "id": 7035,
        "title": "EpiK-Eval: Evaluation for Language Models as Epistemic Models",
        "authors": "Gabriele Prato, Jerry Huang, Prasanna Parthasarathi, Shagun Sodhani, Sarath Chandar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.593"
    },
    {
        "id": 7036,
        "title": "What is Wrong with Language Models that Can Not Tell a Story?",
        "authors": "Ivan Yamshchikov, Alexey Tikhonov",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.wnu-1.8"
    },
    {
        "id": 7037,
        "title": "Transformer and Natural language processing; A recent development.",
        "authors": " Tushar Agarwal, Jitender Jangid, Gaurav Kumar",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "The convergence of transformers and natural language processing (NLP) represents a watershed moment in the realm of artificial intelligence. Recent NLP advancements have been profoundly shaped by the emergence of transformers, a class of deep learning models renowned for their remarkable proficiency in comprehending, generating, and manipulating human language. This abstract offers a succinct exploration of the symbiotic relationship between transformers and NLP, emphasizing their central role in propelling recent process.\r\nThe introduction of transformers heralded a paradigm shift in the realm of NLP, primarily owing to their innovative self-attention mechanism, which empowers them to adeptly capture intricate contextual associations within textual data. Distinguished by their multi-head attention layers and feed-forward networks, the architecture of transformers has ushered in a new era in NLP. Models such as BERT,\r\nGPT-3, and their offshoots have not only redefined but also set the gold standard for a wide array of NLP tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/tjjpt.v44.i1.2225"
    },
    {
        "id": 7038,
        "title": "Evaluating Large Language Models in Relationship Extraction from Unstructured Data: Empirical Study from Holocaust Testimonies",
        "authors": "Isuri Anuradha Nanomi Arachchige,  , Le An Ha, Ruslan Mitkov, Vinitar Nahar,  ,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_013"
    },
    {
        "id": 7039,
        "title": "Neural Language Models in Natural Language Processing",
        "authors": "Zihao Chen",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdacai59742.2023.00104"
    },
    {
        "id": 7040,
        "title": "Ensemble learning with soft-prompted pretrained language models for fact checking",
        "authors": "Shaoqin Huang, Yue Wang, Eugene Y.C. Wong, Lei Yu",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2024.100067"
    },
    {
        "id": 7041,
        "title": "Sign Language-to-Text Dictionary with Lightweight Transformer Models",
        "authors": "Jérôme Fink, Pierre Poitier, Maxime André, Loup Meurice, Benoît Frénay, Anthony Cleve, Bruno Dumas, Laurence Meurant",
        "published": "2023-8",
        "citations": 0,
        "abstract": "The recent advances in deep learning have been beneficial to automatic sign language recognition (SLR). However, free-to-access, usable, and accessible tools are still not widely available to the deaf community. The need for a sign language-to-text dictionary was raised by a bilingual deaf school in Belgium and linguist experts in sign languages (SL) in order to improve the autonomy of students. To meet that need, an efficient SLR system was built based on a specific transformer model. The proposed system is able to recognize 700 different signs, with a top-10 accuracy of 83%. Those results are competitive with other systems in the literature while using 10 times less parameters than existing solutions. The integration of this model into a usable and accessible web application for the dictionary is also introduced. A user-centered human-computer interaction (HCI) methodology was followed to design and implement the user interface. To the best of our knowledge, this is the first publicly released sign language-to-text dictionary using video captured by a standard camera.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/662"
    },
    {
        "id": 7042,
        "title": "ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding",
        "authors": "Ömer Veysel Çağatan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-babylm.14"
    },
    {
        "id": 7043,
        "title": "HTMOT: Hierarchical Topic Modelling Over Time",
        "authors": "Judicael Poumay,  , Ashwin Ittoo,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_092"
    },
    {
        "id": 7044,
        "title": "Data Augmentation for Fake News Detection by Combining Seq2seq and NLI",
        "authors": "Anna Glazkova,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_048"
    },
    {
        "id": 7045,
        "title": "Data-to-text generation using conditional generative adversarial with enhanced transformer",
        "authors": "Elham Seifossadat, Hossein Sameti",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "Abstract\nIn this paper, we propose an enhanced version of the vanilla transformer for data-to-text generation and then use it as the generator of a conditional generative adversarial model to improve the semantic quality and diversity of output sentences. Specifically, by adding a diagonal mask matrix to the attention scores of the encoder and using the history of the attention weights in the decoder, this enhanced version of the vanilla transformer prevents semantic defects in the output text. Also, using this enhanced transformer along with a triplet network, respectively, as the generator and discriminator of conditional generative adversarial network, diversity and semantic quality of sentences are guaranteed. To prove the effectiveness of the proposed model, called conditional generative adversarial with enhanced transformer (CGA-ET), we performed experiments on three different datasets and observed that our proposed model is able to achieve better results than the baselines models in terms of BLEU, METEOR, NIST, ROUGE-L, CIDEr, BERTScore, and SER automatic evaluation metrics as well as human evaluation.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1017/s1351324923000487"
    },
    {
        "id": 7046,
        "title": "Improving Transformer-based Program Repair Model through False Behavior Diagnosis",
        "authors": "Youngkyoung Kim, Misoo Kim, Eunseok Lee",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.865"
    },
    {
        "id": 7047,
        "title": "A Keyword Based Approach to Understanding the Overpenalization of Marginalized Groups by English Marginal Abuse Models on Twitter",
        "authors": "Kyra Yee, Alice Schoenauer Sebag, Olivia Redfield, Matthias Eck, Emily Sheng, Luca Belli",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.trustnlp-1.10"
    },
    {
        "id": 7048,
        "title": "Trustworthiness of Children Stories Generated by Large Language Models",
        "authors": "Prabin Bhandari, Hannah Brennan",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.24"
    },
    {
        "id": 7049,
        "title": "Are transformer-based models more robust than CNN-based models?",
        "authors": "Zhendong Liu, Shuwei Qian, Changhong Xia, Chongjun Wang",
        "published": "2024-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.045"
    },
    {
        "id": 7050,
        "title": "Learning Knowledge-Enhanced Contextual Language Representations for Domain Natural Language Understanding",
        "authors": "Taolin Zhang, Ruyao Xu, Chengyu Wang, Zhongjie Duan, Cen Chen, Minghui Qiu, Dawei Cheng, Xiaofeng He, Weining Qian",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.969"
    },
    {
        "id": 7051,
        "title": "Non-Parametric Memory Guidance for Multi-Document Summarization",
        "authors": "Florian Baud,  , Alex Aussem,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_017"
    },
    {
        "id": 7052,
        "title": "Multilingual Continual Learning Approaches for Text Classification",
        "authors": "Karan Praharaj,  , Irina Matveeva,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_093"
    },
    {
        "id": 7053,
        "title": "Causal interventions expose implicit situation models for commonsense language understanding",
        "authors": "Takateru Yamakoshi, James McClelland, Adele Goldberg, Robert Hawkins",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.839"
    },
    {
        "id": 7054,
        "title": "Validating Predictive Models Of Evaluative Language For Controllable Data2Text Generation",
        "authors": "Maurice Langner, Ralf Klabunde",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.22"
    },
    {
        "id": 7055,
        "title": "Generating clickbait spoilers with an ensemble of large language models",
        "authors": "Mateusz Woźny, Mateusz Lango",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.inlg-main.32"
    },
    {
        "id": 7056,
        "title": "Benchmarking topic models on scientific articles using BERTeley",
        "authors": "Eric Chagnon, Ronald Pandolfi, Jeffrey Donatelli, Daniela Ushizima",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100044"
    },
    {
        "id": 7057,
        "title": "Efficient Transformer Knowledge Distillation: A Performance Review",
        "authors": "Nathan Brown, Ashton Williamson, Tahj Anderson, Logan Lawrence",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-industry.6"
    },
    {
        "id": 7058,
        "title": "Prompting language models improves performance in imbalanced setting",
        "authors": "Jay Mohta",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.sustainlp-1.14"
    },
    {
        "id": 7059,
        "title": "News Category Classification using Natural Language Processing Transformer",
        "authors": "Parvathavarthini S, Shreekanth M, Vignesh Kumar S, Santhosh N S",
        "published": "2023-8-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icaiss58487.2023.10250566"
    },
    {
        "id": 7060,
        "title": "Did the Models Understand Documents? Benchmarking Models for Language Understanding in Document-Level Relation Extraction",
        "authors": "Haotian Chen, Bingsheng Chen, Xiangdong Zhou",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.354"
    },
    {
        "id": 7061,
        "title": "Exploring Transformers as Compact, Data-efficient Language Models",
        "authors": "Clayton Fields, Casey Kennington",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-1.35"
    },
    {
        "id": 7062,
        "title": "Exploring how well Large-scale Masked Language Models can Recognize Grammatical Errors",
        "authors": "Manabu Kimura, Ryo Nagata, Kazuaki Hanawa",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5715/jnlp.30.689"
    },
    {
        "id": 7063,
        "title": "Improving Bias Mitigation through Bias Experts in Natural Language Understanding",
        "authors": "Eojin Jeon, Mingyu Lee, Juhyeong Park, Yeachan Kim, Wing-Lam Mok, SangKeun Lee",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.681"
    },
    {
        "id": 7064,
        "title": "Forming Trees with Treeformers",
        "authors": "Nilay Patel,  , Jeffrey Flanigan,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_090"
    },
    {
        "id": 7065,
        "title": "Understanding Motivation for Enrolling in Postdoc Academy: Succeeding as a Postdoc Using Natural Language Processing",
        "authors": "Ting Sun",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3102/ip.23.2010720"
    },
    {
        "id": 7066,
        "title": "Adapting transformer-based language models for heart disease detection and risk factors extraction",
        "authors": "Essam H. Houssein, Rehab E. Mohamed, Gang Hu, Abdelmgeid A. Ali",
        "published": "2024-4-4",
        "citations": 0,
        "abstract": "AbstractEfficiently treating cardiac patients before the onset of a heart attack relies on the precise prediction of heart disease. Identifying and detecting the risk factors for heart disease such as diabetes mellitus, Coronary Artery Disease (CAD), hyperlipidemia, hypertension, smoking, familial CAD history, obesity, and medications is critical for developing effective preventative and management measures. Although Electronic Health Records (EHRs) have emerged as valuable resources for identifying these risk factors, their unstructured format poses challenges for cardiologists in retrieving relevant information. This research proposed employing transfer learning techniques to automatically extract heart disease risk factors from EHRs. Leveraging transfer learning, a deep learning technique has demonstrated a significant performance in various clinical natural language processing (NLP) applications, particularly in heart disease risk prediction. This study explored the application of transformer-based language models, specifically utilizing pre-trained architectures like BERT (Bidirectional Encoder Representations from Transformers), RoBERTa, BioClinicalBERT, XLNet, and BioBERT for heart disease detection and extraction of related risk factors from clinical notes, using the i2b2 dataset. These transformer models are pre-trained on an extensive corpus of medical literature and clinical records to gain a deep understanding of contextualized language representations. Adapted models are then fine-tuned using annotated datasets specific to heart disease, such as the i2b2 dataset, enabling them to learn patterns and relationships within the domain. These models have demonstrated superior performance in extracting semantic information from EHRs, automating high-performance heart disease risk factor identification, and performing downstream NLP tasks within the clinical domain. This study proposed fine-tuned five widely used transformer-based models, namely BERT, RoBERTa, BioClinicalBERT, XLNet, and BioBERT, using the 2014 i2b2 clinical NLP challenge dataset. The fine-tuned models surpass conventional approaches in predicting the presence of heart disease risk factors with impressive accuracy. The RoBERTa model has achieved the highest performance, with micro F1-scores of 94.27%, while the BERT, BioClinicalBERT, XLNet, and BioBERT models have provided competitive performances with micro F1-scores of 93.73%, 94.03%, 93.97%, and 93.99%, respectively. Finally, a simple ensemble of the five transformer-based models has been proposed, which outperformed the most existing methods in heart disease risk fan, achieving a micro F1-Score of 94.26%. This study demonstrated the efficacy of transfer learning using transformer-based models in enhancing risk prediction and facilitating early intervention for heart disease prevention.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s40537-024-00903-y"
    },
    {
        "id": 7067,
        "title": "How Is a “Kitchen Chair” like a “Farm Horse”? Exploring the Representation of Noun-Noun Compound Semantics in Transformer-based Language Models",
        "authors": "Mark Ormerod, Jesús Martínez del Rincón, Barry Devereux",
        "published": "2024-2-14",
        "citations": 0,
        "abstract": "Abstract\nDespite the success of Transformer-based language models in a wide variety of natural language processing tasks, our understanding of how these models process a given input in order to represent task-relevant information remains incomplete. In this work, we focus on semantic composition and examine how Transformer-based language models represent semantic information related to the meaning of English noun-noun compounds. We probe Transformer-based language models for their knowledge of the thematic relations that link the head nouns and modifier words of compounds (e.g., kitchen chair: a chair located in a kitchen). Firstly, using a dataset featuring groups of compounds with shared lexical or semantic features, we find that token representations of six Transformer-based language models distinguish between pairs of compounds based on whether they use the same thematic relation. Secondly, we utilize fine-grained vector representations of compound semantics derived from human annotations, and find that token vectors from several models elicit a strong signal of the semantic relations used in the compounds. In a novel “compositional probe” setting, where we compare the semantic relation signal in mean-pooled token vectors of compounds to mean-pooled token vectors when the two constituent words appear in separate sentences, we find that the Transformer-based language models that best represent the semantics of noun-noun compounds also do so substantially better than in the control condition where the two constituent works are processed separately. Overall, our results shed light on the ability of Transformer-based language models to support compositional semantic processes in representing the meaning of noun-noun compounds.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1162/coli_a_00495"
    },
    {
        "id": 7068,
        "title": "Semantics Squad at BLP-2023 Task 1: Violence Inciting Bangla Text Detection with Fine-Tuned Transformer-Based Models",
        "authors": "Krishno Dey, Prerona Tarannum, Md. Arid Hasan, Francis Palma",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.28"
    },
    {
        "id": 7069,
        "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning",
        "authors": "Yinheng Li,  ",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_069"
    },
    {
        "id": 7070,
        "title": "A Comparative Analysis of Transformer-based Protein Language Models for Remote Homology Prediction",
        "authors": "Anowarul Kabir, Asher Moldwin, Amarda Shehu",
        "published": "2023-9-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3584371.3612942"
    },
    {
        "id": 7071,
        "title": "Semantics Squad at BLP-2023 Task 2: Sentiment Analysis of Bangla Text with Fine Tuned Transformer Based Models",
        "authors": "Krishno Dey, Md. Arid Hasan, Prerona Tarannum, Francis Palma",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.banglalp-1.41"
    },
    {
        "id": 7072,
        "title": "SOUL: Towards Sentiment and Opinion Understanding of Language",
        "authors": "Yue Deng, Wenxuan Zhang, Sinno Pan, Lidong Bing",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.538"
    },
    {
        "id": 7073,
        "title": "Memory-and-Anticipation Transformer for Online Action Understanding",
        "authors": "Jiahao Wang, Guo Chen, Yifei Huang, Limin Wang, Tong Lu",
        "published": "2023-10-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01271"
    },
    {
        "id": 7074,
        "title": "Making Pre-trained Language Models Better Learn Few-Shot Spoken Language Understanding in More Practical Scenarios",
        "authors": "Yufan Wang, Jie Mei, Bowei Zou, Rui Fan, Tingting He, Ai Ti Aw",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.853"
    },
    {
        "id": 7075,
        "title": "NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models",
        "authors": "Yongchao Chen, Rujul Gandhi, Yang Zhang, Chuchu Fan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.985"
    },
    {
        "id": 7076,
        "title": "Semantic Scene Understanding with Large Language Models on Unmanned Aerial Vehicles",
        "authors": "J. de Curtò, I. de Zarzà, Carlos T. Calafate",
        "published": "2023-2-8",
        "citations": 17,
        "abstract": "Unmanned Aerial Vehicles (UAVs) are able to provide instantaneous visual cues and a high-level data throughput that could be further leveraged to address complex tasks, such as semantically rich scene understanding. In this work, we built on the use of Large Language Models (LLMs) and Visual Language Models (VLMs), together with a state-of-the-art detection pipeline, to provide thorough zero-shot UAV scene literary text descriptions. The generated texts achieve a GUNNING Fog median grade level in the range of 7–12. Applications of this framework could be found in the filming industry and could enhance user experience in theme parks or in the advertisement sector. We demonstrate a low-cost highly efficient state-of-the-art practical implementation of microdrones in a well-controlled and challenging setting, in addition to proposing the use of standardized readability metrics to assess LLM-enhanced descriptions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/drones7020114"
    },
    {
        "id": 7077,
        "title": "Dialogue-based generation of self-driving simulation scenarios using Large Language Models",
        "authors": "Antonio Valerio Miceli Barone, Craig Innes, Alex Lascarides",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.splurobonlp-1.1"
    },
    {
        "id": 7078,
        "title": "Generating Data for Symbolic Language with Large Language Models",
        "authors": "Jiacheng Ye, Chengzu Li, Lingpeng Kong, Tao Yu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.523"
    },
    {
        "id": 7079,
        "title": "Evaluation of Medium-Sized Language Models in German and English Language",
        "authors": "René Peinl, Johannes Wirth",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "Large language models (LLMs) have garnered significant attention, but the definition of “large” lacks clarity. This paper focuses on medium-sized language models (MLMs), defined as having at least six billion parameters but less than 100 billion. The study evaluates MLMs regarding zero-shot generative question answering, which requires models to provide elaborate answers without external document retrieval. The paper introduces an own test dataset and presents results from human evaluation. Results show that combining the best answers from different MLMs yielded an overall correct answer rate of 82.7% which is better than the 60.9% of ChatGPT. The best MLM achieved 71.8% and has 33B parameters, which highlights the importance of using appropriate training data for fine-tuning rather than solely relying on the number of parameters. More fine-grained feedback should be used to further improve the quality of answers. The open source community is quickly closing the gap to the best commercial models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/ijnlc.2024.13101"
    },
    {
        "id": 7080,
        "title": "Corpus Complexity Matters in Pretraining Language Models",
        "authors": "Ameeta Agrawal, Suresh Singh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.sustainlp-1.20"
    },
    {
        "id": 7081,
        "title": "Training Data Extraction From Pre-trained Language Models: A Survey",
        "authors": "Shotaro Ishihara",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.trustnlp-1.23"
    },
    {
        "id": 7082,
        "title": "The Importance of Understanding Language in Large Language Models",
        "authors": "Alaa Youssef, Samantha Stein, Justin Clapp, David Magnus",
        "published": "2023-10-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/15265161.2023.2256614"
    },
    {
        "id": 7083,
        "title": "Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models",
        "authors": "Raghav Jain, Daivik Sojitra, Arkadeep Acharya, Sriparna Saha, Adam Jatowt, Sandipan Dandapat",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.418"
    },
    {
        "id": 7084,
        "title": "Was That a Question? Automatic Classification of Discourse Meaning in Spanish",
        "authors": "Santiago Arróniz,  , Sandra Kübler,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_015"
    },
    {
        "id": 7085,
        "title": "Learning transformer-based attention region with multiple scales for occluded person re-identification",
        "authors": "Zhi Liu, Xingyu Mu, Yunhua Lu, Tingting Zhang, Yingli Tian",
        "published": "2023-3",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cviu.2023.103652"
    },
    {
        "id": 7086,
        "title": "Neural network architecture «transformer»: Artificial Intelligence and its role in Natural Language Processing",
        "authors": "A.M Evstropov, E.A Tarlakovskaya, I.A Sidorov",
        "published": "2023",
        "citations": 0,
        "abstract": "The transformer neural architecture has become a cornerstone of natural language processing (NLP) models. It is a powerful tool for language understanding and generation, enabling machines to process human language that is close to human understanding. NLP has significantly improved in recent years due to pro-gress in artificial intelligence (AI). One of the key developments that has enabled these improvements is the transformer neural network architecture. In this paper, I will explore the transformer architecture, its main concepts, and its application in NLP.",
        "keywords": "",
        "link": "http://dx.doi.org/10.18411/trnio-05-2023-633"
    },
    {
        "id": 7087,
        "title": "Assessment of bidirectional transformer encoder model and attention based bidirectional LSTM language models for fake news detection",
        "authors": "Anshika Choudhary, Anuja Arora",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jretconser.2023.103545"
    },
    {
        "id": 7088,
        "title": "Investigating the Effect of Discourse Connectives on Transformer Surprisal: Language Models Understand Connectives, Even So They Are Surprised",
        "authors": "Yan Cong, Emmanuele Chersoni, Yu-Yin Hsu, Philippe Blache",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.blackboxnlp-1.17"
    },
    {
        "id": 7089,
        "title": "Transformer-based Models for Language Identification: A Comparative Study",
        "authors": "Bharathi Mohan G, R Prasanna Kumar, Elakkiya R, Venkatakrishnan R, Harrieni Shankar, Y Sree Harshitha, Harini K, M Nikhil Reddy",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icscan58655.2023.10394757"
    },
    {
        "id": 7090,
        "title": "Exploring Abstractive Text Summarisation for Podcasts: A Comparative Study of BART and T5 Models",
        "authors": "Parth Saxena,  , Mahmoud El-Haj,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_110"
    },
    {
        "id": 7091,
        "title": "Analysis of the evolution of advanced transformer-based language models: experiments on opinion mining",
        "authors": "Nour Eddine Zekaouiu, Siham Yousfi, Maryem Rhanoui, Mounia Mikram",
        "published": "2023-12-1",
        "citations": 1,
        "abstract": "<p>Opinion mining, also known as sentiment analysis, is a subfield of natural language processing (NLP) that focuses on identifying and extracting subjective information in textual material. This can include determining the overall sentiment of a piece of text (e.g., positive or negative), as well as identifying specific emotions or opinions expressed in the text, that involves the use of advanced machine and deep learning techniques. Recently, transformer-based language models make this task of human emotion analysis intuitive, thanks to the attention mechanism and parallel computation. These advantages make such models very powerful on linguistic tasks, unlike recurrent neural networks that spend a lot of time on sequential processing, making them prone to fail when it comes to processing long text. The scope of our paper aims to study the behaviour of the cutting-edge Transformer-based language models on opinion mining and provide a high-level comparison between them to highlight their key particularities. Additionally, our comparative study shows leads and paves the way for production engineers regarding the approach to focus on and is useful for researchers as it provides guidelines for future research subjects.</p>",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v12.i4.pp1995-2010"
    },
    {
        "id": 7092,
        "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions",
        "authors": "Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, Graham Neubig",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-demo.38"
    },
    {
        "id": 7093,
        "title": "Three Approaches to Client Email Topic Classification",
        "authors": "Branislava Šandrih Todorović,  , Katarina Josipović, Jurij Kodre,  ,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_109"
    },
    {
        "id": 7094,
        "title": "Beyond Information: Is ChatGPT Empathetic Enough?",
        "authors": "Ahmed Belkhir,  , Fatiha Sadat,  ",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_018"
    },
    {
        "id": 7095,
        "title": "huPWKP: A Hungarian Text Simplification Corpus",
        "authors": "Noémi Prótár,  , Dávid Márk Nemeskey,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_097"
    },
    {
        "id": 7096,
        "title": "Explainable Event Detection with Event Trigger Identification as Rationale Extraction",
        "authors": "Hansi Hettiarachchi,  , Tharindu Ranasinghe,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_056"
    },
    {
        "id": 7097,
        "title": "Baby’s CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models",
        "authors": "Zheyu Zhang, Han Yang, Bolei Ma, David Rügamer, Ercong Nie",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.conll-babylm.13"
    },
    {
        "id": 7098,
        "title": "Knowledge representation and acquisition in the era of large language models: Reflections on learning to reason via PAC-Semantics",
        "authors": "Ionela G. Mocanu, Vaishak Belle",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.nlp.2023.100036"
    },
    {
        "id": 7099,
        "title": "A fine-grained comparison of pragmatic language understanding in humans and language models",
        "authors": "Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko, Edward Gibson",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.acl-long.230"
    },
    {
        "id": 7100,
        "title": "Query2doc: Query Expansion with Large Language Models",
        "authors": "Liang Wang, Nan Yang, Furu Wei",
        "published": "2023",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.585"
    },
    {
        "id": 7101,
        "title": "A Computational Analysis of the Voices of Shakespeare’s Characters",
        "authors": "Liviu P. Dinu,  , Ana Sabina Uban,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_033"
    },
    {
        "id": 7102,
        "title": "BhojpuriWordNet: Problems in Translating Hindi Synsets into Bhojpuri",
        "authors": "Imran Ali,  , Praveen Gatla,  ",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26615/978-954-452-092-2_007"
    },
    {
        "id": 7103,
        "title": "Conceptor-Aided Debiasing of Large Language Models",
        "authors": "Li Yifei, Lyle Ungar, João Sedoc",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.661"
    },
    {
        "id": 7104,
        "title": "Probing neural language models for understanding of words of estimative probability",
        "authors": "Damien Sileo, Marie-francine Moens",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.starsem-1.41"
    }
]