[
    {
        "id": 4601,
        "title": "Stochastic Runge-Kutta methods and adaptive SGD-G2 stochastic gradient descent",
        "authors": "Imen Ayadi, Gabriel Turinici",
        "published": "2021-1-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr48806.2021.9412831"
    },
    {
        "id": 4602,
        "title": "AG-SGD: Angle-Based Stochastic Gradient Descent",
        "authors": "Chongya Song, Alexander Pons, Kang Yen",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3055993"
    },
    {
        "id": 4603,
        "title": "SW-SGD: The Sliding Window Stochastic Gradient Descent Algorithm",
        "authors": "Imen Chakroun, Tom Haber, Thomas J. Ashby",
        "published": "2017",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.procs.2017.05.082"
    },
    {
        "id": 4604,
        "title": "Analisis Performa Algoritma Stochastic Gradient Descent (SGD) Dalam Mengklasifikasi Tahu Berformalin",
        "authors": "Fadhila Tangguh Admojo, Yudha Islami Sulistya",
        "published": "2022-3-31",
        "citations": 2,
        "abstract": "Tahu berformalin adalah salah satu jenis makanan yang sering mengandung bahan-bahan kimia yang dapat mengawetkan daripada tahu tanpa formalin. Pada tahu berformalin dapat memberikan tekstur lebih kenyal dan berwarna putih bersih. Penelitian ini bertujuan untuk mengklasifikasikan tahu berformalin dan tahu tidak berformalin. Pada paper ini menggunakan algoritma Stochastic Gradient Descent atau dalam penerapannya lebih dikenal dengan SGD Classifier yang merupakan bagian dari algoritma machine learning untuk klasifikasi, regresi maupun jaringan syaraf tiruan serta algoritma ini sangat efisien pada dataset berskala besar. Penelitian ini mencoba menerapkan algoritma SGD pada dataset tahu berformalin dengan jumlah dataset yakni 11000 yang dimana 5500 data tahu berformalin dan 5500 data tahu tidak berformalin. Setelah dilakukan beberapa tahapan dalam pengujian dengan algoritma SGD maka diperolah hasil akurasi, presisi, recall, f1-score pada model yang masing-masing 82.6% untuk akurasi, 81.7% untuk presisi, 84.1% untuk recall, 83.5% untuk f1-score dan dilakukan pengujian menggunakan 10 data yang tidak termasuk dalam data latih memperoleh performansi rata-rata akurasi sebesar 70%, presisi 71%, recall 70% dan f1-score 70%.",
        "link": "http://dx.doi.org/10.56705/ijodas.v3i1.42"
    },
    {
        "id": 4605,
        "title": "An Improvised Sentiment Analysis Model on Twitter Data Using Stochastic Gradient Descent (SGD) Optimization Algorithm in Stochastic Gate Neural Network (SGNN)",
        "authors": "K. P. Vidyashree, A. B. Rajendra",
        "published": "2023-2-2",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42979-022-01607-x"
    },
    {
        "id": 4606,
        "title": "CD-SGD: Distributed Stochastic Gradient Descent with Compression and Delay Compensation",
        "authors": "Enda Yu, Dezun Dong, Yemao Xu, Shuo Ouyang, Xiangke Liao",
        "published": "2021-8-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3472456.3472508"
    },
    {
        "id": 4607,
        "title": "Using Adaboost and Stochastic gradient descent (SGD) Algorithms with R and Orange Software for Filtering E-mail Spam",
        "authors": "Huwaida T. Elshoush, Esraa A. Dinar",
        "published": "2019-9",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ceec47804.2019.8974319"
    },
    {
        "id": 4608,
        "title": "SVM-SMO-SGD: A hybrid-parallel support vector machine algorithm using sequential minimal optimization with stochastic gradient descent",
        "authors": "Gizen Mutlu, Çiğdem İnan Acı",
        "published": "2022-10",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.parco.2022.102955"
    },
    {
        "id": 4609,
        "title": "MSDF-SGD: Most-Significant Digit-First Stochastic Gradient Descent for Arbitrary-Precision Training",
        "authors": "Changjun Song, Yongming Tang, Jiyuan Liu, Sige Bian, Danni Deng, He Li",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fpl60245.2023.00030"
    },
    {
        "id": 4610,
        "title": "Linear Support Vector Machine (SVM) with Stochastic Gradient Descent (SGD) training and multinomial Nave Bayes (NB) in News Classification",
        "authors": "Feroz Ahmed, Shabina Ghafir",
        "published": "2019-4-30",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26438/ijcse/v7i4.360363"
    },
    {
        "id": 4611,
        "title": "Deep Convolutional Neural Network with a Stochastic Gradient Descent Optimizer (PDCNN-SGD) model for telugu character recognition",
        "authors": "Siva Phaniram Josyula, Reddy M. Babu",
        "published": "2023",
        "citations": 0,
        "abstract": "Telugu Character Recognition (TCR) has received significant attention because of the drastic increase in technological advancements such as multimedia, smartphones and iPods, and paper documents. Offline character recognition is the process of identifying Telugu characters from the scanned image or document whereas online character recognition enables to recognition of characters by the machine while the user writes. Several researchers have attempted to design online TCR models by the use of distinct classification models and feature extraction approaches. It is still necessary to construct automated and intelligent online TCR models, even if many studies have focused on offline TCR models. The Telugu character dataset construction and validation using an Inception and ResNet-based model are presented. The collection of 645 letters in the dataset includes 18 Achus, 38 Hallus, 35 Othulu, 34*16 Guninthamulu and 10 Ankelu. The proposed technique aims to efficiently recognize and identify distinctive Telugu characters online. This model's main preprocessing steps to achieve its goals include normalization, smoothing, and interpolation. Improved recognition performance can be attained by using Stochastic Gradient Descent (SGD) to optimize the model's hyperparameters.",
        "link": "http://dx.doi.org/10.26634/jip.10.1.19250"
    },
    {
        "id": 4612,
        "title": "CP-SGD: Distributed stochastic gradient descent with compression and periodic compensation",
        "authors": "Enda Yu, Dezun Dong, Yemao Xu, Shuo Ouyang, Xiangke Liao",
        "published": "2022-11",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jpdc.2022.05.014"
    },
    {
        "id": 4613,
        "title": "DAC-SGD",
        "authors": "Aijia He, Zehong Chen, Weichen Li, Xingying Li, Hongjun Li, Xin Zhao",
        "published": "2017-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3144789.3144815"
    },
    {
        "id": 4614,
        "title": "Stochastic gradient descent",
        "authors": "Andrew Murphy, David Wang",
        "published": "2018-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.53347/rid-61715"
    },
    {
        "id": 4615,
        "title": "Stochastic Gradient Descent with Gradient Estimator for Categorical Features",
        "authors": "Paul Peseux, Thierry Paquet, Maxime Berar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4439301"
    },
    {
        "id": 4616,
        "title": "Stochastic Gradient Descent",
        "authors": "Guanghui Lan",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-54621-2_777-1"
    },
    {
        "id": 4617,
        "title": "Stochastic Gradient Descent",
        "authors": "Nikhil Ketkar",
        "published": "2017",
        "citations": 126,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4842-2766-4_8"
    },
    {
        "id": 4618,
        "title": "Resampling Stochastic Gradient Descent Cheaply",
        "authors": "Henry Lam, Zitong Wang",
        "published": "2023-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc60868.2023.10408023"
    },
    {
        "id": 4619,
        "title": "OD-SGD",
        "authors": "Yemao Xu, Dezun Dong, Yawei Zhao, Weixia Xu, Xiangke Liao",
        "published": "2020-12-31",
        "citations": 10,
        "abstract": "The training of modern deep learning neural network calls for large amounts of computation, which is often provided by GPUs or other specific accelerators. To scale out to achieve faster training speed, two update algorithms are mainly applied in the distributed training process, i.e., the Synchronous SGD algorithm (SSGD) and Asynchronous SGD algorithm (ASGD). SSGD obtains good convergence point while the training speed is slowed down by the synchronous barrier. ASGD has faster training speed but the convergence point is lower when compared to SSGD. To sufficiently utilize the advantages of SSGD and ASGD, we propose a novel technology named One-step Delay SGD (OD-SGD) to combine their strengths in the training process. Therefore, we can achieve similar convergence point and training speed as SSGD and ASGD separately.\nTo the best of our knowledge, we make the first attempt to combine the features of SSGD and ASGD to improve distributed training performance. Each iteration of OD-SGD contains a global update in the parameter server node and local updates in the worker nodes, the local update is introduced to update and compensate the delayed local weights. We evaluate our proposed algorithm on MNIST, CIFAR-10, and ImageNet datasets. Experimental results show that OD-SGD can obtain similar or even slightly better accuracy than SSGD, while its training speed is much faster, which even exceeds the training speed of ASGD.",
        "link": "http://dx.doi.org/10.1145/3417607"
    },
    {
        "id": 4620,
        "title": "Nyström-SGD: Fast Learning of Kernel-Classifiers with Conditioned Stochastic Gradient Descent",
        "authors": "Lukas Pfahler, Katharina Morik",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-10928-8_13"
    },
    {
        "id": 4621,
        "title": "Multicriteria Scalable Graph Drawing via Stochastic Gradient Descent, (SGD)^2",
        "authors": "Reyan Ahmed, Felice De Luca, Sabin Devkota, Stephen G Kobourov, Mingwei Li",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tvcg.2022.3155564"
    },
    {
        "id": 4622,
        "title": "Adaptive stochastic gradient descent for large-scale learning problems",
        "authors": "Zhuang Yang, Li Ma",
        "published": "No Date",
        "citations": 2,
        "abstract": "Abstract\nAs an effective strategy to enhance stochastic optimization, determining an appropriate step size sequence when performing these algorithms has been warmly encouraged in solving large-scale learning problems. This paper equips stochastic optimization algorithms with an adaptive step size strategy by using the diagonal Barzilai-Borwein (DBB) step size. Specifically, this work proposes a novel stochastic variance reduced algorithm by incorporating DBB into the variance-reduced algorithm, termed mS2GD-DBB. mS2GD-DBB uses a diagonal matrix to approximate the curvature information and accesses to a better curvature information than the conventional BarzilaiBorwein (BB) technique, where the latter works with the scalar approximation of the curvature information. We prove that the proposed algorithm attains a linear convergence rate for strongly convex cases. Moreover, we show that the complexity of the proposed algorithm matches the best known complexity of stochastic optimization algorithms. Numerical experiments suggest that the proposed algorithm shows much promise.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1066512/v1"
    },
    {
        "id": 4623,
        "title": "Scaling Stratified Stochastic Gradient Descent for Distributed Matrix Completion",
        "authors": "Nabil Abubaker, M. Ozan Karsavuran, Cevdet Aykanat",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Stratified SGD (SSGD) is the primary approach for achieving serializable parallel SGD for matrix completion. State-of-the-art parallelizations of SSGD fail to scale due to huge communication overhead. During an SGD epoch, these methods send data proportional to one of the dimensions of the rating matrix. We propose a framework for scalable SSGD through significantly reducing the communication overhead via exchanging point-to-point messages utilizing the sparsity of the input matrix. We provide formulas to represent the essential communication for correctly performing parallel SSGD and we propose a dynamic programming algorithm for efficiently computing them to establish the point-to-point message schedules. This scheme, however, significantly increases the number of messages sent by a processor per epoch from O(K) to O(K^2) for a K-processor system which might limit the scalability. To remedy this, we propose a Hold-and-Combine strategy to limit the upper-bound on the number of messages sent per processor to O(KlgK). We also propose a hypergraph partitioning model that correctly encapsulates reducing the communication volume. Experimental results show that the framework successfully achieves a scalable distributed SSGD through significantly reducing the communication overhead. Our code is publicly available at: github.com/nfabubaker/CESSGD</p><div><br></div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19350536.v1"
    },
    {
        "id": 4624,
        "title": "Stochastic Gradient Descent",
        "authors": "",
        "published": "2021-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781108860604.006"
    },
    {
        "id": 4625,
        "title": "On the Convergence of Stochastic Gradient Descent in Low-Precision Number Formats",
        "authors": "Matteo Cacciola, Antonio Frangioni, Masoud Asgharian, Alireza Ghaffari, Vahid Nia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011795500003411"
    },
    {
        "id": 4626,
        "title": "Scaling Stratified Stochastic Gradient Descent for Distributed Matrix Completion",
        "authors": "Nabil Abubaker, M. Ozan Karsavuran, Cevdet Aykanat",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>Stratified SGD (SSGD) is the primary approach for achieving serializable parallel SGD for matrix completion. State-of-the-art parallelizations of SSGD fail to scale due to huge communication overhead. During an SGD epoch, these methods send data proportional to one of the dimensions of the rating matrix. We propose a framework for scalable SSGD through significantly reducing the communication overhead via exchanging point-to-point messages utilizing the sparsity of the input matrix. We provide formulas to represent the essential communication for correctly performing parallel SSGD and we propose a dynamic programming algorithm for efficiently computing them to establish the point-to-point message schedules. This scheme, however, significantly increases the number of messages sent by a processor per epoch from O(K) to O(K^2) for a K-processor system which might limit the scalability. To remedy this, we propose a Hold-and-Combine strategy to limit the upper-bound on the number of messages sent per processor to O(KlgK). We also propose a hypergraph partitioning model that correctly encapsulates reducing the communication volume. Experimental results show that the framework successfully achieves a scalable distributed SSGD through significantly reducing the communication overhead. Our code is publicly available at: github.com/nfabubaker/CESSGD</p><div><br></div>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19350536"
    },
    {
        "id": 4627,
        "title": "Noisy Stochastic Gradient Descent Algorithm with Stochastic Event-Triggered Mechanism for Communication-Efficient Distributed Learning",
        "authors": "Hezhe Sun, Nachuan Yang, Yuzhe Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4588764"
    },
    {
        "id": 4628,
        "title": "Online Learning: the Stochastic Gradient Descent Family of Algorithms",
        "authors": "Sergios Theodoridis",
        "published": "2020",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-818803-3.00014-3"
    },
    {
        "id": 4629,
        "title": "Comparing the Effectiveness of Support Vector Classifier and Stochastic Gradient Descent in  Hate-Speech Detection",
        "authors": "Dania Ali",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.47611/harp.315"
    },
    {
        "id": 4630,
        "title": "A comparative study of gradient descent and stochastic gradient descent method for optimization",
        "authors": "Sapna Shrimali, Govind S. Sharma, Sunil K. Srivastava",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1063/5.0148736"
    },
    {
        "id": 4631,
        "title": "Variable selection of regularized stochastic gradient descent in logistic regression",
        "authors": "Ping Guo",
        "published": "2022-5-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.54647/mathematics11319"
    },
    {
        "id": 4632,
        "title": "Stochastic Gradient Descent in Continuous Time",
        "authors": "Justin Sirignano, Konstantinos Spiliopoulos",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.2954149"
    },
    {
        "id": 4633,
        "title": "Comparison of Support Vector Machine (SVM), K-Nearest Neighbor (K-NN), and Stochastic Gradient Descent (SGD) for Classifying Corn Leaf Disease based on Histogram of Oriented Gradients (HOG) Feature Extraction",
        "authors": "Firdaus Solihin, Muhammad Syarief, Eka Mala Sari Rochman, Aeri Rachmad",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "Image classification involves categorizing an image's pixels into specific classes based on their unique characteristics. It has diverse applications in everyday life. One such application is the classification of diseases on corn leaves. Corn is a widely consumed staple food in Indonesia, and healthy corn plants are crucial for meeting market demands. Currently, disease identification in corn plants relies on manual checks, which are time-consuming and less effective. This research aims to automate disease identification on corn leaves using the Support Vector Machine (SVM), K-Nearest Neighbor (K-NN) with K=2, and Stochastic Gradient Descent (SGD) algorithms. The classification process utilizes the Histogram of Oriented Gradients (HOG) feature extraction method with a dataset of corn leaf images. The classification results achieved an accuracy of 71.44%, AUC of 79.16%, precision of 70.08%, recall of 71.44%, and f1 score of 67.11%. The highest accuracy was obtained by combining HOG feature extraction with the SGD algorithm.",
        "link": "http://dx.doi.org/10.21831/elinvo.v8i1.55759"
    },
    {
        "id": 4634,
        "title": "P-SGD: A Stochastic Gradient Descent Solution for Privacy-Preserving During Protection Transitions",
        "authors": "Karam Bou-Chaaya, Richard Chbeir, Mahmoud Barhamgi, Philippe Arnould, Djamal Benslimane",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-79382-1_3"
    },
    {
        "id": 4635,
        "title": "Communication-Efficient Distributed Stochastic Gradient Descent with Pooling Operator",
        "authors": "Zhengao Cai, Aiguo Chen, Yi Luo, Jiahao Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4327869"
    },
    {
        "id": 4636,
        "title": "Hyperparameter-free optimizer of stochastic gradient descent that incorporates unit correction and moment estimation",
        "authors": "Kazunori D Yamada",
        "published": "No Date",
        "citations": 2,
        "abstract": "ABSTRACTIn the deep learning era, stochastic gradient descent is the most common method used for optimizing neural network parameters. Among the various mathematical optimization methods, the gradient descent method is the most naive. Adjustment of learning rate is necessary for quick convergence, which is normally done manually with gradient descent. Many optimizers have been developed to control the learning rate and increase convergence speed. Generally, these optimizers adjust the learning rate automatically in response to learning status. These optimizers were gradually improved by incorporating the effective aspects of earlier methods. In this study, we developed a new optimizer: YamAdam. Our optimizer is based on Adam, which utilizes the first and second moments of previous gradients. In addition to the moment estimation system, we incorporated an advantageous part of AdaDelta, namely a unit correction system, into YamAdam. According to benchmark tests on some common datasets, our optimizer showed similar or faster convergent performance compared to the existing methods. YamAdam is an option as an alternative optimizer for deep learning.",
        "link": "http://dx.doi.org/10.1101/348557"
    },
    {
        "id": 4637,
        "title": "Differentially Private Variance Reduced Stochastic Gradient Descent",
        "authors": "Jaewoo Lee",
        "published": "2017-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictcs.2017.60"
    },
    {
        "id": 4638,
        "title": "Stochastic Gradient Descent in Continuous Time: A Central Limit Theorem",
        "authors": "Justin Sirignano, Konstantinos Spiliopoulos",
        "published": "2020-6",
        "citations": 12,
        "abstract": " Stochastic gradient descent in continuous time (SGDCT) provides a computationally efficient method for the statistical learning of continuous-time models, which are widely used in science, engineering, and finance. The SGDCT algorithm follows a (noisy) descent direction along a continuous stream of data. The parameter updates occur in continuous time and satisfy a stochastic differential equation. This paper analyzes the asymptotic convergence rate of the SGDCT algorithm by proving a central limit theorem for strongly convex objective functions and, under slightly stronger conditions, for nonconvex objective functions as well. An [Formula: see text] convergence rate is also proven for the algorithm in the strongly convex case. The mathematical analysis lies at the intersection of stochastic analysis and statistical learning. ",
        "link": "http://dx.doi.org/10.1287/stsy.2019.0050"
    },
    {
        "id": 4639,
        "title": "Fast Armijo line search for stochastic gradient descent",
        "authors": "Sajad Fathi Hafshejani, Daya Gaur, Shahadat Hossain, Robert Benkoczi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nWe give an improved non-monotone line search algorithm for stochastic gradient descent (SGD) for functions that satisfy interpolation conditions. We establish theoretical convergence guarantees for the algorithm for strongly convex, convex and non-convex functions. We conduct a detailed empirical evaluation to validate the theoretical results.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2285238/v1"
    },
    {
        "id": 4640,
        "title": "On Byzantine-Resilient High-Dimensional Stochastic Gradient Descent",
        "authors": "Deepesh Data, Suhas Diggavi",
        "published": "2020-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isit44484.2020.9174363"
    },
    {
        "id": 4641,
        "title": "Static &amp; Dynamic Appointment Scheduling with Stochastic Gradient Descent",
        "authors": "Gary Cheng, Kabir Chandrasekher, Jean Walrand",
        "published": "2019-7",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2019.8814666"
    },
    {
        "id": 4642,
        "title": "Multi-Dimensional Deconvolution with Stochastic Gradient Descent",
        "authors": "M. Ravasi, T. Selvan Pandurangan, N. Luiken",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202210234"
    },
    {
        "id": 4643,
        "title": "A Stochastic Gradient Descent Approach for Stochastic Optimal Control",
        "authors": "Richard Archibald",
        "published": "2020-6",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4208/eajam.190420.200420"
    },
    {
        "id": 4644,
        "title": "Hybrid approximate gradient and stochastic descent for falsification of nonlinear systems",
        "authors": "Shakiba Yaghoubi, Georgios Fainekos",
        "published": "2017-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/acc.2017.7963007"
    },
    {
        "id": 4645,
        "title": "Efficient phase-locking of 60 fiber lasers by stochastic parallel gradient descent algorithm",
        "authors": "Hongxiang Chang, Jiachao Xi, Rongtao Su, Pengfei Ma, Yanxing Ma, Pu Zhou",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3788/col202018.101403"
    },
    {
        "id": 4646,
        "title": "Synthesis of Optimal Feedback Controllers from Data via Stochastic Gradient Descent",
        "authors": "Laura Ferrarotti, Alberto Bemporad",
        "published": "2019-6",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc.2019.8796130"
    },
    {
        "id": 4647,
        "title": "Stochastic gradient descent possibilistic clustering",
        "authors": "Aggeliki Koutsibella, Konstantinos D. Koutroumbas",
        "published": "2020-9-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3411408.3411436"
    },
    {
        "id": 4648,
        "title": "Acoustic Model Optimization Based on Evolutionary Stochastic Gradient Descent with Anchors for Automatic Speech Recognition",
        "authors": "Xiaodong Cui, Michael Picheny",
        "published": "2019-9-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-2620"
    },
    {
        "id": 4649,
        "title": "Fast Distributed Stochastic Nesterov Gradient Descent Algorithm for Image Classification",
        "authors": "Dequan Li, Yuheng Zhang, Yuejin Zhou",
        "published": "2021-10-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac53003.2021.9727635"
    },
    {
        "id": 4650,
        "title": "Fast Convergence for Stochastic and Distributed Gradient Descent in the Interpolation Limit",
        "authors": "Partha P Mitra",
        "published": "2018-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco.2018.8553369"
    },
    {
        "id": 4651,
        "title": "Static Parameter Estimation on SO(3) using Stochastic Gradient Descent for Visual Tracking",
        "authors": "Torstein A. Myhre",
        "published": "2019-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc.2019.8795738"
    },
    {
        "id": 4652,
        "title": "Semantics-Preserving Parallelization of Stochastic Gradient Descent",
        "authors": "Saeed Maleki, Madanlal Musuvathi, Todd Mytkowicz",
        "published": "2018-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipdps.2018.00032"
    },
    {
        "id": 4653,
        "title": "A Stochastic Parallel Gradient Descent Algorithm for Person Re-identification",
        "authors": "Keyang Cheng, Fei Tao",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vcip.2018.8698647"
    },
    {
        "id": 4654,
        "title": "Stochastic Gradient Descent for Optimization of Nuclear Systems",
        "authors": "Austin Williams, Noah Walton, Austin Maryanski, Sandra Bogetic, Wes Hines, Vladimir Sobes",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe use of gradient descent methods for optimizing k-eigenvalue nuclear system has been shown to be useful in the past, but the k-eigenvalue gradients have proved challenging due to their stochastic nature and uncertainty. ADAM is a gradient descent method that accounts for gradients with a stochastic nature. This analysis uses challenge problems constructed to verify if ADAM is a suitable tool to optimize k-eigenvalue systems. ADAM is able to successfully optimize nuclear systems using the gradients of k-eigenvalue problems despite their stochastic nature and uncertainty.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2073277/v1"
    },
    {
        "id": 4655,
        "title": "Guided parallelized stochastic gradient descent for delay compensation",
        "authors": "Anuraganand Sharma",
        "published": "2021-4",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.asoc.2021.107084"
    },
    {
        "id": 4656,
        "title": "EFFORT ALLOCATION AND STATISTICAL INFERENCE FOR 1-DIMENSIONAL MULTISTART STOCHASTIC GRADIENT DESCENT",
        "authors": "Saul Toscano-Palmerin, Peter Frazier",
        "published": "2018-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc.2018.8632402"
    },
    {
        "id": 4657,
        "title": "Stochastic Gradient Descent Classifier-Based Lightweight Intrusion Detection Systems Using the Most Efficient Feature Subsets of Datasets",
        "authors": "Jahongir AZIMJONOV, Taehong Kim",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4378339"
    },
    {
        "id": 4658,
        "title": "Entropy-SGD: biasing gradient descent into wide valleys",
        "authors": "Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, Riccardo Zecchina",
        "published": "2019-12-20",
        "citations": 97,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1742-5468/ab39d9"
    },
    {
        "id": 4659,
        "title": "A Large-Scale Stochastic Gradient Descent Algorithm Over a Graphon",
        "authors": "Yan Chen, Tao Li",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383833"
    },
    {
        "id": 4660,
        "title": "Testing of the stochastic parallel gradient descent algorithm to the alignment of a two-mirror telescope",
        "authors": "",
        "published": "2020",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17586/1023-5086-2020-87-05-31-41"
    },
    {
        "id": 4661,
        "title": "Comparison of the stochastic gradient descent based optimization techniques",
        "authors": "Ersan YAZAN, M. Fatih Talu",
        "published": "2017-9",
        "citations": 49,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/idap.2017.8090299"
    },
    {
        "id": 4662,
        "title": "MindTheStep-AsyncPSGD: Adaptive Asynchronous Parallel Stochastic Gradient Descent",
        "authors": "Karl Backstrom, Marina Papatriantafilou, Philippas Tsigas",
        "published": "2019-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata47090.2019.9006054"
    },
    {
        "id": 4663,
        "title": "Wireless network optimization via stochastic sub-gradient descent: Rate analysis",
        "authors": "Amrit Singh Bedi, Ketan Rajawat",
        "published": "2018-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wcnc.2018.8377002"
    },
    {
        "id": 4664,
        "title": "Machine-Learning Topology Optimization with Stochastic Gradient Descent Optimizer for Heat Conduction Problems",
        "authors": "Yuchao Hua, Lingai Luo, Steven Le Corre, Yilin Fan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4594476"
    },
    {
        "id": 4665,
        "title": "Understanding and Detecting Convergence for Stochastic Gradient Descent with Momentum",
        "authors": "Jerry Chee, Ping Li",
        "published": "2020-12-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata50022.2020.9378129"
    },
    {
        "id": 4666,
        "title": "Asynchronous Stochastic Gradient Descent over Decentralized Datasets",
        "authors": "Yubo Du, Keyou You, Yilin Mo",
        "published": "2020-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icca51439.2020.9264316"
    },
    {
        "id": 4667,
        "title": "Byzantine-Robust Stochastic Gradient Descent for Distributed Low-Rank Matrix Completion",
        "authors": "Xuechao He, Qing Ling, Tianyi Chen",
        "published": "2019-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dsw.2019.8755575"
    },
    {
        "id": 4668,
        "title": "RECENT TRENDS IN STOCHASTIC GRADIENT DESCENT FOR MACHINE LEARNING AND BIG DATA",
        "authors": "David Newton, Raghu Pasupathy, Farzad Yousefian",
        "published": "2018-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc.2018.8632351"
    },
    {
        "id": 4669,
        "title": "Making Asynchronous Stochastic Gradient Descent Work for Transformers",
        "authors": "Alham Fikri Aji, Kenneth Heafield",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d19-5608"
    },
    {
        "id": 4670,
        "title": "Distributed Stochastic Gradient Descent Using LDGM Codes",
        "authors": "Shunsuke Horii, Takahiro Yoshida, Manabu Kobayashi, Toshiyasu Matsushima",
        "published": "2019-7",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isit.2019.8849580"
    },
    {
        "id": 4671,
        "title": "Stochastic gradient descent for linear systems with sequential matrix entry accumulation",
        "authors": "Samrat Mukhopadhyay",
        "published": "2020-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.sigpro.2020.107494"
    },
    {
        "id": 4672,
        "title": "On Constructing Confidence Region for Model Parameters in Stochastic Gradient Descent Via Batch Means",
        "authors": "Yi Zhu, Jing Dong",
        "published": "2021-12-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wsc52266.2021.9715437"
    },
    {
        "id": 4673,
        "title": "Fast max-affine regression via stochastic gradient descent",
        "authors": "Seonho Kim, Kiryung Lee",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/allerton58177.2023.10313409"
    },
    {
        "id": 4674,
        "title": "Constrained Stochastic Gradient Descent: The Good Practice",
        "authors": "Soumava Kumar Roy, Mehrtash Harandi",
        "published": "2017-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dicta.2017.8227420"
    },
    {
        "id": 4675,
        "title": "Guided Stochastic Gradient Descent Algorithm for inconsistent datasets",
        "authors": "Anuraganand Sharma",
        "published": "2018-12",
        "citations": 61,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.asoc.2018.09.038"
    },
    {
        "id": 4676,
        "title": "A Diffusion Approximation Theory of Momentum Stochastic Gradient Descent in Nonconvex Optimization",
        "authors": "Tianyi Liu, Zhehui Chen, Enlu Zhou, Tuo Zhao",
        "published": "2021-12",
        "citations": 1,
        "abstract": " Momentum stochastic gradient descent (MSGD) algorithm has been widely applied to many nonconvex optimization problems in machine learning (e.g., training deep neural networks, variational Bayesian inference, etc.). Despite its empirical success, there is still a lack of theoretical understanding of convergence properties of MSGD. To fill this gap, we propose to analyze the algorithmic behavior of MSGD by diffusion approximations for nonconvex optimization problems with strict saddle points and isolated local optima. Our study shows that the momentum helps escape from saddle points but hurts the convergence within the neighborhood of optima (if without the step size annealing or momentum annealing). Our theoretical discovery partially corroborates the empirical success of MSGD in training deep neural networks. ",
        "link": "http://dx.doi.org/10.1287/stsy.2021.0083"
    },
    {
        "id": 4677,
        "title": "Stochastic Gradient Descent with Logistic Regression Technique for Medical Data Classification",
        "authors": "Raja M, Mohamed Parvees M. Y",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3769853"
    },
    {
        "id": 4678,
        "title": "Communication-efficient Variance-reduced Stochastic Gradient Descent",
        "authors": "Hossein S. Ghadikolaei, Sindri Magnússon",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2020.12.2520"
    },
    {
        "id": 4679,
        "title": "Optimizing Stochastic Gradient Descent Using the Angle Between Gradients",
        "authors": "Chongya Song, Alexander Pons, Kang Yen",
        "published": "2020-12-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/bigdata50022.2020.9378007"
    },
    {
        "id": 4680,
        "title": "Noise Covariance Estimation in Adaptive Kalman Filtering via sequential Mini-batch Stochastic Gradient Descent Algorithms",
        "authors": "Hee-Seung Kim, Lingyi Zhang, Adam Bienkowski, Krishna Pattipati",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>Estimation of unknown noise covariances in a Kalman filter is a\nproblem of significant practical interest in a wide array of applications.\nAlthough this problem has a long history, reliable algorithms for their\nestimation were scant, and necessary and sufficient conditions for\nidentifiability of the covariances were in dispute until recently. Necessary and sufficient conditions for covariance estimation and a batch estimation\nalgorithm. This paper presents stochastic gradient descent (SGD) algorithms for\nnoise covariance estimation in adaptive Kalman filters that are an order of\nmagnitude faster than the batch method for similar or better root mean square\nerror (RMSE) and are applicable to non-stationary systems where the noise covariances\ncan occasionally jump up or down by an unknown magnitude. The computational\nefficiency of the new algorithm stems from adaptive thresholds for convergence,\nrecursive fading memory estimation of the sample cross-correlations of the\ninnovations, and accelerated SGD algorithms. The comparative evaluation of the\nproposed method on a number of test cases demonstrates its computational\nefficiency and accuracy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14671887.v1"
    },
    {
        "id": 4681,
        "title": "Decentralized Differentially Private Without-Replacement Stochastic Gradient Descent",
        "authors": "Richeng Jin, Xiaofan He, Huaiyu Dai",
        "published": "2023-3-22",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089635"
    },
    {
        "id": 4682,
        "title": "On the regularizing property of stochastic gradient descent",
        "authors": "Bangti Jin, Xiliang Lu",
        "published": "2019-1-1",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1088/1361-6420/aaea2a"
    },
    {
        "id": 4683,
        "title": "Noise Covariance Estimation in Adaptive Kalman Filtering via sequential Mini-batch Stochastic Gradient Descent Algorithms",
        "authors": "Hee-Seung Kim, Lingyi Zhang, Adam Bienkowski, Krishna Pattipati",
        "published": "No Date",
        "citations": 1,
        "abstract": "<p>Estimation of unknown noise covariances in a Kalman filter is a\nproblem of significant practical interest in a wide array of applications.\nAlthough this problem has a long history, reliable algorithms for their\nestimation were scant, and necessary and sufficient conditions for\nidentifiability of the covariances were in dispute until recently. Necessary and sufficient conditions for covariance estimation and a batch estimation\nalgorithm. This paper presents stochastic gradient descent (SGD) algorithms for\nnoise covariance estimation in adaptive Kalman filters that are an order of\nmagnitude faster than the batch method for similar or better root mean square\nerror (RMSE) and are applicable to non-stationary systems where the noise covariances\ncan occasionally jump up or down by an unknown magnitude. The computational\nefficiency of the new algorithm stems from adaptive thresholds for convergence,\nrecursive fading memory estimation of the sample cross-correlations of the\ninnovations, and accelerated SGD algorithms. The comparative evaluation of the\nproposed method on a number of test cases demonstrates its computational\nefficiency and accuracy.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.14671887"
    },
    {
        "id": 4684,
        "title": "Decentralized Asynchronous Stochastic Gradient Descent: Convergence Rate Analysis",
        "authors": "Amrit Singh Bedi, Hrusikesha Pradhan, Ketan Rajawat",
        "published": "2018-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spcom.2018.8724408"
    },
    {
        "id": 4685,
        "title": "Stochastic Gradient Descent on Modern Hardware for Business Environment",
        "authors": "Saravanan V, P Ranjana",
        "published": "2023-5-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iciccs56967.2023.10142907"
    },
    {
        "id": 4686,
        "title": "Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks",
        "authors": "Pratik Chaudhari, Stefano Soatto",
        "published": "2018-2",
        "citations": 59,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ita.2018.8503224"
    },
    {
        "id": 4687,
        "title": "Distributed Stochastic Gradient Descent with Cost-Sensitive and Strategic Agents",
        "authors": "Abdullah Basar Akbay, Cihan Tepedelenlioglu",
        "published": "2022-10-31",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ieeeconf56349.2022.10051928"
    },
    {
        "id": 4688,
        "title": "Speeding Up Iterative Closest Point Using Stochastic Gradient Descent",
        "authors": "Fahira Afzal Maken, Fabio Ramos, Lionel Ott",
        "published": "2019-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra.2019.8794011"
    },
    {
        "id": 4689,
        "title": "A Geometric Interpretation of Stochastic Gradient Descent Using Diffusion Metrics",
        "authors": "Rita Fioresi, Pratik Chaudhari, Stefano Soatto",
        "published": "2020-1-15",
        "citations": 3,
        "abstract": "This paper is a step towards developing a geometric understanding of a popular algorithm for training deep neural networks named stochastic gradient descent (SGD). We built upon a recent result which observed that the noise in SGD while training typical networks is highly non-isotropic. That motivated a deterministic model in which the trajectories of our dynamical systems are described via geodesics of a family of metrics arising from a certain diffusion matrix; namely, the covariance of the stochastic gradients in SGD. Our model is analogous to models in general relativity: the role of the electromagnetic field in the latter is played by the gradient of the loss function of a deep network in the former.",
        "link": "http://dx.doi.org/10.3390/e22010101"
    },
    {
        "id": 4690,
        "title": "A new stochastic gradient descent possibilistic clustering algorithm",
        "authors": "Angeliki Koutsimpela, Konstantinos D. Koutroumbas",
        "published": "2022-7-18",
        "citations": 4,
        "abstract": "Several well known clustering algorithms have their own online counterparts, in order to deal effectively with the big data issue, as well as with the case where the data become available in a streaming fashion. However, very few of them follow the stochastic gradient descent philosophy, despite the fact that the latter enjoys certain practical advantages (such as the possibility of (a) running faster than their batch processing counterparts and (b) escaping from local minima of the associated cost function), while, in addition, strong theoretical convergence results have been established for it. In this paper a novel stochastic gradient descent possibilistic clustering algorithm, called O- PCM 2 is introduced. The algorithm is presented in detail and it is rigorously proved that the gradient of the associated cost function tends to zero in the L 2 sense, based on general convergence results established for the family of the stochastic gradient descent algorithms. Furthermore, an additional discussion is provided on the nature of the points where the algorithm may converge. Finally, the performance of the proposed algorithm is tested against other related algorithms, on the basis of both synthetic and real data sets.",
        "link": "http://dx.doi.org/10.3233/aic-210125"
    },
    {
        "id": 4691,
        "title": "Dendrite morphological neurons trained by stochastic gradient descent",
        "authors": "Erik Zamora, Humberto Sossa",
        "published": "2017-10",
        "citations": 34,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2017.04.044"
    },
    {
        "id": 4692,
        "title": "Stochastic Gradient Descent on a Tree: an Adaptive and Robust Approach to Stochastic Convex Optimization",
        "authors": "Sattar Vakili, Sudeep Salgia, Qing Zhao",
        "published": "2019-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/allerton.2019.8919740"
    },
    {
        "id": 4693,
        "title": "Localization Using Stochastic Gradient Descent Method in a 5G Network",
        "authors": "Ankur Pandey, Pinky Pinky, Sudhir Kumar",
        "published": "2018-12",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/indicon45594.2018.8987064"
    },
    {
        "id": 4694,
        "title": "An Efficient, Distributed Stochastic Gradient Descent Algorithm for Deep-Learning Applications",
        "authors": "Guojing Cong, Onkar Bhardwaj, Minwei Feng",
        "published": "2017-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpp.2017.10"
    },
    {
        "id": 4695,
        "title": "Hyper-parameter optimization for support vector machines using stochastic gradient descent and dual coordinate descent",
        "authors": "W.e.i. Jiang, Sauleh Siddiqui",
        "published": "2020-3",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s13675-019-00115-7"
    },
    {
        "id": 4696,
        "title": "A novel approach of designing private federated learning model using DPSGD-Differentially Private Stochastic Gradient Descent optimizer",
        "authors": "SHRUTI YAGNIK, ESAN PANCHAL, PRAMOD TRIPATHI",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe authors have requested that this preprint be removed from Research Square.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2108056/v1"
    },
    {
        "id": 4697,
        "title": "Phase Correction for a Turbulence-Distorted Coherent Combined Vortex Beam Using Stochastic-Parallel-Gradient-Descent Algorithm",
        "authors": "Wenke Xie, Guangwei Qin, Tao Yu, Qiao Xie",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4359087"
    },
    {
        "id": 4698,
        "title": "Mutual Information Based Learning Rate Decay for Stochastic Gradient Descent Training of Deep Neural Networks",
        "authors": "Shrihari Vasudevan",
        "published": "2020-5-17",
        "citations": 8,
        "abstract": "This paper demonstrates a novel approach to training deep neural networks using a Mutual Information (MI)-driven, decaying Learning Rate (LR), Stochastic Gradient Descent (SGD) algorithm. MI between the output of the neural network and true outcomes is used to adaptively set the LR for the network, in every epoch of the training cycle. This idea is extended to layer-wise setting of LR, as MI naturally provides a layer-wise performance metric. A LR range test determining the operating LR range is also proposed. Experiments compared this approach with popular alternatives such as gradient-based adaptive LR algorithms like Adam, RMSprop, and LARS. Competitive to better accuracy outcomes obtained in competitive to better time, demonstrate the feasibility of the metric and approach.",
        "link": "http://dx.doi.org/10.3390/e22050560"
    },
    {
        "id": 4699,
        "title": "Enhanced Stochastic Gradient Descent with Backward Queried Data for Online Learning",
        "authors": "Gio Huh",
        "published": "2020-12-20",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmlant50963.2020.9355978"
    },
    {
        "id": 4700,
        "title": "Stochastic Gradient Descent for Wind Farm Optimization",
        "authors": "Julian Quick, Pierre-Elouan Rethore, Mads Mølgaard Pedersen, Rafael Valotta Rodrigues",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract. It is important to optimize wind turbine positions to mitigate potential wake losses. To perform this optimization, atmospheric conditions, such as the inflow speed and direction, are assigned probability distributions according to measured data, and these conditions are propagated through engineering wake models to estimate the annual energy production (AEP). This study presents stochastic gradient descent (SGD) for wind farm optimization, which is an approach that estimates the gradient of the AEP using Monte Carlo simulation, allowing for the consideration of an arbitrarily large number of atmospheric conditions. This method does not require that the atmospheric conditions be discretized, in contrast to the typical rectangular quadrature approximation of AEP. SGD is demonstrated using wind farms with square boundaries, considering cases with 25, 64, and 100 turbines, and the results are compared to a deterministic optimization approach. It is shown that SGD finds a larger optimal AEP in substantially less time than the deterministic counterpart as the number of wind turbines is increased.\n                        ",
        "link": "http://dx.doi.org/10.5194/wes-2022-104"
    }
]