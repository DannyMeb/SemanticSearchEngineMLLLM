[
    {
        "id": 15105,
        "title": "The effect of amplitude modification in S-shaped activation functions on neural network regression",
        "authors": "Faizal Makhrus",
        "published": "2023",
        "citations": 0,
        "abstract": "Time series forecasting using multilayer feed-forward neural networks (FNN) is potential to give high accuracy. Several factors influence the accuracy. One of them is the choice of activation functions (AFs). There are several AFs commonly used in FNN with their specific characteristics, such as bounded type AFs. They include sigmoid, softsign, arctan, and tanh. This paper investigates the effect of the amplitude in the bounded AFs on the FNNs accuracy. The theoretical investigations use simplified FNN models: linear equation and linear combination. The results show that the higher amplitudes give higher accuracy than typical amplitudes in softsign, arctan, and tanh AFs. However, in sigmoid AF, the amplitude changes do not influence the accuracy. These theoretical results are supported by experiments using the FNN model for time series prediction of 10 foreign exchanges from different continents compared to the US dollar. Based on the experiments, the optimum amplitude of the AFs should be high, that is greater or equal to 100 times of the maximum input values to the FNN, and the accuracy gains up to 310 times.",
        "keywords": "",
        "link": "http://dx.doi.org/10.14311/nnw.2023.33.015"
    },
    {
        "id": 15106,
        "title": "Parabola-Based Artificial Neural Network Activation Functions",
        "authors": "Mikhail Khachumov, Yulia Emelyanova, Vyacheslav Khachumov",
        "published": "2023-9-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/rusautocon58002.2023.10272855"
    },
    {
        "id": 15107,
        "title": "Enhancing neural network classification using fractional-order activation functions",
        "authors": "Meshach Kumar, Utkal Mehta, Giansalvo Cirrincione",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.aiopen.2023.12.003"
    },
    {
        "id": 15108,
        "title": "Adaptive activation Functions with Deep Kronecker Neural Network optimized with Bear Smell Search Algorithm for preventing MANET Cyber security attacks",
        "authors": "E.V.R.M. Kalaimani Shanmugham, Saravanan Dhatchnamurthy, Prabbu Sankar Pakkiri, Neha Garg",
        "published": "2024-3-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/0954898x.2024.2321391"
    },
    {
        "id": 15109,
        "title": "Email Filtering based on Artificial Neural Network: Activation functions and Learning rate analysis",
        "authors": "Abdelmoujoud Assabir, Abdelkhalek Assabir",
        "published": "2023-12-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/commnet60167.2023.10365287"
    },
    {
        "id": 15110,
        "title": "Enhanced Performance of Dynamic Neural Network Model using Wavelet Activation Functions",
        "authors": "Syamsul Bahri, Lailia Awalushaumi, Nurul Fitriyani",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "Both static and dynamic adaptive neural networks have been broadly utilized in mathematical modeling and numerical analysis. This study aimed to enhance the accomplishment of Dynamic Neural Networks (DNN) models by applying wavelet functions as activation functions. Research that models and forecasts the intensity of solar radiation in Mataram City shows that combining B-Spline and Morlet wavelet activation functions can significantly increase the DNN model performance. Wavelet-DNN (W-DNN) was modeled with an identical architecture; the best showed the increase in the model achievement (0.7596 points for in-sample and 0.8502 points for out-sample data). Mainly for out-sample data, the model's performance using the W-DNN+ intervention model increased by 4.0492 points.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24843/lkjiti.2023.v14.i03.p03"
    },
    {
        "id": 15111,
        "title": "Image Segmentation on Convolutional Neural Network (CNN) using Some New Activation Functions",
        "authors": "Arvind Kumar, Sartaj Singh Sodhi",
        "published": "2023-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26713/cma.v14i2.2152"
    },
    {
        "id": 15112,
        "title": "Empirical Loss Landscape Analysis of Neural Network Activation Functions",
        "authors": "Anna Sergeevna Bosman, Andries Engelbrecht, Marde Helbig",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583133.3596321"
    },
    {
        "id": 15113,
        "title": "A comparative analysis of various activation functions and optimizers in a convolutional neural network for hyperspectral image classification",
        "authors": "Eren Can Seyrek, Murat Uysal",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-17546-5"
    },
    {
        "id": 15114,
        "title": "Extending Neural Network Verification to a Larger Family of Piece-wise Linear Activation Functions",
        "authors": "László Antal, Hana Masara, Erika Ábrahám",
        "published": "2023-11-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4204/eptcs.395.4"
    },
    {
        "id": 15115,
        "title": "A Comparative Analysis of the Most Commonly Used Activation Functions in Deep Neural Network",
        "authors": "R. Mahima, M. Maheswari, S. Roshana, E. Priyanka, Neha Mohanan, N. Nandhini",
        "published": "2023-7-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icesc57686.2023.10193390"
    },
    {
        "id": 15116,
        "title": "Multilayer Perceptron with Adaptive Learning Rate and Activation Functions: An Enhanced Convolutional Neural Network Approach for Zhuang Ethnic Cultural Image Classification",
        "authors": "Xie Ganlan, Zhou Wei",
        "published": "2024-1-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icpeca60615.2024.10471034"
    },
    {
        "id": 15117,
        "title": "On The Choice of Activation Functions in Physics-Informed Neural Network for Solving Incompressible Fluid Flows",
        "authors": "Duong V. Dung, Nguyen D. Song, Pramudita S. Palar, Lavi R. Zuhal",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-1803"
    },
    {
        "id": 15118,
        "title": "Energy-Efficient Recurrent Neural Network With MRAM-Based Probabilistic Activation Functions",
        "authors": "Shadi Sheikhfaal, Shaahin Angizi, Ronald F. DeMara",
        "published": "2023-4-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tetc.2022.3202112"
    },
    {
        "id": 15119,
        "title": "Complexity of Neural Network Training and ETR: Extensions with Effectively Continuous Functions",
        "authors": "Teemu Hankala, Miika Hannula, Juha Kontinen, Jonni Virtema",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "The training problem of neural networks (NNs) is known to be ER-complete with respect to ReLU and linear activation functions. We show that the training problem for NNs equipped with arbitrary activation functions is polynomial-time bireducible to the existential theory of the reals extended with the corresponding activation functions. For effectively continuous activation functions (e.g., the sigmoid function), we obtain an inclusion to low levels of the arithmetical hierarchy. Consequently, the sigmoid activation function leads to the existential theory of the reals with the exponential function, and hence the decidability of training NNs using the sigmoid activation function is equivalent to the decidability of the existential theory of the reals with the exponential function, a long-standing open problem. In contrast, we obtain that the training problem is undecidable if sinusoidal activation functions are considered.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i11.29118"
    },
    {
        "id": 15120,
        "title": "ENN: A Neural Network with DCT Adaptive Activation Functions",
        "authors": "Marc Martinez-Gost, Ana Pérez-Neira, Miguel Ángel Lagunas",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jstsp.2024.3361154"
    },
    {
        "id": 15121,
        "title": "Memristive Hopfield neural network dynamics with heterogeneous activation functions and its application",
        "authors": "Quanli Deng, Chunhua Wang, Hairong Lin",
        "published": "2024-1",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.chaos.2023.114387"
    },
    {
        "id": 15122,
        "title": "SecBERT: Privacy-preserving pre-training based neural network inference system",
        "authors": "Hai Huang, Yongjian Wang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106135"
    },
    {
        "id": 15123,
        "title": "Orders-of-coupling representation achieved with a single neural network with optimal neuron activation functions and without nonlinear parameter optimization",
        "authors": "Sergei Manzhos, Manabu Ihara",
        "published": "2023-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.aichem.2023.100013"
    },
    {
        "id": 15124,
        "title": "Mutual Information-Based Neural Network Distillation for Improving Photonic Neural Network Training",
        "authors": "Alexandros Chariton, Nikolaos Passalis, Nikos Pleros, Anastasios Tefas",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-023-11170-y"
    },
    {
        "id": 15125,
        "title": "Recurrent Neural Network Training With Convex Loss and Regularization Functions by Extended Kalman Filtering",
        "authors": "Alberto Bemporad",
        "published": "2023-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tac.2022.3222750"
    },
    {
        "id": 15126,
        "title": "All-Optical Reconfigurable Low-Threshold Nonlinear Activation Functions for High-Precision Neural Network",
        "authors": "Y. Peng, Y. Yuan, S. Cheung, T. Van Vaerenbergh, W. V. Sorin, G. Kurczveil, Aashu Jha, Z. Huang, M. Fiorentino, R. G. Beausoleil",
        "published": "2023",
        "citations": 1,
        "abstract": "All-optical reconfigurable and ultra-low-threshold nonlinear activation functions are experimentally demonstrated based on a high-Q microring resonator (MRR) assisted Mach-Zehnder interferometers (MZIs). We propose a heterogeneously integrated all-optical high-precision neural network for low power consumption.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1364/fio.2023.fw6e.3"
    },
    {
        "id": 15127,
        "title": "The GroupMax Neural Network Approximation of Convex Functions",
        "authors": "Xavier Warin",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3240183"
    },
    {
        "id": 15128,
        "title": "Machine learning of properties of lead-free perovskites with a neural network with additive kernel regression-based neuron activation functions",
        "authors": "Methawee Nukunudompanich, Heejoo Yoon, Lee Hyojae, Keisuke Kameda, Manabu Ihara, Sergei Manzhos",
        "published": "2024-1-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1557/s43580-023-00749-1"
    },
    {
        "id": 15129,
        "title": "Neural Network with Optimal Neuron Activation Functions Based on Additive Gaussian Process Regression",
        "authors": "Sergei Manzhos, Manabu Ihara",
        "published": "2023-9-21",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1021/acs.jpca.3c02949"
    },
    {
        "id": 15130,
        "title": "Best Fit Activation Functions for Attention Mechanism: Comparison and Enhancement",
        "authors": "Maan Alhazmi, Abdulrahman Altahhan",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191885"
    },
    {
        "id": 15131,
        "title": "On-chip Nonlinear Activation and Gradient Functions for Photonic Backpropagation Training and Inference",
        "authors": "F. Ashtiani, M. H. Idjadi",
        "published": "2023-11-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ipc57732.2023.10360521"
    },
    {
        "id": 15132,
        "title": "Multi-Activation Dendritic Neural Network (MA-DNN) Working Example of Dendritic-Based Artificial Neural Network",
        "authors": "Konstantin Tomov, Galina Momcheva",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "Abstract\nThroughout the years neural networks have been based on the perceptron model of the artificial neuron. Attempts to stray from it are few to none. The perceptron simply works and that has discouraged research around other neuron models. New discoveries highlight the importance of dendrites in the neuron, but the perceptron model does not include them. This brings us to the goal of the paper which is to present and test different models of artificial neurons that utilize dendrites to create an artificial neuron that better represents the biological neuron. The authors propose two models. One is made with the purpose of testing the idea of the dendritic neuron. The distinguishing feature of the second model is that it implements activation functions after its dendrites. Results from the second model suggest that it performs as well as or even better than the perceptron model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2478/cait-2023-0030"
    },
    {
        "id": 15133,
        "title": "Noise Impact on a Recurrent Neural Network with a Linear Activation Function",
        "authors": "V. M. Moskvitin,  , N. I. Semenova,  ",
        "published": "2023",
        "citations": 1,
        "abstract": "In this paper we analyze an echo state neural network (ESN) in the presence of uncorrelated additive and multiplicative white Gaussian noise. Here we consider the case where artificial neurons have a linear activation function with different slope coefficients. We consider the influence of the input signal, memory and connection matrices on the accumulation of noise. We have found that the general view of variance and the signal-to-noise ratio of the ESN output signal is similar to only one neuron. The noise is less accumulated in ESN with a diagonal reservoir connection matrix with a large “blurring” coefficient. This is especially true of uncorrelated multiplicative noise.",
        "keywords": "",
        "link": "http://dx.doi.org/10.20537/nd230502"
    },
    {
        "id": 15134,
        "title": "Noise Simulation for the Improvement of Training Deep Neural Network for Printer-Proof Steganography",
        "authors": "Telmo Cunha, Luiz Schirmer, João Marcos, Nuno Gonçalves",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012272300003654"
    },
    {
        "id": 15135,
        "title": "Relaxed Training Procedure for a Binary Neural Network",
        "authors": "",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18178/ijml.2023.13.1.1124"
    },
    {
        "id": 15136,
        "title": "Resource constrained neural network training",
        "authors": "Mariusz Pietrołaj, Marek Blok",
        "published": "2024-1-29",
        "citations": 0,
        "abstract": "AbstractModern applications of neural-network-based AI solutions tend to move from datacenter backends to low-power edge devices. Environmental, computational, and power constraints are inevitable consequences of such a shift. Limiting the bit count of neural network parameters proved to be a valid technique for speeding up and increasing efficiency of the inference process. Hence, it is understandable that a similar approach is gaining momentum in the field of neural network training. In the face of growing complexity of neural network architectures, reducing resources required for preparation of new models would not only improve cost efficiency but also enable a variety of new AI applications on modern personal devices. In this work, we present a deep refinement of neural network parameters limitation with the use of the asymmetric exponent method. In addition to the previous research, we study new techniques of floating-point variables limitation, representation, and rounding. Moreover, by leveraging exponent offset, we present floating-point precision adjustments without an increase in variables’ bit count. The proposed method allowed us to train LeNet, AlexNet and ResNet-18 convolutional neural networks with a custom 8-bit floating-point representation achieving minimal or no results degradation in comparison to baseline 32-bit floating-point variables.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-024-52356-1"
    },
    {
        "id": 15137,
        "title": "Coordinate descent on the Stiefel manifold for deep neural network training",
        "authors": "Estelle Massart, Vinayak Abrol",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-143"
    },
    {
        "id": 15138,
        "title": "Reduction of training computation by network optimization of Integration Neural Network approximator",
        "authors": "Yoshiharu Iwata, Hidefumi Wakamatsu",
        "published": "2023-1-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/sii55687.2023.10039273"
    },
    {
        "id": 15139,
        "title": "Mixed Radial Basis Function Neural Network Training Using Genetic Algorithm",
        "authors": "Taoufyq Elansari, Mohammed Ouanan, Hamid Bourray",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-023-11339-5"
    },
    {
        "id": 15140,
        "title": "Modeling multisource multifrequency acoustic wavefields by a multiscale Fourier feature physics-informed neural network with adaptive activation functions",
        "authors": "Xintao Chai, Zhiyuan Gu, Hang Long, Shaoyong Liu, Taihui Yang, Lei Wang, Fenglin Zhan, Xiaodong Sun, Wenjun Cao",
        "published": "2024-1-9",
        "citations": 0,
        "abstract": " Recently, the physics-informed neural network (PINN) was adopted to solve partial differential equation (PDE)-based forward and inverse problems. Compared to numerical differentiation, a PINN calculates derivatives by mesh-free automatic differentiation without dispersion artifacts. The Fourier feature PINN was applied to solve the frequency-domain acoustic wave equation to model multifrequency scattered wavefields. Although solving for scattered wavefields avoids the source singularity problem, it has drawbacks (e.g., requiring an analytic formula for computing the background wavefield, which only exists for the wave equation for simple models). We evaluated an approach for modeling multisource multifrequency acoustic wavefields using a multiscale Fourier feature mapping (MFFM) PINN with adaptive activations, directly solving for full wavefields instead of scattered wavefields and naturally avoiding the drawbacks of solving the scattered wave equation. For the MFFM, we explored the determination of the maximum and number of Fourier scales. Our inputs to the MFFM were only the spatial coordinates of the subsurface model; this result is lower than that of previous work (improving the efficiency of the PINN while maintaining its accuracy). Because the activation function is extremely important for a PINN, we use an existing technique and adapt it to a new architecture and develop an adaptive amplitude-scaled and phase-shifted sine activation function, which performs the best among the studied activation functions. Experiments show that the MFFM, adaptive activation, an appropriate learning rate, a linearly shrinking NN, and transfer learning greatly improve the convergence rate, accuracy, and efficiency of the PINN for simulating multisource multifrequency wavefields, laying the foundation for applying a PINN to wave equation-based inversion and imaging. We shared our codes, data, and results via a public repository. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1190/geo2023-0394.1"
    },
    {
        "id": 15141,
        "title": "Low-area architecture design of multi-mode activation functions with controllable maximum absolute error for neural network applications",
        "authors": "Shu-Yen Lin, Jung-Chuan Chiang",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.micpro.2023.104952"
    },
    {
        "id": 15142,
        "title": "Attribute-driven streaming edge partitioning with reconciliations for distributed graph neural network training",
        "authors": "Zongshen Mu, Siliang Tang, Yueting Zhuang, Dianhai Yu",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.06.026"
    },
    {
        "id": 15143,
        "title": "End-to-End Neural Network Training for Hyperbox-Based Classification",
        "authors": "Denis Martins, Christian Lülf, Fabian Gieseke",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-33"
    },
    {
        "id": 15144,
        "title": "Activation Functions Effect on Fractal Coding Using Neural Networks",
        "authors": "Rashad A. Al-Jawfi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/iasc.2023.031700"
    },
    {
        "id": 15145,
        "title": "Enriching Neural Network Training Dataset to Improve Worst-Case Performance Guarantees",
        "authors": "Rahul Nellikkath, Spyros Chatzivasileiadis",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/powertech55446.2023.10202770"
    },
    {
        "id": 15146,
        "title": "Neural Network Approximation for Time Splitting Random Functions",
        "authors": "George A. Anastassiou, Dimitra Kouloumpou",
        "published": "2023-5-5",
        "citations": 0,
        "abstract": "In this article we present the multivariate approximation of time splitting random functions defined on a box or RN,N∈N, by neural network operators of quasi-interpolation type. We achieve these approximations by obtaining quantitative-type Jackson inequalities engaging the multivariate modulus of continuity of a related random function or its partial high-order derivatives. We use density functions to define our operators. These derive from the logistic and hyperbolic tangent sigmoid activation functions. Our convergences are both point-wise and uniform. The engaged feed-forward neural networks possess one hidden layer. We finish the article with a great variety of applications.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math11092183"
    },
    {
        "id": 15147,
        "title": "A Radial Basis Function Neural Network for Stochastic Frontier Analyses of General Multivariate Production and Cost Functions",
        "authors": "Parag C. Pendharkar",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-022-11137-5"
    },
    {
        "id": 15148,
        "title": "Neural 3D Rendering with Periodic Activation Functions",
        "authors": "Han-nyoung Lee, Hak-Gu Kim",
        "published": "2023-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15323/mint.2023.4.3.1.1"
    },
    {
        "id": 15149,
        "title": "Models and methods of learning neural networks  with differentiated activation functions",
        "authors": "Dmytro Zelentsov, Shaptala Taras",
        "published": "2023-11-13",
        "citations": 0,
        "abstract": "Analysis of the literature made it clear that the problem associated with improving the performance and acceleration of ANN learning is quite actual, as ANNs are used every day in more and more industries. The concepts of finding more profitable activation functions have been outlined a lot, but changing their behavior as a result of learning is a fresh look at the problem. The aim of the study is to find new models of optimization tasks for the formulated prob-lem and effective methods for their implementation, which would improve the quality of ANN training, in particular by overcoming the problem of local minima. A studied of models and methods for training neural networks using an extended vector of varying parameters is conducted. The training problem is formulated as a continuous mul-tidimensional unconditional optimization problem. The extended vector of varying parameters implies that it includes some parameters of activation functions in addition to weight coeffi-cients. The introduction of additional varying parameters does not change the architecture of a neural network, but makes it impossible to use the back propagation method. A number of gradient methods have been used to solve optimization problems. Different formulations of optimization problems and methods for their solution have been investigated according to ac-curacy and efficiency criteria. The analysis of the results of numerical experiments allowed us to conclude that it is expedient to expand the vector of varying parameters in the tasks of training ANNs with con-tinuous and differentiated activation functions. Despite the increase in the dimensionality of the optimization problem, the efficiency of the new formulation is higher than the generalized one. According to the authors, this is due to the fact that a significant share of computational costs in the generalized formulation falls on attempts to leave the neighborhood of local min-ima, while increasing the dimensionality of the solution space allows this to be done with much lower costs.",
        "keywords": "",
        "link": "http://dx.doi.org/10.34185/1562-9945-6-143-2022-05"
    },
    {
        "id": 15150,
        "title": "Prediction of English Major Training Model in Higher Vocational Education Based on Fuzzy Neural Network Algorithm",
        "authors": "",
        "published": "2023-4-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.55162/mcet.04.134"
    },
    {
        "id": 15151,
        "title": "An Improved Cuckoo Search Algorithm for Optimization of Artificial Neural Network Training",
        "authors": "Pedda Nagyalla Maddaiah, Pournami Pulinthanathu Narayanan",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-023-11411-0"
    },
    {
        "id": 15152,
        "title": "Neural Network Modules Learn Specialized Functions Under Strong Resource Constraints",
        "authors": "Gabriel Béna, Dan F. M Goodman",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1175-0"
    },
    {
        "id": 15153,
        "title": "Performance evaluation of machine learning algorithms and impact of activation functions in artificial neural network classifier for bearing fault diagnosis",
        "authors": "Ganesh Rayjade, Amit Bhagure, Prashant B Kushare, Ramesh Bhandare, Vilas Matsagar, Ashitosh Chaudhary",
        "published": "2024-2-29",
        "citations": 0,
        "abstract": " Machine learning algorithms are used to identify the bearing condition. In this work, different machine learning techniques such as decision tree, logistic regression, support vector machine (SVM), and artificial neural network (ANN) are used and compared to find healthy and faulty conditions of the bearing. The identification of the condition of the bearing is based on vibrations recorded using Fast Fourier Transform Analyzer. The vibration data recorded for the bearings have been used to categorize the condition of the bearing as healthy or faulty by applying machine learning techniques. The dataset of healthy and faulty bearings is collected using a four-channel Fast Fourier Transform Analyzer (FFT) analyzer. However, the statistical feature extraction technique has been used to evaluate the accuracy and performance of artificial neural network, support vector machine, logistic regression, and decision tree algorithms based on their classification accuracy and total costs. The result of the work reveals that the performance of the activation function based artificial neural network (ANN-AF) and SVM algorithm is better than logistic regression and decision tree models. However, it is observed that the use of appropriate activation functions within the ANN-AF technique improves the accuracy of the machine learning model. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10775463241235778"
    },
    {
        "id": 15154,
        "title": "Stacked autoencoder with novel integrated activation functions for the diagnosis of autism spectrum disorder",
        "authors": "Kaviya Elakkiya M,  Dejey",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-08565-2"
    },
    {
        "id": 15155,
        "title": "Activation Functions for Convolutional Neural Networks: Proposals and Experimental Study",
        "authors": "Víctor Manuel Vargas, Pedro Antonio Gutiérrez, Javier Barbero-Gómez, César Hervás-Martínez",
        "published": "2023-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3105444"
    },
    {
        "id": 15156,
        "title": "A robust training of dendritic neuron model neural network for time series prediction",
        "authors": "Ayşe Yilmaz, Ufuk Yolcu",
        "published": "2023-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00521-023-08240-6"
    },
    {
        "id": 15157,
        "title": "Influence of Different Activation Functions on Neural Networks Computational RCS",
        "authors": "Yuanpeng Yang, Wang Wenzhuo, Shi Xinyang, Chonghua Fang",
        "published": "2023-8-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/aces-china60289.2023.10249987"
    },
    {
        "id": 15158,
        "title": "A Wide Analysis of Loss Functions for Image Classification Using Convolution Neural Network",
        "authors": "Janaki Raman Palaniappan",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4674956"
    },
    {
        "id": 15159,
        "title": "Numerical study of the speed’s response of the various intelligent models using the tansig, logsig and purelin activation functions in different layers of artificial neural network",
        "authors": "Zineb Laabid, Aziz Moumen, Khalifa Mansouri, Ali Siadat",
        "published": "2023-3-1",
        "citations": 1,
        "abstract": "<p><span lang=\"EN-US\">Today's world is no longer that of yesterday, the pace with which we live and also the speed is enormous and rapid, that overnight we discover the appearance of new technologies and solutions in all the fields, in particular, that of scientific research. Artificial intelligence plays the main role. Predicting the behavior of new materials using artificial neural networks has become a frequently adopted solution by researchers today. The performance of neural networks depends mainly on the activation functions used. This work was designed to mainly study the impact of these functions on the response speed of an artificial neural network in general, and particularly on the model we are working on to predict the thermomechanical behavior of innovative materials. By using TANSIG, PURELIN and LOGSIG in a feed forward back propagation training by Levenberg-Marquardt algorithm, we were able to generate 9 models. For each of these models, we were interested in analyzing the speed’s response of the network and studying its regression. Thus, this work was able to show us that choosing the right neuron activation function from one layer to another can clearly influence the performance of the results. Depending on the problem studied, the desired objective and the chosen architecture, the activation function can radically change the result and provide us with the expected efficiency. </span></p>",
        "keywords": "",
        "link": "http://dx.doi.org/10.11591/ijai.v12.i1.pp155-161"
    },
    {
        "id": 15160,
        "title": "A fully automatic Bayesian model for adaptive activation functions in artificial neural networks",
        "authors": "Mohamed Fakhfakh, Lotfi Chaari",
        "published": "2023-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/inista59065.2023.10310471"
    },
    {
        "id": 15161,
        "title": "<i>Explananda</i> and <i>explanantia</i> in deep neural network models of neurological network functions",
        "authors": "Mihnea Moldoveanu",
        "published": "2023",
        "citations": 0,
        "abstract": "Abstract\nDepending on what we mean by “explanation,” challenges to the explanatory depth and reach of deep neural network models of visual and other forms of intelligent behavior may need revisions to both the elementary building blocks of neural nets (the explananda) and to the ways in which experimental environments and training protocols are engineered (the explanantia). The two paths assume and imply sharply different conceptions of how an explanation explains and of the explanatory function of models.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1017/s0140525x23001632"
    },
    {
        "id": 15162,
        "title": "A Brief Review of the Most Recent Activation Functions for Neural Networks",
        "authors": "Marina Adriana Mercioni, Stefan Holban",
        "published": "2023-6-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/emes58375.2023.10171705"
    },
    {
        "id": 15163,
        "title": "Explaining the Outputs of Convolutional Neural Network - Recurrent Neural Network (CNN-RNN) based Apparent Personality Detection Models using the Class Activation Maps",
        "authors": "WMKS Ilmini, TGI Fernando",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140224"
    },
    {
        "id": 15164,
        "title": "Preference-based training framework for automatic speech quality assessment using deep neural network",
        "authors": "Cheng-Hung Hu, Yusuke Yasuda, Tomoki Toda",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21437/interspeech.2023-589"
    },
    {
        "id": 15165,
        "title": "Optimizing neural network training with Genetic Algorithms",
        "authors": "Junen Chai",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "In modern society, computer plays an important role among all human beings. Through the increasing development of technology, some problems happened gradually. In order to solve and regenerate the country, individuals should test their strengths. This paper discusses how to use genetic algorithms to optimize neural network training. As an important tool of machine learning, neural networks have made remarkable achievements in dealing with complex tasks. However, the training process of neural networks involves a lot of hyperparameter adjustment and weight optimization, which often requires a lot of time and computing resources. In order to improve the efficiency and performance of neural network training, humans should introduce genetic algorithms as an optimization method. Experiments are conducted on several common datasets to compare the performance of neural network training with Genetic Algorithm optimization against the traditional method. The results indicate that using Genetic Algorithms significantly improves the convergence speed and performance of neural networks while reducing the time and effort spent on hyperparameter tuning. Neural networks optimized using the Genetic Algorithm outperform their counterparts trained under the same time frame.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54254/2755-2721/42/20230780"
    },
    {
        "id": 15166,
        "title": "Artificial neural network-based rapid predictor of biological nerve fiber activation for DBS applications",
        "authors": "Justin Golabek, Matthew Schiefer, Joshua K Wong, Shreya Saxena, Erin Patrick",
        "published": "2023-2-1",
        "citations": 1,
        "abstract": "Abstract\n\nObjective. Computational models are powerful tools that can enable the optimization of deep brain stimulation (DBS). To enhance the clinical practicality of these models, their computational expense and required technical expertise must be minimized. An important aspect of DBS models is the prediction of neural activation in response to electrical stimulation. Existing rapid predictors of activation simplify implementation and reduce prediction runtime, but at the expense of accuracy. We sought to address this issue by leveraging the speed and generalization abilities of artificial neural networks (ANNs) to create a novel predictor of neural fiber activation in response to DBS. Approach. We developed six variations of an ANN-based predictor to predict the response of individual, myelinated axons to extracellular electrical stimulation. ANNs were trained using datasets generated from a finite-element model of an implanted DBS system together with multi-compartment cable models of axons. We evaluated the ANN-based predictors using three white matter pathways derived from group-averaged connectome data within a patient-specific tissue conductivity field, comparing both predicted stimulus activation thresholds and pathway recruitment across a clinically relevant range of stimulus amplitudes and pulse widths. Main results. The top-performing ANN could predict the thresholds of axons with a mean absolute error (MAE) of 0.037 V, and pathway recruitment with an MAE of 0.079%, across all parameters. The ANNs reduced the time required to predict the thresholds of 288 axons by four to five orders of magnitude when compared to multi-compartment cable models. Significance. We demonstrated that ANNs can be fast, accurate, and robust predictors of neural activation in response to DBS.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1741-2552/acb016"
    },
    {
        "id": 15167,
        "title": "modSwish: a new activation function for neural network",
        "authors": "Heena Kalim, Anuradha Chug, Amit Prakash Singh",
        "published": "2024-2-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s12065-024-00908-9"
    },
    {
        "id": 15168,
        "title": "Higher Order Orthogonal Polynomials as Activation Functions in Artificial Neural Networks",
        "authors": "Burak Nebioglu, Alexander I. Iliev",
        "published": "2023-7-6",
        "citations": 2,
        "abstract": "Activation functions are used in Artificial Neural Networks to provide non-linearity to the system. Several different activation functions in use are very well known by almost any AI practitioner however this is not the case for polynomial activation functions. Increasing attention to these valuable mathematical functions can encourage more research and help to fill the gap. During this work, Chebyshev and Hermite orthogonal polynomials were used as activation functions. Calculations were conducted on 3 different datasets with different hyperparameters. According to the results, calculations done by Chebyshev activation functions take less time, but Chebyshev can be more fragile depending on the solved problem. On the other hand, Hermite shows a more robust and generalized behavior, it is less dependent on the problem type, and it improves by necessary adjustments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.55630/sjc.2023.17.1-16"
    },
    {
        "id": 15169,
        "title": "Convolutional graph neural network training scalability for molecular docking",
        "authors": "Kevin Crampon, Alexis Giorkallos, Stephanie Baud, Luiz Angelo Steffenel",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/pdp59025.2023.00042"
    },
    {
        "id": 15170,
        "title": "Retracted: Swimming Training Evaluation Method Based on Convolutional Neural Network",
        "authors": " Complexity",
        "published": "2024-1-24",
        "citations": 0,
        "abstract": "",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2024/9878901"
    },
    {
        "id": 15171,
        "title": "Training Low-Latency Spiking Neural Network with Orthogonal Spiking Neurons",
        "authors": "Yunpeng Yao, Man Wu, Renyuan Zhang",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/newcas57931.2023.10198054"
    },
    {
        "id": 15172,
        "title": "Automatic Evaluation of Neural Network Training Results",
        "authors": "Roman Barinov, Vasiliy Gai, George Kuznetsov, Vladimir Golubenko",
        "published": "2023-1-20",
        "citations": 1,
        "abstract": "This article is dedicated to solving the problem of an insufficient degree of automation of artificial neural network training. Despite the availability of a large number of libraries for training neural networks, machine learning engineers often have to manually control the training process to detect overfitting or underfitting. This article considers the task of automatically estimating neural network training results through an analysis of learning curves. Such analysis allows one to determine one of three possible states of the training process: overfitting, underfitting, and optimal training. We propose several algorithms for extracting feature descriptions from learning curves using mathematical statistics. Further state classification is performed using classical machine learning models. The proposed automatic estimation model serves to improve the degree of automation of neural network training and interpretation of its results, while also taking a step toward constructing self-training models. In most cases when the training process of neural networks leads to overfitting, the developed model determines its onset ahead of the early stopping method by 3–5 epochs.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/computers12020026"
    },
    {
        "id": 15173,
        "title": "Enhancing Deep Neural Network Convergence and Performance: A Hybrid Activation Function Approach by Combining ReLU and ELU Activation Function",
        "authors": "Ritesh Maurya, Divyam Aggarwal, T. Gopalakrishnan, Nageshwar Nath Pandey",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ici60088.2023.10421353"
    },
    {
        "id": 15174,
        "title": "Comparison of an effectiveness of artificial neural networks for various activation functions",
        "authors": "Daniel Florek, Marek Miłosz",
        "published": "2023-3-30",
        "citations": 0,
        "abstract": "Activation functions play an important role in artificial neural networks (ANNs) because they break the linearity in the data transformations that are performed by models. Thanks to the recent spike in interest around the topic of ANNs, new improvements to activation functions are emerging. The paper presents the results of research on the effectiveness of ANNs for ReLU, Leaky ReLU, ELU, and Swish activation functions. Four different data sets, and three different network architectures were used. Results show that Leaky ReLU, ELU and Swish functions work better in deep and more complex architectures which are to alleviate vanishing gradient and dead neurons problems but at the cost of prediction speed. Swish function seems to speed up training process considerably but neither of the three aforementioned functions comes ahead in accuracy in all used datasets.",
        "keywords": "",
        "link": "http://dx.doi.org/10.35784/jcsi.3069"
    },
    {
        "id": 15175,
        "title": "A Wide Analysis of Loss Functions for Image Classification Using Convolution Neural Network",
        "authors": "Janaki Raman Palaniappan",
        "published": "2023-9-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cisct57197.2023.10351209"
    },
    {
        "id": 15176,
        "title": "Neural network operators with hyperbolic tangent functions",
        "authors": "Behar Baxhaku, Purshottam Narain Agrawal",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.119996"
    },
    {
        "id": 15177,
        "title": "Breaking the Quadratic Communication Overhead of Secure Multi-Party Neural Network Training",
        "authors": "Xingyu Lu, Başak Güler",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isit54713.2023.10206617"
    },
    {
        "id": 15178,
        "title": "Autonomous Car Behavioral Training Using Deep Neural Network",
        "authors": "Jiayi Gao",
        "published": "2024-2-28",
        "citations": 0,
        "abstract": "Autonomous driving is becoming increasingly prevalent nowadays. With the help of a number of images of car movement from the Kaggle self-driving dataset, we explore the feasibility of utilizing the images obtained to train a deep neural network to detect and predict the steering angle, which is the critical part of the car behavior. Since deep neural networks have emerged as powerful tools for training autonomous cars and learning about and improving their driving behaviors, we incorporate convolutional layers and additional layers in the deep neural network architecture so that it can capture the behaviors appropriately and provide effective results. We demonstrate that the implementation of this approach is successful and that the corresponding implementation highlights the potential of deep neural network in advancing autonomous car technology. Our comprehensive evaluation suggests that further research should concentrate on refining the network architecture and enhancing perception capabilities in order to deliver promising advances to the field.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/mkny71cuq7"
    },
    {
        "id": 15179,
        "title": "On Data Sampling Strategies for Training Neural Network Speech Separation Models",
        "authors": "William Ravenscroft, Stefan Goetze, Thomas Hain",
        "published": "2023-9-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/eusipco58844.2023.10289800"
    },
    {
        "id": 15180,
        "title": "Neural Network Optimization and Implications of High and Low Gradient Results in Training Vanilla and Hybrid Adaptive Neural Network Models for Effective Signal Power Loss Prediction",
        "authors": "Virginia Chika Ebhota, Thokozani Shongwe",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15866/iremos.v16i5.23622"
    },
    {
        "id": 15181,
        "title": "An improved learning algorithm for training neural network based lattice equalizer",
        "authors": "Zohra Zerdoumi, Lahouaoui Lalaoui",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iw_mss59200.2023.10369515"
    },
    {
        "id": 15182,
        "title": "Applicative Analysis Of Activation Functions For Pneumonia Detection Using Convolutional Neural Networks",
        "authors": "Pratham Tatraiya, Himanshu Priyadarshi, Kulwant Singh, Dhaneshwar Mishra, Ashish Shrivastava",
        "published": "2023-5-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globconet56651.2023.10149937"
    },
    {
        "id": 15183,
        "title": "Quaternionic Convolutional Neural Networks with Trainable Bessel Activation Functions",
        "authors": "Nelson Vieira",
        "published": "2023-9",
        "citations": 1,
        "abstract": "AbstractQuaternionic convolutional neural networks (QCNN) possess the ability to capture both external dependencies between neighboring features and internal latent dependencies within features of an input vector. In this study, we employ QCNN with activation functions based on Bessel-type functions with trainable parameters, for performing classification tasks. Our experimental results demonstrate that this activation function outperforms the traditional ReLU activation function. Throughout our simulations, we explore various network architectures. The use of activation functions with trainable parameters offers several advantages, including enhanced flexibility, adaptability, improved learning, customized model behavior, and automatic feature extraction.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11785-023-01387-z"
    },
    {
        "id": 15184,
        "title": "Artificial Neural Network Modeling for Predicting Thermal Conductivity of EG/Water-Based CNC Nanofluid for Engine Cooling Using Different Activation Functions",
        "authors": "Md. Munirul Hasan, Md Mustafizur Rahman, Mohammad Saiful Islam, Wong Hung Chan, Yasser M. Alginahi, Muhammad Nomani Kabir, Suraya Abu Bakar, Devarajan Ramasamy",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/fhmt.2024.047428"
    },
    {
        "id": 15185,
        "title": "Improvement of the classical artificial neural network simulation model of the parabolic trough solar collector outlet temperature and thermal efficiency using the conformable activation functions",
        "authors": "W. Ajbar, J.E. Solís-Pérez, E. Viera-Martin, A. Parrales, J.F. Gómez-Aguilar, J.A. Hernández",
        "published": "2023-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.segan.2023.101200"
    },
    {
        "id": 15186,
        "title": "Research on Activation Functions in Machine Learning Based Network Coding",
        "authors": "Xin Zhang, Yanbo Yang, Baoshan Li, Minchao Li, Teng Li, Jiawei Zhang",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/nana60121.2023.00058"
    },
    {
        "id": 15187,
        "title": "Clipping-based Neural Network Post Training Quantization for Object Detection",
        "authors": "Cui Liqun, Hu Lei",
        "published": "2023-4-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccect57938.2023.10141287"
    },
    {
        "id": 15188,
        "title": "Constructing the Bounds for Neural Network Training Using Grammatical Evolution",
        "authors": "Ioannis G. Tsoulos, Alexandros Tzallas, Evangelos Karvounis",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "Artificial neural networks are widely established models of computational intelligence that have been tested for their effectiveness in a variety of real-world applications. These models require a set of parameters to be fitted through the use of an optimization technique. However, an issue that researchers often face is finding an efficient range of values for the parameters of the artificial neural network. This paper proposes an innovative technique for generating a promising range of values for the parameters of the artificial neural network. Finding the value field is conducted by a series of rules for partitioning the original set of values or expanding it, the rules of which are generated using grammatical evolution. After finding a promising interval of values, any optimization technique such as a genetic algorithm can be used to train the artificial neural network on that interval of values. The new technique was tested on a wide range of problems from the relevant literature and the results were extremely promising.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/computers12110226"
    },
    {
        "id": 15189,
        "title": "Training of binary neural network models using continuous approximation",
        "authors": "Dmitrij Pavliuchenkov, Anton Trusov, Elena Limonova",
        "published": "2024-4-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3023264"
    },
    {
        "id": 15190,
        "title": "Model of a Predictive Neural Network for Determining the Electric Fields of Training Flight Phases",
        "authors": "Joanna Michalowska",
        "published": "2023-12-25",
        "citations": 0,
        "abstract": "Tests on the content of the electrical component of the electromagnetic field (EMF) were carried out with an NHT3DL broadband meter by Microrad using a 01E (100 kHz ÷ 6.5 GHz) measuring probe. Measurements were made during training flights (Cessna C172, Cessna C152, Aero AT3, and Technam P2006T aircrafts). A neural network was used, the task of which was to learn to predict the successive values of average (ERMS) and instantaneous (EPEAK) electromagnetic fields used here. Such a solution would make it possible to determine the most favorable routes for all aircrafts. This article presents a model of an artificial neural network which aims to predict the intensity of the electrical component of the electromagnetic field. In order to create the developed model, that is, to create a training sequence for the model, a series of measurements was carried out on four types of aircraft (Cessna C172, Cessna C152, Aero AT3, and Technam P2006T). The model was based on long short-term memory (LSTM) layers. The tests carried out showed that the accuracy of the model was higher than that of the reference method. The developed model was able to estimate the electrical component for the vicinity of the routes on which it was trained in order to optimize the exposure of the aircraft to the electrical component of the electromagnetic field. In addition, it allowed for data analysis of the same training flight routes. The reference point for the obtained electric energy results were the normative limits of the electromagnetic field that may affect the crew and passengers during a flight. Monitoring and measuring the electromagnetic field generated by devices is important from an environmental point of view, as well as for the purposes of human body protection and electromagnetic compatibility. In order to improve reliability in general aviation and to adapt to the proposed requirements, aviation training centers are obliged to introduce systems for supervising and analyzing flight parameters.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/en17010126"
    },
    {
        "id": 15191,
        "title": "Simulation of the training path of film creation talents based on artificial neural network",
        "authors": "Yanli Ma",
        "published": "2023-6-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-08656-1"
    },
    {
        "id": 15192,
        "title": "Deep Intuition-Inspired Learning for Advancing Direct Training of Spiking Neural Network",
        "authors": "Shuangming Yang, Haowen Wang, Badong Chen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4331179"
    },
    {
        "id": 15193,
        "title": "Neural Network Based Automatic Modulation Classification with Online Training",
        "authors": "Shuo Zhang, Chris Yakopcic, Tarek M. Taha",
        "published": "2023-6-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccaaw57883.2023.10219282"
    },
    {
        "id": 15194,
        "title": "Efficiently Sampling in Neural Network Training for Click-Through Rate Prediction",
        "authors": "Ergun Biçici, Serdarcan Dilbaz",
        "published": "2023-9-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ubmk59864.2023.10286811"
    },
    {
        "id": 15195,
        "title": "Learning Specialized Activation Functions for Physics-Informed Neural Networks",
        "authors": "Honghui Wang, Lu Lu, Shiji Song null, Gao Huang",
        "published": "2023-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4208/cicp.oa-2023-0058"
    },
    {
        "id": 15196,
        "title": "A deep network-based model of hippocampal memory functions under normal and Alzheimer’s disease conditions",
        "authors": "Tamizharasan Kanagamani, V. Srinivasa Chakravarthy, Balaraman Ravindran, Ramshekhar N. Menon",
        "published": "2023-6-21",
        "citations": 1,
        "abstract": "We present a deep network-based model of the associative memory functions of the hippocampus. The proposed network architecture has two key modules: (1) an autoencoder module which represents the forward and backward projections of the cortico-hippocampal projections and (2) a module that computes familiarity of the stimulus and implements hill-climbing over the familiarity which represents the dynamics of the loops within the hippocampus. The proposed network is used in two simulation studies. In the first part of the study, the network is used to simulate image pattern completion by autoassociation under normal conditions. In the second part of the study, the proposed network is extended to a heteroassociative memory and is used to simulate picture naming task in normal and Alzheimer’s disease (AD) conditions. The network is trained on pictures and names of digits from 0 to 9. The encoder layer of the network is partly damaged to simulate AD conditions. As in case of AD patients, under moderate damage condition, the network recalls superordinate words (“odd” instead of “nine”). Under severe damage conditions, the network shows a null response (“I don’t know”). Neurobiological plausibility of the model is extensively discussed.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fncir.2023.1092933"
    },
    {
        "id": 15197,
        "title": "In-Network DAQ Functions",
        "authors": "Nik Sultana, James Kowalkowski, Michael Wang",
        "published": "2023-7-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2172/1989885"
    },
    {
        "id": 15198,
        "title": "SAR image classification with convolutional neural network using modified functions",
        "authors": "AliAsghar Soltanali, Vahid Ghods, Seyed Farhood Mousavizadeh, Meysam Amirahmadi",
        "published": "2023-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-09415-y"
    },
    {
        "id": 15199,
        "title": "Designing a Convolutional Neural Network for Image Recognition: A Comparative Study of Different Architectures and Training Techniques",
        "authors": "Tapomoy Adhikari",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4366645"
    },
    {
        "id": 15200,
        "title": "Analyzing Training Exercises with Mathematical Algorithms and Convolutional Neural Network",
        "authors": "Szabolcs Róbert Bakos, Miklós Sipos",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cinti59972.2023.10382077"
    },
    {
        "id": 15201,
        "title": "Acceleration of Neural Network training algorithms via FPGA devices",
        "authors": "Gyulai-Nagy Zoltán-Valentin",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.procs.2023.10.259"
    },
    {
        "id": 15202,
        "title": "ANALYSIS OF TRAINING METHODS AND NEURAL NETWORK TOOLS FOR FAKE NEWS DETECTION",
        "authors": "Vitalii Tyshchenko",
        "published": "2023",
        "citations": 0,
        "abstract": "This article analyses various training methods and neural network tools for fake news detection. Approaches to fake news detection based on textual, visual and mixed data are considered, as well as the use of different types of neural networks, such as recurrent neural networks, convolutional neural networks, deep neural networks, generative adversarial networks and others. Also considered are supervised and unsupervised learning methods such as autoencoding neural networks and deep variational autoencoding neural networks.\n\nBased on the analysed studies, attention is drawn to the problems associated with limitations in the volume and quality of data, as well as the lack of efficiency of tools for detecting complex types of fakes. The author analyses neural network-based applications and tools and draws conclusions about their effectiveness and suitability for different types of data and fake detection tasks.\n\nThe study found that machine and deep learning models, as well as adversarial learning methods and special tools for detecting fake media, are effective in detecting fakes. However, the effectiveness and accuracy of these methods and tools can be affected by factors such as data quality, methods used for training and evaluation, and the complexity of the fake media being detected. Based on the analysis of training methods and neural network characteristics, the advantages and disadvantages of fake news detection are identified. Ongoing research and development in this area is crucial to improve the accuracy and reliability of these methods and tools for fake news detection.",
        "keywords": "",
        "link": "http://dx.doi.org/10.28925/2663-4023.2023.20.2034"
    },
    {
        "id": 15203,
        "title": "Multi-label Co-training using a Graph neural network.",
        "authors": "Ragini Kihlman, Maria Fasli",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386952"
    },
    {
        "id": 15204,
        "title": "Enhanced Employee Training Classification Model Using Optimized-HHO Probability Neural Network",
        "authors": "Dekun Chen",
        "published": "2024-2-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/eebda60612.2024.10486024"
    },
    {
        "id": 15205,
        "title": "ECAPA2: A Hybrid Neural Network Architecture and Training Strategy for Robust Speaker Embeddings",
        "authors": "Jenthe Thienpondt, Kris Demuynck",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/asru57964.2023.10389750"
    },
    {
        "id": 15206,
        "title": "Mental abacus training affects high-level executive functions: Comparison of activation of the frontal pole",
        "authors": "Nobuki Watanabe",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "The role of executive function training in supporting child development has been increasingly studied. Executive function is largely related to the prefrontal cortex. The anterior portion of the prefrontal cortex, which is area 10 on the Brodmann map, is essential for the emergence of higher-order executive functions. Accumulating evidence indicates that mental abacus training, which is closely related to mathematics education, activates the prefrontal cortex. Based on these findings, it can be hypothesized that the mental abacus is valuable for training more advanced functions. Therefore, this study analyzed the activation of children’s brains with a focus on the frontal pole (Brodmann area 10). The results illustrated that mental abacus task more strongly activated the brain than piano task, the marshmallow test, or letter–number sequencing tasks. Thus, it was suggested that the mental abacus is valuable for training higher-level executive functions (i.e., frontal pole).",
        "keywords": "",
        "link": "http://dx.doi.org/10.29333/iejme/13220"
    },
    {
        "id": 15207,
        "title": "Multistability analysis of quaternion-valued neural networks with cosine activation functions",
        "authors": "Siyi Yu, Hua Li, Xiaofeng Chen, Dongyuan Lin",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.amc.2023.127849"
    },
    {
        "id": 15208,
        "title": "A Technique for Creating and Training an Artificial Neural Network to Detect Network Traffic Anomalies",
        "authors": "S. O. Ivanov,  ",
        "published": "2024-1-19",
        "citations": 0,
        "abstract": "The article presents a technique for creating and training an artificial neural network to recognize network traffic anomalies using relatively small samples of collected data to generate training data. Various data sources for machine learning and approaches to network traffic analysis are considered. There are data format and the method of generating them from the collected network traffic is described, as well as the steps of the methodology in detail. Using the technique, an artificial neural network was created and trained for the task of recognizing anomalies in the network traffic of the ICMP protocol. The results of testing and comparing various artificial neural network configurations and learning conditions for a given task are presented. The artificial neural network trained according to the method was tested on real network traffic. The presented technique can be applied without requiring changes to detect anomalies of various network protocols and network traffic using a suitable parameterizer and data markup.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17587/it.30.32-41"
    },
    {
        "id": 15209,
        "title": "Limit Cycle of a Single-Neuron System With Smooth Continuous and Binary-Value Activation Functions and Its Circuitry Design",
        "authors": "Jintao Huang, Xiaofeng Liao, Yunhang Zhu",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3314675"
    },
    {
        "id": 15210,
        "title": "OBTAIN: Observational Therapy-Assistance Neural Network for Training State Recognition",
        "authors": "Minxiao Wang, Ning Yang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3263117"
    },
    {
        "id": 15211,
        "title": "Adversarial Neural Network Training for Secure and Robust Brain-to-Brain Communication",
        "authors": "Hossein Ahmadi, Ali Kuhestani, Luca Mesin",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2024.3376657"
    },
    {
        "id": 15212,
        "title": "PREPARATION OF DATA SETS FOR TRAINING A NEURAL NETWORK SYSTEM FOR PERSONAL IDENTIFICATION BY HANDWRITTEN SIGNATURE",
        "authors": "Anastasia Starostina, Oleg Kulyas",
        "published": "2023-8-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32743/unitech.2023.113.8.15867"
    },
    {
        "id": 15213,
        "title": "Exploring Gradient Oscillation in Deep Neural Network Training",
        "authors": "Chedi Morchdi, Yi Zhou, Jie Ding, Bei Wang",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/allerton58177.2023.10313361"
    },
    {
        "id": 15214,
        "title": "Spikeformer: Training high-performance spiking neural network with transformer",
        "authors": "Yudong Li, Yunlin Lei, Xu Yang",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2024.127279"
    },
    {
        "id": 15215,
        "title": "Community synchronization of functional brain network with cognitive training",
        "authors": "Guiyang Lv, Kexin Ma, Ping Zhu, Feiyan Chen, Guoguang He",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191618"
    },
    {
        "id": 15216,
        "title": "GAAF: Searching Activation Functions for Binary Neural Networks Through Genetic Algorithm",
        "authors": "Yanfei Li, Tong Geng, Samuel Stein, Ang Li, Huimin Yu",
        "published": "2023-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26599/tst.2021.9010084"
    },
    {
        "id": 15217,
        "title": "On Approximation by Neural Networks with Optimized Activation Functions and Fixed Weights",
        "authors": "Dansheng Yu, Yunyou Qian null, Fengjun Li",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.4208/ata.oa-2021-0006"
    },
    {
        "id": 15218,
        "title": "Optimizing GANs for Cryptography: The Role and Impact of Activation Functions in Neural Layers Assessing the Cryptographic Strength",
        "authors": "Purushottam Singh, Sandip Dutta, Prashant Pranav",
        "published": "2024-3-12",
        "citations": 0,
        "abstract": "Generative Adversarial Networks (GANs) have surfaced as a transformative approach in the domain of cryptography, introducing a novel paradigm where two neural networks, the generator (akin to Alice) and the discriminator (akin to Bob), are pitted against each other in a cryptographic setting. A third network, representing Eve, attempts to decipher the encrypted information. The efficacy of this encryption–decryption process is deeply intertwined with the choice of activation functions employed within these networks. This study conducted a comparative analysis of four widely used activation functions within a standardized GAN framework. Our recent explorations underscore the superior performance achieved when utilizing the Rectified Linear Unit (ReLU) in the hidden layers combined with the Sigmoid activation function in the output layer. The non-linear nature introduced by the ReLU provides a sophisticated encryption pattern, rendering the deciphering process for Eve intricate. Simultaneously, the Sigmoid function in the output layer guarantees that the encrypted and decrypted messages are confined within a consistent range, facilitating a straightforward comparison with original messages. The amalgamation of these activation functions not only bolsters the encryption strength but also ensures the fidelity of the decrypted messages. These findings not only shed light on the optimal design considerations for GAN-based cryptographic systems but also underscore the potential of investigating hybrid activation functions for enhanced system optimization. In our exploration of cryptographic strength and training efficiency using various activation functions, we discovered that the “ReLU and Sigmoid” combination significantly outperforms the others, demonstrating superior security and a markedly efficient mean training time of 16.51 s per 2000 steps. This highlights the enduring effectiveness of established methodologies in cryptographic applications. This paper elucidates the implications of these choices, advocating for their adoption in GAN-based cryptographic models, given the superior results they yield in ensuring security and accuracy.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app14062379"
    },
    {
        "id": 15219,
        "title": "深層学習用活性化関数によるニューラルネットワーク選点法トモグラフィの改良",
        "authors": "Masaru Teranishi",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1541/ieejeiss.143.694"
    },
    {
        "id": 15220,
        "title": "Self-training and Multi-level Adversarial Network for Domain Adaptive Remote Sensing Image Segmentation",
        "authors": "Yilin Zheng, Lingmin He, Xiangping Wu, Chen Pan",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-023-11341-x"
    },
    {
        "id": 15221,
        "title": "Distinct neural activation in a shared brain network for social and asocial behavior",
        "authors": "Jorge Ferreira",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41684-023-01229-7"
    },
    {
        "id": 15222,
        "title": "Desing of VLSI Architecture for a flexible testbed of Artificial Neural Network for training and testing on FPGA",
        "authors": "",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.31838/jvcs/06.01.06"
    },
    {
        "id": 15223,
        "title": "The Importance of Data Quality in Training a Deep Convolutional Neural Network",
        "authors": "David C. Marcu, Cristian Grava",
        "published": "2023-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/emes58375.2023.10171785"
    },
    {
        "id": 15224,
        "title": "A novel deep neural network model using network deconvolution with attention based activation for crop disease classification",
        "authors": "Nayan Kumar Sarkar, Moirangthem Marjit Singh, Utpal Nandi",
        "published": "2023-7-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11042-023-16125-y"
    },
    {
        "id": 15225,
        "title": "Physics-Informed Neural Networks with Periodic Activation Functions for Solute Transport in Heterogeneous Porous Media",
        "authors": "Salah A. Faroughi, Ramin Soltanmohammadi, Pingki Datta, Seyed Kourosh Mahjour, Shirko Faroughi",
        "published": "2023-12-24",
        "citations": 4,
        "abstract": "Simulating solute transport in heterogeneous porous media poses computational challenges due to the high-resolution meshing required for traditional solvers. To overcome these challenges, this study explores a mesh-free method based on deep learning to accelerate solute transport simulation. We employ Physics-informed Neural Networks (PiNN) with a periodic activation function to solve solute transport problems in both homogeneous and heterogeneous porous media governed by the advection-dispersion equation. Unlike traditional neural networks that rely on large training datasets, PiNNs use strong-form mathematical models to constrain the network in the training phase and simultaneously solve for multiple dependent or independent field variables, such as pressure and solute concentration fields. To demonstrate the effectiveness of using PiNNs with a periodic activation function to resolve solute transport in porous media, we construct PiNNs using two activation functions, sin and tanh, for seven case studies, including 1D and 2D scenarios. The accuracy of the PiNNs’ predictions is then evaluated using absolute point error and mean square error metrics and compared to the ground truth solutions obtained analytically or numerically. Our results demonstrate that the PiNN with sin activation function, compared to tanh activation function, is up to two orders of magnitude more accurate and up to two times faster to train, especially in heterogeneous porous media. Moreover, PiNN’s simultaneous predictions of pressure and concentration fields can reduce computational expenses in terms of inference time by three orders of magnitude compared to FEM simulations for two-dimensional cases.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/math12010063"
    },
    {
        "id": 15226,
        "title": "Training a Neural Network to Predict House Rents Using Artifical Intelligence and Deep Learning",
        "authors": "Chung-Hsing Chao",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icipca59209.2023.10257703"
    },
    {
        "id": 15227,
        "title": "Evaluation of Convergence of Neural Network Regulators Training Methods",
        "authors": "Dmitry Feofilov",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icct58878.2023.10347103"
    },
    {
        "id": 15228,
        "title": "ST-PINN: A Self-Training Physics-Informed Neural Network for Partial Differential Equations",
        "authors": "Junjun Yan, Xinhai Chen, Zhichao Wang, Enqiang Zhoui, Jie Liu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191472"
    },
    {
        "id": 15229,
        "title": "Improving Line Search Methods for Large Scale Neural Network Training",
        "authors": "Philip Kenneweg, Tristan Kenneweg, Barbara Hammer",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acdsa59508.2024.10467724"
    },
    {
        "id": 15230,
        "title": "A Tennis Training Action Analysis Model Based on Graph Convolutional Neural Network",
        "authors": "Xinyu Zhang, Jihua Chen",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3324425"
    },
    {
        "id": 15231,
        "title": "Multimode function multistability of Cohen-Grossberg neural networks with Gaussian activation functions and mixed time delays",
        "authors": "Jiang-Wei Ke, Jin-E Zhang, Ji-Xiang Zhang",
        "published": "2024",
        "citations": 0,
        "abstract": "<abstract><p>This paper explores multimode function multistability of Cohen-Grossberg neural networks (CGNNs) with Gaussian activation functions and mixed time delays. We start by using the geometrical properties of Gaussian functions. The state space is partitioned into $ 3^\\mu $ subspaces, where $ 0\\le \\mu\\le n $. Moreover, through the utilization of Brouwer's fixed point theorem and contraction mapping, some sufficient conditions are acquired to ensure the existence of precisely $ 3^\\mu $ equilibria for $ n $-dimensional CGNNs. Meanwhile, there are $ 2^\\mu $ and $ 3^\\mu-2^\\mu $ multimode function stable and unstable equilibrium points, respectively. Ultimately, two illustrative examples are provided to confirm the efficacy of theoretical results.</p></abstract>",
        "keywords": "",
        "link": "http://dx.doi.org/10.3934/math.2024220"
    },
    {
        "id": 15232,
        "title": "Using Synthetic Training Data in Neural Networks for the Estimation of Fiber Orientation Distribution Functions from Single Shell Data",
        "authors": "Amelie Rauland, Dorit Merhof",
        "published": "2023-4-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isbi53787.2023.10230737"
    },
    {
        "id": 15233,
        "title": "Improved zeroing neural models based on two novel activation functions with exponential behavior",
        "authors": "Dimitrios Gerontitis, Changxin Mo, Predrag S. Stanimirović, Vasilios N. Katsikis",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.tcs.2023.114328"
    },
    {
        "id": 15234,
        "title": "Serf: Towards better training of deep neural networks using log-Softplus ERror activation Function",
        "authors": "Sayan Nag, Mayukh Bhattacharyya, Anuraag Mukherjee, Rohit Kundu",
        "published": "2023-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/wacv56688.2023.00529"
    },
    {
        "id": 15235,
        "title": "Control of BOLD fMRI Responses Via Stimuli Generated with Voxel-Weighted Neural Network Activation Maximization",
        "authors": "Matthew Shinkle, Mark Lescroart",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1167/jov.23.9.5834"
    },
    {
        "id": 15236,
        "title": "Fairness–accuracy tradeoff: activation function choice in a neural network",
        "authors": "Michael B. McCarthy, Sundaraparipurnan Narayanan",
        "published": "2023-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s43681-022-00250-9"
    },
    {
        "id": 15237,
        "title": "Neural speech enhancement with unsupervised pre-training and mixture training",
        "authors": "Xiang Hao, Chenglin Xu, Lei Xie",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.11.013"
    },
    {
        "id": 15238,
        "title": "Application of optical imaging equipment based on deep neural network in basketball training game simulation",
        "authors": "Lin Zeyu",
        "published": "2024-1-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11082-023-06157-x"
    },
    {
        "id": 15239,
        "title": "Experimental Approach Toward Training and Analysing Siamese Deep Neural Network for Sentence with No Repeated Expressions",
        "authors": "Himanshu Tripathi",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10308080"
    },
    {
        "id": 15240,
        "title": "Analysis of sports training and load forecasting using an improved artificial neural network",
        "authors": "Linyao Wang",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-09114-8"
    },
    {
        "id": 15241,
        "title": "Empirical analysis of integrated teaching mode of international trade based on practical training data detection and neural network",
        "authors": "Lifang Chen",
        "published": "2023-6-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-08459-4"
    },
    {
        "id": 15242,
        "title": "Robust Ex-situ Training of Memristor Crossbar-based Neural Network with Limited Precision Weights",
        "authors": "Raqibul Hasan",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3611315.3633245"
    },
    {
        "id": 15243,
        "title": "TRAINING A NEURAL NETWORK TO LEARN THE INVERSE KINEMATIC MAPPING OF A RHINO ROBOT",
        "authors": "Anita Mudigonda, Han P. Bao, P. Soundar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1615/faim1994.690"
    },
    {
        "id": 15244,
        "title": "Adaptive Learning for Fast Training Neural Network in Hybrid Beamforming MIMO System",
        "authors": "Jiajun Yan, Ziyang Shao, Xingzhong Xiong",
        "published": "2023-4-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccect57938.2023.10140900"
    },
    {
        "id": 15245,
        "title": "Comparison of Wavelet-Based Image Processing Methods Before Neural Network Training",
        "authors": "Olesya Sedykh, Maria Oreshina",
        "published": "2023-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/summa60232.2023.10349436"
    },
    {
        "id": 15246,
        "title": "On the Omnipresence of Spurious Local Minima in Certain Neural Network Training Problems",
        "authors": "Constantin Christof, Julia Kowalczyk",
        "published": "2023-6-14",
        "citations": 1,
        "abstract": "AbstractWe study the loss landscape of training problems for deep artificial neural networks with a one-dimensional real output whose activation functions contain an affine segment and whose hidden layers have width at least two. It is shown that such problems possess a continuum of spurious (i.e., not globally optimal) local minima for all target functions that are not affine. In contrast to previous works, our analysis covers all sampling and parameterization regimes, general differentiable loss functions, arbitrary continuous nonpolynomial activation functions, and both the finite- and infinite-dimensional setting. It is further shown that the appearance of the spurious local minima in the considered training problems is a direct consequence of the universal approximation theorem and that the underlying mechanisms also cause, e.g., $$L^p$$\n\nL\np\n\n-best approximation problems to be ill-posed in the sense of Hadamard for all networks that do not have a dense image. The latter result also holds without the assumption of local affine linearity and without any conditions on the hidden layers.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00365-023-09658-w"
    },
    {
        "id": 15247,
        "title": "Abstract Univariate Neural Network Approximation Using a q-Deformed and λ-Parametrized Hyperbolic Tangent Activation Function",
        "authors": "George A. Anastassiou",
        "published": "2023-2-21",
        "citations": 1,
        "abstract": "In this work, we perform univariate approximation with rates, basic and fractional, of continuous functions that take values into an arbitrary Banach space with domain on a closed interval or all reals, by quasi-interpolation neural network operators. These approximations are achieved by deriving Jackson-type inequalities via the first modulus of continuity of the on hand function or its abstract integer derivative or Caputo fractional derivatives. Our operators are expressed via a density function based on a q-deformed and λ-parameterized hyperbolic tangent activation sigmoid function. The convergences are pointwise and uniform. The associated feed-forward neural networks are with one hidden layer.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/fractalfract7030208"
    },
    {
        "id": 15248,
        "title": "Survey on Activation Functions for Optical Neural Networks",
        "authors": "Oceane Destras, Sébastien Le Beux, Felipe Gohring De Magalhães, Gabriela Nicolescu",
        "published": "2024-2-29",
        "citations": 1,
        "abstract": "Integrated photonics arises as a fast and energy-efficient technology for the implementation of artificial neural networks (ANNs). Indeed, with the growing interest in ANNs, photonics shows great promise to overcome current limitations of electronic-based implementation. For example, it has been shown that neural networks integrating optical matrix multiplications can potentially run two orders of magnitude faster than their electronic counterparts. However, the transposition in the optical domain of the activation functions, which is a key feature of ANNs, remains a challenge. There is no direct optical implementation of state-of-the-art activation functions. Currently, most designs require time-consuming and power-hungry electro-optical conversions. In this survey, we review both all-optical and opto-electronic activation functions proposed in the state-of-the-art. We present activation functions with their key characteristics, and we summarize challenges for their use in the context of all-optical neural networks. We then highlight research directions for the implementation of fully optical neural networks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3607533"
    },
    {
        "id": 15249,
        "title": "Spatio-Temporal Pre-Training Enhanced Fast Pure Tansformer Network for Traffic Flow Forecasting",
        "authors": "Junhao Zhang, Junjie Tang, Juncheng Jin, Zehui Qu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191072"
    },
    {
        "id": 15250,
        "title": "Application of BP neural network algorithm in visualization system of sports training management",
        "authors": "Yinghui Zhao",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-08116-w"
    },
    {
        "id": 15251,
        "title": "Zeroth Order GreedyLR: An Adaptive Learning Rate Scheduler for Deep Neural Network Training",
        "authors": "Shreyas Subramanian, Vignesh Ganapathiraman",
        "published": "2023-8-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/prml59573.2023.10348370"
    },
    {
        "id": 15252,
        "title": "Accelerating Deep Neural Network training for autonomous landing guidance via homotopy",
        "authors": "Yang Ni, Binfeng Pan, Pablo Gómez Pérez",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.actaastro.2023.08.029"
    },
    {
        "id": 15253,
        "title": "An Incremental Gray-Box Physical Adversarial Attack on Neural Network Training",
        "authors": "Rabiah Al-qudah, Moayad Aloqaily, Bassem Ouni, Mohsen Guizani, Thierry Lestable",
        "published": "2023-5-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278837"
    },
    {
        "id": 15254,
        "title": "Subnetwork-To-Go: Elastic Neural Network with Dynamic Training and Customizable Inference",
        "authors": "Kai Li, Yi Luo",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446225"
    },
    {
        "id": 15255,
        "title": "A competitive learning scheme for deep neural network pattern classifier training",
        "authors": "Senjing Zheng, Feiying Lan, Marco Castellani",
        "published": "2023-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.asoc.2023.110662"
    },
    {
        "id": 15256,
        "title": "Bayesian optimization for sparse neural networks with trainable activation functions",
        "authors": "Mohamed Fakhfakh, Lotfi Chaari",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tpami.2024.3387073"
    },
    {
        "id": 15257,
        "title": "Detection of Malignant and Benign Skin Lesions using the Influence of Activation Function and Accuracy Analysis in Densely Connected Convolutional Network Compared over Convolutional Neural Network",
        "authors": "T. Nivyashree, P. V. Pramila",
        "published": "2023-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccebs58601.2023.10448550"
    },
    {
        "id": 15258,
        "title": "Convolutional Neural Network Activation Function Performance on Image Recognition of The Batak Script",
        "authors": "Abdul Muis, Elviawaty Muisa Zamzami, Erna Budhiarti Nababan",
        "published": "2024-1-1",
        "citations": 0,
        "abstract": "Deep Learning is a sub-set of Machine learning, Deep Learning is widely used to solve problems in various fields. One of the popular deep learning architectures is The Convolutional Neural Network (CNN), CNN has a layer that transforms feature extraction automatically so it is widely used in image recognition. However, CNN's performance using the tanh function is still relatively low, therefore it is necessary to select the right activation function to improve accuracy performance. This study analyzes the use of the activation function in image recognition of the Batak script. The result of this study is that the CNN model using the ReLU and eLU functions produces the highest accuracy compared to the CNN model using the tanh function. The CNN model using eLU produces the best accuracy performance in the training process, which is 99.71% with an error value of 0.0108. Meanwhile, in the testing process, the highest accuracy value is generated by the CNN Model using the ReLU function with an accuracy of 94.11%, an error value of 0.3282, a precision value of 0.9411, a recall of 0.9411, and an f1-score of 0.9416.",
        "keywords": "",
        "link": "http://dx.doi.org/10.33395/sinkron.v9i1.13192"
    },
    {
        "id": 15259,
        "title": "Data Augmentation and Fine Tuning of Convolutional Neural Network during Training for Person Re-Identification in Video Surveillance Systems",
        "authors": "S. Ye, R. Bohush, H. Chen, S. Ihnatsyeva, S. V. Ablameyko",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3103/s1060992x23040124"
    },
    {
        "id": 15260,
        "title": "Memristive Complex Functions for Design of Deep Neural Network",
        "authors": "Ivan Kipelkin, Svetlana Gerasimova, Tatiana Levanova, Nikolay Gromov",
        "published": "2023-9-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dcna59899.2023.10290538"
    },
    {
        "id": 15261,
        "title": "Latent Inspector: An Interactive Tool for Probing Neural Network Behaviors Through Arbitrary Latent Activation",
        "authors": "Daniel Geißler, Bo Zhou, Paul Lukowicz",
        "published": "2023-8",
        "citations": 0,
        "abstract": "This work presents an active software instrument allowing deep learning architects to interactively inspect neural network models' output behavior from user-manipulated values in any latent layer. Latent Inspector offers multiple dimension reduction techniques to visualize the model's high dimensional latent layer output in human-perceptible, two-dimensional plots. The system is implemented with Node.js front end for interactive user input and Python back end for interacting with the model. By utilizing a general and modular architecture, our proposed solution dynamically adapts to a versatile range of models and data structures. Compared to already existing tools, our asynchronous approach of separating the training process from the inspection offers additional possibilities, such as interactive data generation, by actively working with the model instead of visualizing training logs. Overall, Latent Inspector demonstrates the possibilities as well as the appearing limits for providing a generalized, tool-based concept for enhancing model insight in terms of explainable and transparent AI.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/832"
    },
    {
        "id": 15262,
        "title": "Integrating modeled environmental variability into neural network training for underwater source localization",
        "authors": "Pedro Diniz, Rogério Calazan",
        "published": "2023-6-1",
        "citations": 0,
        "abstract": "Supervised machine learning (ML) is a powerful tool that has been applied to many fields of underwater acoustics, including acoustic inversion. ML algorithms depend on the existence of extensive labeled datasets, which are difficult to obtain for the task of underwater source localization. A feed-forward neural network (FNN) trained on imbalanced or biased data may end up suffering from a problem analogous to model mismatch in matched field processing (MFP), that is, producing incorrect results due to a difference between the environment sampled by the training data and the actual environment. To overcome this issue, physical and numerical propagation models can act as data augmentation tools to compensate for the lack of comprehensive acoustic data. This paper examines how modeled data can be effectively used for training FNNs. Mismatch tests compare the output from a FNN and MFP and show that the network becomes more robust to various kinds of mismatches when trained on diverse environments. A systematic analysis of how the training dataset's variability impacts a FNN's localization performance on experimental data is carried out. Results show that networks trained with synthetic data achieve better and more robust performance than regular MFP when environment variability is taken into account.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1121/10.0019632"
    },
    {
        "id": 15263,
        "title": "Neural Network Models with Integrated Training and Adaptation For Nonlinear Acoustic System Identification",
        "authors": "Svantje Voit, Gerald Enzner",
        "published": "2023-6-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10095123"
    },
    {
        "id": 15264,
        "title": "Bitrate-Performance Optimized Model Training for the Neural Network Coding (NNC) Standard",
        "authors": "Paul Haase, Jonathan Pfaff, Heiko Schwarz, Detlev Marpe, Thomas Wiegand",
        "published": "2023-10-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icip49359.2023.10222133"
    },
    {
        "id": 15265,
        "title": "FENet",
        "authors": "Prajwal Panzade, Daniel Takabi",
        "published": "2023-4-26",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3579987.3586566"
    },
    {
        "id": 15266,
        "title": "Uninorm-like parametric activation functions for human-understandable neural models",
        "authors": "Orsolya Csiszár, Luca Sára Pusztaházi, Lehel Dénes-Fazakas, Michael S. Gashler, Vladik Kreinovich, Gábor Csiszár",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.knosys.2022.110095"
    },
    {
        "id": 15267,
        "title": "Retracted: Application of Neural Network Sample Training Algorithm in Regional Economic Management",
        "authors": "",
        "published": "2023-8-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9780809"
    },
    {
        "id": 15268,
        "title": "Ensuring Trustworthy Neural Network Training via Blockchain",
        "authors": "Edgar Navarro, Kyle J. Standing, Gaby G. Dagher, Tim Andersen",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cogmi58952.2023.00015"
    },
    {
        "id": 15269,
        "title": "Pre-Processing Training Data Improves Accuracy and Generalisability of Convolutional Neural Network Based Landscape Semantic Segmentation",
        "authors": "Andrew Clark, Stuart Phinn, Peter Scarth",
        "published": "2023-6-21",
        "citations": 1,
        "abstract": "Data pre-processing for developing a generalised land use and land cover (LULC) deep learning model using earth observation data is important for the classification of a different date and/or sensor. However, it is unclear how to approach deep learning segmentation problems in earth observation data. In this paper, we trialled different methods of data preparation for Convolutional Neural Network (CNN) training and semantic segmentation of LULC features within aerial photography over the Wet Tropics and Atherton Tablelands, Queensland, Australia. This was conducted by trialling and ranking various training patch selection sampling strategies, patch and batch sizes, data augmentations and scaling and inference strategies. Our results showed: a stratified random sampling approach for producing training patches counteracted class imbalances; a smaller number of larger patches (small batch size) improves model accuracy; data augmentations and scaling are imperative in creating a generalised model able to accurately classify LULC features in imagery from a different date and sensor; and producing the output classification by averaging multiple grids of patches and three rotated versions of each patch produced a more accurate and aesthetic result. Combining the findings from the trials, we fully trained five models on the 2018 training image and applied the model to the 2015 test image. The output LULC classifications achieved an average kappa of 0.84, user accuracy of 0.81, and producer accuracy of 0.87. Future research using CNNs and earth observation data should implement the findings of this project to increase LULC model accuracy and transferability.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/land12071268"
    },
    {
        "id": 15270,
        "title": "Modified convolutional neural network for lung cancer detection: Improved cat swarm-based optimal training",
        "authors": "Vikul J. Pawar, P. Premchand",
        "published": "2023-4-5",
        "citations": 2,
        "abstract": "Lung cancer is the most lethal and severe illness in existence. However, lung cancer patients may live longer if they receive early detection and treatment. In the medical field, the best imaging technique is CT scan imaging as it is more complex for doctors to identify cancer and interpret from CT scan images. Consequently, the computer-aided diagnosis (CAD) is more useful for doctors to find out cancerous nodules. To identify lung cancer, a number of CAD techniques utilising machine learning (ML) and image processing are used nowadays. The goal of this study is to present a novel method for detecting lung cancer that entails four main steps: (i) Pre-processing, (ii) Segmentation, (iii) Feature extraction, and (iv) Classification. ”The input image is first put through a pre-processing step in which the CLAHE model is used to pre-process the image. The segmentation phase of the pre-processed images is then initiated, and it makes use of a modified Level set segmentation method. The retrieved features from the segmented images include statistical features, colour features, and texture features (GLCM, GLRM, and LBP). The Layer Fused Conventional Neural Network (LF-CNN) is then utilised to classify these features in the end. Particularly, layer-wise modification is carried out, and along with that, the LF-CNN is trained by the Modified Cat swarm Optimization (MCSO) Algorithm via selecting optimal weights. The accepted scheme is then compared to the current models in terms of several metrics, including recall, FNR, MCC, FDR, Threat score, FPR, precision, FOR, accuracy, specificity, NPV, FMS, and sensitivity.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3233/web-221801"
    },
    {
        "id": 15271,
        "title": "On Performance of Marine Predators Algorithm in Training of Feed-Forward Neural Network for Identification of Nonlinear Systems",
        "authors": "Ceren Baştemur Baştemur Kaya",
        "published": "2023-8-20",
        "citations": 0,
        "abstract": "Artificial neural networks (ANNs) are used to solve many problems, such as modeling, identification, prediction, and classification. The success of ANN is directly related to the training process. Meta-heuristic algorithms are used extensively for ANN training. Within the scope of this study, a feed-forward artificial neural network (FFNN) is trained using the marine predators algorithm (MPA), one of the current meta-heuristic algorithms. Namely, this study is aimed to evaluate the performance of MPA in ANN training in detail. Identification/modeling of nonlinear systems is chosen as the problem. Six nonlinear systems are used in the applications. Some of them are static, and some are dynamic. Mean squared error (MSE) is utilized as the error metric. Effective training and testing results were obtained using MPA. The best mean error values obtained for six nonlinear systems are 2.3 × 10−4, 1.8 × 10−3, 1.0 × 10−4, 1.0 × 10−4, 1.2 × 10−5, and 2.5 × 10−4. The performance of MPA is compared with 16 meta-heuristic algorithms. The results have shown that the performance of MPA is better than other algorithms in ANN training for the identification of nonlinear systems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/sym15081610"
    },
    {
        "id": 15272,
        "title": "A Formal Characterization of Activation Functions in Deep Neural Networks",
        "authors": "Massi Amrouche, Dušan M. Stipanović",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3187538"
    },
    {
        "id": 15273,
        "title": "RAU: Novel Activation Function for Deep Learning Neural Network",
        "authors": "Et al. Prajesha T M",
        "published": "2023-11-2",
        "citations": 0,
        "abstract": "In Deep learning neural networks (DNNs) activation functions perform a vital role. In each neuron activation function is responsible for generating output signals from given input signals. Hence, activation function is one of the factors that influence the performance of DNN. A novel activation unit RAU (Reciprocal activation unit) is proposed in this paper. Most of the popular algorithms given more importance to positive signals, but proposed method handles the negative and positive inputs equally. The proposed RAU tested with both multiclassification and binary classification datasets. Iris flower and Wisconsin Breast Cancer datasets are used for the analysis. In Breast cancer dataset RAU provides 99.25% and 97.08% accuracy for classification of train and test sets respectively. In Iris dataset RAU provides 99.05% and 97.78% accuracy for the classification of train and test sets. Analysis of the same datasets are performed with the existing activation functions- Sigmoid, RMAF, Swish, Tanh and ReLU. Results showed that RAU performed better than other activation functions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17762/ijritcc.v11i10.8477"
    },
    {
        "id": 15274,
        "title": "Full waveform inversion as training a neural network",
        "authors": "Wensheng Zhang, Zheng Chen",
        "published": "2023-6-1",
        "citations": 0,
        "abstract": "Abstract\nFull waveform inversion (FWI) is usually solved as a nonlinear least-squares problem to minimize the discrepancy between the recorded signal and the synthetic data by some gradient-based optimization methods. In this paper, we investigate acoustic FWI as training a neural network. We recast the time-domain difference scheme into a forward propagation process of vanilla recurrent neural network (RNN) and then find that the parameters of RNN coincide with the physical parameters in the wave equation. As a result, the FWI problem is resolved as training such a kind of neural network. Some stochastic optimization methods such as the Adam optimizer are applied to update the model parameters. We demonstrate our method numerically with three typical models including the benchmark Marmousi model. The numerical results show that the fast convergence and good velocity inversion result can be achieved.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1088/1402-4896/accbb6"
    },
    {
        "id": 15275,
        "title": "A Fault Detection Method of Photovoltaic Cells Based on BP Neural Network Training for Current Trend",
        "authors": "Yupeng Qiu, Kazutaka Itako",
        "published": "2023-8-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ickii58656.2023.10332612"
    },
    {
        "id": 15276,
        "title": "On Training Targets and Activation Functions for Deep Representation Learning in Text-Dependent Speaker Verification",
        "authors": "Achintya Kumar Sarkar, Zheng-Hua Tan",
        "published": "2023-7-17",
        "citations": 2,
        "abstract": "Deep representation learning has gained significant momentum in advancing text-dependent speaker verification (TD-SV) systems. When designing deep neural networks (DNN) for extracting bottleneck (BN) features, the key considerations include training targets, activation functions, and loss functions. In this paper, we systematically study the impact of these choices on the performance of TD-SV. For training targets, we consider speaker identity, time-contrastive learning (TCL), and auto-regressive prediction coding, with the first being supervised and the last two being self-supervised. Furthermore, we study a range of loss functions when speaker identity is used as the training target. With regard to activation functions, we study the widely used sigmoid function, rectified linear unit (ReLU), and Gaussian error linear unit (GELU). We experimentally show that GELU is able to reduce the error rates of TD-SV significantly compared to sigmoid, irrespective of the training target. Among the three training targets, TCL performs the best. Among the various loss functions, cross-entropy, joint-softmax, and focal loss functions outperform the others. Finally, the score-level fusion of different systems is also able to reduce the error rates. To evaluate the representation learning methods, experiments are conducted on the RedDots 2016 challenge database consisting of short utterances for TD-SV systems based on classic Gaussian mixture model-universal background model (GMM-UBM) and i-vector methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/acoustics5030042"
    },
    {
        "id": 15277,
        "title": "Fractional ordering of activation functions for neural networks: A case study on Texas wind turbine",
        "authors": "Bhukya Ramadevi, Venkata Ramana Kasi, Kishore Bingi",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.107308"
    },
    {
        "id": 15278,
        "title": "Influence of Learning Rate Hyper-Parameter and Early Stopping Training Technique in Training Vanilla Neural Network Model for Effective Signal Power Loss Prediction",
        "authors": "Virginia Chika Ebhota, Thokozani Shongwe",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.15866/irecap.v13i5.23606"
    },
    {
        "id": 15279,
        "title": "Examples for separable control Lyapunov functions and their neural network approximation",
        "authors": "Lars Grüne, Mario Sperl",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.02.004"
    },
    {
        "id": 15280,
        "title": "A novel parallel merge neural network with streams of spiking neural network and artificial neural network",
        "authors": "Jie Yang, Junhong Zhao",
        "published": "2023-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.119034"
    },
    {
        "id": 15281,
        "title": "Features of Preparing a Data Set for Training a Neural Network of an Object Tracking System in a Stream",
        "authors": "E. Berezin, E. Pikalov, K. Palaguta",
        "published": "2023-9-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/rusautocon58002.2023.10272741"
    },
    {
        "id": 15282,
        "title": "ARTIFICIAL NEURAL NETWORK TRAINING BASED ON PERFORMANCE AND RISKS ASSESSMENT DATA OF THE INVESTMENT IN DIGITAL ASSETS",
        "authors": "Bohdan Bebeshko",
        "published": "2023",
        "citations": 1,
        "abstract": "The problem of analyzing the results of training artificial neural networks based on data about the efficiency and risks of investing in digital assets, particularly in the context of managing the buying and selling process of cryptocurrencies, has been investigated. The approach for solving this problem is based on the application of game theory as the main principle for forming the architecture of the artificial neural network. Combining two fundamental theories - game theory and neural networks - allows the creation of intuitively understandable and effective intelligent information systems for decision support in various application areas, such as finance, economics, and resource management. Special attention is paid to considering fuzzy parameters and uncertainties in market conditions, reflecting the real circumstances of investing in cryptocurrencies and other digital assets. The article proposes a series of methods for training and adapting the artificial neural network within the developed approach, as well as recommendations for evaluating its effectiveness and stability. The possible areas of application and prospects for further development of this methodology in the context of the digital asset market have been analyzed. The application of the developed methodology for analyzing the results of artificial neural network training has been illustrated, and its high efficiency in predicting investment performance and risks in digital assets has been confirmed. The issues and limitations that may arise during the use of this methodology were highlighted, and possible ways to overcome and improve them have been proposed..",
        "keywords": "",
        "link": "http://dx.doi.org/10.28925/2663-4023.2023.19.135145"
    },
    {
        "id": 15283,
        "title": "An attention-enhanced neural network with distillation training for barcode detection",
        "authors": "Zijian Wang, Zhiyuan Su",
        "published": "2023-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2678979"
    },
    {
        "id": 15284,
        "title": "Neurocontroller Training with Using the Neural Network Emulating the Plant",
        "authors": "Aleksandr A. Voevoda, Victor I. Shipagin",
        "published": "2023-11-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/apeie59731.2023.10347651"
    },
    {
        "id": 15285,
        "title": "Converting Data for Spiking Neural Network Training",
        "authors": "Erik Sadovsky, Maros Jakubec, Roman Jarina",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140803"
    },
    {
        "id": 15286,
        "title": "Standardized motion detection and real time heart rate monitoring of aerobics training based on convolution neural network",
        "authors": "Wenying Chen, Min Li",
        "published": "2023-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ypmed.2023.107642"
    },
    {
        "id": 15287,
        "title": "Fast Heavy Inner Product Identification Between Weights and Inputs in Neural Network Training",
        "authors": "Lianke Qin, Saayan Mitra, Zhao Song, Yuanyuan Yang, Tianyi Zhou",
        "published": "2023-12-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigdata59044.2023.10386943"
    },
    {
        "id": 15288,
        "title": "Ensemble adversarial training based defense against adversarial attacks for machine learning-based intrusion detection system",
        "authors": "Muhammad Shahzad Haroon, Husnain Mansoor Ali",
        "published": "2023",
        "citations": 0,
        "abstract": "In this paper, a defence mechanism is proposed against adversarial attacks. The defence is based on an ensemble classifier that is adversarially trained. This is accomplished by generating adversarial attacks from four different attack methods, i.e., Jacobian-based saliency map attack (JSMA), projected gradient descent (PGD), momentum iterative method (MIM), and fast gradient signed method (FGSM). The adversarial examples are used to identify the robust machine-learning algorithms which eventually participate in the ensemble. The adversarial attacks are divided into seen and unseen attacks. To validate our work, the experiments are conducted using NSLKDD, UNSW-NB15 and CICIDS17 datasets. Grid search for the ensemble is used to optimise results. The parameter used for performance evaluations is accuracy, F1 score and AUC score. It is shown that an adversarially trained ensemble classifier produces better results.",
        "keywords": "",
        "link": "http://dx.doi.org/10.14311/nnw.2023.33.018"
    },
    {
        "id": 15289,
        "title": "Implementation of the SoftMax Activation for Reconfigurable Neural Network Hardware Accelerators",
        "authors": "Vladislav Shatravin, Dmitriy Shashev, Stanislav Shidlovskiy",
        "published": "2023-11-28",
        "citations": 0,
        "abstract": "In recent decades, machine-learning algorithms have been extensively utilized to tackle various complex tasks. To achieve the high performance and efficiency of these algorithms, various hardware accelerators are used. Typically, these devices are specialized for specific neural network architectures and activation functions. However, state-of-the-art complex autonomous and mobile systems may require different algorithms for different tasks. Reconfigurable accelerators can be used to resolve this problem. They possess the capability to support diverse neural network architectures and allow for significant alterations to the implemented model at runtime. Thus, a single device can be used to address entirely different tasks. Our research focuses on dynamically reconfigurable accelerators based on reconfigurable computing environments (RCE). To implement the required neural networks on such devices, their algorithms need to be adapted to the homogeneous structure of RCE. This article proposes the first implementation of the widely used SoftMax activation for hardware accelerators based on RCE. The implementation leverages spatial distribution and incorporates several optimizations to enhance its performance. The timing simulation of the proposed implementation on FPGA shows a high throughput of 1.12 Gbps at 23 MHz. The result is comparable to counterparts lacking reconfiguration capability. However, this flexibility comes at the expense of the increased consumption of logic elements.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app132312784"
    },
    {
        "id": 15290,
        "title": "NEURAL NETWORK DEVELOPMENT AND TRAINING FOR RECOGNIZING THE STATE OF A DRIVER",
        "authors": "Olga Schwetz, Bauyrzhan Smakanov, Levente Kovac, Gjorgi George",
        "published": "2023-4-28",
        "citations": 0,
        "abstract": "The topic of automated road safety support for all road users, the involvement of modern tools and advanced technologies to solve these problems is becoming more and more popular every year and does not lose its relevance. It was made a review of the currently existing works in this area, which showed the main advantages and disadvantages of the developments. It is noted the necessity to create a Kazakhstani automated system for monitoring the driver's condition, which will be available to the general public.\nThe purpose of this study is to develop a new model based on a neural network for recognizing the state of a person driving a car to identify a dangerous state of the driver. The paper deals with the following issues: the relevance of the study, the formalization of the recognition problem and the choice of neural network architecture, which most successfully solves the problem. Separate freeze frames extracted from the video sequence of observing the driver are used for a mathematical description of the problem, a recognition function is built to select three classes corresponding to the driver's state, and feature vectors are selected to correctly determine the condition. For modeling, the architecture of a convolutional neural network was chosen, which allows working with images and video data streams. As a result of the study, a neural network was built and trained to recognize the dangerous state of the driver, taking into account all the factors influencing the process of identifying such a state. The created neural network included two convolutional layers describing the position of the head and the degree of eye openness, and also added two fully connected layers to take into account additional parameters, such as time of a day, duration of the trip, and whether the driver is yawning. Evaluation of modeling accuracy and comparison of the new model with existing ones shows the novelty and practical significance of this work.",
        "keywords": "",
        "link": "http://dx.doi.org/10.52167/1609-1817-2023-125-2-186-195"
    },
    {
        "id": 15291,
        "title": "A modified weighted chimp optimization algorithm for training feed-forward neural network",
        "authors": "Eman A. Atta, Ahmed F. Ali, Ahmed A. Elshamy",
        "published": "2023-3-28",
        "citations": 1,
        "abstract": "Swarm intelligence algorithms (SI) have an excellent ability to search for the optimal solution and they are applying two mechanisms during the search. The first mechanism is exploration, to explore a vast area in the search space, and when they found a promising area they switch from the exploration to the exploitation mechanism. A good SI algorithm can balance the exploration and the exploitation mechanism. In this paper, we propose a modified version of the chimp optimization algorithm (ChOA) to train a feed-forward neural network (FNN). The proposed algorithm is called a modified weighted chimp optimization algorithm (MWChOA). The main drawback of the standard ChOA and the weighted chimp optimization algorithm (WChOA) is they can be trapped in local optima because most of the solutions update their positions based on the position of the four leader solutions in the population. In the proposed algorithm, we reduced the number of leader solutions from four to three, and we found that reducing the number of leader solutions enhances the search and increases the exploration phase in the proposed algorithm, and avoids trapping in local optima. We test the proposed algorithm on the Eleven dataset and compare it against 16 SI algorithms. The results show that the proposed algorithm can achieve success to train the FNN when compare to the other SI algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1371/journal.pone.0282514"
    },
    {
        "id": 15292,
        "title": "Statistical Characterization of Attention Effects on the Contrast Tuning Functions Of Neuronal Populations of a Convolutional Neural Network",
        "authors": "Sudhanshu Srivastava, Miguel P. Eckstein",
        "published": "2023-8-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1167/jov.23.9.5941"
    },
    {
        "id": 15293,
        "title": "A Novel Target Value Standardization Method Based on Cumulative Distribution Functions for Training Artificial Neural Networks",
        "authors": "Wai Meng Kwok, George Streftaris, Sarat Chandra Dass",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscaie57739.2023.10165439"
    },
    {
        "id": 15294,
        "title": "An Efficient Approach for Training Time Minimization in Distributed Split Neural Network",
        "authors": "Eigo Yamamoto, Kimihiro Mizutani",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gcce59613.2023.10315304"
    },
    {
        "id": 15295,
        "title": "Patch-wise Training to Improve Convolutional Neural Network Synthetic Upscaling of Computational Fluid Dynamics Simulations",
        "authors": "John Romano, Alec C. Brodeur, Oktay Baysal",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-1804"
    },
    {
        "id": 15296,
        "title": "A Novel Normalization Method of Transient Electromagnetic Data for Efficient Neural Network Training",
        "authors": "S. He, Y. Wang, H. Cai, A.V. Christiansen, M.R. Asif",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3997/2214-4609.202320125"
    },
    {
        "id": 15297,
        "title": "Retracted: Analysis of Body Behavior Characteristics after Sports Training Based on Convolution Neural Network",
        "authors": "",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1155/2023/9852094"
    },
    {
        "id": 15298,
        "title": "HIN-RNN: A Graph Representation Learning Neural Network for Fraudster Group Detection With No Handcrafted Features",
        "authors": "Saeedreza Shehnepoor, Roberto Togneri, Wei Liu, Mohammed Bennamoun",
        "published": "2023-8",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2021.3123876"
    },
    {
        "id": 15299,
        "title": "Physics-informed neural wavefields with Gabor basis functions",
        "authors": "Tariq Alkhalifah, Xinquan Huang",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106286"
    },
    {
        "id": 15300,
        "title": "Autonomous neural network activation during religious worship experiences using heart rate variability measurements",
        "authors": "Yoshija Walter, Andreas Altorfer",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1080/2153599x.2023.2217238"
    },
    {
        "id": 15301,
        "title": "A new statistical training algorithm for a single multiplicative neuron model artificial neural network",
        "authors": "Hasan Huseyin Gul, Erol Egrioglu, Eren Bas",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s41066-024-00456-8"
    },
    {
        "id": 15302,
        "title": "Neural Network Predictions of Atomic Form Factors and Incoherent Scattering Functions",
        "authors": "B. Mohammedi, H. Benkharfia, B. Beladel, N. Mellel, K. Bessine, N. Moulai",
        "published": "2023-11-23",
        "citations": 0,
        "abstract": "In order to predict atomic form factors and incoherent scattering functions which are used to calculate the coherent and incoherent total scattering cross sections, a technique based on artificial neural networks of the multilayer type was implemented. In this context, two neural models have been developed and compared with those in the literature. This study revealed both the accuracy of the results obtained and the effectiveness of the designed model. The mean relative error for the least estimated property does not exceed 16.5 %. The software realized in this way give a prediction of the above parameters for the input variables Z: Atomic number, x: sin(ϑ/2)/λ and E: Photon energy, and it provides users with flexibility for prediction. The advantages of this technique lie in its very fast handling, due to its ease of use, and in the two integrated networks, which it guarantees for a variety of input parameters such as atomic number, photon energy, and momentum transfer variable.",
        "keywords": "",
        "link": "http://dx.doi.org/10.55981/aij.2023.1275"
    },
    {
        "id": 15303,
        "title": "Consumer Segmentation Based on Multi-layer Feedforward Neural Network with LeakyReLU Activation Function and AdaMod Optimizer",
        "authors": "Yanming Chen, Huanwen Zheng",
        "published": "2023-8-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icbase59196.2023.10303166"
    },
    {
        "id": 15304,
        "title": "Enhancing Convolutional Neural Network Performance through Optimized Sigmoid Activation Function Modeling",
        "authors": "Ye-Rim Youn, Jin-Keun Hong",
        "published": "2023-10-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.47116/apjcri.2023.10.05"
    }
]