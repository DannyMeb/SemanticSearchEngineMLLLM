[
    {
        "id": 20271,
        "title": "Target Binding and Sequence Prediction With LSTMs",
        "authors": "Michael Teti, Rachel StClair, Mirjana Pavlovic, Elan Barenholtz, William Hahn",
        "published": "No Date",
        "citations": 0,
        "abstract": "Deep recurrent neural networks (DRNNs) have recently demonstrated strong performance in sequential data analysis, such as natural language processing. These capabilities make them a promising tool for inferential analysis of sequentially structured bioinformatics data as well. Here, we assessed the ability of Long Short-Term Memory (LSTM) networks, a class of DRNNs, to predict properties of proteins based on their primary structures. The proposed architecture is trained and tested on two different datasets to predict whether a given sequence falls into a certain class or not. The first dataset, directly imported from Uniprot, was used to train the network on whether a given protein contained or did not contain a conserved sequence (homeodomain), and the second dataset, derived by literature mining, was used to train a network on whether a given protein binds or doesn't bind to Artemisinin, a drug typically used to treat malaria. In each case, the model was able to differentiate between the two different classes of sequences it was given with high accuracy, illustrating successful learning and generalization. Upon completion of training, an ROC curve was created using the homeodomain and artemisinin validation datasets. The AUC of these datasets was 0.80 and 0.87 respectively, further indicating the models' effectiveness. Furthermore, using these trained models, it was possible to derive a protocol for sequence detection of homeodomain and binding motif, which are well-documented in literature, and a known Artemisinin binding site, respectively [1-3]. Along with these contributions, we developed a python API to directly connect to Uniprot data sourcing, train deep neural networks on this primary sequence data using TensorFlow, and uniquely visualize the results of this analysis. Such an approach has the potential to drastically increase accuracy and reduce computational time and, current major limitations in informatics, from inquiry to discovery in protein function research",
        "link": "http://dx.doi.org/10.1101/504415"
    },
    {
        "id": 20272,
        "title": "Temperature Prediction at FAST using LSTMs",
        "authors": "Silvia Peiro, Jinhao Ruan",
        "published": "2022-7-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2172/1879659"
    },
    {
        "id": 20273,
        "title": "Toward Expressive Speech Translation: A Unified Sequence-to-Sequence LSTMs Approach for Translating Words and Emphasis",
        "authors": "Quoc Truong Do, Sakriani Sakti, Satoshi Nakamura",
        "published": "2017-8-20",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2017-896"
    },
    {
        "id": 20274,
        "title": "Multiple Path Prediction for Traffic Scenes using LSTMs and Mixture Density Models",
        "authors": "Jaime Fernandez, Suzanne Little, Noel O’connor",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009412204810488"
    },
    {
        "id": 20275,
        "title": "Multiple Path Prediction for Traffic Scenes using LSTMs and Mixture Density Models",
        "authors": "Jaime Fernandez, Suzanne Little, Noel O’connor",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0009412200002550"
    },
    {
        "id": 20276,
        "title": "Investigating Speech Features for Continuous Turn-Taking Prediction Using LSTMs",
        "authors": "Matthew Roddy, Gabriel Skantze, Naomi Harte",
        "published": "2018-9-2",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2018-2124"
    },
    {
        "id": 20277,
        "title": "Performance Analysis of LSTMs for Daily Individual EV Charging Behavior Prediction",
        "authors": "Ahmed Shaharyar Khwaja, Bala Venkatesh, Alagan Anpalagan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In this paper, we evaluate and analyze the performance of long short-term memory networks (LSTMs) for individual electric vehicle (EV) charging behavior prediction over the next day. The charging behavior consists of the charging duration level within a certain upper and lower range, the time slots in which charging will take place, the number of times charging will take place in each time slot, and whetherthenextdaywillbeachargingdayornot.Unlikeexistingwork,weevaluatethebehaviorprediction performance for increasing resolutions of charging duration levels and charging time slots, using varying lengths of training data. The performance of the proposed approach is validated using real EV charging data, and comparison with other machine learning methods shows its generally superior prediction accuracy for all resolutions. We show that the best performance is achieved when around 8-10 months of data are used as training data. It is also shown that although the performance of the LSTMs degrades with increasing resolution, the performance for charging time slot prediction is affected less compared to that for charging duration prediction. We further propose, analyze and evaluate a new technique that improves the charging duration prediction performance.</p>",
        "link": "http://dx.doi.org/10.32920/21408570"
    },
    {
        "id": 20278,
        "title": "Performance Analysis of LSTMs for Daily Individual EV Charging Behavior Prediction",
        "authors": "Ahmed Shaharyar Khwaja, Bala Venkatesh, Alagan Anpalagan",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>In this paper, we evaluate and analyze the performance of long short-term memory networks (LSTMs) for individual electric vehicle (EV) charging behavior prediction over the next day. The charging behavior consists of the charging duration level within a certain upper and lower range, the time slots in which charging will take place, the number of times charging will take place in each time slot, and whetherthenextdaywillbeachargingdayornot.Unlikeexistingwork,weevaluatethebehaviorprediction performance for increasing resolutions of charging duration levels and charging time slots, using varying lengths of training data. The performance of the proposed approach is validated using real EV charging data, and comparison with other machine learning methods shows its generally superior prediction accuracy for all resolutions. We show that the best performance is achieved when around 8-10 months of data are used as training data. It is also shown that although the performance of the LSTMs degrades with increasing resolution, the performance for charging time slot prediction is affected less compared to that for charging duration prediction. We further propose, analyze and evaluate a new technique that improves the charging duration prediction performance.</p>",
        "link": "http://dx.doi.org/10.32920/21408570.v1"
    },
    {
        "id": 20279,
        "title": "Learning and Evaluation Methodologies for Polyphonic Music Sequence Prediction With LSTMs",
        "authors": "Adrien Ycart, Emmanouil Benetos",
        "published": "2020",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/taslp.2020.2987130"
    },
    {
        "id": 20280,
        "title": "Multi-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver based LSTMs",
        "authors": "Nachiket Deo, Mohan M. Trivedi",
        "published": "2018-6",
        "citations": 285,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ivs.2018.8500493"
    },
    {
        "id": 20281,
        "title": "Salinity Time Series Prediction Based on LSTMs Neual Network",
        "authors": "Xingguo Yang, Ruijing Zhang",
        "published": "2019-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icectt.2019.00048"
    },
    {
        "id": 20282,
        "title": "Evolving Deep CNN-LSTMs for Inventory Time Series Prediction",
        "authors": "Ning Xue, Isaac Triguero, Grazziela P. Figueredo, Dario Landa-Silva",
        "published": "2019-6",
        "citations": 33,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec.2019.8789957"
    },
    {
        "id": 20283,
        "title": "Entity aware sequence to sequence learning using LSTMs for estimation of groundwater contamination release history and transport parameters",
        "authors": "Aatish Anshuman, T.I. Eldho",
        "published": "2022-5",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.jhydrol.2022.127662"
    },
    {
        "id": 20284,
        "title": "Analysis and Prediction of Air Pollutant Indices using Bidirectional-Convolutional LSTMs",
        "authors": "Georgios Karampelas, Dionysios N. Sotiropoulos",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisa56318.2022.9904392"
    },
    {
        "id": 20285,
        "title": "Temperature Prediction in Microgrids Using LSTMs: A Case Study",
        "authors": "Ayda Demir, Luis Felipe Gutierrez, Stephen Bayne, Argenis Bilbao",
        "published": "2022-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/compsac54236.2022.00195"
    },
    {
        "id": 20286,
        "title": "Cubic LSTMs for Video Prediction",
        "authors": "Hehe Fan, Linchao Zhu, Yi Yang",
        "published": "2019-7-17",
        "citations": 28,
        "abstract": "Predicting future frames in videos has become a promising direction of research for both computer vision and robot learning communities. The core of this problem involves moving object capture and future motion prediction. While object capture specifies which objects are moving in videos, motion prediction describes their future dynamics. Motivated by this analysis, we propose a Cubic Long Short-Term Memory (CubicLSTM) unit for video prediction. CubicLSTM consists of three branches, i.e., a spatial branch for capturing moving objects, a temporal branch for processing motions, and an output branch for combining the first two branches to generate predicted frames. Stacking multiple CubicLSTM units along the spatial branch and output branch, and then evolving along the temporal branch can form a cubic recurrent neural network (CubicRNN). Experiment shows that CubicRNN produces more accurate video predictions than prior methods on both synthetic and real-world datasets.",
        "link": "http://dx.doi.org/10.1609/aaai.v33i01.33018263"
    },
    {
        "id": 20287,
        "title": "Child-Sum (N2e2n)Tree-Lstms: An Interactive Child-Sum Tree-Lstms to Extract Biomedical Event",
        "authors": "Lei Wang, Han Cao, Liu Yuan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4610636"
    },
    {
        "id": 20288,
        "title": "Financial price prediction based on CEEMD and multi-channel LSTMs",
        "authors": "Yang Liu, Ziqian Zeng, Ruikun Li",
        "published": "2022-11-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1117/12.2659983"
    },
    {
        "id": 20289,
        "title": "OC-0775 Comprehensive proton dose prediction with Bayesian LSTMs",
        "authors": "L. Voss, A. Neishabouri, T. Ortkamp, N. Wahl",
        "published": "2023-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/s0167-8140(23)08716-9"
    },
    {
        "id": 20290,
        "title": "Attention-Based Word Vector Prediction with LSTMs and its Application to the OOV Problem in ASR",
        "authors": "Alejandro Coucheiro-Limeres, Fernando Fernández-Martínez, Rubén San-Segundo, Javier Ferreiros-López",
        "published": "2019-9-15",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2019-2347"
    },
    {
        "id": 20291,
        "title": "Performance Analysis of LSTMs for Daily Individual EV Charging Behavior Prediction",
        "authors": "Ahmed S. Khwaja, Bala Venkatesh, Alagan Anpalagan",
        "published": "2021",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3128491"
    },
    {
        "id": 20292,
        "title": "Stock Price Prediction using Various LSTMs",
        "authors": "Satyala Narayana, Kokkiligadda Stuthi Keerthi, Anneboina Srinivasulu, Vampugani Keerthika, Teki Veera Venkata Satya Prakash",
        "published": "2023-3-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icscds56580.2023.10104913"
    },
    {
        "id": 20293,
        "title": "Automated Lip Reading using Word Level Sentence Prediction based on LSTMs and Attention",
        "authors": "Harshit Singhania",
        "published": "2020-4-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22214/ijraset.2020.4025"
    },
    {
        "id": 20294,
        "title": "Using CNN-LSTMs and Transformer RNNs for COVID19 Impact Prediction",
        "authors": "Vinayak Ashok Bharadi, Sujata Alegavi, Bhushan Nemade",
        "published": "2023-9-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/nmitcon58196.2023.10275886"
    },
    {
        "id": 20295,
        "title": "Wellhead Compressor Failure Prediction Using Attention-based Bidirectional LSTMs with Data Reduction Techniques",
        "authors": "Wirasak Chomphu, Boonserm Kijsirikul",
        "published": "2020-3-9",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3388142.3388154"
    },
    {
        "id": 20296,
        "title": "Enhancing the chimp optimization algorithm to evolve deep LSTMs for accounting profit prediction using adaptive pair reinforced technique",
        "authors": "Chengchen Yang, Tong Wu, Lingzhuo Zeng",
        "published": "2023-11-5",
        "citations": 0,
        "abstract": "AbstractAccurately predicting accounting profit (PAP) plays a vital role in financial analysis and decision-making for businesses. The analysis of a business’s financial achievements offers significant insights and aids in the formulation of strategic plans. This research paper focuses on improving the chimp optimization algorithm (CHOA) to evolve deep long short-term memory (LSTM) models specifically for financial accounting profit prediction. The proposed hybrid approach combines CHOA’s global search capabilities with deep LSTMs’ sequential modeling abilities, considering both the global and temporal aspects of financial data to enhance prediction accuracy. To overcome CHOA’s tendency to get stuck in local minima, a novel updating technique called adaptive pair reinforced (APR) is introduced, resulting in APRCHOA. In addition to well-known conventional prediction models, this study develops five deep LSTM-based models, namely conventional deep LSTM, CHOA (deep LSTM-CHOA), adaptive reinforcement-based genetic algorithm (deep LSTM-ARGA), marine predator algorithm (deep LSTM-MPA), and adaptive reinforced whale optimization algorithm (deep LSTM-ARWOA). To comprehensively evaluate their effectiveness, the developed deep LSTM-APRCHOA models are assessed using statistical error metrics, namely root mean square error (RMSE), bias, and Nash–Sutcliffe efficiency (NSEF). In the validation set, at a lead time of 1 h, the NSEF values for LSTM, LSTM-MPA, LSTM-CHOA, LSTM-ARGA, LSTM-ARWOA, and deep LSTM-APRCHOA were 0.9100, 0.9312, 0.9350, 0.9650, 0.9722, and 0.9801, respectively. The results indicate that among these models, deep LSTM-APRCHOA demonstrates the highest accuracy for financial profit prediction.",
        "link": "http://dx.doi.org/10.1007/s12530-023-09547-4"
    },
    {
        "id": 20297,
        "title": "Child_Sum EATree-LSTMs: Enhanced Attentive Child_Sum Tree-LSTMs for Biomedical Event Extraction",
        "authors": "Lei Wang, Han Cao, Liu Yuan, Xiaoxu Guo, Yachao Cui",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackground\n The tree-structured neural network can deeply extract lexical representations of sentence syntactic structure. Some studies have utilized Recursive Neural Network to detect event triggers.\nMethods\n We incorporate the attention mechanism into Child-Sum Tree-LSTMs for the task of biomedical event triggers. Based on the previous research, we incorporated attention mechanism into Child-Sum Tree-LSTMs to assign an attention weight for the adjacent nodes to detect the biomedical event trigger words. The existing shallow syntactic dependencies in Child-Sum Tree-LSTMs ignore the deep syntactic dependencies. To enhance the effect of attention mechanism, we integrate the enhanced attention mechanism into the Child-Sum Tree-LSTMs model using the deep syntactic dependencies.\nResults\n Our proposed model integrating an enhanced the attention mechanism in Tree-LSTM on MLEE and BioNLP’09 both show best performance. The model also achieves the better performance on almost all of the complex event categories on the test set of BioNLP’09/11/13.\nConclusion\n We evaluate the model performance on the MLEE and BioNLP datasets, and the experimental results demonstrate the advantage of enhanced attention to detect biomedical event trigger words.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2572039/v1"
    },
    {
        "id": 20298,
        "title": "Interpretable Emoji Prediction via Label-Wise Attention LSTMs",
        "authors": "Francesco Barbieri, Luis Espinosa-Anke, Jose Camacho-Collados, Steven Schockaert, Horacio Saggion",
        "published": "2018",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d18-1508"
    },
    {
        "id": 20299,
        "title": "Modeling Spaced Repetition with LSTMs",
        "authors": "Jakub Pokrywka, Marcin Biedalak, Filip Graliński, Krzysztof Biedalak",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011724000003470"
    },
    {
        "id": 20300,
        "title": "Relation classification via sequence features and bi-directional LSTMs",
        "authors": "Yuanfang Ren, Chong Teng, Fei Li, Bo Chen, Donghong Ji",
        "published": "2017-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11859-017-1278-6"
    },
    {
        "id": 20301,
        "title": "Child-Sum (N2E2N)Tree-LSTMs: An Interactive Child-Sum Tree-LSTMs to Extract Biomedical Event",
        "authors": "Lei Wang, Han Cao, Liu Yuan, Yachao Cui, Hongli Yu, Pengfei Sun",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nBackground\n LSTM has been presented to overcome the problem of the gradient explosion and explosion. Tree-LSTM could improve the parallel speed of LSTM, and incorporate relevant information from dependency or syntax trees. Tree-LSTM can update gate and memory vectors from multiple sub-units. Learning edge features can strengthen the expression ability of graph neural networks. However, the original Child-Sum Tree-LSTMs ignores edge features during aggregating the sub-nodes hidden states.\nMethods\n we propose an interaction mechanism that can alternately updating nodes and edges vectors, thus the model can learn the richer nodes vectors. The interaction mechanism attaches the node embedding to its connected link at the first stage. Next, it superimposes the updated edge into the parent node once more. Repeat the above steps from bottom to top. We present five strategies during the alternant renewal process. Meanwhile, we adopt two constituent parsers and two dependency parser to produce the diversified formats, and compare their performances in the experiment result.\nResults\n The proposed model obtains the best performance compared with other methods on the BioNLP’09 and MLEE corpuses.\nConclusion\n The experimental results confirm the effectiveness of the interactive mechanism. The parsing results have little impact on the final model performance, but different parsing formats have different results. CoNLL’2008 Dependencies show competitive and superior performance for each parser.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3293933/v1"
    },
    {
        "id": 20302,
        "title": "Relation Classification via LSTMs Based on Sequence and Tree Structure",
        "authors": "Yuanfei Dai, Wenzhong Guo, Xing Chen, Zuwen Zhang",
        "published": "2018",
        "citations": 12,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2018.2877934"
    },
    {
        "id": 20303,
        "title": "SA-LSTMs: A new advance prediction method of energy consumption in cement raw materials grinding system",
        "authors": "Gang Liu, Kun Wang, Xiaochen Hao, Zhipeng Zhang, Yantao Zhao, Qingquan Xu",
        "published": "2022-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.energy.2021.122768"
    },
    {
        "id": 20304,
        "title": "Vehicle Trajectory Prediction Using LSTMs With Spatial–Temporal Attention Mechanisms",
        "authors": "Lei Lin, Weizi Li, Huikun Bi, Lingqiao Qin",
        "published": "2022-3",
        "citations": 89,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mits.2021.3049404"
    },
    {
        "id": 20305,
        "title": "World Knowledge for Reading Comprehension: Rare Entity Prediction\n            with Hierarchical LSTMs Using External Descriptions",
        "authors": "Teng Long, Emmanuel Bengio, Ryan Lowe, Jackie Chi Kit Cheung, Doina Precup",
        "published": "2017",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/d17-1086"
    },
    {
        "id": 20306,
        "title": "Short-Term Sky Image Prediction using LSTMs from the Past Frames of Sky Images and Synthesized Images by Optical Flow Extrapolation1",
        "authors": "Takekazu Kato, Takaya Nakagawa",
        "published": "2020-9-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icce-taiwan49838.2020.9258039"
    },
    {
        "id": 20307,
        "title": "Child-Sum EATree-LSTMs: enhanced attentive Child-Sum Tree-LSTMs for biomedical event extraction",
        "authors": "Lei Wang, Han Cao, Liu Yuan, Xiaoxu Guo, Yachao Cui",
        "published": "2023-6-15",
        "citations": 2,
        "abstract": "Abstract\nBackground\nTree-structured neural networks have shown promise in extracting lexical representations of sentence syntactic structures, particularly in the detection of event triggers using recursive neural networks.\n\nMethods\nIn this study, we introduce an attention mechanism into Child-Sum Tree-LSTMs for the detection of biomedical event triggers. We incorporate previous researches on assigning attention weights to adjacent nodes and integrate this mechanism into Child-Sum Tree-LSTMs to improve the detection of event trigger words. We also address a limitation of shallow syntactic dependencies in Child-Sum Tree-LSTMs by integrating deep syntactic dependencies to enhance the effect of the attention mechanism.\n\nResults\nOur proposed model, which integrates an enhanced attention mechanism into Tree-LSTM, shows the best performance for the MLEE and BioNLP’09 datasets. Moreover, our model outperforms almost all complex event categories for the BioNLP’09/11/13 test set.\n\nConclusion\nWe evaluate the performance of our proposed model with the MLEE and BioNLP datasets and demonstrate the advantage of an enhanced attention mechanism in detecting biomedical event trigger words.\n",
        "link": "http://dx.doi.org/10.1186/s12859-023-05336-7"
    },
    {
        "id": 20308,
        "title": "Sentence-State LSTMs For Sequence-to-Sequence Learning",
        "authors": "Xuefeng Bai, Yafu Li, Zhirui Zhang, Mingzhou Xu, Boxing Chen, Weihua Luo, Derek Wong, Yue Zhang",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-88480-2_9"
    },
    {
        "id": 20309,
        "title": "LSTMs and Neural Attention Models for Blood Glucose Prediction: Comparative Experiments on Real and Synthetic Data",
        "authors": "Sadegh Mirshekarian, Hui Shen, Razvan Bunescu, Cindy Marling",
        "published": "2019-7",
        "citations": 28,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/embc.2019.8856940"
    },
    {
        "id": 20310,
        "title": "Implementation of SimpleRNN and LSTMs based prediction model for coronavirus disease (Covid-19)",
        "authors": " Priyanka, A Kumari, M Sood",
        "published": "2021-1-1",
        "citations": 3,
        "abstract": "Abstract\nDeep learning is a powerful technique which is inspired by the structure as well as processing power of the human brain. This technique uses deep neural network to perform complex tasks such as time series prediction, image classification, and cancer detection. In this research work, we used Covid-19 time series datasets and with the help of deep learning we built the model for prediction of Covid-19 cases. For the model building, we used two deep learning neural networks, Recurrent Neural Networks (RNN) and Long Short Term Memory Networks (LSTMs). We built a prediction model using RNN in the first instance and subsequently the second model was built using LSTMs. Out of these two neural networks, we got promising results from the model based on LSTMs with an overall accuracy of 98%. As the cases of Covid-19 are increasing day-by-day at a very high rate, we proposed these models using neural networks to help in predicting the future trends of Covid-19 confirmed, deaths and recovered cases.",
        "link": "http://dx.doi.org/10.1088/1757-899x/1022/1/012015"
    },
    {
        "id": 20311,
        "title": "Enlarging Forecast Horizon for Residential Load Prediction using Sequence-To-Sequence LSTM",
        "authors": "Abhishu Oza, Dhaval K Patel",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.36227/techrxiv.170792791.13616658/v1"
    },
    {
        "id": 20312,
        "title": "Cracking the black box of deep sequence-based protein-protein interaction prediction",
        "authors": "Judith Bernett, David B. Blumenthal, Markus List",
        "published": "2023",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14293/gof.23.40"
    },
    {
        "id": 20313,
        "title": "Customized Impression Prediction From Radiology Reports Using BERT and LSTMs",
        "authors": "Batuhan Gundogdu, Utku Pamuksuz, Jonathan H. Chung, Jessica M. Telleria, Peng Liu, Farrukh Khan, Paul J. Chang",
        "published": "2023-8",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tai.2021.3086435"
    },
    {
        "id": 20314,
        "title": "Combined Prediction Model of Gas Concentration Based on Indicators Dynamic Optimization and Bi-LSTMs",
        "authors": "Yujie Peng, Dazhao Song, Liming Qiu, Honglei Wang, Xueqiu He, Qiang Liu",
        "published": "2023-3-7",
        "citations": 5,
        "abstract": "In order to accurately predict the gas concentration, find out the gas abnormal emission in advance, and take effective measures to reduce the gas concentration in time, this paper analyzes multivariate monitoring data and proposes a new dynamic combined prediction method of gas concentration. Spearman’s rank correlation coefficient is applied for the dynamic optimization of prediction indicators. The time series and spatial topology features of the optimized indicators are extracted and input into the combined prediction model of gas concentration based on indicators dynamic optimization and Bi-LSTMs (Bi-directional Long Short-term Memory), which can predict the gas concentration for the next 30 min. The results show that the other gas concentration, temperature, and humidity indicators are strongly correlated with the gas concentration to be predicted, and Spearman’s rank correlation coefficient is up to 0.92 at most. The average R2 of predicted value and real value is 0.965, and the average prediction efficiency R for gas abnormal or normal emission is 79.9%. Compared with the other models, the proposed dynamic optimized indicators combined model is more accurate, and the missing alarm of gas abnormal emission is significantly alleviated, which greatly improves the early alarming accuracy. It can assist the safety monitoring personnel in decision making and has certain significance to improve the safety production efficiency of coal mines.",
        "link": "http://dx.doi.org/10.3390/s23062883"
    },
    {
        "id": 20315,
        "title": "Dimensionality reduction and ensemble of LSTMs for antimicrobial resistance prediction",
        "authors": "Àlvar Hernàndez-Carnerero, Miquel Sànchez-Marrè, Inmaculada Mora-Jiménez, Cristina Soguero-Ruiz, Sergio Martínez-Agüero, Joaquín Álvarez-Rodríguez",
        "published": "2023-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.artmed.2023.102508"
    },
    {
        "id": 20316,
        "title": "Human action prediction in collaborative environments based on shared-weight LSTMs with feature dimensionality reduction",
        "authors": "Tomislav Petković, Luka Petrović, Ivan Marković, Ivan Petrović",
        "published": "2022-9",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.asoc.2022.109245"
    },
    {
        "id": 20317,
        "title": "Impute Water Temperature in the Swiss River Network Using LSTMs",
        "authors": "Benjamin Fankhauser, Vidushi Bigler, Kaspar Riesen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012358100003654"
    },
    {
        "id": 20318,
        "title": "Cross-species regulatory sequence activity prediction",
        "authors": "David R. Kelley",
        "published": "No Date",
        "citations": 7,
        "abstract": "AbstractMachine learning algorithms trained to predict the regulatory activity of nucleic acid sequences have revealed principles of gene regulation and guided genetic variation analysis. While the human genome has been extensively annotated and studied, model organisms have been less explored. Model organism genomes offer both additional training sequences and unique annotations describing tissue and cell states unavailable in humans. Here, we develop a strategy to train deep convolutional neural networks simultaneously on multiple genomes and apply it to learn sequence predictors for large compendia of human and mouse data. Training on both genomes improves gene expression prediction accuracy on held out sequences. We further demonstrate a novel and powerful transfer learning approach to use mouse regulatory models to analyze human genetic variants associated with molecular phenotypes and disease. Together these techniques unleash thousands of non-human epigenetic and transcriptional profiles toward more effective investigation of how gene regulation affects human disease.",
        "link": "http://dx.doi.org/10.1101/660563"
    },
    {
        "id": 20319,
        "title": "Autistic Savants and Mathematical sequence prediction.",
        "authors": "Anil Kumar Bheemaiah",
        "published": "No Date",
        "citations": 0,
        "abstract": "In the third paper in a series of papers on autism savants, detection of giftedness and the use of mental arithmetic as an intervention in autism and a practice of metal wellness,  we describe the use of python scripts towards primality detection exercises, of both small primes and arbitrary sized numbers and several other exercises including sequence prediction, inspired by branch prediction architectures. Sequence prediction as a mental exercise, is used as infotainment and as a wellness exercise, and a possible intervention in ASD. Several prediction mechanisms inspired by data and prediction algorithms are described. Keywords: ASD, Autism Savants, Education  For The Gifted, primality detection, data mining, basket of associations, sequences, branch prediction, plotting graphs.",
        "link": "http://dx.doi.org/10.31234/osf.io/es293"
    },
    {
        "id": 20320,
        "title": "Dictionary Augmented Sequence-to-Sequence Neural Network for Grapheme to Phoneme Prediction",
        "authors": "Antoine Bruguier, Anton Bakhtin, Dravyansh Sharma",
        "published": "2018-9-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2018-2061"
    },
    {
        "id": 20321,
        "title": "Combining Residual Networks with LSTMs for Lipreading",
        "authors": "Themos Stafylakis, Georgios Tzimiropoulos",
        "published": "2017-8-20",
        "citations": 145,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2017-85"
    },
    {
        "id": 20322,
        "title": "Multistep Traffic Speed Prediction: A Sequence-to-Sequence Spatio-Temporal Attention Model",
        "authors": "Di Yang, Peng Wang, Lihong Yuan, Hong Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4564494"
    },
    {
        "id": 20323,
        "title": "Ensemble of LSTMs and Feature Selection for Human Action Prediction",
        "authors": "Tomislav Petković, Luka Petrović, Ivan Marković, Ivan Petrović",
        "published": "2022",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-95892-3_33"
    },
    {
        "id": 20324,
        "title": "Child-Sum (N2E2N)Tree-LSTMs: An interactive Child-Sum Tree-LSTMs to extract biomedical event",
        "authors": "Lei Wang, Han Cao, Liu Yuan",
        "published": "2024-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.sasc.2024.200075"
    },
    {
        "id": 20325,
        "title": "Sentiment Analysis with CNNs and LSTMs",
        "authors": "mohit Negi, Jatin Kaushik, Alok Dahiya, Deepak Kaushik",
        "published": "No Date",
        "citations": 0,
        "abstract": "In this paper we describe our attempt at producing a state-of-the-art senti- ment classifier using Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTMs) networks. Our sys- tem leverages a large amount of unlabeled data to pre-train word embeddings. We then use a subset of the unlabeled data to fine tune the embeddings using distant su- pervision. The final CNNs and LSTMs are trained on the SemEval-2017 Twitter dataset where the embeddings are fined tuned again. To boost performances we ensemble several CNNs and LSTMs to- gether. Our approach achieved first rank on all of the five English subtasks amongst 40 teams.",
        "link": "http://dx.doi.org/10.14293/s2199-1006.1.sor-.pp3potq.v1"
    },
    {
        "id": 20326,
        "title": "Pre-training with pseudo-labeling for regulatory sequence prediction",
        "authors": "Raphaël Mourad",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractPredicting molecular processes using deep learning is a promising approach to provide biological insights for non-coding SNPs identified in genome-wide association studies. However, most deep learning methods rely on supervised learning which requires DNA sequences associated with functional data, and whose amount is severely limited by the finite size of the human genome. Conversely, the amount of mammalian DNA sequences is growing exponentially due to ongoing large-scale sequencing projects, but in most cases without functional data. To alleviate the limitations of supervised learning, we propose a novel semi-supervised learning based on pseudo-labeling, which allows to explot unannotated DNA sequences from numerous genomes during model pre-training. The approach is very flexible and can be used to train any neural architecture including state-of-the-art models, and shows in certain situations strong predictive performance improvements compared to standard supervised learning in most cases.",
        "link": "http://dx.doi.org/10.1101/2023.12.21.572780"
    },
    {
        "id": 20327,
        "title": "DeepGOPlus: Improved protein function prediction from sequence",
        "authors": "Maxat Kulmanov, Robert Hoehndorf",
        "published": "No Date",
        "citations": 11,
        "abstract": "ABSTRACTProtein function prediction is one of the major tasks of bioinformatics that can help in wide range of biological problems such as understanding disease mechanisms or finding drug targets. Many methods are available for predicting protein functions from sequence based features, protein–protein interaction networks, protein structure or literature. However, other than sequence, most of the features are difficult to obtain or not available for many proteins thereby limiting their scope. Furthermore, the performance of sequence-based function prediction methods is often lower than methods that incorporate multiple features and predicting protein functions may require a lot of time.We developed a novel method for predicting protein functions from sequence alone which combines deep convolutional neural network (CNN) model with sequence similarity based predictions. Our CNN model scans the sequence for motifs which are predictive for protein functions and combines this with functions of similar proteins. We evaluate the performance of DeepGOPlus on the CAFA3 dataset and significantly improve the performance of predictions of biological processes and cellular components with Fmax of 0.47 and 0.70, respectively, using only the amino acid sequence of proteins as input. DeepGOPlus can annotate around 40 protein sequences per second, thereby making fast and accurate function predictions available for a wide range of proteins.",
        "link": "http://dx.doi.org/10.1101/615260"
    },
    {
        "id": 20328,
        "title": "Wei2GO: weighted sequence similarity-based protein function prediction",
        "authors": "Maarten J.M.F Reijnders",
        "published": "No Date",
        "citations": 4,
        "abstract": "AbstractBackgroundProtein function prediction is an important part of bioinformatics and genomics studies. There are many different predictors available, however most of these are in the form of web-servers instead of open-source locally installable versions. Such local versions are necessary to perform large scale genomics studies due to the presence of limitations imposed by web servers such as queues, prediction speed, and updatability of databases.MethodsThis paper describes Wei2GO: a weighted sequence similarity and python-based open-source protein function prediction software. It uses DIAMOND and HMMScan sequence alignment searches against the UniProtKB and Pfam databases respectively, transfers Gene Ontology terms from the reference protein to the query protein, and uses a weighing algorithm to calculate a score for the Gene Ontology annotations.ResultsWei2GO is compared against the Argot2 and Argot2.5 web servers, which use a similar concept, and DeepGOPlus which acts as a reference. Wei2GO shows an increase in performance according to precision and recall curves, Fmax scores, and Smin scores for biological process and molecular function ontologies. Computational time compared to Argot2 and Argot2.5 is decreased from several hours to several minutes.AvailabilityWei2GO is written in Python 3, and can be found at https://gitlab.com/mreijnders/Wei2GO",
        "link": "http://dx.doi.org/10.1101/2020.04.24.059501"
    },
    {
        "id": 20329,
        "title": "A Noval Sequence-to-Sequence Based Deep Learning Model for Satellite Cloud Image Time Series Prediction",
        "authors": "Jie Lian, Shixin Wu, Qin Zhao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4705284"
    },
    {
        "id": 20330,
        "title": "Reducing the Computational Complexity of Two-Dimensional LSTMs",
        "authors": "Bo Li, Tara N. Sainath",
        "published": "2017-8-20",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2017-1164"
    },
    {
        "id": 20331,
        "title": "MellisAI - An AI Generated Music Composer Using RNN-LSTMs",
        "authors": "N. Hari Kumar,  , P. S Ashwin, Haritha Ananthakrishnan",
        "published": "2020-2",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18178/ijmlc.2020.10.2.927"
    },
    {
        "id": 20332,
        "title": "Improved sequence-to-sequence ship trajectory prediction based on AIS",
        "authors": "Junfeng Yuan, Wenhao Fang, Jilin Zhang, Yuyu Yin, Jian Wan, Yinjie Wang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nDue to the high density of ships and the complex traffic environment in coastal areas, developing ship trajectory prediction methods is an imperative task for effective collision avoidance. The performance of most previous methods is limited by the issues of regional discrepancy of Automatic Identification System (AIS) data and singularity in trajectory feature extraction. To address these two issues, this paper proposes an improved ship trajectory prediction framework based on AIS. The proposed framework mainly consists of two parts: (1) A trajectory data preprocessing module including the extraction of trajectories based on time and ship speed for imputing missing values based on which AIS data from different regions is normalized to keep the time interval consistency. (2) A sequence-to-sequence ship trajectory prediction model based on double CNNs called DCNN. The encoder of DCNN is composed of two parallel structures, namely the global temporal convolution and local temporal convolution, to extract both long-term and short-term dependencies of the ship trajectories. The decoder is composed of RNN and its variants which are used to generate the predicted ship trajectory sequence. The experimental results show that the proposed framework can effectively predict the ship's trajectory, so as to predict the ship encounter in different regional traffic modes in advance, and help the ship to actively avoid collision.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3821423/v1"
    },
    {
        "id": 20333,
        "title": "Bike-Share Demand Prediction using Attention based Sequence to Sequence and Conditional Variational AutoEncoder",
        "authors": "Tomohiro Mimura, Shin Ishiguro, Satoshi Kawasaki, Yusuke Fukazawa",
        "published": "2019-11-5",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3356995.3364543"
    },
    {
        "id": 20334,
        "title": "Machine Learning in Finance: Towards Online Prediction of Loan Defaults Using Sequential Data with LSTMs",
        "authors": "V. A. Kandappan, A. G. Rekha",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-16-1696-9_5"
    },
    {
        "id": 20335,
        "title": "ThermoFinder: A sequence-based thermophilic proteins prediction framework",
        "authors": "Han Yu, Xiaozhou Luo",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractMotivationThermophilic proteins are important for academic research and industrial processes, and various computational methods have been developed to identify and screen them. However, their performance has been limited due to the lack of high-quality labeled data and efficient models for representing protein. Here, we proposed a novel sequence-based thermophilic proteins prediction framework, called ThermoFinder.ResultsIn this study, we demonstrated that ThermoFinder outperforms previous state-of-the-art tools on two benchmark datasets, and feature ablation experiments confirmed the effectiveness of our approach. Additionally, ThermoFinder exhibited exceptional performance and consistency across two newly constructed datasets, one of these was specifically constructed for the regression-based prediction of temperature optimum values directly derived from protein sequences. The feature importance analysis, using shapley additive explanations, further validated the advantages of ThermoFinder. We believe that ThermoFinder will be a valuable and comprehensive framework for predicting thermophilic proteins.",
        "link": "http://dx.doi.org/10.1101/2024.01.02.573852"
    },
    {
        "id": 20336,
        "title": "Wind power regression prediction based on stacked LSTMs with attention mechanisms for evaluating technological improvement effects of wind turbines",
        "authors": "Lingxing Kong, Kailong Liu, Deyi Fu, Boyong Liu, Jingkai Ma, Huini Sun, Shuang Bai",
        "published": "2023-7-2",
        "citations": 0,
        "abstract": "Accurately evaluating the technological improvement effects of wind turbines is crucial for wind farm operators. To this end, this paper proposes an innovative approach that employs a wind power regression model which leverages external environmental information to predict the output power of wind turbines. The effectiveness of technological improvements can be evaluated by comparing the predicted output power with the measured output power. In this paper, a model called stacked LSTM networks with attention mechanisms is designed. In the proposed model, the stacked LSTM networks are used to enhance the nonlinear fitting ability and capture deeper features of the input sequence. Furthermore, temporal attention mechanisms are employed to make the model focus on important time-series information of the data. In addition, a hierarchical attention mechanism is designed to explore the correlation among the outputs of the stacked LSTM networks and enrich the model’s output information. The experiments on the data from a wind farm show that the proposed method outperforms various wind power prediction benchmarks, achieving lower RMSE, MAE, and MAPE values of 142.82, 104.2, and 4.85%, respectively.",
        "link": "http://dx.doi.org/10.3233/jifs-230403"
    },
    {
        "id": 20337,
        "title": "A Multi-decoder Recurrent Network for Vessel Trajectory Prediction Using Multi-memory LSTMs",
        "authors": "Meng Chen, Chi Zhang, Tengteng Qu, Bo Chen, Chengqi Cheng, Haojiang Deng",
        "published": "2022-10-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icus55513.2022.9986985"
    },
    {
        "id": 20338,
        "title": "Sequence-to-Sequence Video Prediction by Learning Hierarchical Representations",
        "authors": "Kun Fan, Chungin Joung, Seungjun Baek",
        "published": "2020-11-23",
        "citations": 3,
        "abstract": "Video prediction which maps a sequence of past video frames into realistic future video frames is a challenging task because it is difficult to generate realistic frames and model the coherent relationship between consecutive video frames. In this paper, we propose a hierarchical sequence-to-sequence prediction approach to address this challenge. We present an end-to-end trainable architecture in which the frame generator automatically encodes input frames into different levels of latent Convolutional Neural Network (CNN) features, and then recursively generates future frames conditioned on the estimated hierarchical CNN features and previous prediction. Our design is intended to automatically learn hierarchical representations of video and their temporal dynamics. Convolutional Long Short-Term Memory (ConvLSTM) is used in combination with skip connections so as to separately capture the sequential structures of multiple levels of hierarchy of features. We adopt Scheduled Sampling for training our recurrent network in order to facilitate convergence and to produce high-quality sequence predictions. We evaluate our method on the Bouncing Balls, Moving MNIST, and KTH human action dataset, and report favorable results as compared to existing methods.",
        "link": "http://dx.doi.org/10.3390/app10228288"
    },
    {
        "id": 20339,
        "title": "Whispered Speech to Neutral Speech Conversion Using Bidirectional LSTMs",
        "authors": "G. Nisha Meenakshi, Prasanta Kumar Ghosh",
        "published": "2018-9-2",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2018-1487"
    },
    {
        "id": 20340,
        "title": "Memory Time Span in LSTMs for Multi-Speaker Source Separation",
        "authors": "Jeroen Zegers, Hugo van Hamme",
        "published": "2018-9-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2018-2082"
    },
    {
        "id": 20341,
        "title": "Transformers Versus LSTMs for Electronic Trading",
        "authors": "Paul Bilokon, Yitao Qiu",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4577922"
    },
    {
        "id": 20342,
        "title": "Peer Review #1 of \"Context dependent prediction in DNA sequence using neural networks (v0.2)\"",
        "authors": "",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.13666v0.2/reviews/1"
    },
    {
        "id": 20343,
        "title": "Peer Review #4 of \"Context dependent prediction in DNA sequence using neural networks (v0.1)\"",
        "authors": "",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.13666v0.1/reviews/4"
    },
    {
        "id": 20344,
        "title": "Peer Review #1 of \"Context dependent prediction in DNA sequence using neural networks (v0.1)\"",
        "authors": "",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.13666v0.1/reviews/1"
    },
    {
        "id": 20345,
        "title": "Semi-supervised learning improves regulatory sequence prediction with unlabeled sequences",
        "authors": "Raphaël Mourad",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nMotivation: Genome-wide association studies have systematically identified thousands of single nucleotide polymor-phisms (SNPs) associated with complex genetic diseases. However, the majority of those SNPs were found in non-coding genomic regions, preventing the understanding of the underlying causal mechanism. Predicting molecular processes based on the DNA sequence represents a promising approach to understand the role of those non-coding SNPs. Over the past years, deep learning was successfully applied to regulatory sequence prediction using supervised learning. Supervised learning required DNA sequences associated with functional data for training, whose amount is strongly limited by the finite size of the human genome. Conversely, the amount of mammalian DNA sequences is exponentially increasing due to ongoing large sequencing projects, but without functional data in most cases. \nResults: To alleviate the limitations of supervised learning, we propose a paradigm shift with semi-supervised learning, which does not only exploit labeled sequences (e.g. human genome with ChIP-seq experiment), but also unlabeled sequences available in much larger amounts (e.g. from other species without ChIP-seq experiment, such as chimpanzee). Our approach is flexible and can be plugged into any neural architecture including shallow and deep networks, and shows strong predictive performance improvements compared to supervised learning in most cases (up to 70%). \nAvailability and implementation: https://forgemia.inra.fr/raphael.mourad/deepgnn",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2358823/v1"
    },
    {
        "id": 20346,
        "title": "Deep Recurrent Neural Network for Protein Function Prediction from Sequence",
        "authors": "Xueliang Leon Liu",
        "published": "No Date",
        "citations": 35,
        "abstract": "AbstractAs high-throughput biological sequencing becomes faster and cheaper, the need to extract useful information from sequencing becomes ever more paramount, often limited by low-throughput experimental characterizations. For proteins, accurate prediction of their functions directly from their primary amino-acid sequences has been a long standing challenge. Here, machine learning using artificial recurrent neural networks (RNN) was applied towards classification of protein function directly from primary sequence without sequence alignment, heuristic scoring or feature engineering. The RNN models containing long-short-term-memory (LSTM) units trained on public, annotated datasets from UniProt achieved high performance for in-class prediction of four important protein functions tested, particularly compared to other machine learning algorithms using sequence-derived protein features. RNN models were used also for out-of-class predictions of phylogenetically distinct protein families with similar functions, including proteins of the CRISPR-associated nuclease, ferritin-like iron storage and cytochrome P450 families. Applying the trained RNN models on the partially unannotated UniRef100 database predicted not only candidates validated by existing annotations but also currently unannotated sequences. Some RNN predictions for the ferritin-like iron sequestering function were experimentally validated, even though their sequences differ significantly from known, characterized proteins and from each other and cannot be easily predicted using popular bioinformatics methods. As sequencing and experimental characterization data increases rapidly, the machine-learning approach based on RNN could be useful for discovery and prediction of homologues for a wide range of protein functions.",
        "link": "http://dx.doi.org/10.1101/103994"
    },
    {
        "id": 20347,
        "title": "Peer Review #3 of \"Context dependent prediction in DNA sequence using neural networks (v0.2)\"",
        "authors": "",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.13666v0.2/reviews/3"
    },
    {
        "id": 20348,
        "title": "Peer Review #4 of \"Context dependent prediction in DNA sequence using neural networks (v0.2)\"",
        "authors": "",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.13666v0.2/reviews/4"
    },
    {
        "id": 20349,
        "title": "Peer Review #2 of \"Context dependent prediction in DNA sequence using neural networks (v0.1)\"",
        "authors": "",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.13666v0.1/reviews/2"
    },
    {
        "id": 20350,
        "title": "Peer Review #3 of \"Context dependent prediction in DNA sequence using neural networks (v0.1)\"",
        "authors": "",
        "published": "2022-9-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.13666v0.1/reviews/3"
    },
    {
        "id": 20351,
        "title": "Intrusion Prediction With System-Call Sequence-to-Sequence Model",
        "authors": "Shaohua Lv, Jian Wang, Yinqi Yang, Jiqiang Liu",
        "published": "2018",
        "citations": 18,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2018.2881561"
    },
    {
        "id": 20352,
        "title": "Body posture prediction based on the sequence to sequence model",
        "authors": "Qili Chen, Jinjin Jiang",
        "published": "2019-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccis48116.2019.9073724"
    },
    {
        "id": 20353,
        "title": "Efficient Prediction of Region-wide Traffic States in Public Bus Networks using LSTMs",
        "authors": "Marcos Amaris, Mayuri A. Morais, Raphael Y. De Camargo",
        "published": "2021-9-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc48978.2021.9564881"
    },
    {
        "id": 20354,
        "title": "Spatio-Temporal Convolutional LSTMs for Tumor Growth Prediction by Learning 4D Longitudinal Patient Data",
        "authors": "Ling Zhang, Le Lu, Xiaosong Wang, Robert M. Zhu, Mohammadhadi Bagheri, Ronald M. Summers, Jianhua Yao",
        "published": "2020-4",
        "citations": 39,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tmi.2019.2943841"
    },
    {
        "id": 20355,
        "title": "5. Deep Learning: RNNs and LSTMs",
        "authors": "",
        "published": "2020-11-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1515/9781683924715-006"
    },
    {
        "id": 20356,
        "title": "Separable Convolutional LSTMs for Faster Video Segmentation",
        "authors": "Andreas Pfeuffer, Klaus Dietmayer",
        "published": "2019-10",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itsc.2019.8917487"
    },
    {
        "id": 20357,
        "title": "Sequence-to-sequence prediction of spatiotemporal systems",
        "authors": "Guorui Shen, Jürgen Kurths, Ye Yuan",
        "published": "2020-2-1",
        "citations": 9,
        "abstract": "We propose a novel type of neural networks known as “attention-based sequence-to-sequence architecture” for a model-free prediction of spatiotemporal systems. This architecture is composed of an encoder and a decoder in which the encoder acts upon a given input sequence and then the decoder yields another output sequence to make a multistep prediction at a time. In order to demonstrate the potential of this approach, we train the neural network using data numerically sampled from the Korteweg–de Vries equation—which describes the interaction between solitary waves—and then predict its future evolution. Furthermore, we validate the applicability of the approach on datasets sampled from the chaotic Lorenz system and three other partial differential equations. The results show that the proposed method can achieve good performance in predicting the evolutionary behavior of studied spatiotemporal dynamics. To the best of our knowledge, this work is the first attempt at applying attention-based sequence-to-sequence architecture to the prediction task of solitary waves.",
        "link": "http://dx.doi.org/10.1063/1.5133405"
    },
    {
        "id": 20358,
        "title": "PS4: a Next-Generation Dataset for Protein Single Sequence Secondary Structure Prediction",
        "authors": "Omar Peracha",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractProtein secondary structure prediction is a subproblem of protein folding. A lightweight algorithm capable of accurately predicting secondary structure from only the protein residue sequence could provide a useful input for tertiary structure prediction, alleviating the reliance on MSA typically seen in today’s best-performing models. Unfortunately, existing datasets for secondary structure prediction are small, creating a bottleneck. We present PS4, a dataset of 18,731 non-redundant protein chains and their respective secondary structure labels. Each chain is identified, and the dataset is also non-redundant against other secondary structure datasets commonly seen in the literature. We perform ablation studies by training secondary structure prediction algorithms on the PS4 training set, and obtain state-of-the-art accuracy on the CB513 test set in zero shots.",
        "link": "http://dx.doi.org/10.1101/2023.02.28.530456"
    },
    {
        "id": 20359,
        "title": "LSTMs for Hydrological Modelling in Swiss Catchments",
        "authors": "Christina Lott, Leonardo Martins, Jonas Weiss, Thomas Brunschwiler, Peter Molnar",
        "published": "No Date",
        "citations": 0,
        "abstract": "Simulation of the catchment rainfall-runoff transformation with physically based watershed models is a traditional way to predict streamflow and other hydrological variables at catchment scales. However, the calibration of such models requires large data inputs and computational power and contains many parameters which are often impossible to constrain or validate. An alternative approach is to use data-driven machine learning for streamflow prediction.In the past few years, LSTM (long short-term memory) models and its variants have been explored in rainfall-runoff modelling. Typical applications use daily climate variables as inputs and model the rainfall-runoff transformation processes with different timescales of memory. This is especially useful as delays in runoff production by snow accumulation and melt, soil water storage, evapotranspiration, etc., can be included. In contrast to feed-forward ANNs (artificial neural networks), LSTMs are capable of maintaining the sequential temporal order of inputs, and compared to RNNs (recurrent neural networks), of learning the long-term dependencies. [1]However, current work on LSTMs mostly focuses on the USA, the UK and Brazil, where CAMELS datasets are available [1, 2, 3]. Catchments at higher altitudes with snow-driven dynamics and sometimes glaciers are present in small number in these datasets (if at all). Systematic applications of LSTMs for streamflow prediction in climates where a significant part of the catchments are snow and ice dominated are missing. In this work, an FS-LSTM (fast slow-LSTM) previously applied in Brazil is adapted for Swiss catchments to fill this gap [3]. The FS-LSTM explored builds on the work of Hoedt et al. (2021) that imposed mass constraints on an LSTM, called MC-LSTM [4]. FS-LSTM adds a fast and slow part for streamflow, containing rainfall and soil moisture respectively. We will discuss benchmark results against an existing semi-distributed conceptual model widely used in Switzerland for streamflow simulation [5].&#160;References:[1]: Kratzert et al., Rainfall-runoff modelling using Long Short-Term Memory (LSTM) networks, 2018.[2]: Lees et al., Hydrological concept formation inside long short-term memory (LSTM) networks, 2022.[3]: Quinones et al., Fast-Slow Streamflow Model Using Mass-Conserving LSTM, 2021.[4]: Hoedt et al., MC-LSTM: Mass-Conserving LSTM, 2021.[5]: Viviroli et al., An introduction to the hydrological modelling system PREVAH and its pre- and post-processing-tools, 2009.",
        "link": "http://dx.doi.org/10.5194/egusphere-egu23-14399"
    },
    {
        "id": 20360,
        "title": "Transition-based DRS Parsing Using Stack-LSTMs",
        "authors": "Kilian Evang",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w19-1202"
    },
    {
        "id": 20361,
        "title": "Generating Steganographic Text with LSTMs",
        "authors": "Tina Fang, Martin Jaggi, Katerina Argyraki",
        "published": "2017",
        "citations": 55,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p17-3017"
    },
    {
        "id": 20362,
        "title": "Mandarin Prosody Boundary Prediction based on Sequence-to-sequence Model",
        "authors": "Yajing Yan, Jiaolong Jiang, Hongwu Yang",
        "published": "2020-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/itnec48623.2020.9084900"
    },
    {
        "id": 20363,
        "title": "Sequence-to-sequence prediction of personal computer software by recurrent neural network",
        "authors": "Qichuan Yang, Zhiqiang He, Fujiang Ge, Yang Zhang",
        "published": "2017-5",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7965952"
    },
    {
        "id": 20364,
        "title": "LSTMs Compose—and Learn—Bottom-Up",
        "authors": "Naomi Saphra, Adam Lopez",
        "published": "2020",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.findings-emnlp.252"
    },
    {
        "id": 20365,
        "title": "Friendly Neighbors: Contextualized Sequence-to-Sequence Link Prediction",
        "authors": "Adrian Kochsiek, Apoorv Saxena, Inderjeet Nair, Rainer Gemulla",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.repl4nlp-1.11"
    },
    {
        "id": 20366,
        "title": "Prediction of SARS-CoV-2 spike protein mutations using Sequence-to-Sequence and Transformer models",
        "authors": "Hamed Ahmadi, Vahid Nikoofard, Hossein Nikoofard, Rouhollah Abdolvahab, Narges Nikoofard, Mahdi Esmaeilzadeh",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractIn the study of viral epidemics, having information about the structural evolution of the virus can be very helpful in controlling the disease and making vaccines. Various deep learning and natural language processing techniques (NLP) can be used to analyze genetic structure of viruses, namely to predict their mutations. In this paper, by using Sequence-to-Sequence (Seq2Seq) model with Long Short-Term Memory (LSTM) cell and Transformer model with the attention mechanism, we investigate the spike protein mutations of SARS-CoV-2 virus. We make time-series datasets of the spike protein sequences of this virus and generate upcoming spike protein sequences. We also determine the mutations of the generated spike protein sequences, by comparing these sequences with the Wuhan spike protein sequence. We train the models to make predictions in December 2021, February 2022, and October 2022. Furthermore, we find that some of our generated spike protein sequences have been reported in December 2021 and February 2022, which belong to Delta and Omicron variants. The results obtained in the present study could be useful for prediction of future mutations of SARS-CoV-2 and other viruses.",
        "link": "http://dx.doi.org/10.1101/2023.01.23.525130"
    },
    {
        "id": 20367,
        "title": "Gender Classification from Tweets Using LSTMs and Transformers",
        "authors": "Omar Alaaeldein",
        "published": "2022-5-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/miucc55081.2022.9781702"
    },
    {
        "id": 20368,
        "title": "Wind Power Forecasting using LSTMs",
        "authors": "Sanjana Vijayshankar, Jennifer King, Peter Seiler",
        "published": "2021-12-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683121"
    },
    {
        "id": 20369,
        "title": "Empowering Sentence Representation with Semantic-Enhanced Tree-LSTMs",
        "authors": "Andrew H. Chris, Fei Lee, Woods Ali",
        "published": "No Date",
        "citations": 0,
        "abstract": "The Semantic-Enhanced Tree-LSTM (SeT-LSTM) network, a novel advancement in the realm of linguistic modeling, marks a significant step forward from traditional Tree-based Long Short Term Memory (LSTM) networks. By intricately weaving in the nuances of typed grammatical dependencies, SeT-LSTMs offer a more nuanced understanding of language semantics. Traditional models often overlook the semantic shift caused by variations in word or phrase roles, a gap this paper aims to fill by focusing on the types of grammatical connections, or typed dependencies, within sentences. Our proposed architecture, dubbed the Semantic Relationship-Guided LSTM (SRG-LSTM), leverages a control mechanism to model the interplay between sequence elements. Additionally, we present a novel Tree-LSTM variant, the Semantic Dependency Tree-LSTM (SDT-LSTM), which integrates dependency parse structures with dependency types for more robust sentence embedding. The SDT-LSTM demonstrates superior performance in Semantic Relatedness Scoring and Sentiment Analysis compared to its predecessors. Qualitatively, it shows resilience to changes in sentence voice and heightened sensitivity to nominal alterations, aligning well with human intuition. This research underlines the pivotal role of grammatical relationships in sentence understanding and paves the way for further exploration in this domain.",
        "link": "http://dx.doi.org/10.20944/preprints202311.1766.v1"
    },
    {
        "id": 20370,
        "title": "Peer Review #2 of \"PhosVarDeep: deep-learning based prediction of phospho-variants using sequence information (v0.1)\"",
        "authors": "",
        "published": "2022-3-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7287/peerj.12847v0.1/reviews/2"
    }
]