[
    {
        "id": 4901,
        "title": "Survey on Activation Functions: A Comparative Study between state-of-the-art Activation Functions and Oscillatory Activation Functions",
        "authors": "Prahitha Movva",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2250"
    },
    {
        "id": 4902,
        "title": "Survey on recent activation functions with emphasis on oscillating activation functions",
        "authors": "Bhavya Raitani",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2429"
    },
    {
        "id": 4903,
        "title": "Oscillating activation functions and other recent progress in activation functions: A survey",
        "authors": "Anuran Roy",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2407"
    },
    {
        "id": 4904,
        "title": "Activation Functions",
        "authors": "",
        "published": "2022-5-16",
        "citations": 0,
        "abstract": "This book describes the functions frequently used in deep neural networks. For this\npurpose, 37 activation functions are explained both mathematically and visually, and\ngiven with their LaTeX implementations due to their common use in scientific articles.",
        "link": "http://dx.doi.org/10.3726/b19631"
    },
    {
        "id": 4905,
        "title": "Activation Functions",
        "authors": "Mark Liu",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b23383-6"
    },
    {
        "id": 4906,
        "title": "Flexible Activation Bag: Learning Activation Functions in Autoencoder Networks",
        "authors": "Hendrik Klopries, Andreas Schwung",
        "published": "2023-4-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icit58465.2023.10143113"
    },
    {
        "id": 4907,
        "title": "Learning Task-specific Activation Functions using Genetic Programming",
        "authors": "Mina Basirat, Peter Roth",
        "published": "2019",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007408205330540"
    },
    {
        "id": 4908,
        "title": "A survey on recently proposed activation functions for Deep Learning",
        "authors": "Murilo Gustineli",
        "published": "No Date",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2245"
    },
    {
        "id": 4909,
        "title": "Learning Task-specific Activation Functions using Genetic Programming",
        "authors": "Mina Basirat, Peter Roth",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0007408200002108"
    },
    {
        "id": 4910,
        "title": "Verify It Yourself: A Note on Activation Functions’ Influence on Fast DeepFake Detection",
        "authors": "Piotr Kawa, Piotr Syga",
        "published": "2021",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010581700002998"
    },
    {
        "id": 4911,
        "title": "Verify It Yourself: A Note on Activation Functions’ Influence on Fast DeepFake Detection",
        "authors": "Piotr Kawa, Piotr Syga",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010581707790784"
    },
    {
        "id": 4912,
        "title": "On The Choice of Activation Functions in Physics-Informed Neural Network for Solving Incompressib...",
        "authors": "",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2514/6.2023-1803.vid"
    },
    {
        "id": 4913,
        "title": "Learning Nonseparable Sparse Regularizers Via Multivariate Activation Functions",
        "authors": "Xin Xu, Zhouchen Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4769821"
    },
    {
        "id": 4914,
        "title": "S*ReLU: Learning Piecewise Linear Activation Functions via Particle Swarm Optimization",
        "authors": "Mina Basirat, Peter Roth",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010338506450652"
    },
    {
        "id": 4915,
        "title": "Optimizing Neural Networks: A Comparative Analysis of Activation Functions in Deep Learning",
        "authors": "Mainak Mitra, Soumit Roy, Vikram Maghnani",
        "published": "2019-3-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21275/sr231205140623"
    },
    {
        "id": 4916,
        "title": "Cryptocurrency Price Estimation Using Hyperparameterized Oscillatory Activation Functions in LSTM Networks",
        "authors": "Pragya Mishra, Shubham Bharadwaj",
        "published": "No Date",
        "citations": 0,
        "abstract": "Activation functions are critical components of neural networks, helping the model learn highly-intricate dependencies, trends, and patterns. Non-linear activation functions allow the model to behave as a functional approximator, learning complex decision boundaries and multi-dimensional patterns in the data. Activation functions can be combined with one another to learn better representations with the objective of improving gradient flow, performance metrics reducing training time and computational cost. Recent work on oscillatory activation functions\\cite{noel2021growing}\\cite{noel2021biologically} showcased their ability to perform competitively on image classification tasks using a compact architecture. Our work proposes the utilization of these oscillatory activation functions for predicting the volume-weighted average of Bitcoin on the G-Research Cryptocurrency Dataset. We utilize a popular LSTM architecture for this task achieving competitive results when compared to popular activation functions formally used.",
        "link": "http://dx.doi.org/10.31224/osf.io/g5a28"
    },
    {
        "id": 4917,
        "title": "CAB39L functions as a tumour suppressor in gastric cancer through LKB1-mediated AMPK activation",
        "authors": "WEILIN LI",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.59a6b349d462b80290b552bb"
    },
    {
        "id": 4918,
        "title": "CMOS-enabled Activation Functions for Low-power Hardware Neural Networks",
        "authors": "Zhihao Chen, Amin Farjudian, James Greer",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>A hardware neuron that can generate a broad class of activation functions is introduced. The requirements on the explicit form of activation functions within neural networks is not stringent. It is shown that the use of standard transistors can readily provide a variety of nonlinear transfer functions that can serve as activation functions. The similarities and differences of the hardware generated activation functions to commonly used mathematical activation functions are presented. A hardware neural network (HNN) based upon the neuron design is evaluated for pattern recognition. The accuracy and robustness of the HNN for the application is found to be comparable to its software counterpart even with moderate performance degradation due to perturbations to the inputs and component variability. This CMOS compatible neuron allows for HNN designs that are readily manufacturable leading to a practical, low-power platform for including HNNs into applications such as the Internet of Things (IoT) and systems-on-a- chip (SoC).</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22134818.v1"
    },
    {
        "id": 4919,
        "title": "Evaluation of Sigmoid and ReLU Activation Functions Using Asymptotic Method",
        "authors": "",
        "published": "2022-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7176/ncs/13-05"
    },
    {
        "id": 4920,
        "title": "CMOS-enabled Activation Functions for Low-power Hardware Neural Networks",
        "authors": "Zhihao Chen, Amin Farjudian, James Greer",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>A hardware neuron that can generate a broad class of activation functions is introduced. The requirements on the explicit form of activation functions within neural networks is not stringent. It is shown that the use of standard transistors can readily provide a variety of nonlinear transfer functions that can serve as activation functions. The similarities and differences of the hardware generated activation functions to commonly used mathematical activation functions are presented. A hardware neural network (HNN) based upon the neuron design is evaluated for pattern recognition. The accuracy and robustness of the HNN for the application is found to be comparable to its software counterpart even with moderate performance degradation due to perturbations to the inputs and component variability. This CMOS compatible neuron allows for HNN designs that are readily manufacturable leading to a practical, low-power platform for including HNNs into applications such as the Internet of Things (IoT) and systems-on-a- chip (SoC).</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.22134818"
    },
    {
        "id": 4921,
        "title": "Activation Functions: Experimentation and Comparison",
        "authors": "Disha Gangadia",
        "published": "2021-4-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/i2ct51068.2021.9417890"
    },
    {
        "id": 4922,
        "title": "Deep networks with ReLU activation functions can be smooth statistical models",
        "authors": "Joseph Rynkiewicz",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2022.es2022-20"
    },
    {
        "id": 4923,
        "title": "On Transformative Adaptive Activation Functions in Neural Networks for Gene Expression Inference",
        "authors": "Vladimír Kunc, Jiří Kléma",
        "published": "No Date",
        "citations": 6,
        "abstract": "AbstractMotivationGene expression profiling was made cheaper by the NIH LINCS program that profiles only ~1, 000 selected landmark genes and uses them to reconstruct the whole profile. The D–GEX method employs neural networks to infer the whole profile. However, the original D–GEX can be further significantly improved.ResultsWe have analyzed the D–GEX method and determined that the inference can be improved using a logistic sigmoid activation function instead of the hyperbolic tangent. Moreover, we propose a novel transformative adaptive activation function that improves the gene expression inference even further and which generalizes several existing adaptive activation functions. Our improved neural network achieves average mean absolute error of 0.1340 which is a significant improvement over our reimplementation of the original D–GEX which achieves average mean absolute error 0.1637",
        "link": "http://dx.doi.org/10.1101/587287"
    },
    {
        "id": 4924,
        "title": "Activation and Functions of Plasmacytoid Dendritic Cells",
        "authors": "Dipyaman Ganguly",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-5595-2_3"
    },
    {
        "id": 4925,
        "title": "Is Melanopsin Activation Affecting Large Field Color Matching Functions?",
        "authors": "Pablo Barrionuevo, Clemente Paz Filgueira, Dingcai Cao",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractColor theory is based on the exclusive activation of cones. However, since the discovery of melanopsin expressing cells in the human retina, evidence of its intrusion in brightness and color vision is increasing. We aimed to assess if differences between peripheral or large field and foveal color matches can be accounted for melanopsin activation or rod intrusion. Photopic color matches by young observers showed that differences between extrafoveal and foveal results cannot be explained by rod intrusion. Furthermore, statistical analyses on existing color matching functions suggest a role of melanopsin activation, particularly, in Large Field S Fundamentals.",
        "link": "http://dx.doi.org/10.1101/2022.03.11.484009"
    },
    {
        "id": 4926,
        "title": "Evaluation of Oscillatory Activation Functions on Retinal Fundus Multi-disease Image Dataset (RFMiD)",
        "authors": "Shubham Bharadwaj, Shubham Gajbhiye",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2206"
    },
    {
        "id": 4927,
        "title": "Learning Combinations of Activation Functions",
        "authors": "Franco Manessi, Alessandro Rozza",
        "published": "2018-8",
        "citations": 30,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr.2018.8545362"
    },
    {
        "id": 4928,
        "title": "Artificial Neural Network Activation Functions in Exact Analytical Form (Heaviside, ReLU, PReLU, ELU, SELU, ELiSH)",
        "authors": "Rami Alkhatib",
        "published": "No Date",
        "citations": 0,
        "abstract": "Activation functions are fundamental elements in artificial neural\nnetworks. The mathematical formulation of some activation functions (e.g.\nHeaviside function and Rectified Linear Unit function) are not expressed in an\nexplicit closed form. This made them numerically unstable and computationally\ncomplex during estimation. This paper introduces a novel explicit analytic form\nfor those activation functions. The proposed mathematical equations match\nexactly the original definition of the studied activation function. The\nproposed equations can be adapted better in optimization, forward and backward\npropagation algorithm employed in an artificial neural network.",
        "link": "http://dx.doi.org/10.36227/techrxiv.15096888"
    },
    {
        "id": 4929,
        "title": "Artificial Neural Network Activation Functions in Exact Analytical Form (Heaviside, ReLU, PReLU, ELU, SELU, ELiSH)",
        "authors": "Rami Alkhatib",
        "published": "No Date",
        "citations": 0,
        "abstract": "Activation functions are fundamental elements in artificial neural\nnetworks. The mathematical formulation of some activation functions (e.g.\nHeaviside function and Rectified Linear Unit function) are not expressed in an\nexplicit closed form. This made them numerically unstable and computationally\ncomplex during estimation. This paper introduces a novel explicit analytic form\nfor those activation functions. The proposed mathematical equations match\nexactly the original definition of the studied activation function. The\nproposed equations can be adapted better in optimization, forward and backward\npropagation algorithm employed in an artificial neural network.",
        "link": "http://dx.doi.org/10.36227/techrxiv.15096888.v1"
    },
    {
        "id": 4930,
        "title": "Continuous control using Deep Q Learning with Oscillatory Activation functions",
        "authors": "Prith Sharma, Aditya Raj Sahoo, Sushant Sinha",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2274"
    },
    {
        "id": 4931,
        "title": "Comparison of Activation Functions in Convolution Neural Network",
        "authors": "Maria Pavlova",
        "published": "2020-10-29",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/telecom50385.2020.9299559"
    },
    {
        "id": 4932,
        "title": "Integrated Innate Immunity—Combining Activation and Effector Functions",
        "authors": "Tom P. Monie",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-12-804464-3.00004-1"
    },
    {
        "id": 4933,
        "title": "NFT artwork generation using oscillatory activation functions in GANs",
        "authors": "Prith Sharma, Aditya Raj Sahoo, Sushant Sinha, Shubham Bharadwaj",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2225"
    },
    {
        "id": 4934,
        "title": "Performance Analysis of Backpropagation Artificial Neural Networks with Various Activation Functions and Network Sizes",
        "authors": "Hamed Hosseinzadeh",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThis paper conducts a comprehensive performance analysis of Back Propagation Artificial Neural Networks (BP-ANNs) utilizing various activation functions. Activation functions play a crucial role in shaping neural networks' behavior and learning capabilities. Through systematic evaluation across diverse network sizes (numbers of hidden layers and neurons), this study assesses the impact of commonly employed activation functions—such as Sigmoidalm, Tanh, Cloglog, Aranda, and others—on the convergence speed and accuracy of BP-ANNs. The findings provide empirical insights essential for optimizing neural network artificial intelligence architectures tailored to specific applications and datasets.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-4141485/v1"
    },
    {
        "id": 4935,
        "title": "Performance Evaluation of Activation Functions in Extreme Learning Machine",
        "authors": "Karol Struniawski, Aleksandra Konopka, Ryszard Kozera",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-31"
    },
    {
        "id": 4936,
        "title": "Memristive Hopfield Neural Network Dynamics with Heterogeneous Activation Functions and its Application",
        "authors": "Quanli Deng, Chunhua Wang, Hairong Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4579583"
    },
    {
        "id": 4937,
        "title": "Analysis of Features of Different Activation Functions",
        "authors": "Kewen Liu",
        "published": "2021-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cds52072.2021.00078"
    },
    {
        "id": 4938,
        "title": "Activation, attention and awareness",
        "authors": "Albert Kok",
        "published": "2019-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9780429451171-4"
    },
    {
        "id": 4939,
        "title": "Improved Echo State Network With Multiple Activation Functions",
        "authors": "Shoujing Zheng, Fanjun Li, Xingshang Li",
        "published": "2022-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac57257.2022.10055912"
    },
    {
        "id": 4940,
        "title": "Evaluation of Oscillatory Activation Functions on MNIST dataset using GANs",
        "authors": "Prith Sharma, Sushant Sinha, Shubham Bharadwaj, Aditya Raj Sahoo",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31224/2195"
    },
    {
        "id": 4941,
        "title": "Discovering Parametric Activation Functions",
        "authors": "Garrett Bingham, Risto Miikkulainen",
        "published": "2022-4",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2022.01.001"
    },
    {
        "id": 4942,
        "title": "Hankel determinant for certain new classes of analytic functions associated the activation functions",
        "authors": "YueJuan Sun, Muhammad Arif, Khalil Ullah, Lei Shi, Muhammad Imran Faisal",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.heliyon.2023.e21449"
    },
    {
        "id": 4943,
        "title": "Fractional activation functions in feedforward artificial neural networks",
        "authors": "Alexander Ivanov",
        "published": "2018-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siela.2018.8447139"
    },
    {
        "id": 4944,
        "title": "Multistability Analysis of Complex-Valued Neural Networks with Sine and Cosine Activation Functions",
        "authors": "Liu Yang, Weiqiang Gong, Qiang Li",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4545101"
    },
    {
        "id": 4945,
        "title": "Enhancing Deep Learning Models for Image Classification using Hybrid Activation Functions",
        "authors": "Zhiqiang Zhang, Xiaoming Li, Yihe Yang, Zhiyong Shi",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn the era of big data, efficient data processing has become a crucial issue for scientific development. Image classification, as one of the core tasks in the field of computer vision, holds great significance for achieving automated and intelligent applications. Nonlinear activation functions play a crucial role in neural networks, as they can introduce nonlinear properties and improve the representation and learning ability of the model. Therefore, it is essential to investigate the performance of different nonlinear activation functions on image classification tasks to optimize model performance and improve data processing efficiency. This paper is based on three nonlinear activation functions, namely, the cosine linear unit (CosLU), derivative exponential linear unit (DELU), and rectified linear unit with nonnegative slope (ReLUN), proposed by E. Pishchik in 2023, to study their performance on image classification tasks. We selected two datasets, CIFAR-10 and CIFAR-100, and employed these three activation functions to train five progressively deepening network models. By comparing them with the ReLU activation function and between the two datasets, we expanded the number of classes in the dataset to provide a more comprehensive evaluation of these activation functions. The experimental results demonstrate that when trained on the CIFAR-10 dataset, the cosine linear unit (CosLU) activation function outperforms ReLU, while the derivative exponential linear unit (DELU) activation function exhibits poor performance, and the rectified linear unit with nonnegative slope (ReLUN) activation function performs similarly to ReLU. However, when trained on the CIFAR-100 dataset, the effectiveness of these activation functions significantly decreases. Additionally, we observed that activation functions with trainable parameters tend to exhibit an overall performance trend that improves as the model size increases. Furthermore, we identified a characteristic shared by most activation functions with trainable parameters, indicating that the larger the model is, the better the overall performance trend may become.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3574353/v1"
    },
    {
        "id": 4946,
        "title": "Resource efficient activation functions for neural network accelerators",
        "authors": "Adedamola Wuraola, Nitish Patel",
        "published": "2022-4",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2021.11.032"
    },
    {
        "id": 4947,
        "title": "Efficient Evaluation of Activation Functions over Encrypted Data",
        "authors": "Patricia Thaine, Sergey Gorbunov, Gerald Penn",
        "published": "2019-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spw.2019.00022"
    },
    {
        "id": 4948,
        "title": "Reconfigurable all-optical nonlinear activation functions",
        "authors": "Aashu Jha, Chaoran Huang, Paul R. Prucnal",
        "published": "2020-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipc47351.2020.9252358"
    },
    {
        "id": 4949,
        "title": "Parabola-Based Artificial Neural Network Activation Functions",
        "authors": "Mikhail Khachumov, Yulia Emelyanova, Vyacheslav Khachumov",
        "published": "2023-9-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/rusautocon58002.2023.10272855"
    },
    {
        "id": 4950,
        "title": "Parameterizing Activation Functions for Adversarial Robustness",
        "authors": "Sihui Dai, Saeed Mahloujifar, Prateek Mittal",
        "published": "2022-5",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spw54247.2022.9833884"
    },
    {
        "id": 4951,
        "title": "Best Fit Activation Functions for Attention Mechanism: Comparison and Enhancement",
        "authors": "Maan Alhazmi, Abdulrahman Altahhan",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191885"
    },
    {
        "id": 4952,
        "title": "A comparison of activation functions in artificial neural networks",
        "authors": "Cenk Bircanoğlu, Nafiz Arıca",
        "published": "2018-5",
        "citations": 23,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/siu.2018.8404724"
    },
    {
        "id": 4953,
        "title": "An Evaluation of Parametric Activation Functions for Deep Learning",
        "authors": "Luke B. Godfrey",
        "published": "2019-10",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc.2019.8913972"
    },
    {
        "id": 4954,
        "title": "Construction of activation functions for wavelet neural networks",
        "authors": "Andrey B. Stepanov",
        "published": "2017-5",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/scm.2017.7970597"
    },
    {
        "id": 4955,
        "title": "Design of nonlinear segmentation activation functions for object detection",
        "authors": "",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.25236/ajcis.2022.051311"
    },
    {
        "id": 4956,
        "title": "A new class of activation functions. Some related problems and applications",
        "authors": "Nikolay V. Kyurkchiev",
        "published": "2020-5-17",
        "citations": 4,
        "abstract": "The cumulative distribution function (cdf) of the discrete two--parameter bathtub hazard distribution has important role in the fields of population dynamics, reliability analysis and life testing experiments.В Also of interest to the specialists is the task of approximating the Heaviside function by new (cdf) in Hausdorff sense.В We define new activation function and family of new recurrence generated functions and study the ''saturation'' by these families.В In this paper we analyze some intrinsic properties of the new Topp-Leone-G-Family with baseline ''deterministic-type'' (cdf) - (NTLG-DT).В Some numerical examples with real data from Biostatistics, Population dynamics and Signal theory, illustrating our results are given.В It is shown that the study of the two characteristics - \"confidential curves\" and ''super saturation'' is a must when choosing the right model.В Some related problems are discussed, as an example to the Approximation Theory.",
        "link": "http://dx.doi.org/10.11145/j.biomath.2020.05.033"
    },
    {
        "id": 4957,
        "title": "The Most Used Activation Functions: Classic Versus Current",
        "authors": "Marina Adriana Mercioni, Stefan Holban",
        "published": "2020-5",
        "citations": 37,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/das49615.2020.9108942"
    },
    {
        "id": 4958,
        "title": "Non-uniform Piecewise Linear Activation Functions in Deep Neural Networks",
        "authors": "Zezhou Zhu, Yuan Dong",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956345"
    },
    {
        "id": 4959,
        "title": "Activation Functions and Universal Approximation Theory",
        "authors": "",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811254185_0007"
    },
    {
        "id": 4960,
        "title": "Activation functions and their characteristics in deep neural networks",
        "authors": "Bin Ding, Huimin Qian, Jun Zhou",
        "published": "2018-6",
        "citations": 138,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc.2018.8407425"
    },
    {
        "id": 4961,
        "title": "Neuroevolutionary based convolutional neural network with adaptive activation functions",
        "authors": "Roxana ZahediNasab, Hadis Mohseni",
        "published": "2020-3",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2019.11.090"
    },
    {
        "id": 4962,
        "title": "MRI Brain Tumor Prediction using Azure Streamlit Framework and Analysis of CNN Activation Functions",
        "authors": "Vipin Saxena,  , Sugandha Singh, Karm Veer Singh",
        "published": "2023-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.17485/ijst/v16i37.427"
    },
    {
        "id": 4963,
        "title": "Comparing Activation Functions in Machine Learning for Finite Element Simulations in Thermomechanical Forming",
        "authors": "Olivier Pantalé",
        "published": "2023-11-25",
        "citations": 0,
        "abstract": "Finite element (FE) simulations have been effective in simulating thermomechanical forming processes, yet challenges arise when applying them to new materials due to nonlinear behaviors. To address this, machine learning techniques and artificial neural networks play an increasingly vital role in developing complex models. This paper presents an innovative approach to parameter identification in flow laws, utilizing an artificial neural network that learns directly from test data and automatically generates a Fortran subroutine for the Abaqus standard or explicit FE codes. We investigate the impact of activation functions on prediction and computational efficiency by comparing Sigmoid, Tanh, ReLU, Swish, Softplus, and the less common Exponential function. Despite its infrequent use, the Exponential function demonstrates noteworthy performance and reduced computation times. Model validation involves comparing predictive capabilities with experimental data from compression tests, and numerical simulations confirm the numerical implementation in the Abaqus explicit FE code.",
        "link": "http://dx.doi.org/10.3390/a16120537"
    },
    {
        "id": 4964,
        "title": "Investigative study of various activation functions for speech recognition",
        "authors": "Hari Krishna Vydana, Anil Kumar Vuppala",
        "published": "2017-3",
        "citations": 14,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ncc.2017.8077043"
    },
    {
        "id": 4965,
        "title": "Co-activation and eEMG-feedback for Restoring Hand-Functions",
        "authors": "Christian Klauer, Emilia Ambrosini, Simona Ferrante, Alessandra Pedrocchi",
        "published": "2019-6",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/ecc.2019.8795989"
    },
    {
        "id": 4966,
        "title": "Efficient activation functions for embedded inference engines",
        "authors": "Adedamola Wuraola, Nitish Patel, Sing Kiong Nguang",
        "published": "2021-6",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neucom.2021.02.030"
    },
    {
        "id": 4967,
        "title": "vReLU Activation Functions for Artificial Neural Networks",
        "authors": "He Hu",
        "published": "2018-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/fskd.2018.8687140"
    },
    {
        "id": 4968,
        "title": "On-chip Nonlinear Activation and Gradient Functions for Photonic Backpropagation Training and Inference",
        "authors": "F. Ashtiani, M. H. Idjadi",
        "published": "2023-11-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ipc57732.2023.10360521"
    },
    {
        "id": 4969,
        "title": "Efficient Two-Party Privacy-Preserving Protocols for Activation Functions",
        "authors": "Lin Fu, Bin Dai",
        "published": "2022-12-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccc56324.2022.10065786"
    },
    {
        "id": 4970,
        "title": "Learning specialized activation functions with the Piecewise Linear Unit",
        "authors": "Yucong Zhou, Zezhou Zhu, Zhao Zhong",
        "published": "2021-10",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.01188"
    },
    {
        "id": 4971,
        "title": "EvoDNN - An Evolutionary Deep Neural Network with Heterogeneous Activation Functions",
        "authors": "Peiyu Cui, Boris Shabash, Kay C. Wiese",
        "published": "2019-6",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cec.2019.8789964"
    },
    {
        "id": 4972,
        "title": "ResNet-18 comparative analysis of various activation functions for image classification",
        "authors": "Gaurav Kumar Pandey, Sumit Srivastava",
        "published": "2023-4-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icict57646.2023.10134464"
    },
    {
        "id": 4973,
        "title": "Programmable, high-speed all-optical nonlinear activation functions for neuromorphic photonics",
        "authors": "Aashu Jha, Chaoran Huang, Paul R. Prucnal",
        "published": "2021",
        "citations": 3,
        "abstract": "We experimentally demonstrate programmable, all-optical nonlinear activation functions, including: rectified linear unit (ReLU), inverse-ReLU and quadratic functions, on a silicon-nitride platform at a remarkable speed of 10 Gbps, enabling real-time processing on all-purpose photonic hardware.",
        "link": "http://dx.doi.org/10.1364/ofc.2021.tu5h.3"
    },
    {
        "id": 4974,
        "title": "Review for \"CD96 functions as a co‐stimulatory receptor to enhance CD8 &lt;sup&gt;+&lt;/sup&gt; T cell activation and effector responses\"",
        "authors": "",
        "published": "2019-12-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/eji.201948405/v2/review1"
    },
    {
        "id": 4975,
        "title": "Salt Body Flooding Using Activation Functions From Machine Learning",
        "authors": "A. Alali, B. Sun, V. Kazei, T. Alkhalifah",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3997/2214-4609.202011667"
    },
    {
        "id": 4976,
        "title": "Implementation of activation functions for ELM based classifiers",
        "authors": "Satyakam Baraha, Pradyut Kumar Biswal",
        "published": "2017-3",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/wispnet.2017.8299920"
    },
    {
        "id": 4977,
        "title": "Transformers with Learnable Activation Functions",
        "authors": "Haishuo Fang, Ji-Ung Lee, Nafise Sadat Moosavi, Iryna Gurevych",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-eacl.181"
    },
    {
        "id": 4978,
        "title": "Arrestin-Dependent ERK Activation and Its Disruption",
        "authors": "Louis M. Luttrell, Benjamin W. Spiller",
        "published": "2017",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-57553-7_15"
    },
    {
        "id": 4979,
        "title": "Oxytocin: functions and activation",
        "authors": "Hassan Nima HABIB, Khalaf Abd Al-Razzaq AL-RISHDY, Waleed Yousif KASSIM",
        "published": "2020-2-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.29327/multi.2020004"
    },
    {
        "id": 4980,
        "title": "Review for \"CD96 functions as a co‐stimulatory receptor to enhance CD8 &lt;sup&gt;+&lt;/sup&gt; T cell activation and effector responses\"",
        "authors": "",
        "published": "2019-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1002/eji.201948405/v1/review2"
    },
    {
        "id": 4981,
        "title": "Investigation of activation functions in deep belief network",
        "authors": "Mian Mian Lau, King Hann Lim",
        "published": "2017-4",
        "citations": 31,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccre.2017.7935070"
    },
    {
        "id": 4982,
        "title": "Hardware implementation of hyperbolic tangent and sigmoid activation functions",
        "authors": "Z. Hajduk",
        "published": "2018-8-13",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.24425/bpas.2018.124272"
    },
    {
        "id": 4983,
        "title": "Improving Graph Convolutional Networks with Non-Parametric Activation Functions",
        "authors": "Simone Scardapane, Steven Van Vaerenbergh, Danilo Comminiello, Aurelio Uncini",
        "published": "2018-9",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco.2018.8553465"
    },
    {
        "id": 4984,
        "title": "Flex-SFU: Accelerating DNN Activation Functions by Non-Uniform Piecewise Approximation",
        "authors": "Enrico Reggiani, Renzo Andri, Lukas Cavigelli",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/dac56929.2023.10247855"
    },
    {
        "id": 4985,
        "title": "The Effect of Activation Functions in a Chest X-ray Image Classification",
        "authors": "Charles Chinedu Nworu, Emmanuel John Ekpenyong‬, Chisimkwuo John, Christian Nduka Onyeukwu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nThe choice of activation functons is very important in deep learning. This is because activation functions are capable of capturing non-linear patterns in a data. The most popular actiavtion function is the Rectified Linear Unit (ReLU) but it suffers from gradient vanishing problem. Therefore, we examined the modifications of the ReLU activation function to determine its effectiveness (accuracy) and efficiency (time complexity). The effectiveness and efficiency verified by conducting an empirical experiment using X-ray images that contains pneumonia and normal samples. Our experiments show that the modified ReLU, ReLU6 performed better in terms of low generalization error (97.05% training accuracy and 78.21% test accuracy). The sensitivity measure also suggest that the ELU is capable of correctly predicting more than half of the positive cases with 52.14% probability. For efficiency, the GELU shows the lowest training time when compared with other activation functions. This will allow practitioners in this field to choose activation functions based on effectiveness and efficiency\n\nJEL Classification: C63 , C67 , I10",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-2064395/v1"
    },
    {
        "id": 4986,
        "title": "Sufficient Conditions for Persistency of Excitation with Step and ReLU Activation Functions",
        "authors": "Tyler Lekang, Andrew Lamperski",
        "published": "2022-12-6",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc51059.2022.9992794"
    },
    {
        "id": 4987,
        "title": "PARABOLIC INTEGRODIFFERENTIAL SPLINES AS ACTIVATION FUNCTIONS TO INCREASE THE EFFICIENCY OF INFORMATION PROCESSING BY NEURAL NETWORKS",
        "authors": "",
        "published": "2020-12-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14357/08696527200415"
    },
    {
        "id": 4988,
        "title": "Comparative Study of Different Activation Functions for Anomalous Sound Detection",
        "authors": "Youssef Abdelrahman Ahmed, Hisham Othman, Mohammed A.-M. Salem",
        "published": "2021-12-19",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icm52667.2021.9664952"
    },
    {
        "id": 4989,
        "title": "Enhancing 3D implicit shape representation by leveraging periodic activation functions",
        "authors": "Kanika Singla, Parmanand Astya",
        "published": "2021-10-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ispcc53510.2021.9609352"
    },
    {
        "id": 4990,
        "title": "Hardware Implementation for Multiple Activation Functions",
        "authors": "Chih-Hsiang Chang, Hsu-Yu Kao, Shih-Hsu Huang",
        "published": "2019-5",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icce-tw46550.2019.8991981"
    },
    {
        "id": 4991,
        "title": "Finite-time stabilization control of complex-valued neural networks with discontinuous activation functions and time delay",
        "authors": "Zengyun Wang, Jinde Cao",
        "published": "2019-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/chicc.2019.8865680"
    },
    {
        "id": 4992,
        "title": "Activation Functions",
        "authors": "Ovidiu Calin",
        "published": "2020",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-36721-3_2"
    },
    {
        "id": 4993,
        "title": "Criminal Detection: Study of Activation Functions and Optimizers",
        "authors": "",
        "published": "2021-4-5",
        "citations": 0,
        "abstract": "Face is the primary means of recognizing a person, transmitting information, communicating with others, and inferring people’s feelings, among others. Our faces will reveal more than we think. A facial image may show personal characteristics such as ethnicity, gender, age, fitness, emotion, psychology, and occupation. In addition to the recent specialisation of deep learning models, the exponential output and memory space growth of computer machines has greatly increased the role of images in recognising semantic patterns. Facial photographs can reveal those personality features in the same way as a textual message on social media reveals the author's individual characteristics. We investigate a new degree of image comprehension by using deep learning to infer a criminal proclivity from facial images. A convolutional neural network (CNN) deep learning model is used to differentiate between criminal and non-criminal facial images. Using tenfold cross-validation on a set of 5500 face pictures, the model's confusion matrix, training, and test accuracies are registered. In learning to achieve the highest test accuracy, CNN was more reliable than the SNN, which was 8% better than the SNN's test accuracy. Finally, CNN's dissection and visualization of convolutional layers showed that CNN distinguished the two sets of images based on the shape of the face, eyebrows, top of the eye, pupils, nostrils, and lips. In this project we focus on Activation functions and optimizers. Activation functions are of two types Saturated and Non-Saturated. Here we use non saturated activation functions like ReLU, SELU and SOFTMAX. When we combine ReLU and SOFTMAX, we get 99.3 percentages as test accuracy. By combining SELU and SOFTMAX we get 99.6 as test accuracy. Therefore, SELU and SOFTMAX combination give the better accuracy",
        "link": "http://dx.doi.org/10.30534/ijatcse/2021/931022021"
    },
    {
        "id": 4994,
        "title": "GPU-based empirical evaluation of activation functions in convolutional neural networks",
        "authors": "Raniah Zaheer, Humera Shaziya",
        "published": "2018-1",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icisc.2018.8398903"
    },
    {
        "id": 4995,
        "title": "A comparative study of Activation functions for Diabetes detection using Convolution Neural Networks (CNN)",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.55487/ijcih.v1i2.14"
    },
    {
        "id": 4996,
        "title": "Comparison of Performance by Activation Functions on Deep Image Prior",
        "authors": "Shohei Fujii, Hitoshi Hayashi",
        "published": "2019-2",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icaiic.2019.8669063"
    },
    {
        "id": 4997,
        "title": "Impact of Activation Functions of Neural Network in Mammographic Masses",
        "authors": "Rashmi Agrawal, Neha Gupta",
        "published": "2022-4-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icoei53556.2022.9776775"
    },
    {
        "id": 4998,
        "title": "Target Recognition Based on CNN with LeakyReLU and PReLU Activation Functions",
        "authors": "Tongtong Jiang, Jinyong Cheng",
        "published": "2019-8",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/sdpc.2019.00136"
    },
    {
        "id": 4999,
        "title": "Activation Functions Effect on Fractal Coding Using Neural Networks",
        "authors": "Rashad A. Al-Jawfi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32604/iasc.2023.031700"
    },
    {
        "id": 5000,
        "title": "Neural networks with distributed delays and Hölder continuous activation functions",
        "authors": "Nasser-eddine Tatar",
        "published": "2018",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18514/mmn.2018.1175"
    }
]