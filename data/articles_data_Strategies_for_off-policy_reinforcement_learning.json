[
    {
        "id": 31505,
        "title": "Off-Policy Reinforcement Learning for Optimal Control of a Two Wheeled Self Balancing Robot",
        "authors": "Athira Mullachery, Shaikshavali Chitraganti",
        "published": "2023-12-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc61519.2023.10441833"
    },
    {
        "id": 31506,
        "title": "H<sub>∞</sub> Optimal Distributed Tracking Control of Network Distributed Systems over Directed Networks via Off-Policy Reinforcement Learning",
        "authors": "Gulnihal Kucuksayacigil",
        "published": "2023-6-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ecc57647.2023.10178321"
    },
    {
        "id": 31507,
        "title": "Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes",
        "authors": "Andrew Bennett, Nathan Kallus",
        "published": "2023-9-26",
        "citations": 0,
        "abstract": " In applications of offline reinforcement learning to observational data, such as in healthcare or education, a general concern is that observed actions might be affected by unobserved factors, inducing confounding and biasing estimates derived assuming a perfect Markov decision process (MDP) model. In “Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes,” A. Bennett and N. Kallus tackle this by considering off-policy evaluation in a partially observed MDP (POMDP). Specifically, they consider estimating the value of a given target policy in an unknown POMDP, given observations of trajectories generated by a different and unknown policy, which may depend on the unobserved states. They consider both when the target policy value can be identified the observed data and, given identification, how best to estimate it. Both these problems are addressed by extending the framework of proximal causal inference to POMDP settings, using sequences of so-called bridge functions. This results in a novel framework for off-policy evaluation in POMDPs that they term proximal reinforcement learning, which they validate in various empirical settings. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1287/opre.2021.0781"
    },
    {
        "id": 31508,
        "title": "An Efficient Off-Policy Reinforcement Learning Algorithm for the Continuous-Time LQR Problem",
        "authors": "Victor G. Lopez, Matthias A. Müller",
        "published": "2023-12-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10384256"
    },
    {
        "id": 31509,
        "title": "Timing-Aware Resilience of Data-driven Off-policy Reinforcement Learning for Discrete-Time Systems",
        "authors": "Lijing Zhai, Filippos Fotiadis, Kyriakos G. Vamvoudakis, Jérôme Hugues",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/acc55779.2023.10155865"
    },
    {
        "id": 31510,
        "title": "Off-policy evaluation for tabular reinforcement learning with synthetic trajectories",
        "authors": "Weiwei Wang, Yuqiang Li, Xianyi Wu",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11222-023-10351-y"
    },
    {
        "id": 31511,
        "title": "Reliable Off-Policy Evaluation for Reinforcement Learning",
        "authors": "Jie Wang, Rui Gao, Hongyuan Zha",
        "published": "2024-3",
        "citations": 2,
        "abstract": " Off-policy evaluation is an important topic in reinforcement learning, which estimates the expected cumulative reward of a target policy using logged trajectory data generated from a different behavior policy, without execution of the target policy. It is imperative to quantify the uncertainty of the off-policy estimate before deployment of the target policy. Here we leverage methodologies from (Wasserstein) distributionally robust optimization to provide robust and optimistic cumulative reward estimates. With proper selection of the size of the distributional uncertainty set, these estimates serve as confidence bounds with nonasymptotic and asymptotic guarantees under stochastic or adversarial environments. We also generalize those results to batch reinforcement learning. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1287/opre.2022.2382"
    },
    {
        "id": 31512,
        "title": "Off-policy and on-policy reinforcement learning with the Tsetlin machine",
        "authors": "Saeed Rahimi Gorji, Ole-Christoffer Granmo",
        "published": "2023-4",
        "citations": 2,
        "abstract": "AbstractThe Tsetlin Machine is a recent supervised learning algorithm that has obtained competitive accuracy- and resource usage results across several benchmarks. It has been used for convolution, classification, and regression, producing interpretable rules in propositional logic. In this paper, we introduce the first framework for reinforcement learning based on the Tsetlin Machine. Our framework integrates the value iteration algorithm with the regression Tsetlin Machine as the value function approximator. To obtain accurate off-policy state-value estimation, we propose a modified Tsetlin Machine feedback mechanism that adapts to the dynamic nature of value iteration. In particular, we show that the Tsetlin Machine is able to unlearn and recover from the misleading experiences that often occur at the beginning of training. A key challenge that we address is mapping the intrinsically continuous nature of state-value learning to the propositional Tsetlin Machine architecture, leveraging probabilistic updates. While accurate off-policy, this mechanism learns significantly slower than neural networks on-policy. However, by introducing multi-step temporal-difference learning in combination with high-frequency propositional logic patterns, we are able to close the performance gap. Several gridworld instances document that our framework can outperform comparable neural network models, despite being based on simple one-level AND-rules in propositional logic. Finally, we propose how the class of models learnt by our Tsetlin Machine for the gridworld problem can be translated into a more understandable graph structure. The graph structure captures the state-value function approximation and the corresponding policy found by the Tsetlin Machine.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10489-022-04297-3"
    },
    {
        "id": 31513,
        "title": "On the Reuse Bias in Off-Policy Reinforcement Learning",
        "authors": "Chengyang Ying, Zhongkai Hao, Xinning Zhou, Hang Su, Dong Yan, Jun Zhu",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Importance sampling (IS) is a popular technique in off-policy evaluation, which re-weights the return of trajectories in the replay buffer to boost sample efficiency. However, training with IS can be unstable and previous attempts to address this issue mainly focus on analyzing the variance of IS. In this paper, we reveal that the instability is also related to a new notion of Reuse Bias of IS --- the bias in off-policy evaluation caused by the reuse of the replay buffer for evaluation and optimization. We theoretically show that the off-policy evaluation and optimization of the current policy with the data from the replay buffer result in an overestimation of the objective, which may cause an erroneous gradient update and degenerate the performance. We further provide a high-probability upper bound of the Reuse Bias and show that controlling one term of the upper bound can control the Reuse Bias by introducing the concept of stability for off-policy algorithms. Based on these analyses, we present a novel yet simple Bias-Regularized Importance Sampling (BIRIS) framework along with practical algorithms, which can alleviate the negative impact of the Reuse Bias, and show that our BIRIS can significantly reduce the Reuse Bias empirically. Moreover, extensive experimental results show that our BIRIS-based methods can significantly improve the sample efficiency on a series of continuous control tasks in MuJoCo.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/502"
    },
    {
        "id": 31514,
        "title": "An Improved Trust-Region Method for Off-Policy Deep Reinforcement Learning",
        "authors": "Hepeng Li, Xiangnan Zhong, Haibo He",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191837"
    },
    {
        "id": 31515,
        "title": "A Continuous Off-Policy Reinforcement Learning Scheme for Optimal Motion Planning in Simply-Connected Workspaces",
        "authors": "Panagiotis Rousseas, Charalampos P. Bechlioulis, Kostas J. Kyriakopoulos",
        "published": "2023-5-29",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161189"
    },
    {
        "id": 31516,
        "title": "An Approach for DC Motor Speed Control with Off-Policy Reinforcement Learning Method",
        "authors": "Sevilay TÜFENKÇİ, Gürkan KAVURAN, Celaleddin YEROĞLU",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "In the literature, interest in automatic control systems that do not require human intervention and perform at the desired level increases day by day. In this study, a Twin Delay Deep Deterministic Policy Gradient (TD3), a reinforcement learning algorithm, automatically controls a DC motor system. A reinforcement learning method is an approach that learns what should be done to reach the goal and observes the results that come out with the interaction of both itself and the environment. The proposed method aims to adjust the voltage value applied to the input of the DC motor in order to reach output with single input and single output structure to the desired speed.",
        "keywords": "",
        "link": "http://dx.doi.org/10.17694/bajece.1114868"
    },
    {
        "id": 31517,
        "title": "Autonomous navigation of mobile robots in unknown environments using off-policy reinforcement learning with curriculum learning",
        "authors": "Yan Yin, Zhiyu Chen, Gang Liu, Jiasong Yin, Jianwei Guo",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2024.123202"
    },
    {
        "id": 31518,
        "title": "Rethinking Population-assisted Off-policy Reinforcement Learning",
        "authors": "Bowen Zheng, Ran Cheng",
        "published": "2023-7-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583131.3590512"
    },
    {
        "id": 31519,
        "title": "Fast Link Scheduling in Wireless Networks Using Regularized Off-Policy Reinforcement Learning",
        "authors": "Sagnik Bhattacharya, Ayan Banerjee, Subrahmanya Swamy Peruru, Kothapalli Venkata Srinivas",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lnet.2023.3264486"
    },
    {
        "id": 31520,
        "title": "Multi-Horizon Learning in Procedurally-Generated Environments for Off-Policy Reinforcement Learning (Student Abstract)",
        "authors": "Raja Farrukh Ali, Kevin Duong, Nasik Muhammad Nafi, William Hsu",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Value estimates at multiple timescales can help create advanced discounting functions and allow agents to form more effective predictive models of their environment. In this work, we investigate learning over multiple horizons concurrently for off-policy reinforcement learning by using an advantage-based action selection method and introducing architectural improvements. Our proposed agent learns over multiple horizons simultaneously, while using either exponential or hyperbolic discounting functions. We implement our approach on Rainbow, a value-based off-policy algorithm, and test on Procgen, a collection of procedurally-generated environments, to demonstrate the effectiveness of this approach, specifically to evaluate the agent's performance in previously unseen scenarios.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i13.26935"
    },
    {
        "id": 31521,
        "title": "Safety-Aware Optimal Control of Nonlinear Systems Using Off-Policy Reinforcement Learning*",
        "authors": "Mingduo Lin, Bo Zhao, Hongbing Xia, Derong Liu",
        "published": "2023-10-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/csis-iac60628.2023.10363836"
    },
    {
        "id": 31522,
        "title": "Robust hierarchical games of linear discrete-time systems based on off-policy model-free reinforcement learning",
        "authors": "Xiao Ma, Yuan Yuan",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jfranklin.2024.106711"
    },
    {
        "id": 31523,
        "title": "Particle Swarm Optimization  Method Combined with off  Policy Reinforcement Learning  Algorithm for the Discovery of  High Utility Itemset",
        "authors": "K. Logeswaran, P. Suresh, S. Anandamurugan",
        "published": "2023-3-28",
        "citations": 3,
        "abstract": "Mining of High Utility Itemset (HUI) is an area of high importance in data mining that involves numerous methodologies for addressing it effectively. When the diversity of items and size of an item is quite vast in the given dataset, then the problem search space that needs to be solved by conventional exact approaches to High Utility Itemset Mining (HUIM) also increases in terms of exponential. This factual issue has made the researchers to choose alternate yet efficient approaches based on Evolutionary Computation (EC) to solve the HUIM problem. Particle Swarm Optimization (PSO) is an EC-based approach that has drawn the attention of many researchers to unravel different NP-Hard problems in real-time. Variants of PSO techniques have been established in recent years to increase the efficiency of the HUIs mining process. In PSO, the Minimization of execution time and generation of reasonable decent solutions were greatly influenced by the PSO control parameters namely Acceleration Coefficient and  and Inertia Weight. The proposed approach is called Adaptive Particle Swarm Optimization using Reinforcement Learning with Off Policy (APSO-RLOFF), which employs the Reinforcement Learning (RL) concept to achieve the adaptive online calibration of PSO control and, in turn, to increase the performance of PSO. The state-of-the-art RL approach called the Q-Learning algorithm is employed in the APSO-RLOFF approach. In RL, state-action utility values are estimated during each episode using Q-Learning. Extensive tests are carried out on four benchmark datasets to evaluate the performance of the suggested technique. An exact approach called HUP-Miner and three EC-based approaches, namely HUPEUMU-GRAM, HUIM-BPSO, and AGA_RLOFF, are used to relate the performance of the anticipated approach. From the outcome, it is inferred that the performance metrics of APSO-RLOFF, namely no of discovered HUIs and execution time, outstrip the previously considered EC computations.\n ",
        "keywords": "",
        "link": "http://dx.doi.org/10.5755/j01.itc.52.1.31949"
    },
    {
        "id": 31524,
        "title": "A multiagent reinforcement learning framework for off-policy evaluation in two-sided markets",
        "authors": "Chengchun Shi, Runzhe Wan, Ge Song, Shikai Luo, Hongtu Zhu, Rui Song",
        "published": "2023-12-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1214/22-aoas1700"
    },
    {
        "id": 31525,
        "title": "Two-player nonlinear Stackelberg differential game via off-policy integral reinforcement learning",
        "authors": "Xiaohong Cui, Jiayu Chen, Yang Cui, Suan Xu",
        "published": "2024-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jfranklin.2024.106812"
    },
    {
        "id": 31526,
        "title": "Reliability assessment of off-policy deep reinforcement learning: A benchmark for aerodynamics",
        "authors": "Sandrine Berger, Andrea Arroyo Ramo, Valentin Guillet, Thibault Lahire, Brice Martin, Thierry Jardin, Emmanuel Rachelson, Michaël Bauerheim",
        "published": "2024",
        "citations": 0,
        "abstract": "Abstract\nDeep reinforcement learning (DRL) is promising for solving control problems in fluid mechanics, but it is a new field with many open questions. Possibilities are numerous and guidelines are rare concerning the choice of algorithms or best formulations for a given problem. Besides, DRL algorithms learn a control policy by collecting samples from an environment, which may be very costly when used with Computational Fluid Dynamics (CFD) solvers. Algorithms must therefore minimize the number of samples required for learning (sample efficiency) and generate a usable policy from each training (reliability). This paper aims to (a) evaluate three existing algorithms (DDPG, TD3, and SAC) on a fluid mechanics problem with respect to reliability and sample efficiency across a range of training configurations, (b) establish a fluid mechanics benchmark of increasing data collection cost, and (c) provide practical guidelines and insights for the fluid dynamics practitioner. The benchmark consists in controlling an airfoil to reach a target. The problem is solved with either a low-cost low-order model or with a high-fidelity CFD approach. The study found that DDPG and TD3 have learning stability issues highly dependent on DRL hyperparameters and reward formulation, requiring therefore significant tuning. In contrast, SAC is shown to be both reliable and sample efficient across a wide range of parameter setups, making it well suited to solve fluid mechanics problems and set up new cases without tremendous effort. In particular, SAC is resistant to small replay buffers, which could be critical if full-flow fields were to be stored.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1017/dce.2023.28"
    },
    {
        "id": 31527,
        "title": "Enhancing Off-Policy Constrained Reinforcement Learning through Adaptive Ensemble C Estimation",
        "authors": "Hengrui Zhang, Youfang Lin, Shuo Shen, Sheng Han, Kai Lv",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "In the domain of real-world agents, the application of Reinforcement Learning (RL) remains challenging due to the necessity for safety constraints. Previously, Constrained Reinforcement Learning (CRL) has predominantly focused on on-policy algorithms. Although these algorithms exhibit a degree of efficacy, their interactivity efficiency in real-world settings is sub-optimal, highlighting the demand for more efficient off-policy methods. However, off-policy CRL algorithms grapple with challenges in precise estimation of the C-function, particularly due to the fluctuations in the constrained Lagrange multiplier. Addressing this gap, our study focuses on the nuances of C-value estimation in off-policy CRL and introduces the Adaptive Ensemble C-learning (AEC) approach to reduce these inaccuracies. Building on state-of-the-art off-policy algorithms, we propose AEC-based CRL algorithms designed for enhanced task optimization. Extensive experiments on nine constrained robotics tasks reveal the superior interaction efficiency and performance of our algorithms in comparison to preceding methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i19.30177"
    },
    {
        "id": 31528,
        "title": "Off-policy two-dimensional reinforcement learning for optimal tracking control of batch processes with network-induced dropout and disturbances",
        "authors": "Xueying Jiang, Min Huang, Huiyuan Shi, Xingwei Wang, Yanfeng Zhang",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.isatra.2023.11.011"
    },
    {
        "id": 31529,
        "title": "Optimization of a Deep Reinforcement Learning Policy for Construction Manufacturing Control",
        "authors": "Ian Flood, Xiaoyan Zhou",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012091400003546"
    },
    {
        "id": 31530,
        "title": "Off‐policy correction algorithm for double Q network based on deep reinforcement learning",
        "authors": "Qingbo Zhang, Manlu Liu, Heng Wang, Weimin Qian, Xinglang Zhang",
        "published": "2023-12",
        "citations": 0,
        "abstract": "AbstractA deep reinforcement learning (DRL) method based on the deep deterministic policy gradient (DDPG) algorithm is proposed to address the problems of a mismatch between the needed training samples and the actual training samples during the training of intelligence, the overestimation and underestimation of the existence of Q‐values, and the insufficient dynamism of the intelligence policy exploration. This method introduces the Actor‐Critic Off‐Policy Correction (AC‐Off‐POC) reinforcement learning framework and an improved double Q‐value learning method, which enables the value function network in the target task to provide a more accurate evaluation of the policy network and converge to the optimal policy more quickly and stably to obtain higher value returns. The method is applied to multiple MuJoCo tasks on the Open AI Gym simulation platform. The experimental results show that it is better than the DDPG algorithm based solely on the different policy correction framework (AC‐Off‐POC) and the conventional DRL algorithm. The value of returns and stability of the double‐Q‐network off‐policy correction algorithm for the deep deterministic policy gradient (DCAOP‐DDPG) proposed by the authors are significantly higher than those of other DRL algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1049/csy2.12102"
    },
    {
        "id": 31531,
        "title": "Off‐policy model‐based end‐to‐end safe reinforcement learning",
        "authors": "Soha Kanso, Mayank Shekhar Jha, Didier Theilliol",
        "published": "2024-3-10",
        "citations": 0,
        "abstract": "AbstractSafety and stability considerations play a crucial role in the development of learning based strategies for control design of systems that require high levels of safety. Safe reinforcement learning (RL) based approaches traditionally seek learning of the control laws that are optimal with respect to system performance whilst ensuring system stability and safety. In this article, an off‐policy safe RL based approach is proposed for nonlinear systems affine in control in continuous time. In this novel work, safety and stability are guaranteed during initialization and exploration phases by adjusting the control input with the solution of a quadratic programming problem combining both input to state stable‐control Lyapunov function and robust control barrier function (R‐CBF) conditions. Moreover, the safety of the learned policy is assured by augmenting the cost function with a CBF to maintain safety and optimize performance simultaneously. Novel mathematically rigorous proofs are provided to establish the stability and safety guarantees, offering a sound theoretical foundation for the approach. To demonstrate the effectiveness of the algorithm, two examples are presented: engine surge and stall dynamics, and an unstable nonlinear system.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/rnc.7109"
    },
    {
        "id": 31532,
        "title": "Re-attentive experience replay in off-policy reinforcement learning",
        "authors": "Wei Wei, Da Wang, Lin Li, Jiye Liang",
        "published": "2024-2-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06505-8"
    },
    {
        "id": 31533,
        "title": "Robust control design for zero-sum differential games problem based on off-policy reinforcement learning technique",
        "authors": "Hongji Zhuang, Hongxu Zhu, Shufan Wu, Xiaoliang Wang, Zhongcheng Mu, Qiang Shen",
        "published": "2023-12-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s42401-023-00263-0"
    },
    {
        "id": 31534,
        "title": "Efficient Exploitation for Off-Policy Reinforcement Learning via a Novel Multi-scale Tabular Buffer",
        "authors": "Shicheng Guo, Wei Xue, Wei Zhao, Yuanxia Shen, Gaohang Yu",
        "published": "2023-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/acie58528.2023.00007"
    },
    {
        "id": 31535,
        "title": "On-policy and off-policy Q-learning strategies for spacecraft systems: An approach for time-varying discrete-time without controllability assumption of augmented system",
        "authors": "Hoang Nguyen, Hoang Bach Dang, Phuong Nam Dao",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ast.2024.108972"
    },
    {
        "id": 31536,
        "title": "Off‐policy reinforcement learning algorithm for robust optimal control of uncertain nonlinear systems",
        "authors": "Ali Amirparast, S. Kamal Hosseini Sani",
        "published": "2024-5-25",
        "citations": 0,
        "abstract": "AbstractIn this article an optimal control scheme is proposed to solve robust control problem for matched and unmatched system. In the proposed optimal approach the value functions are designed such that the obtained optimal control law guarantees asymptotic stability of the uncertain nonlinear system. Since the proposed robust optimal control problem is not straightforward to solve, an off‐policy reinforcement‐learning algorithm based on neural networks approximation is developed to obtain robust optimal control law iteratively. The robust control law for matched uncertain systems can be achieved via proposed off‐policy learning algorithm without requiring exact knowledge of system's dynamics. The advantages of the proposed robust optimal controller are verified by comparative simulations on an uncertain model of a car suspension system and a mathematical nonlinear model.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/rnc.7278"
    },
    {
        "id": 31537,
        "title": "Exploring the Use of Invalid Action Masking in Reinforcement Learning: A Comparative Study of On-Policy and Off-Policy Algorithms in Real-Time Strategy Games",
        "authors": "Yueqi Hou, Xiaolong Liang, Jiaqiang Zhang, Qisong Yang, Aiwu Yang, Ning Wang",
        "published": "2023-7-18",
        "citations": 1,
        "abstract": "Invalid action masking is a practical technique in deep reinforcement learning to prevent agents from taking invalid actions. Existing approaches rely on action masking during policy training and utilization. This study focuses on developing reinforcement learning algorithms that incorporate action masking during training but can be used without action masking during policy execution. The study begins by conducting a theoretical analysis to elucidate the distinction between naive policy gradient and invalid action policy gradient. Based on this analysis, we demonstrate that the naive policy gradient is a valid gradient and is equivalent to the proposed composite objective algorithm, which optimizes both the masked policy and the original policy in parallel. Moreover, we propose an off-policy algorithm for invalid action masking that employs the masked policy for sampling while optimizing the original policy. To compare the effectiveness of these algorithms, experiments are conducted using a simplified real-time strategy (RTS) game simulator called Gym-μRTS. Based on empirical findings, we recommend utilizing the off-policy algorithm for addressing most tasks while employing the composite objective algorithm for handling more complex tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app13148283"
    },
    {
        "id": 31538,
        "title": "Distributed Randomized Multiagent Policy Iteration in Reinforcement Learning",
        "authors": "Weipeng Zhang",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4489715"
    },
    {
        "id": 31539,
        "title": "Autonomous pricing using policy gradient reinforcement learning",
        "authors": "Kevin Michael Frick",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4527452"
    },
    {
        "id": 31540,
        "title": "Cautious policy programming: exploiting KL regularization for monotonic policy improvement in reinforcement learning",
        "authors": "Lingwei Zhu, Takamitsu Matsubara",
        "published": "2023-11",
        "citations": 0,
        "abstract": "AbstractIn this paper, we propose cautious policy programming (CPP), a novel value-based reinforcement learning (RL) algorithm that exploits the idea of monotonic policy improvement during learning. Based on the nature of entropy-regularized RL, we derive a new entropy-regularization-aware lower bound of policy improvement that depends on the expected policy advantage function but not on state-action-space-wise maximization as in prior work. CPP leverages this lower bound as a criterion for adjusting the degree of a policy update for alleviating policy oscillation. Different from similar algorithms that are mostly theory-oriented, we also propose a novel interpolation scheme that makes CPP better scale in high dimensional control problems. We demonstrate that the proposed algorithm can trade off performance and stability in both didactic classic control problems and challenging high-dimensional Atari games.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06368-z"
    },
    {
        "id": 31541,
        "title": "Efficient evaluation of natural stochastic policies in off-line reinforcement learning",
        "authors": "Nathan Kallus, Masatoshi Uehara",
        "published": "2024-2-12",
        "citations": 0,
        "abstract": "Summary\nWe study the efficient off-policy evaluation of natural stochastic policies, which are defined in terms of deviations from the unknown behaviour policy. This is a departure from the literature on off-policy evaluation that largely considers the evaluation of explicitly specified policies. Crucially, off-line reinforcement learning with natural stochastic policies can help alleviate issues of weak overlap, lead to policies that build upon current practice and improve policies’ implementability in practice. Compared with the classic case of a prespecified evaluation policy, when evaluating natural stochastic policies, the efficiency bound, which measures the best-achievable estimation error, is inflated since the evaluation policy itself is unknown. In this paper we derive the efficiency bounds of two major types of natural stochastic policies: tilting policies and modified treatment policies. We then propose efficient nonparametric estimators that attain the efficiency bounds under lax conditions and enjoy a partial double robustness property.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1093/biomet/asad059"
    },
    {
        "id": 31542,
        "title": "Off-policy deep reinforcement learning with automatic entropy adjustment for adaptive online grid emergency control",
        "authors": "Ying Zhang, Meng Yue, Jianhui Wang",
        "published": "2023-4",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.epsr.2023.109136"
    },
    {
        "id": 31543,
        "title": "Confidence optimally modulates decision policy in reinforcement learning",
        "authors": "kobe desender, Tom Verguts",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1083-0"
    },
    {
        "id": 31544,
        "title": "Reinforcement Learning Based NSGA-II for Energy-Delay Trade-Off in IAB mmWave Het-Nets",
        "authors": "Wen Shang, Vasilis Friderikos",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278969"
    },
    {
        "id": 31545,
        "title": "An Off-Policy Reinforcement Learning-Based Adaptive Optimization Method for Dynamic Resource Allocation Problem",
        "authors": "Baiyang He, Ying Meng, Lixin Tang",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3338237"
    },
    {
        "id": 31546,
        "title": "Policy-Based Reinforcement Learning in the Generalized Rock-Paper-Scissors Game",
        "authors": "Imre Gergely Mali, Gabriela Czibula",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-92"
    },
    {
        "id": 31547,
        "title": "Adaptive Optimal Control of Discrete-Time Linear Systems with Discounted Value: Off-Policy Reinforcement Learning",
        "authors": "Liao Zhu, Chunxiuzi Liu, Jingsheng Xu, Ping Guo",
        "published": "2023-9-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/docs60977.2023.10294994"
    },
    {
        "id": 31548,
        "title": "Off-policy reinforcement learning for tracking control of discrete-time Markov jump linear systems with completely unknown dynamics",
        "authors": "Zhen Huang, Yidong Tu, Haiyang Fang, Hai Wang, Liang Zhang, Kaibo Shi, Shuping He",
        "published": "2023-2",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jfranklin.2022.10.052"
    },
    {
        "id": 31549,
        "title": "M2S2: A Policy Transfer Framework for Deep Reinforcement Learning",
        "authors": "Huashan Liu, Dezhen Yin, Xiangjian Li",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450392"
    },
    {
        "id": 31550,
        "title": "Deep Reinforcement Learning Based Optimization and Risk Control of Trading Strategies",
        "authors": "Mengrui Bao",
        "published": "2024-4-13",
        "citations": 0,
        "abstract": "Deep reinforcement learning (DRL) based optimization leverages advanced machine learning techniques to solve complex decision-making problems in various domains.  Deep learning in trading involves the application of sophisticated neural network architectures to analyze financial data and make predictions in stock markets, forex, commodities, and other trading domains. Deep learning algorithms, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs), traders can extract valuable insights from vast amounts of historical market data, including price movements, trading volumes, and market sentiment. These models can learn complex patterns and relationships within the data, enabling them to forecast future market trends, identify potential trading opportunities, and manage risks more effectively. Deep learning in trading has shown promise in improving decision-making processes, enhancing trading strategies, and achieving higher returns, although challenges such as data scarcity, model interpretability, and overfitting remain areas of ongoing research and development. In this paper evaluated the stock market to achieve the financial gain to achieve the significant improvement in prediction with the stock trading. The data related to the stock market are optimized with stock trend data of 7,000 stocks situated in the United States. The estimated stock trade data were computed and processed with the Multiagent Q-learning model for the data detection and classification. A deep Q-network is developed to adaptively pick distinct action subsets and train the market making agent based on the inventory states. The experimental findings demonstrate the effectiveness of our suggested methodology in feature learning and superiority in accuracy improvement when compared to deep learning-based modelling of frequency trading patterns and standard signal processing approaches for stock trend prediction. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/jes.1943"
    },
    {
        "id": 31551,
        "title": "A Hybrid Online Off-Policy Reinforcement Learning Agent Framework Supported by Transformers",
        "authors": "Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez, Luis Jimenez-Linares, David Muñoz-Valero, Jun Liu",
        "published": "2023-12",
        "citations": 1,
        "abstract": " Reinforcement learning (RL) is a powerful technique that allows agents to learn optimal decision-making policies through interactions with an environment. However, traditional RL algorithms suffer from several limitations such as the need for large amounts of data and long-term credit assignment, i.e. the problem of determining which actions actually produce a certain reward. Recently, Transformers have shown their capacity to address these constraints in this area of learning in an offline setting. This paper proposes a framework that uses Transformers to enhance the training of online off-policy RL agents and address the challenges described above through self-attention. The proposal introduces a hybrid agent with a mixed policy that combines an online off-policy agent with an offline Transformer agent using the Decision Transformer architecture. By sequentially exchanging the experience replay buffer between the agents, the agent’s learning training efficiency is improved in the first iterations and so is the training of Transformer-based RL agents in situations with limited data availability or unknown environments. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1142/s012906572350065x"
    },
    {
        "id": 31552,
        "title": "Off-Policy Risk-Sensitive Reinforcement Learning-Based Constrained Robust Optimal Control",
        "authors": "Cong Li, Qingchen Liu, Zhehua Zhou, Martin Buss, Fangzhou Liu",
        "published": "2023-4",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tsmc.2022.3213750"
    },
    {
        "id": 31553,
        "title": "Efficient Dimensionality Reduction Strategies for Quantum Reinforcement Learning",
        "authors": "Eva Andrés, M. P. Cuéllar, G. Navarro",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3318173"
    },
    {
        "id": 31554,
        "title": "Optimization of Charging Strategies for New Energy Vehicles Based on Reinforcement Learning Algorithms",
        "authors": "",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23977/jaip.2024.070112"
    },
    {
        "id": 31555,
        "title": "Risk-Sensitive Policy with Distributional Reinforcement Learning",
        "authors": "Thibaut Théate, Damien Ernst",
        "published": "2023-6-30",
        "citations": 3,
        "abstract": "Classical reinforcement learning (RL) techniques are generally concerned with the design of decision-making policies driven by the maximisation of the expected outcome. Nevertheless, this approach does not take into consideration the potential risk associated with the actions taken, which may be critical in certain applications. To address that issue, the present research work introduces a novel methodology based on distributional RL to derive sequential decision-making policies that are sensitive to the risk, the latter being modelled by the tail of the return probability distribution. The core idea is to replace the Q function generally standing at the core of learning schemes in RL by another function, taking into account both the expected return and the risk. Named the risk-based utility function U, it can be extracted from the random return distribution Z naturally learnt by any distributional RL algorithm. This enables the spanning of the complete potential trade-off between risk minimisation and expected return maximisation, in contrast to fully risk-averse methodologies. Fundamentally, this research yields a truly practical and accessible solution for learning risk-sensitive policies with minimal modification to the distributional RL algorithm, with an emphasis on the interpretability of the resulting decision-making process.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/a16070325"
    },
    {
        "id": 31556,
        "title": "FWA-RL: Fireworks Algorithm with Policy Gradient for Reinforcement Learning",
        "authors": "Maiyue Chen, Ying Tan",
        "published": "2023-7-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cec53210.2023.10254081"
    },
    {
        "id": 31557,
        "title": "Reward Function Design for Stand-Off Tracking of Reinforcement Learning",
        "authors": "Yeontaek Jung, Jinrae Kim, Seong-hun Kim, Youdan Kim",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-1440"
    },
    {
        "id": 31558,
        "title": "Learning Risk-Aware Costmaps via Inverse Reinforcement Learning for Off-Road Navigation",
        "authors": "Samuel Triest, Mateo Guaman Castro, Parv Maheshwari, Matthew Sivaprakasam, Wenshan Wang, Sebastian Scherer",
        "published": "2023-5-29",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161268"
    },
    {
        "id": 31559,
        "title": "Research on Reinforcement Learning Explainable Strategies Based on Advantage Saliency",
        "authors": "Dandan Yan",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "Deep reinforcement learning is increasingly being used in difficult environments with sparse rewards and high-dimensional inputs, and it performs well, but its decision-making processes are largely unclear and difficult to explain to end users. Saliency map methods explain an agent's behavior by highlighting state features relevant for the agent to take an action. In this paper, we use the perturbation-based saliency map method, propose the use of advantage function to replace the existing method of calculating state saliency, realize the combination of advantage function and perturbation-based saliency map. A saliency map is generated by noting the saliency of the dependent elements of the agent's chosen action in the Atari game environment. Experimental comparisons show that our method generates more accurate explanatory saliency maps.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54097/fcis.v3i1.6348"
    },
    {
        "id": 31560,
        "title": "Vertical Take-Off and Landing System Control Using Deep Reinforcement Learning",
        "authors": "Haitham M. Al-radhi, Khaled A. El-Metwally",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/esmarta59349.2023.10293679"
    },
    {
        "id": 31561,
        "title": "REINFORCEMENT LEARNING: APPLICATION AND ADVANCES TOWARDS STABLE CONTROL STRATEGIES, 53-57. SI",
        "authors": "Abhishek Kumar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2316/j.2023.201-0347"
    },
    {
        "id": 31562,
        "title": "Reward poisoning attacks in deep reinforcement learning based on exploration strategies",
        "authors": "Kanting Cai, Xiangbin Zhu, Zhaolong Hu",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126578"
    },
    {
        "id": 31563,
        "title": "Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning",
        "authors": "Patrick Emami, Xiangyu Zhang, David Biagioni, Ahmed S. Zamzam",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10384223"
    },
    {
        "id": 31564,
        "title": "Multi-agent off-policy actor-critic algorithm for distributed multi-task reinforcement learning",
        "authors": "Miloš S. Stanković, Marko Beko, Nemanja Ilić, Srdjan S. Stanković",
        "published": "2023-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ejcon.2023.100853"
    },
    {
        "id": 31565,
        "title": "<mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" altimg=\"si9.svg\"><mml:msub><mml:mi mathvariant=\"script\">H</mml:mi><mml:mi>∞</mml:mi></mml:msub></mml:math> Tracking learning control for discrete-time Markov jump systems: A parallel off-policy reinforcement learning",
        "authors": "Xuewen Zhang, Jianwei Xia, Jing Wang, Xiangyong Chen, Hao Shen",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jfranklin.2023.10.008"
    },
    {
        "id": 31566,
        "title": "Model-Based Off-Policy Deep Reinforcement Learning With Model-Embedding",
        "authors": "Xiaoyu Tan, Chao Qu, Junwu Xiong, James Zhang, Xihe Qiu, Yaochu Jin",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tetci.2024.3369636"
    },
    {
        "id": 31567,
        "title": "Adaptive Cyber Defense Technique Based on Multiagent Reinforcement Learning Strategies",
        "authors": "Adel Alshamrani, Abdullah Alshahrani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/iasc.2023.032835"
    },
    {
        "id": 31568,
        "title": "Object Detection Using Policy-Based Reinforcement Learning",
        "authors": "Keong-Hun Choi, Jong-Eun Ha",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/iccas59377.2023.10316786"
    },
    {
        "id": 31569,
        "title": "Adversarial Proximal Policy Optimisation for Robust Reinforcement Learning",
        "authors": "Bilkan Ince, Hyo-Sang Shin, Antonios Tsourdos",
        "published": "2024-1-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2024-1697"
    },
    {
        "id": 31570,
        "title": "Reinforcement Learning for Multi-Well SAGD Optimization: A Policy Gradient Approach",
        "authors": "J. L. Guevara, J. Trivedi",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "Abstract\nFinding an optimal steam injection strategy for a SAGD process is considered a major challenge due to the complex dynamics of the physical phenomena. Recently, reinforcement learning (RL) has been presented as alternative to conventional methods (e.g., adjoint-optimization, model predictive control) as an effective way to address the cited challenge. In general, RL represents a model-free strategy where an agent is trained to find the optimal policy - the action at every time step that will maximize cumulative long-term performance of a given process- only by continuous interactions with the environment (e.g., SAGD process). This environment is modeled as a Markov-Decision-Process (MDP) and a state must be defined to characterize it. During the interaction process, at each time step, the agent executes an action, receives a scalar reward (e.g., net present value) due to the action taken and observes the new state (e.g., pressure distribution of the reservoir) of the environment. This process continuous for a number of simulations or episodes until convergence is achieved. One approach to solve the RL problem is to parametrize the policy using well-known methods, e.g., linear functions, SVR, neural networks, etc. This approach is based on maximizing the performance of the process with respect to the parameters of the policy. Using the Monte Carlo algorithm, after every episode a long-term performance of the process is obtained and the parameters of the policy are updated using gradient-ascent methods. In this work policy gradient is used to find the steam injection policy that maximizes cumulative net present value of a SAGD process. The environment is represented by a reservoir simulation model inspired by northern Alberta reservoir and the policy is parametrized using a deep neural network. Results show that optimal steam injection can be characterized in two regions: 1) an increase or slight increase of steam injection rates, and 2) a sharp decrease until reaching the minimum value. Furthermore, the first region's objective appears to be more of pressure maintenance using high steam injection rates. In the second region, the objective is to collect more reward or achieve high values of daily net present value due to the reduction of steam injection while keeping high oil production values.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2118/213104-ms"
    },
    {
        "id": 31571,
        "title": "Sparse Bayesian Network-Based Disturbance Observer for Policy-Based Reinforcement Learning",
        "authors": "HyeonBeen Park",
        "published": "2023-10-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/iccas59377.2023.10316987"
    },
    {
        "id": 31572,
        "title": "Policy Gradients for Probabilistic Constrained Reinforcement Learning",
        "authors": "Weiqin Chen, Dharmashankar Subramanian, Santiago Paternain",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089763"
    },
    {
        "id": 31573,
        "title": "Reinforcement Learning Explained via Reinforcement Learning: Towards Explainable Policies through Predictive Explanation",
        "authors": "Léo Saulières, Martin Cooper, Florence Bannay",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011619600003393"
    },
    {
        "id": 31574,
        "title": "Impact of Reinforcement Strategies on Students’ Learning Behaviors in Classroom at Primary Level",
        "authors": "",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.47205/jdss.2023(4-iv)25"
    },
    {
        "id": 31575,
        "title": "A Deep Reinforcement Learning Technique for PNPSC Net Player Strategies",
        "authors": "Edwin Michael Bearss, Mikel Petty",
        "published": "2023-4-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3564746.3587011"
    },
    {
        "id": 31576,
        "title": "Distributed Policy Gradient with Heterogeneous Computations for Federated Reinforcement Learning",
        "authors": "Ye Zhu, Xiaowen Gong",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089771"
    },
    {
        "id": 31577,
        "title": "Optimization of a Robust Reinforcement Learning Policy",
        "authors": "Bilkan Ince, Hyo-Sang Shin, Antonios Tsourdos",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-0967"
    },
    {
        "id": 31578,
        "title": "Off-policy Imitation Learning from Visual Inputs",
        "authors": "Zhihao Cheng, Li Shen, Dacheng Tao",
        "published": "2023-5-29",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icra48891.2023.10161566"
    },
    {
        "id": 31579,
        "title": "Decision Support System for Optimizing Tactics and Strategies of Sports Competition Using Reinforcement Learning Algorithm",
        "authors": "Zhiyong Xu",
        "published": "2024-4-4",
        "citations": 0,
        "abstract": "This paper presented an augmented reality secure edge machine learning dynamic optimization model for detecting volleyball movement recognition using a decision support system. The proposed Edge Augmented Decision Support System Blockchain (EdgeAu-DSSBC) model aims to address the limitations of centralized machine learning models, such as high latency, network congestion, and data privacy concerns, by utilizing edge computing and dynamic optimization techniques. The proposed EdgeAu-DSSBC system consists of two main components: an augmented reality interface that allows users to interact with the system and an edge machine learning algorithm that performs the recognition task. The system uses dynamic optimization techniques to optimize the parameters of the machine learning algorithm in real time based on the feedback received from the users. The decision support system provides additional guidance to the users and helps them to make informed decisions based on the recognition results. The EdgeAu-DSSBC model was evaluated using a dataset of volleyball movement videos and compared to existing centralized and edge-based machine-learning models. The experimental results demonstrate that the EdgeAu-DSSBC exhibits improved performance with an accuracy of 99%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.52783/jes.1304"
    },
    {
        "id": 31580,
        "title": "Advances in Value-based, Policy-based, and Deep Learning-based Reinforcement Learning",
        "authors": "Haewon Byeon",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140838"
    },
    {
        "id": 31581,
        "title": "Robust Driving Policy Learning with Guided Meta Reinforcement Learning",
        "authors": "Kanghoon Lee, Jiachen Li, David Isele, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochendorfer",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10422469"
    },
    {
        "id": 31582,
        "title": "Fast and Robust Training and Deployment of Deep Reinforcement Learning Based Navigation Policy",
        "authors": "Weizhi Tao, Hailong Huang",
        "published": "2023-10-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icus58632.2023.10318224"
    },
    {
        "id": 31583,
        "title": "Sentiment Driven Reinforcement Learning Trading Strategies to Enhance Market Performance",
        "authors": "Rajesh Rohilla, Raaghav Raj Maiya, Ritvik Bharti",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icccnt56998.2023.10307400"
    },
    {
        "id": 31584,
        "title": "Escape Route Strategies in Complex Emergency Situations using Deep Reinforcement Learning",
        "authors": "Tim Wächter, Jan Rexilius, Matthias König",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ie57519.2023.10179101"
    },
    {
        "id": 31585,
        "title": "Comparisons of Different Dynamic Game Reinforcement Learning Optimal Strategies for A Non-holonomic Vehicle",
        "authors": "Lingyi Xu, Zoran Gajic",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciss59072.2024.10480181"
    },
    {
        "id": 31586,
        "title": "Policy ensemble gradient for continuous control problems in deep reinforcement learning",
        "authors": "Guoqiang Liu, Gang Chen, Victoria Huang",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neucom.2023.126381"
    },
    {
        "id": 31587,
        "title": "Deep Reinforcement Learning: Policy Gradients for US Equities Trading",
        "authors": "Miquel Noguer i Alonso, Himanshu Agrawal, David Pacheco Aznar",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4645453"
    },
    {
        "id": 31588,
        "title": "Generative Adversarial Inverse Reinforcement Learning With Deep Deterministic Policy Gradient",
        "authors": "Ming Zhan, Jingjing Fan, Jianying Guo",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3305453"
    },
    {
        "id": 31589,
        "title": "Finding individual strategies for storage units in electricity market models using deep reinforcement learning",
        "authors": "Nick Harder, Anke Weidlich, Philipp Staudt",
        "published": "2023-10-19",
        "citations": 1,
        "abstract": "AbstractModeling energy storage units realistically is challenging as their decision-making is not governed by a marginal cost pricing strategy but relies on expected electricity prices. Existing electricity market models often use centralized rule-based bidding or global optimization approaches, which may not accurately capture the competitive behavior of market participants. To address this issue, we present a novel method using multi-agent deep reinforcement learning to model individual strategies in electricity market models. We demonstrate the practical applicability of our approach using a detailed model of the German wholesale electricity market with a complete fleet of pumped hydro energy storage units represented as learning agents. We compare the results to widely used modeling approaches and demonstrate that the proposed method performs well and can accurately represent the competitive behavior of market participants. To understand the benefits of using reinforcement learning, we analyze overall profits, aggregated dispatch, and individual behavior of energy storage units. The proposed method can improve the accuracy and realism of electricity market modeling and help policymakers make informed decisions for future market designs and policies.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s42162-023-00293-0"
    },
    {
        "id": 31590,
        "title": "Improved Soft Actor-Critic: Mixing Prioritized Off-Policy Samples With On-Policy Experiences",
        "authors": "Chayan Banerjee, Zhiyong Chen, Nasimul Noman",
        "published": "2024-3",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3174051"
    },
    {
        "id": 31591,
        "title": "Molecule generation using transformers and policy gradient reinforcement learning",
        "authors": "Eyal Mazuz, Guy Shtar, Bracha Shapira, Lior Rokach",
        "published": "2023-5-31",
        "citations": 11,
        "abstract": "AbstractGenerating novel valid molecules is often a difficult task, because the vast chemical space relies on the intuition of experienced chemists. In recent years, deep learning models have helped accelerate this process. These advanced models can also help identify suitable molecules for disease treatment. In this paper, we propose Taiga, a transformer-based architecture for the generation of molecules with desired properties. Using a two-stage approach, we first treat the problem as a language modeling task of predicting the next token, using SMILES strings. Then, we use reinforcement learning to optimize molecular properties such as QED. This approach allows our model to learn the underlying rules of chemistry and more easily optimize for molecules with desired properties. Our evaluation of Taiga, which was performed with multiple datasets and tasks, shows that Taiga is comparable to, or even outperforms, state-of-the-art baselines for molecule optimization, with improvements in the QED ranging from 2 to over 20 percent. The improvement was demonstrated both on datasets containing lead molecules and random molecules. We also show that with its two stages, Taiga is capable of generating molecules with higher biological property scores than the same model without reinforcement learning.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-023-35648-w"
    },
    {
        "id": 31592,
        "title": "Supervised reinforcement learning for recommending treatment strategies in sepsis",
        "authors": "Sun Haoran, Chen Qiuming, Yang Yujing",
        "published": "2023-8-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2689783"
    },
    {
        "id": 31593,
        "title": "Learning in two dimensions and controlling in three: Generalizable drag reduction strategies for flows past circular cylinders through deep reinforcement learning",
        "authors": "Michail Chatzimanolakis, Pascal Weber, Petros Koumoutsakos",
        "published": "2024-4-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1103/physrevfluids.9.043902"
    },
    {
        "id": 31594,
        "title": "Drones Tracking Adaptation Using Reinforcement Learning: Proximal Policy optimization",
        "authors": "Esra Alhadhrami, Amal El Fallah Seghrouchni, Frederic Barbaresco, Raed Abu Zitar",
        "published": "2023-5-24",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/irs57608.2023.10172435"
    },
    {
        "id": 31595,
        "title": "A Routing Optimization Policy Using Graph Convolution Deep Reinforcement Learning",
        "authors": "Yongan Guo, Qingpeng Wu, Hao She",
        "published": "2023-8-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccc57788.2023.10233329"
    },
    {
        "id": 31596,
        "title": "Logic Optimization Sequence Tuning Based on Policy Search Deep Reinforcement Learning",
        "authors": "Yu Jin, Haijiao Huang, Wenzhe Ye, Xuebing Zhang",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cstic58779.2023.10219326"
    },
    {
        "id": 31597,
        "title": "Visual Navigation for Obstacle Avoidance Using Deep Reinforcement Learning with Policy Optimization",
        "authors": "Ali Parsbin, Mahdi Akraminia",
        "published": "2023-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icrom60803.2023.10412549"
    },
    {
        "id": 31598,
        "title": "Vaccine allocation policy optimization and budget sharing mechanism using reinforcement learning",
        "authors": "David Rey, Ahmed W. Hammad, Meead Saberi",
        "published": "2023-2",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.omega.2022.102783"
    },
    {
        "id": 31599,
        "title": "Max-Policy Sharing for Multi-Agent Reinforcement Learning in Autonomous Mobility on Demand",
        "authors": "Ebtehal T. Alotaibi, Michael Herrmann",
        "published": "2023-7-29",
        "citations": 0,
        "abstract": "Autonomous-Mobility-on-Demand (AMoD) systems can revolutionize urban transportation by providing mobility as a service without car ownership. However, optimizing the performance of AMoD systems presents a challenge due to competing objectives of reducing customer wait times and increasing system utilization while minimizing empty miles. To address this challenge, this study compares the performance of max-policy sharing agents and independent learners in an AMoD system using reinforcement learning. The results demonstrate the advantages of the max-policy sharing approach in improving Quality of Service (QoS) indicators such as completed orders, empty miles, lost customers due to competition, and out-of-charge events. The study identifies the importance of striking a balance between competition and cooperation among individual autonomous vehicles and tuning the frequency of policy sharing to avoid suboptimal policies. The findings suggest that the max-policy sharing approach has the potential to accelerate learning in multi-agent reinforcement learning systems, particularly under conditions of low exploration.",
        "keywords": "",
        "link": "http://dx.doi.org/10.5121/csit.2023.131333"
    },
    {
        "id": 31600,
        "title": "Distributed randomized multiagent policy iteration in reinforcement learning",
        "authors": "Weipeng Zhang",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.rico.2023.100255"
    },
    {
        "id": 31601,
        "title": "Reinforcement and Curriculum Learning for Off-Road Navigation of an UGV with a 3D LiDAR",
        "authors": "Manuel Sánchez, Jesús Morales, Jorge L. Martínez",
        "published": "2023-3-18",
        "citations": 3,
        "abstract": "This paper presents the use of deep Reinforcement Learning (RL) for autonomous navigation of an Unmanned Ground Vehicle (UGV) with an onboard three-dimensional (3D) Light Detection and Ranging (LiDAR) sensor in off-road environments. For training, both the robotic simulator Gazebo and the Curriculum Learning paradigm are applied. Furthermore, an Actor–Critic Neural Network (NN) scheme is chosen with a suitable state and a custom reward function. To employ the 3D LiDAR data as part of the input state of the NNs, a virtual two-dimensional (2D) traversability scanner is developed. The resulting Actor NN has been successfully tested in both real and simulated experiments and favorably compared with a previous reactive navigation approach on the same UGV.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23063239"
    },
    {
        "id": 31602,
        "title": "Offline Deep Reinforcement Learning and Off-Policy Evaluation for Personalized Basal Insulin Control in Type 1 Diabetes",
        "authors": "Taiyu Zhu, Kezhi Li, Pantelis Georgiou",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/jbhi.2023.3303367"
    },
    {
        "id": 31603,
        "title": "Policy Optimization with Augmented Value Targets for Generalization in Reinforcement Learning",
        "authors": "Nasik Muhammad Nafi, Giovanni Poggi-Corradini, William Hsu",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191507"
    },
    {
        "id": 31604,
        "title": "Determining Optimal Incentive Policy for Decentralized Distributed Systems Using Reinforcement Learning",
        "authors": "Elitsa Pankovska, Ashish Rajendra Sai, Harald Vranken",
        "published": "2023-5-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icbc56567.2023.10174946"
    },
    {
        "id": 31605,
        "title": "Developing Driving Strategies Efficiently: A Skill-Based Hierarchical Reinforcement Learning Approach",
        "authors": "Yigit Gurses, Kaan Buyukdemirci, Yildiray Yildiz",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lcsys.2024.3349511"
    },
    {
        "id": 31606,
        "title": "Consumer-Centric Home Energy Management System Using Trust Region Policy Optimization- Based Multi-Agent Deep Reinforcement Learning",
        "authors": "Kuthsav Thattai, Jayashri Ravishankar, Chaojie Li",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/powertech55446.2023.10202803"
    },
    {
        "id": 31607,
        "title": "Reinforcement Learning for Optimizing Can-Order Policy with the Rolling Horizon Method",
        "authors": "Jiseong Noh",
        "published": "2023-7-7",
        "citations": 1,
        "abstract": "This study presents a novel approach to a mixed-integer linear programming (MILP) model for periodic inventory management that combines reinforcement learning algorithms. The rolling horizon method (RHM) is a multi-period optimization approach that is applied to handle new information in updated markets. The RHM faces a limitation in easily determining a prediction horizon; to overcome this, a dynamic RHM is developed in which RL algorithms optimize the prediction horizon of the RHM. The state vector consisted of the order-up-to-level, real demand, total cost, holding cost, and backorder cost, whereas the action included the prediction horizon and forecasting demand for the next time step. The performance of the proposed model was validated through two experiments conducted in cases with stable and uncertain demand patterns. The results showed the effectiveness of the proposed approach in inventory management, particularly when the proximal policy optimization (PPO) algorithm was used for training compared with other reinforcement learning algorithms. This study signifies important advancements in both the theoretical and practical aspects of multi-item inventory management.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/systems11070350"
    },
    {
        "id": 31608,
        "title": "PPO-ABR: Proximal Policy Optimization based Deep Reinforcement Learning for Adaptive BitRate streaming",
        "authors": "Mandan Naresh, Paresh Saxena, Manik Gupta",
        "published": "2023-6-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iwcmc58020.2023.10182379"
    },
    {
        "id": 31609,
        "title": "Quantum reinforcement learning via policy iteration",
        "authors": "El Amine Cherrat, Iordanis Kerenidis, Anupam Prakash",
        "published": "2023-12",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s42484-023-00116-1"
    },
    {
        "id": 31610,
        "title": "Predictive reinforcement learning in non-stationary environments using weighted mixture policy",
        "authors": "Hossein Pourshamsaei, Amin Nobakhti",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.asoc.2024.111305"
    },
    {
        "id": 31611,
        "title": "TOWARDS EFFECTIVE STRATEGIES FOR MOBILE ROBOT USING REINFORCEMENT LEARNING AND GRAPH ALGORITHMS",
        "authors": "Sofiia Shaposhnikova, Dmytro Omelian",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "Abstract. This research paper explores the use of Reinforcement Learning (RL) and traditional graph algorithms like A* for mobile robots in the field of path planning and strategy development. The paper conducts a comprehensive analysis of these algorithms by evaluating their performance in terms of efficiency, scalability, and applicability in real-world scenarios. The results of the study show that while both RL and A* algorithms have their benefits and limitations, RL algorithms have the potential to provide more effective and scalable solutions for mobile robots in real-world applications. The paper also provides ongoing research directions aimed at improving the performance of these algorithms and concludes by offering valuable insights for researchers and practitioners working in the field of mobile robots.\r\nThe purpose of this project is to evaluate the performance of these algorithms, identify their benefits and limitations, and contribute to the development of more effective and practical solutions for mobile robots in real-world applications. The results of this study will be valuable for researchers and practitioners working in the field of mobile robots, as it will provide a comprehensive analysis of the use of RL and A* algorithms and offer ongoing research directions for improving their performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.15673/atbp.v15i2.2522"
    },
    {
        "id": 31612,
        "title": "SupervisorBot: NLP-Annotated Real-Time Recommendations of Psychotherapy Treatment Strategies with Deep Reinforcement Learning",
        "authors": "Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf",
        "published": "2023-8",
        "citations": 4,
        "abstract": "We present a novel recommendation system designed to provide real-time treatment strategies to therapists during psychotherapy sessions. Our system utilizes a turn-level rating mechanism that forecasts the therapeutic outcome by calculating a similarity score between the profound representation of a scoring inventory and the patient's current spoken sentence. By transcribing and segmenting the continuous audio stream into patient and therapist turns, our system conducts immediate evaluation of their therapeutic working alliance. The resulting dialogue pairs, along with their computed working alliance ratings, are then utilized in a deep reinforcement learning recommendation system. In this system, the sessions are treated as users, while the topics are treated as items. To showcase the system's effectiveness, we not only evaluate its performance using an existing dataset of psychotherapy sessions but also demonstrate its practicality through a web app. Through this demo, we aim to provide a tangible and engaging experience of our recommendation system in action.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/837"
    },
    {
        "id": 31613,
        "title": "Energy and spectrum efficient cell switch-off with channel and power allocation in ultra-dense networks: A deep reinforcement learning approach",
        "authors": "Zahra Rezaei, Behrouz Shahgholi Ghahfarokhi",
        "published": "2023-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.comnet.2023.109912"
    },
    {
        "id": 31614,
        "title": "Nearly Optimal Control for Mixed Zero-Sum Game Based on Off-Policy Integral Reinforcement Learning",
        "authors": "Ruizhuo Song, Gaofu Yang, Frank L. Lewis",
        "published": "2024-2",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2022.3191847"
    },
    {
        "id": 31615,
        "title": "Psychotherapy AI Companion with Reinforcement Learning Recommendations and Interpretable Policy Dynamics",
        "authors": "Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf",
        "published": "2023-4-30",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3543873.3587623"
    },
    {
        "id": 31616,
        "title": "Reinforcement learning policy recommendation for interbank network stability",
        "authors": "Alessio Brini, Gabriele Tedeschi, Daniele Tantari",
        "published": "2023-8",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jfs.2023.101139"
    },
    {
        "id": 31617,
        "title": "Comparison of Deep Reinforcement Learning-Based Guidance Strategies under Non-Ideal Conditions",
        "authors": "Sevket Utku Aydinli, Ali Turker Kutay",
        "published": "2023-7-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/codit58514.2023.10284472"
    },
    {
        "id": 31618,
        "title": "Social Engineering Attack-Defense Strategies Based on Reinforcement Learning",
        "authors": "Rundong Yang, Kangfeng Zheng, Xiujuan Wang, Bin Wu, Chunhua Wu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/csse.2023.038917"
    },
    {
        "id": 31619,
        "title": "Comparison of WiFi Interference Mitigation Strategies in DSME Networks: Leveraging Reinforcement Learning with Expected SARSA",
        "authors": "Ivonne Andrea Mantilla Gonzalez, Volker Turau",
        "published": "2023-9-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/meditcom58224.2023.10266605"
    },
    {
        "id": 31620,
        "title": "Advanced Deep Reinforcement Learning Strategies for Enhanced Autonomous Vehicle Navigation Systems",
        "authors": "M Dhinakaran, R.Thalapathi Rajasekaran, V. Balaji, V. Aarthi, S. Ambika",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ic457434.2024.10486336"
    },
    {
        "id": 31621,
        "title": "Logic + Reinforcement Learning + Deep Learning: A Survey",
        "authors": "Andreas Bueff, Vaishak Belle",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011746300003393"
    },
    {
        "id": 31622,
        "title": "Parallel Bootstrap-Based On-Policy Deep Reinforcement Learning for Continuous Fluid Flow Control Applications",
        "authors": "Jonathan Viquerat, Elie Hachem",
        "published": "2023-7-14",
        "citations": 1,
        "abstract": "The coupling of deep reinforcement learning to numerical flow control problems has recently received considerable attention, leading to groundbreaking results and opening new perspectives for the domain. Due to the usually high computational cost of fluid dynamics solvers, the use of parallel environments during the learning process represents an essential ingredient to attain efficient control in a reasonable time. Yet, most of the deep reinforcement learning literature for flow control relies on on-policy algorithms, for which the massively parallel transition collection may break theoretical assumptions and lead to suboptimal control models. To overcome this issue, we propose a parallelism pattern relying on partial-trajectory buffers terminated by a return bootstrapping step, allowing a flexible use of parallel environments while preserving the on-policiness of the updates. This approach is illustrated on a CPU-intensive continuous flow control problem from the literature.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/fluids8070208"
    },
    {
        "id": 31623,
        "title": "Combining Evolution and Deep Reinforcement Learning for Policy Search: A Survey",
        "authors": "Olivier Sigaud",
        "published": "2023-9-30",
        "citations": 5,
        "abstract": "Deep neuroevolution and deep Reinforcement Learning have received a lot of attention over the past few years. Some works have compared them, highlighting their pros and cons, but an emerging trend combines them so as to benefit from the best of both worlds. In this article, we provide a survey of this emerging trend by organizing the literature into related groups of works and casting all the existing combinations in each group into a generic framework. We systematically cover all easily available papers irrespective of their publication status, focusing on the combination mechanisms rather than on the experimental results. In total, we cover 45 algorithms more recent than 2017. We hope this effort will favor the growth of the domain by facilitating the understanding of the relationships between the methods, leading to deeper analyses, outlining missing useful comparisons and suggesting new combinations of mechanisms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3569096"
    },
    {
        "id": 31624,
        "title": "On-Policy Data-Driven Linear Quadratic Regulator via Model Reference Adaptive Reinforcement Learning",
        "authors": "Marco Borghesi, Alessandro Bosso, Giuseppe Notarstefano",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383516"
    },
    {
        "id": 31625,
        "title": "Reinforcement learning based monotonic policy for online resource allocation",
        "authors": "Pankaj Mishra, Ahmed Moustafa",
        "published": "2023-1",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.future.2021.09.023"
    },
    {
        "id": 31626,
        "title": "Reinforcement Learning based Security Policy to Mitigate Wormhole, Blackhole and Grayhole Attacks in MANET",
        "authors": "Ankita Chourasia, Sanjiv Tokekar",
        "published": "2024-2-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ic457434.2024.10486553"
    },
    {
        "id": 31627,
        "title": "Investigating navigation strategies in the Morris Water Maze through deep reinforcement learning",
        "authors": "Andrew Liu, Alla Borisyuk",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.12.004"
    },
    {
        "id": 31628,
        "title": "On Effectiveness of Exploration Strategies in Deep Reinforcement Learning for Power Allocation in Multi-Carrier Wireless Systems",
        "authors": "Amna Kopic, Kenan Turbic, Haris Gacanin",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccworkshops57953.2023.10283481"
    },
    {
        "id": 31629,
        "title": "Hosting Capacity Assessment Strategies and Reinforcement Learning Methods for Coordinated Voltage Control in Electricity Distribution Networks: A Review",
        "authors": "Jude Suchithra, Duane Robinson, Amin Rajabi",
        "published": "2023-3-1",
        "citations": 5,
        "abstract": "Increasing connection rates of rooftop photovoltaic (PV) systems to electricity distribution networks has become a major concern for the distribution network service providers (DNSPs) due to the inability of existing network infrastructure to accommodate high levels of PV penetration while maintaining voltage regulation and other operational requirements. The solution to this dilemma is to undertake a hosting capacity (HC) study to identify the maximum penetration limit of rooftop PV generation and take necessary actions to enhance the HC of the network. This paper presents a comprehensive review of two topics: HC assessment strategies and reinforcement learning (RL)-based coordinated voltage control schemes. In this paper, the RL-based coordinated voltage control schemes are identified as a means to enhance the HC of electricity distribution networks. RL-based algorithms have been widely used in many power system applications in recent years due to their precise, efficient and model-free decision-making capabilities. A large portion of this paper is dedicated to reviewing RL concepts and recently published literature on RL-based coordinated voltage control schemes. A non-exhaustive classification of RL algorithms for voltage control is presented and key RL parameters for the voltage control problem are identified. Furthermore, critical challenges and risk factors of adopting RL-based methods for coordinated voltage control are discussed.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/en16052371"
    },
    {
        "id": 31630,
        "title": "Decision-Making Strategies for Close-Range Air Combat Based on Reinforcement Learning with Variable-Scale Actions",
        "authors": "Lixin Wang, Jin Wang, Hailiang Liu, Ting Yue",
        "published": "2023-4-26",
        "citations": 2,
        "abstract": "The current research into decision-making strategies for air combat focuses on the performance of algorithms, while the selection of actions is often ignored, and the actions are often fixed in amplitude and limited in number in order to improve the convergence efficiency, making the strategy unable to give full play to the maneuverability of the aircraft. In this paper, a decision-making strategy for close-range air combat based on reinforcement learning with variable-scale actions is proposed; the actions are the variable-scale virtual pursuit angles and speeds. Firstly, a trajectory prediction method consisting of a real-time prediction, correction, and judgment of errors is proposed. The back propagation (BP) neural network and the long and short term memory (LSTM) neural network are used as base prediction network and correction prediction network, respectively. Secondly, the past, current, and future positions of the target aircraft are used as virtual pursuit points, and they are converted into virtual pursuit angles as the track angle commands using angle guidance law. Then, the proximity policy optimization (PPO) algorithm is applied to train the agent. The simulation results show that the attacking aircraft that uses the strategy proposed in this paper has a higher win rate during air combat and the attacking aircraft’s maneuverability is fully utilized.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/aerospace10050401"
    },
    {
        "id": 31631,
        "title": "Adaptive Evolutionary Reinforcement Learning with Policy Direction",
        "authors": "Caibo Dong, Dazi Li",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "AbstractEvolutionary Reinforcement Learning (ERL) has garnered widespread attention in recent years due to its inherent robustness and parallelism. However, the integration of Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) remains relatively rudimentary and lacks dynamism, which can impact the convergence performance of ERL algorithms. In this study, a dynamic adaptive module is introduced to balance the Evolution Strategies (ES) and RL training within ERL. By incorporating elite strategies, this module leverages advantageous individuals to elevate the overall population's performance. Additionally, RL strategy updates often lack guidance from the population. To address this, we incorporate the strategies of the best individuals from the population, providing valuable policy direction. This is achieved through the formulation of a loss function that employs either L1 or L2 regularization to facilitate RL training. The proposed framework is referred to as Adaptive Evolutionary Reinforcement Learning (AERL). The effectiveness of our framework is evaluated by adopting Soft Actor-Critic (SAC) as the RL algorithm and comparing it with other algorithms in the MuJoCo environment. The results underscore the outstanding convergence performance of our proposed Adaptive Evolutionary Soft Actor-Critic (AESAC) algorithm. Furthermore, ablation experiments are conducted to emphasize the necessity of these two improvements. It is worth noting that the enhancements in AESAC are realized at the population level, enabling broader exploration and effectively reducing the risk of falling into local optima.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-024-11548-6"
    },
    {
        "id": 31632,
        "title": "A new approach for drone tracking with drone using Proximal Policy Optimization based distributed deep reinforcement learning",
        "authors": "Ziya Tan, Mehmet Karaköse",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.softx.2023.101497"
    },
    {
        "id": 31633,
        "title": "Adaptive exploration network policy for effective exploration in reinforcement learning",
        "authors": "Min Li, William Zhu",
        "published": "2023-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2667206"
    },
    {
        "id": 31634,
        "title": "Dual policy iteration-reinforcement learning to optimize the detection quality of passive remote sensing device",
        "authors": "Rui Guo, Zhonghao Fu",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.sigpro.2023.109002"
    },
    {
        "id": 31635,
        "title": "Ensemble Reinforcement Learning in Continuous Spaces -- A Hierarchical Multi-Step Approach for Policy Training",
        "authors": "Gang Chen, Victoria Huang",
        "published": "2023-8",
        "citations": 1,
        "abstract": "Actor-critic deep reinforcement learning (DRL) algorithms have recently achieved prominent success in tackling various challenging reinforcement learning (RL) problems, particularly complex control tasks with high-dimensional continuous state and action spaces. Nevertheless, existing research showed that actor-critic DRL algorithms often failed to explore their learning environments effectively, resulting in limited learning stability and performance. To address this limitation, several ensemble DRL algorithms have been proposed lately to boost exploration and stabilize the learning process. However, most of existing ensemble algorithms do not explicitly train all base learners towards jointly optimizing the performance of the ensemble. In this paper, we propose a new technique to train an ensemble of base learners based on an innovative multi-step integration method. This training technique enables us to develop a new hierarchical learning algorithm for ensemble DRL that effectively promotes inter-learner collaboration through stable inter-learner parameter sharing. The design of our new algorithm is verified theoretically. The algorithm is also shown empirically to outperform several state-of-the-art DRL algorithms on multiple benchmark RL problems.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/391"
    },
    {
        "id": 31636,
        "title": "Efficient learning of power grid voltage control strategies via model-based deep reinforcement learning",
        "authors": "Ramij Raja Hossain, Tianzhixi Yin, Yan Du, Renke Huang, Jie Tan, Wenhao Yu, Yuan Liu, Qiuhua Huang",
        "published": "2023-11-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06422-w"
    },
    {
        "id": 31637,
        "title": "Plug-and-Play Model-Agnostic Counterfactual Policy Synthesis for Deep Reinforcement Learning-Based Recommendation",
        "authors": "Siyu Wang, Xiaocong Chen, Julian McAuley, Sally Cripps, Lina Yao",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3329808"
    },
    {
        "id": 31638,
        "title": "Off-Policy Reinforcement based on a Safe Model Eco-Driving Education for Fully-Automated, Connected Hybrid Vehicles",
        "authors": "Soumitra S Pande, Neeraja B, K Kalyan Kumar, Singarapu Sathish, Lakkakula Mounika, Jyoti Prasad Patra",
        "published": "2023-3-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icears56392.2023.10085149"
    },
    {
        "id": 31639,
        "title": "Reinforcement learning for intensive care medicine: actionable clinical insights from novel approaches to reward shaping and off-policy model evaluation",
        "authors": "Luca F. Roggeveen, Ali el Hassouni, Harm-Jan de Grooth, Armand R. J. Girbes, Mark Hoogendoorn, Paul W. G. Elbers,  ",
        "published": "2024-3-25",
        "citations": 0,
        "abstract": "Abstract\nBackground\nReinforcement learning (RL) holds great promise for intensive care medicine given the abundant availability of data and frequent sequential decision-making. But despite the emergence of promising algorithms, RL driven bedside clinical decision support is still far from reality. Major challenges include trust and safety. To help address these issues, we introduce cross off-policy evaluation and policy restriction and show how detailed policy analysis may increase clinical interpretability. As an example, we apply these in the setting of RL to optimise ventilator settings in intubated covid-19 patients.\n\nMethods\nWith data from the Dutch ICU Data Warehouse and using an exhaustive hyperparameter grid search, we identified an optimal set of Dueling Double-Deep Q Network RL models. The state space comprised ventilator, medication, and clinical data. The action space focused on positive end-expiratory pressure (peep) and fraction of inspired oxygen (FiO2) concentration. We used gas exchange indices as interim rewards, and mortality and state duration as final rewards. We designed a novel evaluation method called cross off-policy evaluation (OPE) to assess the efficacy of models under varying weightings between the interim and terminal reward components. In addition, we implemented policy restriction to prevent potentially hazardous model actions. We introduce delta-Q to compare physician versus policy action quality and in-depth policy inspection using visualisations.\n\nResults\nWe created trajectories for 1118 intensive care unit (ICU) admissions and trained 69,120 models using 8 model architectures with 128 hyperparameter combinations. For each model, policy restrictions were applied. In the first evaluation step, 17,182/138,240 policies had good performance, but cross-OPE revealed suboptimal performance for 44% of those by varying the reward function used for evaluation. Clinical policy inspection facilitated assessment of action decisions for individual patients, including identification of action space regions that may benefit most from optimisation.\n\nConclusion\nCross-OPE can serve as a robust evaluation framework for safe RL model implementation by identifying policies with good generalisability. Policy restriction helps prevent potentially unsafe model recommendations. Finally, the novel delta-Q metric can be used to operationalise RL models in clinical practice. Our findings offer a promising pathway towards application of RL in intensive care medicine and beyond.\n",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s40635-024-00614-x"
    },
    {
        "id": 31640,
        "title": "Bi-Level Off-Policy Reinforcement Learning for Two-Timescale Volt/VAR Control in Active Distribution Networks",
        "authors": "Haotian Liu, Wenchuan Wu, Yao Wang",
        "published": "2023-1",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tpwrs.2022.3168700"
    },
    {
        "id": 31641,
        "title": "Research on Reinforcement-Learning-Based Truck Platooning Control Strategies in Highway On-Ramp Regions",
        "authors": "Jiajia Chen, Zheng Zhou, Yue Duan, Biao Yu",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "With the development of autonomous driving technology, truck platooning control has become a reality. Truck platooning can improve road capacity by maintaining a minor headway. Platooning systems can significantly reduce fuel consumption and emissions, especially for trucks. In this study, we designed a Platoon-MAPPO algorithm to implement truck platooning control based on multi-agent reinforcement learning for a platooning facing an on-ramp scenario on highway. A centralized training, decentralized execution algorithm was used in this paper. Each truck only computes its actions, avoiding the data computation delay problem caused by centralized computation. Each truck considers the truck status in front of and behind itself, maximizing the overall gain of the platooning and improving the global operational efficiency. In terms of performance evaluation, we used the traditional rule-based platooning following model as a benchmark. To ensure fairness, the model used the same network structure and traffic scenario as our proposed model. The simulation results show that the algorithm proposed in this paper has good performance and improves the overall efficiency of the platoon while guaranteeing traffic safety. The average energy consumption decreased by 14.8%, and the road occupancy rate decreased by 43.3%.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/wevj14100273"
    },
    {
        "id": 31642,
        "title": "System of Automated Dynamic Search for Treatment Strategies Based on Reinforcement Learning Methods",
        "authors": "Irina Kashirina, Maria Demchenko, Yulia Bondarenko",
        "published": "2023-11-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/summa60232.2023.10349573"
    },
    {
        "id": 31643,
        "title": "Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing",
        "authors": "Yang Ni, Danny Abraham, Mariam Issa, Yeseong Kim, Pietro Mercati, Mohsen Imani",
        "published": "2023-6-5",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583781.3590298"
    },
    {
        "id": 31644,
        "title": "Reinforcement Learning-Based Intelligent Control Strategies for Optimal Power Management in Advanced Power Distribution Systems: A Survey",
        "authors": "Mudhafar Al-Saadi, Maher Al-Greer, Michael Short",
        "published": "2023-2-6",
        "citations": 11,
        "abstract": "Intelligent energy management in renewable-based power distribution applications, such as microgrids, smart grids, smart buildings, and EV systems, is becoming increasingly important in the context of the transition toward the decentralization, digitalization, and decarbonization of energy networks. Arguably, many challenges can be overcome, and benefits leveraged, in this transition by the adoption of intelligent autonomous computer-based decision-making through the introduction of smart technologies, specifically artificial intelligence. Unlike other numerical or soft computing optimization methods, the control based on artificial intelligence allows the decentralized power units to collaborate in making the best decision of fulfilling the administrator’s needs, rather than only a primitive decentralization based only on the division of tasks. Among the smart approaches, reinforcement learning stands as the most relevant and successful, particularly in power distribution management applications. The reason is it does not need an accurate model for attaining an optimized solution regarding the interaction with the environment. Accordingly, there is an ongoing need to accomplish a clear, up-to-date, vision of the development level, especially with the lack of recent comprehensive detailed reviews of this vitally important research field. Therefore, this paper fulfills the need and presents a comprehensive review of the state-of-the-art successful and distinguished intelligent control strategies-based RL in optimizing the management of power flow and distribution. Wherein extensive importance is given to the classification of the literature on emerging strategies, the proposals based on RL multiagent, and the multiagent primary secondary control of managing power flow in micro and smart grids, particularly the energy storage. As a result, 126 of the most relevant, recent, and non-incremental have been reviewed and put into relevant categories. Furthermore, salient features have been identified of the major positive and negative, of each selection.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/en16041608"
    },
    {
        "id": 31645,
        "title": "Adaptive Policy Learning for Offline-to-Online Reinforcement Learning",
        "authors": "Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, Jing Jiang",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Conventional reinforcement learning (RL) needs an environment to collect fresh data, which is impractical when online interactions are costly. Offline RL provides an alternative solution by directly learning from the previously collected dataset. However, it will yield unsatisfactory performance if the quality of the offline datasets is poor. In this paper, we consider an offline-to-online setting where the agent is first learned from the offline dataset and then trained online, and propose a framework called Adaptive Policy Learning for effectively taking advantage of offline and online data. Specifically, we explicitly consider the difference between the online and offline data and apply an adaptive update scheme accordingly, that is, a pessimistic update strategy for the offline dataset and an optimistic/greedy update scheme for the online dataset. Such a simple and effective method provides a way to mix the offline and online RL and achieve the best of both worlds. We further provide two detailed algorithms for implementing the framework through embedding value or policy-based RL algorithms into it. Finally, we conduct extensive experiments on popular continuous control tasks, and results show that our algorithm can learn the expert policy with high sample efficiency even when the quality of offline dataset is poor, e.g., random dataset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i9.26345"
    },
    {
        "id": 31646,
        "title": "Intelligent Game Strategies in Target-Missile-Defender Engagement Using Curriculum-Based Deep Reinforcement Learning",
        "authors": "Xiaopeng Gong, Wanchun Chen, Zhongyuan Chen",
        "published": "2023-1-31",
        "citations": 3,
        "abstract": "Aiming at the attack and defense game problem in the target-missile-defender three-body confrontation scenario, intelligent game strategies based on deep reinforcement learning are proposed, including an attack strategy applicable to attacking missiles and active defense strategy applicable to a target/defender. First, based on the classical three-body adversarial research, the reinforcement learning algorithm is introduced to improve the purposefulness of the algorithm training. The action spaces the reward and punishment conditions of both attack and defense confrontation are considered in the reward function design. Through the analysis of the sign of the action space and design of the reward function in the adversarial form, the combat requirements can be satisfied in both the missile and target/defender training. Then, a curriculum-based deep reinforcement learning algorithm is applied to train the agents and a convergent game strategy is obtained. The simulation results show that the attack strategy of the missile can maneuver according to the battlefield situation and can successfully hit the target after avoiding the defender. The active defense strategy enables the less capable target/defender to achieve the effect similar to a network adversarial attack on the missile agent, shielding targets from attack against missiles with superior maneuverability on the battlefield.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/aerospace10020133"
    },
    {
        "id": 31647,
        "title": "Re-exploring control strategies in a non-Markovian open quantum system by reinforcement learning",
        "authors": "Amine Jaouadi, Etienne Mangaud, Michèle Desouter-Lecomte",
        "published": "2024-1-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1103/physreva.109.013104"
    },
    {
        "id": 31648,
        "title": "Active structural control framework using policy-gradient reinforcement learning",
        "authors": "Soheila Sadeghi Eshkevari, Soheil Sadeghi Eshkevari, Debarshi Sen, Shamim N. Pakzad",
        "published": "2023-1",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engstruct.2022.115122"
    },
    {
        "id": 31649,
        "title": "Deep Reinforcement Learning: A Study of Reinforcement Learning with Neural Networks in Industrial Automation",
        "authors": "Asiri Iroshan",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4386667"
    },
    {
        "id": 31650,
        "title": "Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement Learning (Extended Abstract)",
        "authors": "Xiaoteng Ma, Shuai Ma, Li Xia, Qianchuan Zhao",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Keeping risk under control is often more crucial than maximizing expected rewards in real-world decision-making situations, such as finance, robotics, autonomous driving, etc. The most natural choice of risk measures is variance, while it penalizes the upside volatility as much as the downside part. Instead, the (downside) semivariance, which captures negative deviation of a random variable under its mean, is more suitable for risk-averse proposes. This paper aims at optimizing the mean-semivariance (MSV) criterion in reinforcement learning w.r.t. steady reward distribution. Since semivariance is time-inconsistent and does not satisfy the standard Bellman equation, the traditional dynamic programming methods are inapplicable to MSV problems directly. To tackle this challenge, we resort to Perturbation Analysis (PA) theory and establish the performance difference formula for MSV. We reveal that the MSV problem can be solved by iteratively solving a sequence of RL problems with a policy-dependent reward function. Further, we propose two on-policy algorithms based on the policy gradient theory and the trust region method. Finally, we conduct diverse experiments from simple bandit problems to continuous control tasks in MuJoCo, which demonstrate the effectiveness of our proposed methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/784"
    },
    {
        "id": 31651,
        "title": "Enhancing Multi-Agent Reinforcement Learning: Set Function Approximation and Dynamic Policy Adaptation",
        "authors": "Jayant Singh, Jing Zhou, Baltasar Beferull-Lozano",
        "published": "2023-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iecon51785.2023.10312543"
    },
    {
        "id": 31652,
        "title": "Sophisticated Swarm Reinforcement Learning by Incorporating Inverse Reinforcement Learning",
        "authors": "Yasuaki Kuroe, Kenya Takeuchi",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/smc53992.2023.10394525"
    },
    {
        "id": 31653,
        "title": "Trade-Off Between Robustness and Rewards Adversarial Training for Deep Reinforcement Learning Under Large Perturbations",
        "authors": "Jeffrey Huang, Ho Jin Choi, Nadia Figueroa",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lra.2023.3324590"
    },
    {
        "id": 31654,
        "title": "Artificial intelligence for decentralized water systems: A smart planning agent based on reinforcement learning for off-grid camp water infrastructures",
        "authors": "Christos Makropoulos, Dimitrios Bouziotas",
        "published": "2023-5-1",
        "citations": 4,
        "abstract": "Abstract\n\nThe planning and management of decentralized technologies in water systems is one of the promising, yet overlooked, domains where artificial intelligence (AI) can be successfully applied. In this study, we develop and deploy a reinforcement learning (RL)-based ‘smart planning agent’ capable of designing alternative decentralized water systems under demanding operational contexts. The agent's aim is to identify optimal water infrastructure configurations (i.e., proposed decisions on water management options and interventions) for different conditions with regard to climate, occupancy and water technology availability in a demanding, off-grid setting, i.e., a water system with high requirements of independence from centralized infrastructure. The agent is coupled with a source-to-tap water cycle simulation model capable of assessing and stress-testing the proposed configurations under different conditions. The approach is demonstrated in the case of a military camp deployed abroad for peacekeeping operations. The agent is tasked with selecting optimal interventions from an array of real-world camp water management technologies and evaluating their efficiency under highly variable, operational conditions explored through simulation. The results show that RL can be a useful addition to the arsenal of decision support systems (DSS) for distributed water system planning and management, especially under challenging, highly variable conditions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.2166/hydro.2023.168"
    },
    {
        "id": 31655,
        "title": "Factored Particle Swarm Optimization for Policy Co-training in Reinforcement Learning",
        "authors": "Kordel K. France, John W. Sheppard",
        "published": "2023-7-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3583131.3590376"
    },
    {
        "id": 31656,
        "title": "Development of Active Decoy Guidance Policy by Utilising Multi-Agent Reinforcement Learning",
        "authors": "Enver Bildik, Burak Yuksek, Antonios Tsourdos, Gokhan Inalhan",
        "published": "2023-1-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-2668"
    },
    {
        "id": 31657,
        "title": "Modeling limit order trading with a continuous action policy for deep reinforcement learning",
        "authors": "Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.05.051"
    },
    {
        "id": 31658,
        "title": "Probabilistic Policy Blending for Shared Autonomy using Deep Reinforcement Learning",
        "authors": "Saurav Singh, Jamison Heard",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ro-man57019.2023.10309604"
    },
    {
        "id": 31659,
        "title": "Cache Policy Design via Reinforcement Learning for Cellular Networks in Non-Stationary Environment",
        "authors": "Ashvin Srinivasan, Mohsen Amidzadeh, Junshan Zhang, Olav Tirkkonen",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccworkshops57953.2023.10283680"
    },
    {
        "id": 31660,
        "title": "A parallel heterogeneous policy deep reinforcement learning algorithm for bipedal walking motion design",
        "authors": "Chunguang Li, Mengru Li, Chongben Tao",
        "published": "2023-8-8",
        "citations": 2,
        "abstract": "Considering the dynamics and non-linear characteristics of biped robots, gait optimization is an extremely challenging task. To tackle this issue, a parallel heterogeneous policy Deep Reinforcement Learning (DRL) algorithm for gait optimization is proposed. Firstly, the Deep Deterministic Policy Gradient (DDPG) algorithm is used as the main architecture to run multiple biped robots in parallel to interact with the environment. And the network is shared to improve the training efficiency. Furthermore, heterogeneous experience replay is employed instead of the traditional experience replay mechanism to optimize the utilization of experience. Secondly, according to the walking characteristics of biped robots, a biped robot periodic gait is designed with reference to sinusoidal curves. The periodic gait takes into account the effects of foot lift height, walking period, foot lift speed and ground contact force of the biped robot. Finally, different environments and different biped robot models pose challenges for different optimization algorithms. Thus, a unified gait optimization framework for biped robots based on the RoboCup3D platform is established. Comparative experiments were conducted using the unified gait optimization framework, and the experimental results show that the method outlined in this paper can make the biped robot walk faster and more stably.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnbot.2023.1205775"
    },
    {
        "id": 31661,
        "title": "Model gradient: unified model and policy learning in model-based reinforcement learning",
        "authors": "Chengxing Jia, Fuxiang Zhang, Tian Xu, Jing-Cheng Pang, Zongzhang Zhang, Yang Yu",
        "published": "2024-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11704-023-3150-5"
    },
    {
        "id": 31662,
        "title": "Playing Various Strategies in Dominion with Deep Reinforcement Learning",
        "authors": "Jasper Gerigk, Steve Engels",
        "published": "2023-10-6",
        "citations": 0,
        "abstract": "Deck-building games, like Dominion, present an unsolved challenge for game AI research. The complexity arising from card interactions and the relative strength of strategies depending on the game configuration result in computer agents being limited to simple strategies. This paper describes the first application of recent advances in Geometric Deep Learning to deck-building games. We utilize a comprehensive multiset-based game representation and train the policy using a Soft Actor-Critic algorithm adapted to support variable-size sets of actions. The proposed model is the first successful learning-based agent that makes all decisions without relying on heuristics and supports a broader set of game configurations. It exceeds the performance of all previous learning-based approaches and is only outperformed by search-based approaches in certain game configurations. In addition, the paper presents modifications that induce agents to exhibit novel human-like play strategies. Finally, we show that learning strong strategies based on card combinations requires a reinforcement learning algorithm capable of discovering and executing a precise strategy while ignoring simpler suboptimal policies with higher immediate rewards.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aiide.v19i1.27518"
    },
    {
        "id": 31663,
        "title": "Balancing policy constraint and ensemble size in uncertainty-based offline reinforcement learning",
        "authors": "Alex Beeson, Giovanni Montana",
        "published": "2024-1",
        "citations": 0,
        "abstract": "AbstractOffline reinforcement learning agents seek optimal policies from fixed data sets. With environmental interaction prohibited, agents face significant challenges in preventing errors in value estimates from compounding and subsequently causing the learning process to collapse. Uncertainty estimation using ensembles compensates for this by penalising high-variance value estimates, allowing agents to learn robust policies based on data-driven actions. However, the requirement for large ensembles to facilitate sufficient penalisation results in significant computational overhead. In this work, we examine the role of policy constraints as a mechanism for regulating uncertainty, and the corresponding balance between level of constraint and ensemble size. By incorporating behavioural cloning into policy updates, we show empirically that sufficient penalisation can be achieved with a much smaller ensemble size, substantially reducing computational demand while retaining state-of-the-art performance on benchmarking tasks. Furthermore, we show how such an approach can facilitate stable online fine tuning, allowing for continued policy improvement while avoiding severe performance drops.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06458-y"
    },
    {
        "id": 31664,
        "title": "Designing Traffic Management Strategies Using Reinforcement Learning Techniques",
        "authors": "Christine Taylor, Erik Vargo, Emily Bromberg, Tyler Manderfield",
        "published": "2023-10",
        "citations": 0,
        "abstract": " The future vision for traffic flow management is one that leverages advanced automation to assist human decision-makers in the identification of potential constraints and the development of resolution strategies. What makes this problem so challenging is the inherent uncertainty associated with forecasting these constraints, leaving human decision-makers reliant on experience to devise effective traffic management initiatives to mitigate demand in excess of resource capacity. This paper proposes to employ artificial intelligence-based methods to recommend traffic management initiatives under forecast uncertainty and to do so in a real-time planning context. The proposed algorithm consists of 1) a policy network that is generated offline using an Expert Iteration algorithm, 2) a statistical model that updates the likelihood of constraint futures based on observations, and 3) a Monte Carlo tree search algorithm that explores possible combinations of traffic management initiatives to identify the recommended actions for the current decision. The skill introduced by each of the algorithmic components is assessed for a case study focused on managing arrivals into the Atlanta Hartsfield–Jackson International Airport over 92 validation days. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/1.d0339"
    },
    {
        "id": 31665,
        "title": "Imitation Learning-Based Performance-Power Trade-Off Uncore Frequency Scaling Policy for Multicore System",
        "authors": "Baonan Xiao, Jianfeng Yang, Xianxian Qi",
        "published": "2023-1-28",
        "citations": 0,
        "abstract": "As the importance of uncore components, such as shared cache slices and memory controllers, increases in processor architecture, the percentage of uncore power consumption in the overall power consumption of multicore processors rises significantly. To maximize the power efficiency of a multicore processor system, we investigate the uncore frequency scaling (UFS) policy and propose a novel imitation learning-based uncore frequency control policy. This policy performs online learning based on the DAgger algorithm and converts the annotation cost of online aggregation data into fine-tuning of the expert model. This design optimizes the online learning efficiency and improves the generality of the UFS policy on unseen loads. On the other hand, we shift our policy optimization target to Performance Per Watt (PPW), i.e., the power efficiency of the processor, to avoid saving a percentage of power while losing a larger percentage of performance. The experimental results show that our proposed policy outperforms the current advanced UFS policy in the benchmark test sequence of SPEC CPU2017. Our policy has a maximum improvement of about 10% relative to the performance-first policies. In the unseen processor load, the tuning decision made by our policy after collecting 50 aggregation data can maintain the processor stably near the optimal power efficiency state.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s23031449"
    },
    {
        "id": 31666,
        "title": "Model-Based Offline Reinforcement Learning with Uncertainty Estimation and Policy Constraint",
        "authors": "Jin Zhu, Chunhui Du, Geir E. Dullerud",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2024.3372939"
    },
    {
        "id": 31667,
        "title": "Multi-objective Reinforcement Learning with Path Integral Policy Improvement",
        "authors": "Ryo Ariizumi, Hayato Sago, Toru Asai, Shun-Ichi Azuma",
        "published": "2023-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/sice59929.2023.10354223"
    },
    {
        "id": 31668,
        "title": "Connecting planning horizons in mining complexes with reinforcement learning and stochastic programming",
        "authors": "Zachary Levinson, Roussos Dimitrakopoulos",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.resourpol.2023.104136"
    },
    {
        "id": 31669,
        "title": "Reinforcement learning of LQR control policy by a double inverted-pendulum biomechanical model",
        "authors": "Kamran Iqbal, Muhammad Haras",
        "published": "2023-4-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icit58465.2023.10143150"
    },
    {
        "id": 31670,
        "title": "MEPE: A Minimalist Ensemble Policy Evaluation Operator for Deep Reinforcement Learning",
        "authors": "Qiang He, Xinwen Hou",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10448009"
    },
    {
        "id": 31671,
        "title": "Deep Reinforcement Learning Framework with Representation Learning for Concurrent Negotiation",
        "authors": "Ryoga Miyajima, Katsuhide Fujita",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012336000003636"
    },
    {
        "id": 31672,
        "title": "Policy transfer of reinforcement learning-based flow control: From two- to three-dimensional environment",
        "authors": "",
        "published": "2023-5-1",
        "citations": 2,
        "abstract": "In the current paper, the zero-mass synthetic jet flow control combined with a proximal policy optimization (PPO) algorithm in deep reinforcement learning is constructed, and a policy transfer strategy which is trained in two-dimensional (2D) environment and migrated to three-dimensional (3D) environment is proposed and analyzed. By policy, we mean the flow control strategy of the agent learned by interacting with environment through deep reinforcement learning (DRL) algorithm. Through comprehensive evaluations of vortex separation in the cylindrical boundary layer and wake region at different Reynolds (Re) numbers, the PPO model trained in the 2D environment can reduce the drag coefficient by approximately 6.3%, 18.6%, and 23.7% at Re = 100, 200, and 300, respectively, when the spanwise length of the 3D environment is equal to the cylinder's diameter. Moreover, when the spanwise length is three times the diameter, the drag reduction capability is about 5.8%, 15.4%, and 13.1% at the three Re numbers, respectively. Additionally, the PPO model trained in the 2D environment also demonstrated outstanding migration learning capability in a new 3D flow field environment with varying Re numbers, successfully suppressing vortex shedding and reducing drag coefficient. Furthermore, the results illustrate that the model trained at high Re numbers could still reduce the drag coefficient in the 3D environment with low Re numbers, while the model trained at low Re numbers was not as effective at achieving drag reduction in the environments under high Re numbers. Overall, the proposed policy transfer strategy has been proven to be an effective method applying DRL agent trained in 2D flow to a new 3D environment.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1063/5.0147190"
    },
    {
        "id": 31673,
        "title": "Neuroevolutionary diversity policy search for multi-objective reinforcement learning",
        "authors": "Dan Zhou, Jiqing Du, Sachiyo Arai",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.119932"
    },
    {
        "id": 31674,
        "title": "Quantum Natural Policy Gradients: Towards Sample-Efficient Reinforcement Learning",
        "authors": "Nico Meyer, Daniel D. Scherer, Axel Plinge, Christopher Mutschler, Michael J. Hartmann",
        "published": "2023-9-17",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/qce57702.2023.10181"
    },
    {
        "id": 31675,
        "title": "PI-ELM: Reinforcement learning-based adaptable policy improvement for dynamical system",
        "authors": "Yingbai Hu, Xu Wang, Yueyue Liu, Weiping Ding, Alois Knoll",
        "published": "2023-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ins.2023.119700"
    },
    {
        "id": 31676,
        "title": "Progressive Diversifying Policy for Multi-Agent Reinforcement Learning",
        "authors": "Shaoqi Sun, Yuanzhao Zhai, Kele Xu, Dawei Feng, Bo Ding",
        "published": "2023-6-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp49357.2023.10096125"
    },
    {
        "id": 31677,
        "title": "Model free Reinforcement Learning to determine pricing policy for car parking lots",
        "authors": "Karri Sowmya, Meera M. Dhabu",
        "published": "2023-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.120532"
    },
    {
        "id": 31678,
        "title": "RLAR: A Reinforcement Learning Abductive Reasoner",
        "authors": "Mostafa ElHayani",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012425000003636"
    },
    {
        "id": 31679,
        "title": "Modeling market trading strategies of the intermediary entity for microgrids: A reinforcement learning-based approach",
        "authors": "Sanaz Ghanbari, Salah Bahramara, Hêmin Golpîra",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.epsr.2023.109989"
    },
    {
        "id": 31680,
        "title": "Optimizing Forwarding Strategies in Named Data Networking Using Reinforcement Learning",
        "authors": "Zhafirah Naghmah Ahmad, Fika Triana, Revita Rachel, Ridha Muldina Negara, Ratna Mayasari, Sri Astuti, Syamsul Rizal",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icwt58823.2023.10335311"
    },
    {
        "id": 31681,
        "title": "CPRL: Change Point Detection and Reinforcement Learning to Optimize Cache Placement Strategies",
        "authors": "Javane Rostampoor, Raviraj Adve, Ali Afana, Yahia Ahmed",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tcomm.2023.3341856"
    },
    {
        "id": 31682,
        "title": "Learning of Navigating Policy for Rotational Maze using DQN Reinforcement Learning",
        "authors": "Junheon An, Youngwan Cho",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5391/jkiis.2023.33.6.548"
    },
    {
        "id": 31683,
        "title": "Reinforcement learning for automatic detection of effective strategies for self-regulated learning",
        "authors": "Ikenna Osakwe, Guanliang Chen, Yizhou Fan, Mladen Rakovic, Xinyu Li, Shaveen Singh, Inge Molenaar, Maria Bannert, Dragan Gašević",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.caeai.2023.100181"
    },
    {
        "id": 31684,
        "title": "A novel adaptive fuzzy reinforcement learning controller for a platoon of off-axle hitching tractor-trailers with a prescribed performance and path curvature compensation",
        "authors": "Omid Elhaki, Khoshnam Shojaei",
        "published": "2023-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ejcon.2022.100735"
    },
    {
        "id": 31685,
        "title": "Proximal Policy Optimization-Based Reinforcement Learning and Hybrid Approaches to Explore the Cross Array Task Optimal Solution",
        "authors": "Samuel Corecco, Giorgia Adorni, Luca Maria Gambardella",
        "published": "2023-11-20",
        "citations": 0,
        "abstract": "In an era characterised by rapid technological advancement, the application of algorithmic approaches to address complex problems has become crucial across various disciplines. Within the realm of education, there is growing recognition of the pivotal role played by computational thinking (CT). This skill set has emerged as indispensable in our ever-evolving digital landscape, accompanied by an equal need for effective methods to assess and measure these skills. This research places its focus on the Cross Array Task (CAT), an educational activity designed within the Swiss educational system to assess students’ algorithmic skills. Its primary objective is to evaluate pupils’ ability to deconstruct complex problems into manageable steps and systematically formulate sequential strategies. The CAT has proven its effectiveness as an educational tool in tracking and monitoring the development of CT skills throughout compulsory education. Additionally, this task presents an enthralling avenue for algorithmic research, owing to its inherent complexity and the necessity to scrutinise the intricate interplay between different strategies and the structural aspects of this activity. This task, deeply rooted in logical reasoning and intricate problem solving, often poses a substantial challenge for human solvers striving for optimal solutions. Consequently, the exploration of computational power to unearth optimal solutions or uncover less intuitive strategies presents a captivating and promising endeavour. This paper explores two distinct algorithmic approaches to the CAT problem. The first approach combines clustering, random search, and move selection to find optimal solutions. The second approach employs reinforcement learning techniques focusing on the Proximal Policy Optimization (PPO) model. The findings of this research hold the potential to deepen our understanding of how machines can effectively tackle complex challenges like the CAT problem but also have broad implications, particularly in educational contexts, where these approaches can be seamlessly integrated into existing tools as a tutoring mechanism, offering assistance to students encountering difficulties. This can ultimately enhance students’ CT and problem-solving abilities, leading to an enriched educational experience.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/make5040082"
    },
    {
        "id": 31686,
        "title": "Q-LEARNING, POLICY ITERATION AND ACTOR-CRITIC REINFORCEMENT LEARNING COMBINED WITH METAHEURISTIC ALGORITHMS IN SERVO SYSTEM CONTROL",
        "authors": "Iuliu Alexandru Zamfirache, Radu-Emil Precup, Emil M. Petriu",
        "published": "2023-12-16",
        "citations": 0,
        "abstract": "This paper carries out the performance analysis of three control system structures and approaches, which combine Reinforcement Learning (RL) and Metaheuristic Algorithms (MAs) as representative optimization algorithms. In the first approach, the Gravitational Search Algorithm (GSA) is employed to initialize the parameters (weights and biases) of the Neural Networks (NNs) involved in Deep Q-Learning by replacing the traditional way of initializing the NNs based on random generated values. In the second approach, the Grey Wolf Optimizer (GWO) algorithm is employed to train the policy NN in Policy Iteration RL-based control. In the third approach, the GWO algorithm is employed as a critic in an Actor-Critic framework, and used to evaluate the performance of the actor NN. The goal of this paper is to analyze all three RL-based control approaches, aiming to determine which one represents the best fit for solving the proposed control optimization problem. The performance analysis is based on non-parametric statistical tests conducted on the data obtained from real-time experimental results specific to nonlinear servo system position control.",
        "keywords": "",
        "link": "http://dx.doi.org/10.22190/fume231011044z"
    },
    {
        "id": 31687,
        "title": "Policy-based Reinforcement Learning for Generalisation in Interactive Text-based Environments",
        "authors": "Edan Toledo, Jan Buys, Jonathan Shock",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.eacl-main.88"
    },
    {
        "id": 31688,
        "title": "Towards Hierarchical Policy Learning for Conversational Recommendation with Hypergraph-based Reinforcement Learning",
        "authors": "Sen Zhao, Wei Wei, Yifan Liu, Ziyang Wang, Wendi Li, Xian-Ling Mao, Shuai Zhu, Minghui Yang, Zujie Wen",
        "published": "2023-8",
        "citations": 0,
        "abstract": "Conversational recommendation systems (CRS) aim to timely and proactively acquire user dynamic preferred attributes through conversations for item recommendation. In each turn of CRS, there naturally have two decision-making processes with different roles that influence each other: 1) director, which is to select the follow-up option (i.e., ask or recommend) that is more effective for reducing the action space and acquiring user preferences; and 2) actor, which is to accordingly choose primitive actions (i.e., asked attribute or recommended item) to estimate the effectiveness of the director’s option. However, existing methods heavily rely on a unified decision-making module or heuristic rules, while neglecting to distinguish the roles of different decision procedures, as well as the mutual influences between them. To address this, we propose a novel Director-Actor Hierarchical Conversational Recommender (DAHCR), where the director selects the most effective option, followed by the actor accordingly choosing primitive actions that satisfy user preferences. Specifically, we develop a dynamic hypergraph to model user preferences and introduce an intrinsic motivation to train from weak supervision over the director. Finally, to alleviate the bad effect of model bias on the mutual influence between the director and actor, we model the director’s option by sampling from a categorical distribution. Extensive experiments demonstrate that DAHCR outperforms state-of-the-art methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/273"
    },
    {
        "id": 31689,
        "title": "Weighted Policy Constraints for Offline Reinforcement Learning",
        "authors": "Zhiyong Peng, Changlin Han, Yadong Liu, Zongtan Zhou",
        "published": "2023-6-26",
        "citations": 2,
        "abstract": "Offline reinforcement learning (RL) aims to learn policy from the passively collected offline dataset. Applying existing RL methods on the static dataset straightforwardly will raise distribution shift, causing these unconstrained RL methods to fail. To cope with the distribution shift problem, a common practice in offline RL is to constrain the policy explicitly or implicitly close to behavioral policy. However, the available dataset usually contains sub-optimal or inferior actions, constraining the policy near all these actions will make the policy inevitably learn inferior behaviors, limiting the performance of the algorithm. Based on this observation, we propose a weighted policy constraints (wPC) method that only constrains the learned policy to desirable behaviors, making room for policy improvement on other parts. Our algorithm outperforms existing state-of-the-art offline RL algorithms on the D4RL offline gym datasets. Moreover, the proposed algorithm is simple to implement with few hyper-parameters, making the proposed wPC algorithm a robust offline RL method with low computational complexity.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i8.26130"
    },
    {
        "id": 31690,
        "title": "Focused Crawler Based on Reinforcement Learning and Decaying Epsilon-Greedy Exploration Policy",
        "authors": "Parisa Begum Kaleel, Shina Sheen",
        "published": "2023",
        "citations": 0,
        "abstract": "In order to serve a diversified user base with a range of purposes, general search engines offer search results for a wide variety of topics and material categories on the internet. While Focused Crawlers (FC) deliver more specialized and targeted results inside particular domains or verticals, general search engines give a wider coverage of the web. For a vertical search engine, the performance of a focused crawler is extremely important, and several ways of improvement are applied. We propose an intelligent, focused crawler which uses Reinforcement Learning (RL) to prioritize the hyperlinks for long-term profit. Our implementation differs from other RL based works by encouraging learning at an early stage using a decaying ϵ-greedy policy to select the next link and hence enables the crawler to use the experience gained to improve its performance with more relevant pages. With an increase in the infertility rate all over the world, searching for information regarding the issues and details about artificial reproduction treatments available is in need by many people. Hence, we have considered infertility domain as a case study and collected web pages from scratch. We compare the performance of crawling tasks following ϵ-greedy and decaying ϵ-greedy policies. Experimental results show that crawlers following a decaying ϵ-greedy policy demonstrate better performance",
        "keywords": "",
        "link": "http://dx.doi.org/10.34028/iajit/20/5/14"
    },
    {
        "id": 31691,
        "title": "Discovering Symbolic Policy for Building Control using Reinforcement Learning",
        "authors": "Soo Kyung Kim, Chihyeon Song, Weizhe Chen, Jinkyoo Park, Saman Mostafavi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.ifacol.2023.10.1848"
    },
    {
        "id": 31692,
        "title": "Learning Explicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning via Polarization Policy Gradient",
        "authors": "Wubing Chen, Wenbin Li, Xiao Liu, Shangdong Yang, Yang Gao",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "Cooperative multi-agent policy gradient (MAPG) algorithms have recently attracted wide attention and are regarded as a general scheme for the multi-agent system. Credit assignment plays an important role in MAPG and can induce cooperation among multiple agents. However, most MAPG algorithms cannot achieve good credit assignment because of the game-theoretic pathology known as centralized-decentralized mismatch. To address this issue, this paper presents a novel method, Multi-Agent Polarization Policy Gradient (MAPPG). MAPPG takes a simple but efficient polarization function to transform the optimal consistency of joint and individual actions into easily realized constraints, thus enabling efficient credit assignment in MAPPG. Theoretically, we prove that individual policies of MAPPG can converge to the global optimum. Empirically, we evaluate MAPPG on the well-known matrix game and differential game, and verify that MAPPG can converge to the global optimum for both discrete and continuous action spaces. We also evaluate MAPPG on a set of StarCraft II micromanagement tasks and demonstrate that MAPPG outperforms the state-of-the-art MAPG algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i10.26364"
    },
    {
        "id": 31693,
        "title": "Prescribed intelligent elliptical pursuing by UAVs: A reinforcement learning policy",
        "authors": "Yi Xia, Xingling Shao, Tianyun Ding, Jun Liu",
        "published": "2024-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2024.123547"
    },
    {
        "id": 31694,
        "title": "Multi-Skill Policy Transfer by Option-based Deep Reinforcement Learning for Autonomous Driving",
        "authors": "Bo Wei, Jianxin Zhao, Yinuo Zhao, Feng Tian",
        "published": "2023-8-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/bigcom61073.2023.00045"
    },
    {
        "id": 31695,
        "title": "Non‐zero‐sum games of discrete‐time Markov jump systems with unknown dynamics: An off‐policy reinforcement learning method",
        "authors": "Xuewen Zhang, Hao Shen, Feng Li, Jing Wang",
        "published": "2024-1-25",
        "citations": 1,
        "abstract": "AbstractThis article concentrates on the non‐zero‐sum games problem of discrete‐time Markov jump systems without requiring the system dynamics information. First, the multiplayer non‐zero‐sum games problem can be converted to solve a set of coupled game algebraic Riccati equations, which is difficult to be solved directly. Then, to obtain the optimal control policies, a model‐based algorithm adapting the policy iteration approach is proposed. However, the model‐based algorithm relies on system dynamics information, which has the limitations in practice. Subsequently, an off‐policy reinforcement learning algorithm is given to get rid of the dependence on system dynamics information, which only uses the information of system states and inputs. Moreover, the proof of convergence and Nash equilibrium are also given. Finally, a numerical example is given to demonstrate the effectiveness of the proposed algorithms.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/rnc.7021"
    },
    {
        "id": 31696,
        "title": "Enhancing photovoltaic parameter estimation: integration of non-linear hunting and reinforcement learning strategies with golden jackal optimizer",
        "authors": "Chappani Sankaran Sundar Ganesh, Chandrasekaran Kumar, Manoharan Premkumar, Bizuwork Derebew",
        "published": "2024-2-2",
        "citations": 3,
        "abstract": "AbstractThe advancement of Photovoltaic (PV) systems hinges on the precise optimization of their parameters. Among the numerous optimization techniques, the effectiveness of each often rests on their inherent parameters. This research introduces a new methodology, the Reinforcement Learning-based Golden Jackal Optimizer (RL-GJO). This approach uniquely combines reinforcement learning with the Golden Jackal Optimizer to enhance its efficiency and adaptability in handling various optimization problems. Furthermore, the research incorporates an advanced non-linear hunting strategy to optimize the algorithm’s performance. The proposed algorithm is first validated using 29 CEC2017 benchmark test functions and five engineering-constrained design problems. Secondly, rigorous testing on PV parameter estimation benchmark datasets, including the single-diode model, double-diode model, three-diode model, and a representative PV module, was carried out to highlight the superiority of RL-GJO. The results were compelling: the root mean square error values achieved by RL-GJO were markedly lower than those of the original algorithm and other prevalent optimization methods. The synergy between reinforcement learning and GJO in this approach facilitates faster convergence and improved solution quality. This integration not only improves the performance metrics but also ensures a more efficient optimization process, especially in complex PV scenarios. With an average Freidman’s rank test values of 1.564 for numerical and engineering design problems and 1.742 for parameter estimation problems, the proposed RL-GJO is performing better than the original GJO and other peers. The proposed RL-GJO stands out as a reliable tool for PV parameter estimation. By seamlessly combining reinforcement learning with the golden jackal optimizer, it sets a new benchmark in PV optimization, indicating a promising avenue for future research and applications.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-024-52670-8"
    },
    {
        "id": 31697,
        "title": "Identifying environmentally sustainable pavement management strategies via deep reinforcement learning",
        "authors": "Ali Kazemeini, Omar Swei",
        "published": "2023-3",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jclepro.2023.136124"
    },
    {
        "id": 31698,
        "title": "A multi-agent reinforcement learning framework for optimizing financial trading strategies based on TimesNet",
        "authors": "Yuling Huang, Chujin Zhou, Kai Cui, Xiaoping Lu",
        "published": "2024-3",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.eswa.2023.121502"
    },
    {
        "id": 31699,
        "title": "Inverse Reinforcement Learning Integrated Reinforcement Learning for Single Intersection Traffic Signal Control",
        "authors": "Shiyi Gu, Tingting Zhang, Ya Zhang",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iai59504.2023.10327510"
    },
    {
        "id": 31700,
        "title": "Preventive maintenance policy in photovoltaic systems using Reinforcement Learning",
        "authors": "E.T Bacalhau, L Casacio, F Barbosa, Felipe Yamada, L Guimarães",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.19124/ima.2023.01.17"
    },
    {
        "id": 31701,
        "title": "Policy-Independent Behavioral Metric-Based Representation for Deep Reinforcement Learning",
        "authors": "Weijian Liao, Zongzhang Zhang, Yang Yu",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Behavioral metrics can calculate the distance between states or state-action pairs from the rewards and transitions difference. By virtue of their capability to filter out task-irrelevant information in theory, using them to shape a state embedding space becomes a new trend of representation learning for deep reinforcement learning (RL), especially when there are explicit distracting factors in observation backgrounds. However, due to the tight coupling between the metric and the RL policy, such metric-based methods may result in less informative embedding spaces which can weaken their aid to the baseline RL algorithm and even consume more samples to learn. We resolve this by proposing a new behavioral metric. It decouples the learning of RL policy and metric owing to its independence on RL policy. We theoretically justify its scalability to continuous state and action spaces and design a practical way to incorporate it into an RL procedure as a representation learning target. We evaluate our approach on DeepMind control tasks with default and distracting backgrounds. By statistically reliable evaluation protocols, our experiments demonstrate our approach is superior to previous metric-based methods in terms of sample efficiency and asymptotic performance in both backgrounds.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i7.26052"
    },
    {
        "id": 31702,
        "title": "Evolving Constrained Reinforcement Learning Policy",
        "authors": "Chengpeng Hu, Jiyuan Pei, Jialin Liu, Xin Yao",
        "published": "2023-6-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191982"
    },
    {
        "id": 31703,
        "title": "Human skill knowledge guided global trajectory policy reinforcement learning method",
        "authors": "Yajing Zang, Pengfei Wang, Fusheng Zha, Wei Guo, Chuanfeng Li, Lining Sun",
        "published": "2024-3-15",
        "citations": 0,
        "abstract": "Traditional trajectory learning methods based on Imitation Learning (IL) only learn the existing trajectory knowledge from human demonstration. In this way, it can not adapt the trajectory knowledge to the task environment by interacting with the environment and fine-tuning the policy. To address this problem, a global trajectory learning method which combinines IL with Reinforcement Learning (RL) to adapt the knowledge policy to the environment is proposed. In this paper, IL is proposed to acquire basic trajectory skills, and then learns the agent will explore and exploit more policy which is applicable to the current environment by RL. The basic trajectory skills include the knowledge policy and the time stage information in the whole task space to help learn the time series of the trajectory, and are used to guide the subsequent RL process. Notably, neural networks are not used to model the action policy and the Q value of RL during the RL process. Instead, they are sampled and updated in the whole task space and then transferred to the networks after the RL process through Behavior Cloning (BC) to get continuous and smooth global trajectory policy. The feasibility and the effectiveness of the method was validated in a custom Gym environment of a flower drawing task. And then, we executed the learned policy in the real-world robot drawing experiment.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3389/fnbot.2024.1368243"
    },
    {
        "id": 31704,
        "title": "Deep reinforcement learning for cost-optimal condition-based maintenance policy of offshore wind turbine components",
        "authors": "Jianda Cheng, Yan Liu, Wei Li, Tianyun Li",
        "published": "2023-9",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.oceaneng.2023.115062"
    }
]