[
    {
        "id": 33505,
        "title": "Dynamic Path Planning for Autonomous Vehicles Using Adaptive Reinforcement Learning",
        "authors": "Karim Wahdan, Nourhan Ehab, Yasmin Mansy, Amr El Mougy",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012363300003636"
    },
    {
        "id": 33506,
        "title": "Adaptive Action Supervision in Reinforcement Learning from Real-World Multi-Agent Demonstrations",
        "authors": "Keisuke Fujii, Kazushi Tsutsui, Atom Scott, Hiroshi Nakahara, Naoya Takeishi, Yoshinobu Kawahara",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012261100003636"
    },
    {
        "id": 33507,
        "title": "MERGE: Meta Reinforcement Learning for Tunable RL Agents at the Edge",
        "authors": "Sharda Tripathi, Carla Fabiana Chiasserini",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437278"
    },
    {
        "id": 33508,
        "title": "Deep Reinforcement Learning and Transfer Learning Methods Used in Autonomous Financial Trading Agents",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012194000003636"
    },
    {
        "id": 33509,
        "title": "Integrated and Adaptive Guidance and Control for Endoatmospheric Missiles via Reinforcement Meta-Learning",
        "authors": "Brian Gaudet, Roberto Furfaro",
        "published": "2023-1-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-2638"
    },
    {
        "id": 33510,
        "title": "Molecule Builder: Environment for Testing Reinforcement Learning Agents",
        "authors": "Petr Hyner, Jan Hůla, Mikoláš Janota",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012257900003595"
    },
    {
        "id": 33511,
        "title": "Reinforcement Learning Explained via Reinforcement Learning: Towards Explainable Policies through Predictive Explanation",
        "authors": "Léo Saulières, Martin Cooper, Florence Bannay",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011619600003393"
    },
    {
        "id": 33512,
        "title": "Logic + Reinforcement Learning + Deep Learning: A Survey",
        "authors": "Andreas Bueff, Vaishak Belle",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011746300003393"
    },
    {
        "id": 33513,
        "title": "A Study Toward Multi-Objective Multiagent Reinforcement Learning Considering Worst Case and Fairness Among Agents",
        "authors": "Toshihiro Matsui",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011687100003393"
    },
    {
        "id": 33514,
        "title": "RLAR: A Reinforcement Learning Abductive Reasoner",
        "authors": "Mostafa ElHayani",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012425000003636"
    },
    {
        "id": 33515,
        "title": "PenGym: Pentesting Training Framework for Reinforcement Learning Agents",
        "authors": "Thanh Nguyen, Zhi Chen, Kento Hasegawa, Kazuhide Fukushima, Razvan Beuran",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012367300003648"
    },
    {
        "id": 33516,
        "title": "Deep Reinforcement Learning Framework with Representation Learning for Concurrent Negotiation",
        "authors": "Ryoga Miyajima, Katsuhide Fujita",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012336000003636"
    },
    {
        "id": 33517,
        "title": "Towards Self-Adaptive Resilient Swarms Using Multi-Agent Reinforcement Learning",
        "authors": "Rafael Pina, Varuna De Silva, Corentin Artaud",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012462800003654"
    },
    {
        "id": 33518,
        "title": "Towards an Adaptive Pedagogical Agent in a Reading Intervention Using Reinforcement Learning",
        "authors": "Anna Riedmann, Birgit Lugrin",
        "published": "2023-9-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3570945.3607320"
    },
    {
        "id": 33519,
        "title": "Turn-Based Multi-Agent Reinforcement Learning Model Checking",
        "authors": "Dennis Gross",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011872800003393"
    },
    {
        "id": 33520,
        "title": "Competitive Reinforcement Learning Agents with Adaptive Networks",
        "authors": "Herman Pareli Nordaunet, Trym Bø, Evan Jåsund Kassab, Frank Veenstra, Ulysse Côté-Allard",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccma59762.2023.10374802"
    },
    {
        "id": 33521,
        "title": "Probabilistic Model Checking of Stochastic Reinforcement Learning Policies",
        "authors": "Dennis Gross, Helge Spieker",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012357700003636"
    },
    {
        "id": 33522,
        "title": "Autonomous Drone Takeoff and Navigation Using Reinforcement Learning",
        "authors": "Sana Ikli, Ilhem Quenel",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012296300003636"
    },
    {
        "id": 33523,
        "title": "Task Scheduling: A Reinforcement Learning Based Approach",
        "authors": "Ciprian Paduraru, Catalina Patilea, Stefan Iordache",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011826100003393"
    },
    {
        "id": 33524,
        "title": "Multi-Agent Deep Reinforcement Learning for Collaborative Task Scheduling",
        "authors": "Mali Gergely",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012434700003636"
    },
    {
        "id": 33525,
        "title": "A Recent Publications Survey on Reinforcement Learning for Selecting Parameters of Meta-Heuristic and Machine Learning Algorithms",
        "authors": "Maria Chernigovskaya, Andrey Kharitonov, Klaus Turowski",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011954300003488"
    },
    {
        "id": 33526,
        "title": "Teaching Reinforcement Learning Agents via Reinforcement Learning",
        "authors": "Kun Yang, Chengshuai Shi, Cong Shen",
        "published": "2023-3-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ciss56502.2023.10089695"
    },
    {
        "id": 33527,
        "title": "ADAPTIVE LEARNING THROUGH AI: REINFORCEMENT LEARNING IN TEACHING MULTIPLICATION TABLES",
        "authors": "Lara Drožđek, Igor Pesek",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.21125/inted.2024.1186"
    },
    {
        "id": 33528,
        "title": "Farsighter: Efficient Multi-Step Exploration for Deep Reinforcement Learning",
        "authors": "Yongshuai Liu, Xin Liu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011800600003393"
    },
    {
        "id": 33529,
        "title": "A Supervised Learning Approach to Robust Reinforcement Learning for Job Shop Scheduling",
        "authors": "Christoph Schmidl, Thiago Simão, Nils Jansen",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012473600003636"
    },
    {
        "id": 33530,
        "title": "Contextual Online Imitation Learning (COIL): Using Guide Policies in Reinforcement Learning",
        "authors": "Alexander Hill, Marc Groefsema, Matthia Sabatelli, Raffaella Carloni, Marco Grzegorczyk",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012312700003636"
    },
    {
        "id": 33531,
        "title": "Automatic Facility Layout Design System Using Deep Reinforcement Learning",
        "authors": "Hikaru Ikeda, Hiroyuki Nakagawa, Tatsuhiro Tsuchiya",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011678500003393"
    },
    {
        "id": 33532,
        "title": "Multi-Agent Archive-Based Inverse Reinforcement Learning by Improving Suboptimal Experts",
        "authors": "Shunsuke Ueki, Keiki Takadama",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012475100003636"
    },
    {
        "id": 33533,
        "title": "Learning Quadruped Locomotion Method Integrating Meta-Learning and Reinforcement Learning",
        "authors": "Xiafu Lv, Runming He, Chuangshi Wang, Yang Ou, Hong Wang, Zhenglin Chen",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450573"
    },
    {
        "id": 33534,
        "title": "Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents<sup>*</sup>",
        "authors": "Foozhan Ataiefard, Hadi Hemmati",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00099"
    },
    {
        "id": 33535,
        "title": "Adaptive Traffic Grooming Using Reinforcement Learning in Multilayer Elastic Optical Networks",
        "authors": "Takafumi Tanaka",
        "published": "2023",
        "citations": 0,
        "abstract": "We introduce a traffic grooming technique for multilayer networks that uses reinforcement learning. We confirm its superior performance over heuristic methods as regards its ability to meet several key requirements such as blocking probability and energy consumption.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1364/ofc.2023.tu2d.6"
    },
    {
        "id": 33536,
        "title": "Multi-Agent Quantum Reinforcement Learning Using Evolutionary Optimization",
        "authors": "Michael Kölle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas Nüßlein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012382800003636"
    },
    {
        "id": 33537,
        "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
        "authors": "Michael Kölle, Mohamad Hgog, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012383900003636"
    },
    {
        "id": 33538,
        "title": "An Adaptive Agent Decision Model Based on Deep Reinforcement  Learning and Autonomous Learning",
        "authors": "",
        "published": "2023-7-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.33168/jliss.2023.0309"
    },
    {
        "id": 33539,
        "title": "Meta-DAMS: Delay-Aware Multipath Scheduler using Hybrid Meta Reinforcement Learning",
        "authors": "Amir Sepahi, Lin Cai, Wenjun Yang, Jianping Pan",
        "published": "2023-10-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/vtc2023-fall60731.2023.10333611"
    },
    {
        "id": 33540,
        "title": "Adaptive Coding is Optimal in Reinforcement Learning",
        "authors": "Aldo Rustichini, Magdalena Soukupova, Stefano Palminteri",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4320894"
    },
    {
        "id": 33541,
        "title": "A Reinforcement Learning Environment for Directed Quantum Circuit Synthesis",
        "authors": "Michael Kölle, Tom Schubert, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012383200003636"
    },
    {
        "id": 33542,
        "title": "LSTM, ConvLSTM, MDN-RNN and GridLSTM Memory-based Deep Reinforcement Learning",
        "authors": "Fernando Duarte, Nuno Lau, Artur Pereira, Luís Reis",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011664900003393"
    },
    {
        "id": 33543,
        "title": "Targeted Adversarial Attacks on Deep Reinforcement Learning Policies via Model Checking",
        "authors": "Dennis Gross, Thiago Simão, Nils Jansen, Guillermo Pérez",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011693200003393"
    },
    {
        "id": 33544,
        "title": "Deep W-Networks: Solving Multi-Objective Optimisation Problems with Deep Reinforcement Learning",
        "authors": "Jernej Hribar, Luke Hackett, Ivana Dusparic",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011610300003393"
    },
    {
        "id": 33545,
        "title": "Subgoal Reachability in Goal Conditioned Hierarchical Reinforcement Learning",
        "authors": "Michał Bortkiewicz, Jakub Łyskawa, Paweł Wawrzyński, Mateusz Ostaszewski, Artur Grudkowski, Bartłomiej Sobieski, Tomasz Trzciński",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012326200003636"
    },
    {
        "id": 33546,
        "title": "Spontaneous emergence of eyes in reinforcement learning agents",
        "authors": "Zongfu Yu",
        "published": "2024-3-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3000685"
    },
    {
        "id": 33547,
        "title": "Variational Skill Embeddings for Meta Reinforcement Learning",
        "authors": "Jen-Tzung Chien, Weiwei Lai",
        "published": "2023-6-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ijcnn54540.2023.10191425"
    },
    {
        "id": 33548,
        "title": "Learning key steps to attack deep reinforcement learning agents",
        "authors": "Chien-Min Yu, Ming-Hsin Chen, Hsuan-Tien Lin",
        "published": "2023-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s10994-023-06318-9"
    },
    {
        "id": 33549,
        "title": "Adaptive Traffic Grooming Using Reinforcement Learning in Multilayer Elastic Optical Networks",
        "authors": "Takafumi Tanaka",
        "published": "2023-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ofc49934.2023.10117077"
    },
    {
        "id": 33550,
        "title": "DRL4HFC: Deep Reinforcement Learning for Container-Based Scheduling in Hybrid Fog/Cloud System",
        "authors": "Ameni Kallel, Molka Rekik, Mahdi Khemakhem",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012356800003636"
    },
    {
        "id": 33551,
        "title": "Variational Quantum Circuit Design for Quantum Reinforcement Learning on Continuous Environments",
        "authors": "Georg Kruse, Theodora-Augustina Drăgan, Robert Wille, Jeanette Miriam Lorenz",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012353100003636"
    },
    {
        "id": 33552,
        "title": "Reinforcement Learning Models for Adaptive Low Voltage Power System Operation",
        "authors": "Eleni Stai, Matteo Guscetti, Mathias Duckheim, Gabriela Hug",
        "published": "2023-6-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/powertech55446.2023.10202750"
    },
    {
        "id": 33553,
        "title": "A model-based approach to meta-Reinforcement Learning: Transformers and tree search",
        "authors": "Brieuc Pinon, Raphaël Jungers, Jean-Charles Delvenne",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14428/esann/2023.es2023-117"
    },
    {
        "id": 33554,
        "title": "Integration of Efficient Deep Q-Network Techniques Into QT-Opt Reinforcement Learning Structure",
        "authors": "Shudao Wei, Chenxing Li, Jan Seyler, Shahram Eivazi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011715000003393"
    },
    {
        "id": 33555,
        "title": "Meta-Reinforcement Learning with Transformer Networks for Space Guidance Applications",
        "authors": "Lorenzo Federici, Roberto Furfaro",
        "published": "2024-1-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2024-2061"
    },
    {
        "id": 33556,
        "title": "APAM: Adaptive Pre-Training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-Tailed Learning",
        "authors": "Bo Dong, Yiming Xu, Sunyi Chi, Zhenyu Shi, Zheng Du",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00283"
    },
    {
        "id": 33557,
        "title": "Learning Adaptive Cruise Control for Autonomous Vehicles Using End-to-End Deep Reinforcement Learning",
        "authors": "Mingfeng Yuan, Jinjun Shan",
        "published": "2023-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iecon51785.2023.10311652"
    },
    {
        "id": 33558,
        "title": "Channel-Adaptive Early Exiting Using Reinforcement Learning for Multivariate Time Series Classification",
        "authors": "Leonardos Pantiskas, Kees Verstoep, Mark Hoogendoorn, Henri Bal",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icmla58977.2023.00073"
    },
    {
        "id": 33559,
        "title": "A Deep Reinforcement Learning Framework for Training SARSA Agents (DQSARSA Agents) to Improve Consensus in Blockchain Networks",
        "authors": "Zakariya Qawaqneh, Arafat Abu Mallouh",
        "published": "2023-7-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iceccme57830.2023.10253427"
    },
    {
        "id": 33560,
        "title": "Data Augmentation Through Expert-Guided Symmetry Detection to Improve Performance in Offline Reinforcement Learning",
        "authors": "Giorgio Angelotti, Nicolas Drougard, Caroline Chanel",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011633400003393"
    },
    {
        "id": 33561,
        "title": "Reward Design for Deep Reinforcement Learning Towards Imparting Commonsense Knowledge in Text-Based Scenario",
        "authors": "Ryota Kubo, Fumito Uwano, Manabu Ohta",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012456900003636"
    },
    {
        "id": 33562,
        "title": "Temporal encoding in deep reinforcement learning agents",
        "authors": "Dongyan Lin, Ann Zixiang Huang, Blake Aaron Richards",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "AbstractNeuroscientists have observed both cells in the brain that fire at specific points in time, known as “time cells”, and cells whose activity steadily increases or decreases over time, known as “ramping cells”. It is speculated that time and ramping cells support temporal computations in the brain and carry mnemonic information. However, due to the limitations in animal experiments, it is difficult to determine how these cells really contribute to behavior. Here, we show that time cells and ramping cells naturally emerge in the recurrent neural networks of deep reinforcement learning models performing simulated interval timing and working memory tasks, which have learned to estimate expected rewards in the future. We show that these cells do indeed carry information about time and items stored in working memory, but they contribute to behavior in large part by providing a dynamic representation on which policy can be computed. Moreover, the information that they do carry depends on both the task demands and the variables provided to the models. Our results suggest that time cells and ramping cells could contribute to temporal and mnemonic calculations, but the way in which they do so may be complex and unintuitive to human observers.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1038/s41598-023-49847-y"
    },
    {
        "id": 33563,
        "title": "Intelligent Systems Context-Aware Adaptive Reinforcement Learning Approach for Autonomous Vehicles",
        "authors": "Chandrasekaran Sakthivel, V. Chandrasekar",
        "published": "2023-2-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icctech57499.2023.00012"
    },
    {
        "id": 33564,
        "title": "The Reinforcement-Learning-Based Adaptive Neighbor Discovery Algorithm for Data Transmission Enabled IoT",
        "authors": "Abhishek Singla",
        "published": "2023-11-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/incoft60753.2023.10425185"
    },
    {
        "id": 33565,
        "title": "Enhancing HVAC control systems through transfer learning with deep reinforcement learning agents",
        "authors": "Kevlyn Kadamala, Des Chambers, Enda Barrett",
        "published": "2024-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.segy.2024.100131"
    },
    {
        "id": 33566,
        "title": "RoMA: Resilient Multi-Agent Reinforcement Learning with Dynamic Participating Agents",
        "authors": "Xuting Tang, Jia Xu, Shusen Wang",
        "published": "2023-11-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cloudnet59005.2023.10490082"
    },
    {
        "id": 33567,
        "title": "Meta-ATMoS+: A Meta-Reinforcement Learning Framework for Threat Mitigation in Software-Defined Networks",
        "authors": "Hauton Tsang, Mohammad A. Salahuddin, Noura Limam, Raouf Boutaba",
        "published": "2023-10-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/lcn58197.2023.10223403"
    },
    {
        "id": 33568,
        "title": "An Adaptive Analytical FPGA Placement flow based on Reinforcement Learning",
        "authors": "C. Barn, S. Vermeulen, S. Areibi, G. Grewal",
        "published": "2023-12-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icm60448.2023.10378891"
    },
    {
        "id": 33569,
        "title": "Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning",
        "authors": "Ryan Shea, Zhou Yu",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.emnlp-main.110"
    },
    {
        "id": 33570,
        "title": "Boosting Deep Reinforcement Learning Agents with Generative Data Augmentation",
        "authors": "Tasos Papagiannis, Georgios Alexandridis, Andreas Stafylopatis",
        "published": "2023-12-29",
        "citations": 0,
        "abstract": "Data augmentation is a promising technique in improving exploration and convergence speed in deep reinforcement learning methodologies. In this work, we propose a data augmentation framework based on generative models for creating completely novel states and increasing diversity. For this purpose, a diffusion model is used to generate artificial states (learning the distribution of original, collected states), while an additional model is trained to predict the action executed between two consecutive states. These models are combined to create synthetic data for cases of high and low immediate rewards, which are encountered less frequently during the agent’s interaction with the environment. During the training process, the synthetic samples are mixed with the actually observed data in order to speed up agent learning. The proposed methodology is tested on the Atari 2600 framework, producing realistic and diverse synthetic data which improve training in most cases. Specifically, the agent is evaluated on three heterogeneous games, achieving a reward increase of up to 31%, although the results indicate performance variance among the different environments. The augmentation models are independent of the learning process and can be integrated to different algorithms, as well as different environments, with slight adaptations.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/app14010330"
    },
    {
        "id": 33571,
        "title": "Playing Card Games on Adjustable Shuffling with Reinforcement Learning Agents",
        "authors": "Ryotaro Suzuki, Fukuta Naoki",
        "published": "2023-7-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iiai-aai59060.2023.00132"
    },
    {
        "id": 33572,
        "title": "PARL: A Dialog System Framework with Prompts as Actions for Reinforcement Learning",
        "authors": "Tao Xiang, Yangzhe Li, Monika Wintergerst, Ana Pecini, Dominika Młynarczyk, Georg Groh",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011725200003393"
    },
    {
        "id": 33573,
        "title": "Proposal of a Signal Control Method Using Deep Reinforcement Learning with Pedestrian Traffic Flow",
        "authors": "Akimasa Murata, Yuichi Sei, Yasuyuki Tahara, Akihiko Ohsuga",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011665000003393"
    },
    {
        "id": 33574,
        "title": "Morphing Aircraft Adaptive Attitude Control Based on Deep Reinforcement Learning",
        "authors": "Shaojie Ma, Junpeng Hui, Yuhang Wang, Xuan Zhang",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10241099"
    },
    {
        "id": 33575,
        "title": "Distributed Reinforcement Learning Based Adaptive Antenna Pattern Selection for Wireless Communication Networks",
        "authors": "Yu-An Chen",
        "published": "2023-7-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icce-taiwan58799.2023.10226834"
    },
    {
        "id": 33576,
        "title": "Discrete-Time Reinforcement Learning Adaptive Control for Non-Gaussian Distribution of Sampling Intervals",
        "authors": "C. Treesatayapun",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3269441"
    },
    {
        "id": 33577,
        "title": "Adaptive Approximation Strategy to Reduce Approximation Error in Reinforcement Learning",
        "authors": "Min Li, William Zhu",
        "published": "2023-4-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccea58433.2023.10135234"
    },
    {
        "id": 33578,
        "title": "Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores",
        "authors": "Shukai Liu, Chenming Wu, Ying Li, Liangjun Zhang",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10341990"
    },
    {
        "id": 33579,
        "title": "Adaptive User Scheduling and Resource Allocation in Wireless Federated Learning Networks: A Deep Reinforcement Learning Approach",
        "authors": "Changxiang Wu, Yijing Ren, Daniel K. C. So",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279678"
    },
    {
        "id": 33580,
        "title": "Contrastive Learning-Based Bayes-Adaptive Meta-Reinforcement Learning for Active Pantograph Control in High-Speed Railways",
        "authors": "Hui Wang, Zhiwei Han, Xufan Wang, Yanbo Wu, Zhigang Liu",
        "published": "2024-3",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tte.2023.3293095"
    },
    {
        "id": 33581,
        "title": "Adaptive Water Environment Optimization Strategy Based on Reinforcement Learning",
        "authors": "Tianjiao Dang, Jifa Liu",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14733/cadaps.2024.s23.1-18"
    },
    {
        "id": 33582,
        "title": "Bridging the Reality Gap of Reinforcement Learning based Traffic Signal Control using Domain Randomization and Meta Learning",
        "authors": "Arthur Müller, Matthia Sabatelli",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10421987"
    },
    {
        "id": 33583,
        "title": "FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks",
        "authors": "Buse G. A. Tekgul,  N. Asokan",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3627106.3627128"
    },
    {
        "id": 33584,
        "title": "High Frequency Trading with Deep Reinforcement Learning Agents Under a Directional Changes Sampling Framework",
        "authors": "George Rayment, Michael Kampouridis",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10371966"
    },
    {
        "id": 33585,
        "title": "Toward an adaptive deep reinforcement learning agent for maritime platform defense",
        "authors": "Jared Markowitz, Edward W. Staley",
        "published": "2023-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2663831"
    },
    {
        "id": 33586,
        "title": "Personalized Adaptive Cruise Control with Deep Reinforcement Learning",
        "authors": "Zhuocheng Han, Xuelian Zheng, Yuanyuan Ren, Xiansheng Li, Qingju Wang",
        "published": "2023",
        "citations": 0,
        "abstract": "With the increasing of vehicle intelligence, how to integrate driving style into the autonomous driving decision-making strategies and enhance the driver's trust in the autonomous driving system has become a hot topic. In this paper, a new personalized adaptive cruise control algorithm taking the consideration of driver car-following style is designed. By filtering and reconstructing the driving data in the NGSIM database, indicators characterizing the car-following style are extracted, and K-means is used to cluster the car-following style into three categories: aggressive, general and conservative. A classification identification model is established to realize the online identification of the car-following style. The adaptive cruise controller is designed based on the Dueling Double Deep Q-Network algorithm, and driver car-following style is integrated into the reward function. Corresponding weight coefficients are set according to different working conditions, and the fuzzy rule is used to adjust the weight coefficients of the reward function in real time. The simulation platform is built based on Carsim and Matlab/Simulink to verify the performance of the proposed algorithm. The simulation results showed that the personalized adaptive cruise control algorithm can achieve accurate identification of the driver's car-following style and achieve stable control that incorporates the driver's car-following style. The research can provide reference for the subsequent implementation of more advanced personalized autonomous driving functions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.54941/ahfe1003421"
    },
    {
        "id": 33587,
        "title": "Design of Adaptive PID Controller Based on Deep Reinforcement Learning",
        "authors": "Xinyuan Yang",
        "published": "2023-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccasit58768.2023.10351739"
    },
    {
        "id": 33588,
        "title": "Accelerate Training of Reinforcement Learning Agent by Utilization of Current and Previous Experience",
        "authors": "Chenxing Li, Yinlong Liu, Zhenshan Bing, Fabian Schreier, Jan Seyler, Shahram Eivazi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011745600003393"
    },
    {
        "id": 33589,
        "title": "Adaptive Duty Cycle Control for Optimal Battery Energy Storage System Charging by Reinforcement Learning",
        "authors": "Richard Wiencek, Sagnika Ghosh",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cai54212.2023.00013"
    },
    {
        "id": 33590,
        "title": "Adaptive Reinforcement learning with Dij-Huff Method to Secure Optimal Route in Smart Healthcare System",
        "authors": "K.S. Madhuri, M. Jithendranath",
        "published": "2023-2-14",
        "citations": 0,
        "abstract": "The Wireless Sensor Network (WSN) is a multi-hop wireless network that contains multiple sensor nodes agreed in a self-organized mode. The significant advancement in the healthcare, the security of the medical data became huge disputes for healthcare services. Intrusion Detection System increasingly demands automatic and intelligent intrusion detection approaches to handle threats caused by a growing number of attackers in the WSN environment. Reinforcement Learning is a fundamental approach to improving routing efficiency. This approach introduces Adaptive Reinforcement learning with Dij-Huff Method (ARDM) for secure optimal route in WSN. Adaptive Reinforcement Learning (ARL) is a machine learning technique that selects the best forwarder node. The node energy, node Received Signal Strength, and node delay parameters determine the forwarder nodes in the WSN. Furthermore, the Dij-Huff Procedure (DHP) is a mixture of Dijkstra’s algorithm and Huffman coding. Dijkstra’s algorithm is applied to discover the sensor nodes with the highest energy and the best possible distance route. Huffman coding computes the binary hop count to offer each hop security. The simulation results and their comparison with conventional protocol demonstrate that the proposed scheme has a better detection ratio, a better throughput, and increased energy efficiency.",
        "keywords": "",
        "link": "http://dx.doi.org/10.18137/cardiometry.2022.25.11311139"
    },
    {
        "id": 33591,
        "title": "Intelligent Robot Path Planning and Navigation based on  Reinforcement Learning and Adaptive Control",
        "authors": "",
        "published": "2023-7-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.33168/jliss.2023.0318"
    },
    {
        "id": 33592,
        "title": "Adaptive overlay selection at the SD-WAN edges: A reinforcement learning approach with networked agents",
        "authors": "Alessio Botta, Roberto Canonico, Annalisa Navarro, Giovanni Stanco, Giorgio Ventre",
        "published": "2024-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.comnet.2024.110310"
    },
    {
        "id": 33593,
        "title": "Robust Driving Policy Learning with Guided Meta Reinforcement Learning",
        "authors": "Kanghoon Lee, Jiachen Li, David Isele, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochendorfer",
        "published": "2023-9-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itsc57777.2023.10422469"
    },
    {
        "id": 33594,
        "title": "Continual Meta-Reinforcement Learning for UAV-Aided Vehicular Wireless Networks",
        "authors": "Riccardo Marini, Sangwoo Park, Osvaldo Simeone, Chiara Buratti",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10279524"
    },
    {
        "id": 33595,
        "title": "Line of Sight Curvature for Missile Guidance using Reinforcement Meta-Learning",
        "authors": "Brian Gaudet, Roberto Furfaro",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-1627"
    },
    {
        "id": 33596,
        "title": "Modifying Neural Networks in Adversarial Agents of Multi-agent Reinforcement Learning Systems",
        "authors": "Neshat Elhami Fard, Rastko R. Selmic",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/med59994.2023.10185760"
    },
    {
        "id": 33597,
        "title": "Abstract PR001: Learning to adapt: Rational personalization of adaptive therapy using deep reinforcement learning",
        "authors": "Kit Gallagher, Maximilian Strobl, Robert Gatenby, Philip Maini, Alexander Anderson",
        "published": "2024-2-1",
        "citations": 0,
        "abstract": "Abstract\nStandard-of-care treatment regimens have long been designed for maximal cell kill, yet these strategies often fail when applied to metastatic cancers due to the emergence of drug resistance. Adaptive treatment strategies have been developed as an alternative approach, dynamically adjusting treatment to suppress the growth of treatment-resistant populations, delaying, or even preventing, tumor progression. Promising clinical results in prostate cancer indicate the potential to optimize adaptive treatment protocols. We propose the application of deep reinforcement learning (DRL) to guide adaptive drug scheduling, and demonstrate that these treatment schedules can outperform the current adaptive protocols in a mathematical model calibrated to prostate cancer dynamics, more than doubling the time to progression. We show that the DRL strategies are robust to patient variability, including both tumor dynamics and clinical monitoring schedules. We demonstrate that the DRL framework can produce interpretable, adaptive strategies based on a single tumor burden threshold, replicating and informing optimal treatment strategies. Given the DRL framework has no knowledge of the underlying mathematical tumor model, this demonstrates the capability of DRL to help develop treatment strategies in novel or complex settings. Finally, we propose a five-step pathway combining mechanistic modeling with the DRL framework to translate this approach to the clinic with limited information for a given patient. Integrating conventional tools to ensure the interpretability of traditionally `black-box' DRL models, we generate personalized treatment schedules that consistently outperform clinical standard-of-care protocols.\nCitation Format: Kit Gallagher, Maximilian Strobl, Robert Gatenby, Philip Maini, Alexander Anderson. Learning to adapt: Rational personalization of adaptive therapy using deep reinforcement learning [abstract]. In: Proceedings of the AACR Special Conference in Cancer Research: Translating Cancer Evolution and Data Science: The Next Frontier; 2023 Dec 3-6; Boston, Massachusetts. Philadelphia (PA): AACR; Cancer Res 2024;84(3 Suppl_2):Abstract nr PR001.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1158/1538-7445.canevol23-pr001"
    },
    {
        "id": 33598,
        "title": "Privacy-Preserving Deep Reinforcement Learning Based On Adaptive Noise For Sepsis Treatment",
        "authors": "Feng Yang, Xiangfei Zhang, Xinyu Zhao, Qingchen Zhang",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/swc57546.2023.10449170"
    },
    {
        "id": 33599,
        "title": "FP-WDDQN: An improved deep reinforcement learning algorithm for adaptive traffic signal control",
        "authors": "Xiao Zhang, Xiaolong Xu",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdmw60847.2023.00015"
    },
    {
        "id": 33600,
        "title": "Optimizing Forensic Investigation and Security Surveillance with Deep Reinforcement Learning Techniques",
        "authors": "T J Nandhini, K Thinakaran",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdsaai59313.2023.10452551"
    },
    {
        "id": 33601,
        "title": "Adaptive Online Optimization Control Method Based on Reinforcement Learning",
        "authors": "Zhaolei Wang, Chunmei Yu, Kunfeng Lu, Ruiguang Hu, Xu Huang, Ludi Wang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10450340"
    },
    {
        "id": 33602,
        "title": "Multi-UAV Adaptive Path Planning Using Deep Reinforcement Learning",
        "authors": "Jonas Westheider, Julius Rückin, Marija Popović",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10342516"
    },
    {
        "id": 33603,
        "title": "Adaptive Cyber Defense Technique Based on Multiagent Reinforcement Learning Strategies",
        "authors": "Adel Alshamrani, Abdullah Alshahrani",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/iasc.2023.032835"
    },
    {
        "id": 33604,
        "title": "Adaptive Streaming Scheme with Reinforcement Learning in Edge Computing Environments",
        "authors": "Jeongho Kang, Kwangsue Chung",
        "published": "2023-1-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icoin56518.2023.10048966"
    },
    {
        "id": 33605,
        "title": "Adaptive Reward Shifting Based on Behavior Proximity for Offline Reinforcement Learning",
        "authors": "Zhe Zhang, Xiaoyang Tan",
        "published": "2023-8",
        "citations": 0,
        "abstract": "One of the major challenges of the current offline reinforcement learning research is to deal with the distribution shift problem due to the change in state-action visitations for the new policy. To address this issue, we present a novel reward shifting-based method. Specifically, to regularize the behavior of the new policy at each state, we modify the reward to be received by the new policy by shifting it adaptively according to its proximity to the behavior policy, and apply the reward shifting along opposite directions for in-distribution actions and the ones not. In this way we are able to guide the learning procedure of the new policy itself by influencing the consequence of its actions explicitly, helping it to achieve a better balance between behavior constraints and policy improvement. Empirical results on the popular D4RL benchmarks show that the proposed method obtains competitive performance compared to the state-of-art baselines.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/514"
    },
    {
        "id": 33606,
        "title": "Adaptive manufacturing: dynamic resource allocation using multi-agent reinforcement learning",
        "authors": "David Heik, Fouad Bahrpeyma, Dirk Reichelt",
        "published": "2024-2-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.33968/2024.52"
    },
    {
        "id": 33607,
        "title": "Environment-Aware Adaptive Reinforcement Learning-Based Routing for Vehicular Ad Hoc Networks",
        "authors": "Yi Jiang, Jinlin Zhu, Kexin Yang",
        "published": "2023-12-20",
        "citations": 0,
        "abstract": "With the rapid development of the intelligent transportation system (ITS), routing in vehicular ad hoc networks (VANETs) has become a popular research topic. The high mobility of vehicles in urban streets poses serious challenges to routing protocols and has a significant impact on network performance. Existing topology-based routing is not suitable for highly dynamic VANETs, thereby making location-based routing protocols the preferred choice due to their scalability. However, the working environment of VANETs is complex and interference-prone. In wireless-network communication, the channel contention introduced by the high density of vehicles, coupled with urban structures, significantly increases the difficulty of designing high-quality communication protocols. In this context, compared to topology-based routing protocols, location-based geographic routing is widely employed in VANETs due to its avoidance of the route construction and maintenance phases. Considering the characteristics of VANETs, this paper proposes a novel environment-aware adaptive reinforcement routing (EARR) protocol aimed at establishing reliable connections between source and destination nodes. The protocol adopts periodic beacons to perceive and explore the surrounding environment, thereby constructing a local topology. By applying reinforcement learning to the vehicle network’s route selection, it adaptively adjusts the Q table through the perception of multiple metrics from beacons, including vehicle speed, available bandwidth, signal-reception strength, etc., thereby assisting the selection of relay vehicles and alleviating the challenges posed by the high dynamics, shadow fading, and limited bandwidth in VANETs. The combination of reinforcement learning and beacons accelerates the establishment of end-to-end routes, thereby guiding each vehicle to choose the optimal next hop and forming suboptimal routes throughout the entire communication process. The adaptive adjustment feature of the protocol enables it to address sudden link interruptions, thereby enhancing communication reliability. In experiments, the EARR protocol demonstrates significant improvements across various performance metrics compared to existing routing protocols. Throughout the simulation process, the EARR protocol maintains a consistently high packet-delivery rate and throughput compared to other protocols, as well as demonstrates stable performance across various scenarios. Finally, the proposed protocol demonstrates relatively consistent standardized latency and low overhead in all experiments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/s24010040"
    },
    {
        "id": 33608,
        "title": "UAV Control Method Combining Reptile Meta-Reinforcement Learning and Generative Adversarial Imitation Learning",
        "authors": "Shui Jiang, Yanning Ge, Xu Yang, Wencheng Yang, Hui Cui",
        "published": "2024-3-20",
        "citations": 0,
        "abstract": "Reinforcement learning (RL) is pivotal in empowering Unmanned Aerial Vehicles (UAVs) to navigate and make decisions efficiently and intelligently within complex and dynamic surroundings. Despite its significance, RL is hampered by inherent limitations such as low sample efficiency, restricted generalization capabilities, and a heavy reliance on the intricacies of reward function design. These challenges often render single-method RL approaches inadequate, particularly in the context of UAV operations where high costs and safety risks in real-world applications cannot be overlooked. To address these issues, this paper introduces a novel RL framework that synergistically integrates meta-learning and imitation learning. By leveraging the Reptile algorithm from meta-learning and Generative Adversarial Imitation Learning (GAIL), coupled with state normalization techniques for processing state data, this framework significantly enhances the model’s adaptability. It achieves this by identifying and leveraging commonalities across various tasks, allowing for swift adaptation to new challenges without the need for complex reward function designs. To ascertain the efficacy of this integrated approach, we conducted simulation experiments within both two-dimensional environments. The empirical results clearly indicate that our GAIL-enhanced Reptile method surpasses conventional single-method RL algorithms in terms of training efficiency. This evidence underscores the potential of combining meta-learning and imitation learning to surmount the traditional barriers faced by reinforcement learning in UAV trajectory planning and decision-making processes.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/fi16030105"
    },
    {
        "id": 33609,
        "title": "Object Detection for Reinforcement Learning Agents",
        "authors": "Benjamin Van Oostendorp",
        "published": "2023-12-31",
        "citations": 0,
        "abstract": "In traditional reinforcement learning applications with images as input, the observation for the agent to learn from, is an image. In these models, a Convolutional Neural Network (CNN) is typically used to extract the features before for the learning process, in order to maximize the cumulative reward. In this paper, a different approach for pre-processing the input for reinforcement learning agents is considered. The proposed approach uses object detectors instead instead of CNNs, and converts each input image into bounding boxes and object locations for the agent to learn from.",
        "keywords": "",
        "link": "http://dx.doi.org/10.52846/stccj.2023.3.2.51"
    },
    {
        "id": 33610,
        "title": "Meta Reinforcement Learning for Rate Adaptation",
        "authors": "Abdelhak Bentaleb, May Lim, Mehmet N. Akcay, Ali C. Begen, Roger Zimmermann",
        "published": "2023-5-17",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/infocom53939.2023.10228951"
    },
    {
        "id": 33611,
        "title": "MRLCC: an adaptive cloud task scheduling method based on meta reinforcement learning",
        "authors": "Xi Xiu, Jialun Li, Yujie Long, Weigang Wu",
        "published": "2023-5-10",
        "citations": 2,
        "abstract": "AbstractTask scheduling is a complex problem in cloud computing, and attracts many researchers’ interests. Recently, many deep reinforcement learning (DRL)-based methods have been proposed to learn the scheduling policy through interacting with the environment. However, most DRL methods focus on a specific environment, which may lead to a weak adaptability to new environments because they have low sample efficiency and require full retraining to learn updated policies for new environments. To overcome the weakness and reduce the time consumption of adapting to new environment, we propose a task scheduling method based on meta reinforcement learning called MRLCC. Through comparing MRLCC and baseline algorithms on the performance of shortening makespan in different environments, we can find that MRLCC is able to adapt to different environments quickly and has a high sample efficiency. Besides, the experimental results demonstrate that MRLCC can maintain a high utilization rate over all baseline algorithms after a few steps of gradient update.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1186/s13677-023-00440-8"
    },
    {
        "id": 33612,
        "title": "A Comparison of Reinforcement Learning and Deep Trajectory Based Stochastic Control Agents for Stepwise Mean-Variance Hedging",
        "authors": "Ali Fathi, Bernhard Hientzsch",
        "published": "2023",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2139/ssrn.4360692"
    },
    {
        "id": 33613,
        "title": "Deep Reinforcement Learning Approach for Flocking Control Problem of Multi-Agents in Obstruction Environment",
        "authors": "Li Meng, Jin Cheng, Han Zhang, Yujiao Dong",
        "published": "2023-7-24",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.23919/ccc58697.2023.10239748"
    },
    {
        "id": 33614,
        "title": "Comparative Analysis of Photovoltaic MPPT P&amp;O Algorithm and Reinforcement Learning Agents Utilizing Fuzzy Logic Reward System",
        "authors": "Richard Wiencek, Sagnika Ghosh",
        "published": "2023-10-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/naps58826.2023.10318563"
    },
    {
        "id": 33615,
        "title": "Multi-Agent Dynamic Area Coverage Based on Reinforcement Learning with Connected Agents",
        "authors": "Fatih Aydemir, Aydin Cetin",
        "published": "2023",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32604/csse.2023.031116"
    },
    {
        "id": 33616,
        "title": "Robust interplanetary trajectory design under multiple uncertainties via meta-reinforcement learning",
        "authors": "Lorenzo Federici, Alessandro Zavoli",
        "published": "2024-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.actaastro.2023.10.018"
    },
    {
        "id": 33617,
        "title": "Adaptive Ordered Information Extraction with Deep Reinforcement Learning",
        "authors": "Wenhao Huang, Jiaqing Liang, Zhixu Li, Yanghua Xiao, Chuanjun Ji",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.18653/v1/2023.findings-acl.863"
    },
    {
        "id": 33618,
        "title": "AUV Adaptive PID Control Method Based on Deep Reinforcement Learning",
        "authors": "Rui Liu, Zhiyan Cui, Yue Lian, Kai Li, Chengyi Liao, Xiaopeng Su",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10451043"
    },
    {
        "id": 33619,
        "title": "PPO-ABR: Proximal Policy Optimization based Deep Reinforcement Learning for Adaptive BitRate streaming",
        "authors": "Mandan Naresh, Paresh Saxena, Manik Gupta",
        "published": "2023-6-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iwcmc58020.2023.10182379"
    },
    {
        "id": 33620,
        "title": "Domain Adaptation of Reinforcement Learning Agents based on Network Service Proximity",
        "authors": "Kaushik Dey, Satheesh K. Perepu, Pallab Dasgupta, Abir Das",
        "published": "2023-6-19",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/netsoft57336.2023.10175507"
    },
    {
        "id": 33621,
        "title": "Quantum Reinforcement Learning for Solving a Stochastic Frozen Lake Environment and the Impact of Quantum Architecture Choices",
        "authors": "Theodora-Augustina Drăgan, Maureen Monnet, Christian Mendl, Jeanette Lorenz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0011673400003393"
    },
    {
        "id": 33622,
        "title": "Adaptive Individual Q-Learning—A Multiagent Reinforcement Learning Method for Coordination Optimization",
        "authors": "Zhen Zhang, Dongqing Wang",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tnnls.2024.3385097"
    },
    {
        "id": 33623,
        "title": "Meta Reinforcement Learning for Strategic IoT Deployments Coverage in Disaster-Response UAV Swarms",
        "authors": "Marwan Dhuheir, Aiman Erbad, Ala Al-Fuqaha",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10436973"
    },
    {
        "id": 33624,
        "title": "Aquarium: A Comprehensive Framework for Exploring Predator-Prey Dynamics Through Multi-Agent Reinforcement Learning Algorithms",
        "authors": "Michael Kölle, Yannick Erpelding, Fabian Ritz, Thomy Phan, Steffen Illium, Claudia Linnhoff-Popien",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.5220/0012382300003636"
    },
    {
        "id": 33625,
        "title": "Meta-Critic Reinforcement Learning for IOS-Assisted Multi-User Communications in Dynamic Environments",
        "authors": "Qinpei Luo, Boya Di, Zhu Han",
        "published": "2023-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/vtc2023-spring57618.2023.10199513"
    },
    {
        "id": 33626,
        "title": "A Rational Analysis of the Optimism Bias using Meta-Reinforcement Learning",
        "authors": "Johannes A. Schubert, Akshay K. Jagadish, Marcel Binz, Eric Schulz",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.32470/ccn.2023.1260-0"
    },
    {
        "id": 33627,
        "title": "Reinforcement learning and meta-decision-making",
        "authors": "Pieter Verbeke, Tom Verguts",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.cobeha.2024.101374"
    },
    {
        "id": 33628,
        "title": "Scaling Offline Evaluation of Reinforcement Learning Agents through Abstraction",
        "authors": "Josiah P. Hanna",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "A critical challenge for the widescale adoption of reinforcement learning (RL) is the need to give domain experts assurance that learned policies will improve decision-making -- and not lead to unacceptable behavior. To meet this challenge, my work aims to develop new methods for offline policy evaluation in real world RL domains. There has been much recent interest in offline evaluation and many advances. However, recent benchmarking efforts have also shown that there remains a substantial gap between current state-of-the-art methods and real world domains such as robotics. Towards scalable offline evaluation, my group is investigating the use of methods for abstraction and representation learning. In this new faculty highlight, I will present our recent results that show the promise of this direction for scaling offline evaluation in RL domains. I will then describe future directions in this line of that work which will further realize the promise of offline policy evaluation for increasing confidence in deployed RL.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i20.30283"
    },
    {
        "id": 33629,
        "title": "Deep-Q-Based Reinforcement Learning Method to Predict Accuracy of Atari Gaming Set Classification",
        "authors": "N Gobinathan, R Ponnusamy",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icdsaai59313.2023.10452644"
    },
    {
        "id": 33630,
        "title": "Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games",
        "authors": "Jonas Gillberg, Joakim Bergdahl, Alessandro Sestini, Andrew Eakins, Linus Gisslén",
        "published": "2023-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cog57401.2023.10333194"
    },
    {
        "id": 33631,
        "title": "Application of Reinforcement Learning Agents to Space Habitat Resource Management",
        "authors": "Matthew R. Rines, Michael G. Balchanos, Dimitri N. Mavris",
        "published": "2023-1-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-2376"
    },
    {
        "id": 33632,
        "title": "Deep hierarchical reinforcement learning for collaborative object transportation by heterogeneous agents",
        "authors": "Maram Hasan, Rajdeep Niyogi",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.compeleceng.2023.109066"
    },
    {
        "id": 33633,
        "title": "Adaptive Voltage and Frequency Regulation for Secondary Control via Reinforcement Learning for Islanded Microgrids",
        "authors": "Kouhyar Sheida, Mohammad Seyedi, Farzad Ferdowsi",
        "published": "2024-2-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tpec60005.2024.10472240"
    },
    {
        "id": 33634,
        "title": "Adaptive Routing with Hierarchical Reinforcement Learning on Dragonfly Networks",
        "authors": "Xuhong Cai, Mo Li, Xingyan Shi, Jiayou Shen, Chensizhu Wu, Yi Chen",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278794"
    },
    {
        "id": 33635,
        "title": "A Low Latency Adaptive Coding Spike Framework for Deep Reinforcement Learning",
        "authors": "Lang Qin, Rui Yan, Huajin Tang",
        "published": "2023-8",
        "citations": 1,
        "abstract": "In recent years, spiking neural networks (SNNs) have been used in reinforcement learning (RL) due to their low power consumption and event-driven features. However, spiking reinforcement learning (SRL), which suffers from fixed coding methods, still faces the problems of high latency and poor versatility. In this paper, we use learnable matrix multiplication to encode and decode spikes, improving the flexibility of the coders and thus reducing latency. Meanwhile, we train the SNNs using the direct training method and use two different structures for online and offline RL algorithms, which gives our model a wider range of applications. Extensive experiments have revealed that our method achieves optimal performance with ultra-low latency (as low as 0.8% of other SRL methods) and excellent energy efficiency (up to 5X the DNNs) in different algorithms and different environments.",
        "keywords": "",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/340"
    },
    {
        "id": 33636,
        "title": "Adaptive path planning using Gaussian process regression: a reinforcement learning approach",
        "authors": "Lanqing Zhao",
        "published": "2023-12-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.3012570"
    },
    {
        "id": 33637,
        "title": "Adaptive Inverse Deep Reinforcement Lyapunov learning control for a floating wind turbine",
        "authors": "Hadi Mohammadian KhalafAnsar, Jafar Keighobadi",
        "published": "2023-6-28",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.24200/sci.2023.61871.7532"
    },
    {
        "id": 33638,
        "title": "Analysis of Mode Switching Metrics for Deep Reinforcement Learning-Based Adaptive Modulation",
        "authors": "Wanqing Shi, Xiaohong Shen, Haiyan Wang",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/itoec57671.2023.10291670"
    },
    {
        "id": 33639,
        "title": "An Adaptive Task Scheduling Approach for Cloud Computing Using Deep Reinforcement Learning",
        "authors": "Pedram Amini, Amir Kalbasi",
        "published": "2024-5-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dchpc60845.2024.10454081"
    },
    {
        "id": 33640,
        "title": "Reinforcement learning control with function approximation via multivariate simplex splines",
        "authors": "Yiting Feng, Ye Zhou, Hann Woei Ho, Nor Ashidi Mat Isa",
        "published": "2023-3-8",
        "citations": 0,
        "abstract": "SummaryIn the field of optimal control for continuous nonlinear systems, function approximation methods are often employed to overcome the curse of dimensionality. Compared to other global function approximators like neural networks, multivariate splines can be easily evaluated and adapted on a local basis with linearity in the parameters. In this work, a multivariate spline based reinforcement learning (RL) strategy is proposed for solving the continuous‐time nonlinear control problem. Based on the classic value iteration method, multivariate splines are integrated into RL algorithms to approximate continuous value functions and policy functions from discrete action and value samples. Hence, the determined splines with updated coefficients can be utilized in continuous control of nonlinear systems. In the simulation experiment, the performance of the spline‐based RL control is evaluated in controlling an under‐actuated inverted pendulum. The proposed method is compared with the value iteration based discrete control strategy and the neural network based continuous control strategy. The simulation results indicate that the proposed method based on multivariate splines has better control performance with less state oscillations, energy consumption and convergence time in comparison with discrete value iteration and neural network based RL, and the adoption of simplex splines improves the function approximation efficiency with less computation time than neural network optimization.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1002/acs.3579"
    },
    {
        "id": 33641,
        "title": "Deep reinforcement learning for adaptive mesh refinement",
        "authors": "Corbin Foucart, Aaron Charous, Pierre F.J. Lermusiaux",
        "published": "2023-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.jcp.2023.112381"
    },
    {
        "id": 33642,
        "title": "On-Policy Data-Driven Linear Quadratic Regulator via Model Reference Adaptive Reinforcement Learning",
        "authors": "Marco Borghesi, Alessandro Bosso, Giuseppe Notarstefano",
        "published": "2023-12-13",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/cdc49753.2023.10383516"
    },
    {
        "id": 33643,
        "title": "Cooperative Adaptive Cruise Control and Energy Management System for HEVs Using Multi-Agent Deep Reinforcement Learning",
        "authors": "Shailesh Hegde",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.46720/fwc2023-sca-055"
    },
    {
        "id": 33644,
        "title": "Adaptive optimal secure wind power generation control for variable speed wind turbine systems via reinforcement learning",
        "authors": "Mahmood Mazare",
        "published": "2024-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.apenergy.2023.122034"
    },
    {
        "id": 33645,
        "title": "Adaptive short-time Fourier transform based on reinforcement learning",
        "authors": "Weikun Zhao, Chaofeng Wang, Ya Jiang, Wenbin Lin",
        "published": "2023-1-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccece58074.2023.10135451"
    },
    {
        "id": 33646,
        "title": "Causal Meta-Reinforcement Learning for Multimodal Remote Sensing Data Classification",
        "authors": "Wei Zhang, Xuesong Wang, Haoyu Wang, Yuhu Cheng",
        "published": "2024-3-16",
        "citations": 0,
        "abstract": "Multimodal remote sensing data classification can enhance a model’s ability to distinguish land features through multimodal data fusion. In this context, how to help models understand the relationship between multimodal data and target tasks has become the focus of researchers. Inspired by the human feedback learning mechanism, causal reasoning mechanism, and knowledge induction mechanism, this paper integrates causal learning, reinforcement learning, and meta learning into a unified remote sensing data classification framework and proposes causal meta-reinforcement learning (CMRL). First, based on the feedback learning mechanism, we overcame the limitations of traditional implicit optimization of fusion features and customized a reinforcement learning environment for multimodal remote sensing data classification tasks. Through feedback interactive learning between agents and the environment, we helped the agents understand the complex relationships between multimodal data and labels, thereby achieving full mining of multimodal complementary information.Second, based on the causal inference mechanism, we designed causal distribution prediction actions, classification rewards, and causal intervention rewards, capturing pure causal factors in multimodal data and preventing false statistical associations between non-causal factors and class labels. Finally, based on the knowledge induction mechanism, we designed a bi-layer optimization mechanism based on meta-learning. By constructing a meta training task and meta validation task simulation model in the generalization scenario of unseen data, we helped the model induce cross-task shared knowledge, thereby improving its generalization ability for unseen multimodal data. The experimental results on multiple sets of multimodal datasets showed that the proposed method achieved state-of-the-art performance in multimodal remote sensing data classification tasks.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/rs16061055"
    },
    {
        "id": 33647,
        "title": "Trajectory Design for Unmanned Aerial Vehicles via Meta-Reinforcement Learning",
        "authors": "Ziyang Lu, Xueyuan Wang, M. Cenk Gursoy",
        "published": "2023-5-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/infocomwkshps57453.2023.10226090"
    },
    {
        "id": 33648,
        "title": "Reinforcement learning-based aggregation for robot swarms",
        "authors": "Arash Sadeghi Amjadi, Cem Bilaloğlu, Ali Emre Turgut, Seongin Na, Erol Şahin, Tomáš Krajník, Farshad Arvin",
        "published": "2023-9-15",
        "citations": 0,
        "abstract": "Aggregation, the gathering of individuals into a single group as observed in animals such as birds, bees, and amoeba, is known to provide protection against predators or resistance to adverse environmental conditions for the whole. Cue-based aggregation, where environmental cues determine the location of aggregation, is known to be challenging when the swarm density is low. Here, we propose a novel aggregation method applicable to real robots in low-density swarms. Previously, Landmark-Based Aggregation (LBA) method had used odometric dead-reckoning coupled with visual landmarks and yielded better aggregation in low-density swarms. However, the method’s performance was affected adversely by odometry drift, jeopardizing its application in real-world scenarios. In this article, a novel Reinforcement Learning-based Aggregation method, RLA, is proposed to increase aggregation robustness, thus making aggregation possible for real robots in low-density swarm settings. Systematic experiments conducted in a kinematic-based simulator and on real robots have shown that the RLA method yielded larger aggregates, is more robust to odometry noise than the LBA method, and adapts better to environmental changes while not being sensitive to parameter tuning, making it better deployable under real-world conditions.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1177/10597123231202593"
    },
    {
        "id": 33649,
        "title": "Discrete-time robust event-triggered actuator fault-tolerant control based on adaptive networks and reinforcement learning",
        "authors": "C. Treesatayapun",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.neunet.2023.08.003"
    },
    {
        "id": 33650,
        "title": "Federated Meta Reinforcement Learning for Personalized Tasks",
        "authors": "Wentao Liu, Xiaolong Xu, Jintao Wu, Jielin Jiang",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.26599/tst.2023.9010066"
    },
    {
        "id": 33651,
        "title": "Simulated Autonomous Driving Using Reinforcement Learning: A Comparative Study on Unity’s ML-Agents Framework",
        "authors": "Yusef Savid, Reza Mahmoudi, Rytis Maskeliūnas, Robertas Damaševičius",
        "published": "2023-5-14",
        "citations": 3,
        "abstract": "Advancements in artificial intelligence are leading researchers to find use cases that were not as straightforward to solve in the past. The use case of simulated autonomous driving has been known as a notoriously difficult task to automate, but advancements in the field of reinforcement learning have made it possible to reach satisfactory results. In this paper, we explore the use of the Unity ML-Agents toolkit to train intelligent agents to navigate a racing track in a simulated environment using RL algorithms. The paper compares the performance of several different RL algorithms and configurations on the task of training kart agents to successfully traverse a racing track and identifies the most effective approach for training kart agents to navigate a racing track and avoid obstacles in that track. The best results, value loss of 0.0013 and a cumulative reward of 0.761, were yielded using the Proximal Policy Optimization algorithm. After successfully choosing a model and algorithm that can traverse the track with ease, different objects were added to the track and another model (which used behavioral cloning as a pre-training option) was trained to avoid such obstacles. The aforementioned model resulted in a value loss of 0.001 and a cumulative reward of 0.068, proving that behavioral cloning can help achieve satisfactory results where the in game agents are able to avoid obstacles more efficiently and complete the track with human-like performance, allowing for a deployment of intelligent agents in racing simulators.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/info14050290"
    },
    {
        "id": 33652,
        "title": "Deep-reinforcement-learning-based range-adaptive distributed power control for cellular-V2X",
        "authors": "Wooyeol Yang, Han-Shin Jo",
        "published": "2023-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.icte.2022.07.008"
    },
    {
        "id": 33653,
        "title": "Adaptive Evolutionary Reinforcement Learning with Policy Direction",
        "authors": "Caibo Dong, Dazi Li",
        "published": "2024-2-23",
        "citations": 0,
        "abstract": "AbstractEvolutionary Reinforcement Learning (ERL) has garnered widespread attention in recent years due to its inherent robustness and parallelism. However, the integration of Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) remains relatively rudimentary and lacks dynamism, which can impact the convergence performance of ERL algorithms. In this study, a dynamic adaptive module is introduced to balance the Evolution Strategies (ES) and RL training within ERL. By incorporating elite strategies, this module leverages advantageous individuals to elevate the overall population's performance. Additionally, RL strategy updates often lack guidance from the population. To address this, we incorporate the strategies of the best individuals from the population, providing valuable policy direction. This is achieved through the formulation of a loss function that employs either L1 or L2 regularization to facilitate RL training. The proposed framework is referred to as Adaptive Evolutionary Reinforcement Learning (AERL). The effectiveness of our framework is evaluated by adopting Soft Actor-Critic (SAC) as the RL algorithm and comparing it with other algorithms in the MuJoCo environment. The results underscore the outstanding convergence performance of our proposed Adaptive Evolutionary Soft Actor-Critic (AESAC) algorithm. Furthermore, ablation experiments are conducted to emphasize the necessity of these two improvements. It is worth noting that the enhancements in AESAC are realized at the population level, enabling broader exploration and effectively reducing the risk of falling into local optima.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s11063-024-11548-6"
    },
    {
        "id": 33654,
        "title": "An Adaptive Hybrid Automatic Repeat Request (A-HARQ) Scheme Based on Reinforcement Learning",
        "authors": "Shih-Yang Lin, Miao-Hui Yang, Shuo Jia",
        "published": "2023-10-3",
        "citations": 1,
        "abstract": "V2X communication is susceptible to attenuation and fading caused by external interference. This interference often leads to bit error and poor quality and stability of the wireless link, and it can easily disrupt packet transmission. In order to enhance communication reliability, the 3rd Generation Partnership Project (3GPP) introduced the Hybrid Automatic Repeat Request (HARQ) technology for both 4G and 5G systems. Nevertheless, it can be improved for poor communication conditions (e.g., heavy traffic flow, long-distance transmission), especially in advanced or cooperative driving scenarios. In this paper, we propose an Adaptive Hybrid Automatic Repeat Request (A-HARQ) scheme that can reduce the average block error rate, the average number of retransmissions, and the round-trip time (RTT). It adapts the Q-learning model to select the timing and frequency of retransmission to enhance the transmission reliability. We also design some transmission schemes—K-repetition, T-delay and [T, K]-overlap—which are used to shorten latency and avoid packet collision. Compared with the conventional 5G HARQ, our simulation results show that the proposed A-HARQ scheme decreases the system’s average BLER, the number of retransmissions, and the RTT to 5.55%, 1.55 ms, and 0.97 ms, respectively.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/electronics12194127"
    },
    {
        "id": 33655,
        "title": "A Deep Reinforcement Learning Decision-Making Approach for Adaptive Cruise Control in Autonomous Vehicles",
        "authors": "Dany Ghraizi, Reine Talj, Clovis Francis",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icar58858.2023.10406331"
    },
    {
        "id": 33656,
        "title": "Meta-GNAS: Meta-reinforcement learning for graph neural architecture search",
        "authors": "YuFei Li, Jia Wu, TianJin Deng",
        "published": "2023-8",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.engappai.2023.106300"
    },
    {
        "id": 33657,
        "title": "Adaptive Deep Reinforcement Learning Approach for Service Migration in MEC-Enabled Vehicular Networks",
        "authors": "Sabri Khamari, Abdennour Rachedi, Toufik Ahmed, Mohamed Mosbah",
        "published": "2023-7-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iscc58397.2023.10218103"
    },
    {
        "id": 33658,
        "title": "MOEA with adaptive operator based on reinforcement learning for weapon target assignment",
        "authors": "Shiqi Zou, Xiaoping Shi, Shenmin Song",
        "published": "2024",
        "citations": 0,
        "abstract": "<abstract><p>Weapon target assignment (WTA) is a typical problem in the command and control of modern warfare. Despite the significance of the problem, traditional algorithms still have shortcomings in terms of efficiency, solution quality, and generalization. This paper presents a novel multi-objective evolutionary optimization algorithm (MOEA) that integrates a deep Q-network (DQN)-based adaptive mutation operator and a greedy-based crossover operator, designed to enhance the solution quality for the multi-objective WTA (MO-WTA). Our approach (NSGA-DRL) evolves NSGA-II by embedding these operators to strike a balance between exploration and exploitation. The DQN-based adaptive mutation operator is developed for predicting high-quality solutions, thereby improving the exploration process and maintaining diversity within the population. In parallel, the greedy-based crossover operator employs domain knowledge to minimize ineffective searches, focusing on exploitation and expediting convergence. Ablation studies revealed that our proposed operators significantly boost the algorithm performance. In particular, the DQN mutation operator shows its predictive effectiveness in identifying candidate solutions. The proposed NSGA-DRL outperforms state-and-art MOEAs in solving MO-WTA problems by generating high-quality solutions.</p></abstract>",
        "keywords": "",
        "link": "http://dx.doi.org/10.3934/era.2024069"
    },
    {
        "id": 33659,
        "title": "Cooperative Adaptive Cruise Control Based on Reinforcement Learning for Heavy-Duty BEVs",
        "authors": "Matteo Acquarone, Federico Miretti, Daniela Misul, Luca Sassara",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/access.2023.3331827"
    },
    {
        "id": 33660,
        "title": "Adaptive exploration network policy for effective exploration in reinforcement learning",
        "authors": "Min Li, William Zhu",
        "published": "2023-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2667206"
    },
    {
        "id": 33661,
        "title": "Distributed web hacking by adaptive consensus-based reinforcement learning",
        "authors": "Nemanja Ilić, Dejan Dašić, Miljan Vučetić, Aleksej Makarov, Ranko Petrović",
        "published": "2024-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.artint.2023.104032"
    },
    {
        "id": 33662,
        "title": "Exploration of Adaptive Environment Design Strategy Based on Reinforcement Learning in CAD Environment",
        "authors": "Xiaolong Chen, Lin Chen",
        "published": "2024-3-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14733/cadaps.2024.s23.175-190"
    },
    {
        "id": 33663,
        "title": "Safety Validation for Deep Reinforcement Learning Based Aircraft Separation Assurance with Adaptive Stress Testing",
        "authors": "Wei Guo, Marc Brittain, Peng Wei",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/dasc58513.2023.10311238"
    },
    {
        "id": 33664,
        "title": "Meta Reinforcement Learning based Energy Management in Microgrids under Extreme Weather Events",
        "authors": "Yibing Dang, Jiangjiao Xu, Dongdong Li",
        "published": "2023-12-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/aees59800.2023.10469455"
    },
    {
        "id": 33665,
        "title": "Meta Federated Reinforcement Learning for Distributed Resource Allocation",
        "authors": "Zelin Ji, Zhijin Qin, Xiaoming Tao",
        "published": "2024",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/twc.2023.3345363"
    },
    {
        "id": 33666,
        "title": "Robot Arm Movement Control by Model-based Reinforcement Learning using Machine Learning Regression Techniques and Particle Swarm Optimization",
        "authors": "Meta Mueangprasert, Pisak Chermprayong, Kittipong Boonlong",
        "published": "2023-1-18",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ica-symp56348.2023.10044940"
    },
    {
        "id": 33667,
        "title": "Correction: Landmark based guidance for reinforcement learning agents under partial observability",
        "authors": "Alper Demir, Erkin Çilden, Faruk Polat",
        "published": "2023-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s13042-022-01763-9"
    },
    {
        "id": 33668,
        "title": "Autonomous cyber warfare agents: dynamic reinforcement learning for defensive cyber operations",
        "authors": "David Bierbrauer, Robert Schabinger, Caleb Carlin, Jonathan Mullin, John Pavlik, Nathaniel D. Bastian",
        "published": "2023-6-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2663093"
    },
    {
        "id": 33669,
        "title": "Explaining Reinforcement Learning Agents through Counterfactual Action Outcomes",
        "authors": "Yotam Amitai, Yael Septon, Ofra Amir",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Explainable reinforcement learning (XRL) methods aim to help elucidate agent policies and decision-making processes. The majority of XRL approaches focus on local explanations, seeking to shed light on the reasons an agent acts the way it does at a specific world state. While such explanations are both useful and necessary, they typically do not portray the outcomes of the agent's selected choice of action. \nIn this work, we propose ``COViz'', a new local explanation method that visually compares the outcome of an agent's chosen action to a counterfactual one. In contrast to most local explanations that provide state-limited observations of the agent's motivation, our method depicts alternative trajectories the agent could have taken from the given state and their outcomes. \nWe evaluated the usefulness of COViz in supporting people's understanding of agents' preferences and compare it with reward decomposition, a local explanation method that describes an agent's expected utility for different actions by decomposing it into meaningful reward types. Furthermore, we examine the complementary benefits of integrating both methods. Our results show that such integration significantly improved participants' performance.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i9.28863"
    },
    {
        "id": 33670,
        "title": "Learning from Symmetry: Meta-Reinforcement Learning with Symmetrical Behaviors and Language Instructions",
        "authors": "Xiangtong Yao, Zhenshan Bing, Genghang Zhuang, Kejia Chen, Hongkuan Zhou, Kai Huang, Alois Knoll",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10341769"
    },
    {
        "id": 33671,
        "title": "Deep Reinforcement Meta-Learning and Self-Organization in Complex Systems: Applications to Traffic Signal Control",
        "authors": "Marcin Korecki",
        "published": "2023-6-27",
        "citations": 3,
        "abstract": "We studied the ability of deep reinforcement learning and self-organizing approaches to adapt to dynamic complex systems, using the applied example of traffic signal control in a simulated urban environment. We highlight the general limitations of deep learning for control in complex systems, even when employing state-of-the-art meta-learning methods, and contrast it with self-organization-based methods. Accordingly, we argue that complex systems are a good and challenging study environment for developing and improving meta-learning approaches. At the same time, we point to the importance of baselines to which meta-learning methods can be compared and present a self-organizing analytic traffic signal control that outperforms state-of-the-art meta-learning in some scenarios. We also show that meta-learning methods outperform classical learning methods in our simulated environment (around 1.5–2× improvement, in most scenarios). Our conclusions are that, in order to develop effective meta-learning methods that are able to adapt to a variety of conditions, it is necessary to test them in demanding, complex settings (such as, for example, urban traffic control) and compare them against established methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.3390/e25070982"
    },
    {
        "id": 33672,
        "title": "Adaptive PD Control Using Deep Reinforcement Learning for Local-Remote Teleoperation with Stochastic Time Delays",
        "authors": "Luc McCutcheon, Saber Fallah",
        "published": "2023-10-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iros55552.2023.10341953"
    },
    {
        "id": 33673,
        "title": "Deep reinforcement learning based adaptive threshold multi-tasks offloading approach in MEC",
        "authors": "Liting Mu, Bin Ge, Chenxing Xia, Cai Wu",
        "published": "2023-8",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.comnet.2023.109803"
    },
    {
        "id": 33674,
        "title": "Adaptive Policy Learning for Offline-to-Online Reinforcement Learning",
        "authors": "Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, Jing Jiang",
        "published": "2023-6-26",
        "citations": 0,
        "abstract": "Conventional reinforcement learning (RL) needs an environment to collect fresh data, which is impractical when online interactions are costly. Offline RL provides an alternative solution by directly learning from the previously collected dataset. However, it will yield unsatisfactory performance if the quality of the offline datasets is poor. In this paper, we consider an offline-to-online setting where the agent is first learned from the offline dataset and then trained online, and propose a framework called Adaptive Policy Learning for effectively taking advantage of offline and online data. Specifically, we explicitly consider the difference between the online and offline data and apply an adaptive update scheme accordingly, that is, a pessimistic update strategy for the offline dataset and an optimistic/greedy update scheme for the online dataset. Such a simple and effective method provides a way to mix the offline and online RL and achieve the best of both worlds. We further provide two detailed algorithms for implementing the framework through embedding value or policy-based RL algorithms into it. Finally, we conduct extensive experiments on popular continuous control tasks, and results show that our algorithm can learn the expert policy with high sample efficiency even when the quality of offline dataset is poor, e.g., random dataset.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i9.26345"
    },
    {
        "id": 33675,
        "title": "QoE-Aware Adaptive Bitrate Algorithm Based on Subepisodic Deep Reinforcement Learning for DASH",
        "authors": "Dujia Yang, Changjian Song, Jian Wang, Rangang Zhu, Jun'An Yang",
        "published": "2023-2-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1145/3587716.3587733"
    },
    {
        "id": 33676,
        "title": "Towards an Adaptive e-Learning System Based on Deep Learner Profile, Machine Learning Approach, and Reinforcement Learning",
        "authors": "Riad Mustapha, Gouraguine Soukaina, Qbadou Mohammed, Aoula Es-Sâadia",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.14569/ijacsa.2023.0140528"
    },
    {
        "id": 33677,
        "title": "Reinforcement Learning Based Adaptive Intelligent Control for State-Constrained Nonlinear Multi-agent Systems",
        "authors": "Junhe Liu, Lei Yan, Yi Peng",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/iccsse59359.2023.10245355"
    },
    {
        "id": 33678,
        "title": "Proximal Bellman Mappings for Reinforcement Learning and Their Application to Robust Adaptive Filtering",
        "authors": "Yuki Akiyama, Konstantinos Slavakis",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10446701"
    },
    {
        "id": 33679,
        "title": "MARLMUI: Multi-Agent Reinforcement Learning Approach in Mobile Adaptive User Interface",
        "authors": "Dmitry A. Vidmanov, Alexander N. Alfimtsev",
        "published": "2023-3-16",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/reepe57272.2023.10086785"
    },
    {
        "id": 33680,
        "title": "Adaptive Multi-Teacher Knowledge Distillation with Meta-Learning",
        "authors": "Hailin Zhang, Defang Chen, Can Wang",
        "published": "2023-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00333"
    },
    {
        "id": 33681,
        "title": "SAMBA: Scenario-Adaptive Meta-Learning for mmWave Beam Alignment",
        "authors": "Ziyi Xu, Shuoyao Wang, Ying-Jun Angela Zhang",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/gcwkshps58843.2023.10465061"
    },
    {
        "id": 33682,
        "title": "Adaptive traffic light control based on reinforcement learning under different stages of autonomy",
        "authors": "Zhuohang Xu, Libin Zhang, Fan Qi",
        "published": "2023-5-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ccdc58219.2023.10327174"
    },
    {
        "id": 33683,
        "title": "Adaptive Discretization in Online Reinforcement Learning",
        "authors": "Sean R. Sinclair, Siddhartha Banerjee, Christina Lee Yu",
        "published": "2023-9",
        "citations": 2,
        "abstract": " Adaptive Discretization in Reinforcement Learning  Performance guarantees for RL algorithms are typically for worst case instances, which are pathological by design and not observed in meaningful applications. Moreover, many domains (such as computer systems and networking applications) have large state-action spaces and require algorithms to execute with low latency. This phenomenon highlights a trifecta of goals for practical RL algorithms: low sample, storage, and computational complexity. In this work, we develop an algorithmic framework for nonparametric RL with data-driven adaptive discretization. Our framework has provably better sample, storage, and computational complexity than uniform discretization or kernel regression methods. Moreover, we highlight how the performance guarantees are min-max optimal with respect to a novel instance-specific complexity measure that captures structure in facility location and newsvendor models. ",
        "keywords": "",
        "link": "http://dx.doi.org/10.1287/opre.2022.2396"
    },
    {
        "id": 33684,
        "title": "Efficient hyperparameters optimization through model-based reinforcement learning with experience exploiting and meta-learning",
        "authors": "Xiyuan Liu, Jia Wu, Senpeng Chen",
        "published": "2023-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1007/s00500-023-08050-x"
    },
    {
        "id": 33685,
        "title": "DREAM: Distributed Reinforcement Learning Enabled Adaptive Mixed-Critical NoC",
        "authors": "Nidhi Anantharajaiah, Yunhe Xu, Fabian Lesniak, Tanja Harbaum, Juergen Becker",
        "published": "2023-6-20",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/isvlsi59464.2023.10238569"
    },
    {
        "id": 33686,
        "title": "Spacecraft Adaptive Deep Reinforcement Learning Guidance with Input State Uncertainties in Relative Motion Scenario",
        "authors": "Andrea Brandonisio, Lorenzo Capra, Michèle Lavagna",
        "published": "2023-1-23",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-1439"
    },
    {
        "id": 33687,
        "title": "Learning to Adapt: Communication Load Balancing via Adaptive Deep Reinforcement Learning",
        "authors": "Di Wu, Yi Tian Xu, Jimmy Li, Michael Jenkin, Ekram Hossain, Seowoo Jang, Yan Xin, Charlie Zhang, Xue Liu, Gregory Dudek",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437528"
    },
    {
        "id": 33688,
        "title": "Hierarchical Meta-Reinforcement Learning for Resource-Efficient Slicing in O-RAN",
        "authors": "Xianfu Chen, Celimuge Wu, Zhifeng Zhao, Yong Xiao, Shiwen Mao, Yusheng Ji",
        "published": "2023-12-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/globecom54140.2023.10437350"
    },
    {
        "id": 33689,
        "title": "A Comparison of Partially and Fully Integrated Guidance and Flight Control Optimized with Reinforcement Meta-Learning",
        "authors": "Brian Gaudet, Roberto Furfaro",
        "published": "2023-1-23",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.2514/6.2023-1628"
    },
    {
        "id": 33690,
        "title": "Generalizable Task Representation Learning for Offline Meta-Reinforcement Learning with Data Limitations",
        "authors": "Renzhe Zhou, Chen-Xiao Gao, Zongzhang Zhang, Yang Yu",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Generalization and sample efficiency have been long-standing issues concerning reinforcement learning, and thus the field of Offline Meta-Reinforcement Learning (OMRL) has gained increasing attention due to its potential of solving a wide range of problems with static and limited offline data. Existing OMRL methods often assume sufficient training tasks and data coverage to apply contrastive learning to extract task representations. However, such assumptions are not applicable in several real-world applications and thus undermine the generalization ability of the representations. In this paper, we consider OMRL with two types of data limitations: limited training tasks and limited behavior diversity and propose a novel algorithm called GENTLE for learning generalizable task representations in the face of data limitations. GENTLE employs Task Auto-Encoder (TAE), which is an encoder-decoder architecture to extract the characteristics of the tasks. Unlike existing methods, TAE is optimized solely by reconstruction of the state transition and reward, which captures the generative structure of the task models and produces generalizable representations when training tasks are limited. To alleviate the effect of limited behavior diversity, we consistently construct pseudo-transitions to align the data distribution used to train TAE with the data distribution encountered during testing. Empirically, GENTLE significantly outperforms existing OMRL methods on both in-distribution tasks and out-of-distribution tasks across both the given-context protocol and the one-shot protocol.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i15.29658"
    },
    {
        "id": 33691,
        "title": "Deep reinforcement learning based efficient access scheduling algorithm with an adaptive number of devices for federated learning IoT systems",
        "authors": "Zheng Guan, Zengwen Wang, Yu Cai, Xue Wang",
        "published": "2023-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.iot.2023.100980"
    },
    {
        "id": 33692,
        "title": "Collaborative Edge Caching: a Meta Reinforcement Learning Approach with Edge Sampling",
        "authors": "Yinan Mao, Bowei He, Shiji Zhou, Chen Ma, Zhi Wang",
        "published": "2023-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icme55011.2023.00171"
    },
    {
        "id": 33693,
        "title": "Control algorithms for guidance of autonomous flying agents using reinforcement learning",
        "authors": "Christopher D. Hsu, Franklin Shedleski, Bethany Allik",
        "published": "2023-6-13",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1117/12.2663072"
    },
    {
        "id": 33694,
        "title": "Quantum Multi-Agent Meta Reinforcement Learning",
        "authors": "Won Joon Yun, Jihong Park, Joongheon Kim",
        "published": "2023-6-26",
        "citations": 6,
        "abstract": "Although quantum supremacy is yet to come, there has recently been an increasing interest in identifying the potential of quantum machine learning (QML) in the looming era of practical quantum computing. Motivated by this, in this article we re-design multi-agent reinforcement learning (MARL) based on the unique characteristics of quantum neural networks (QNNs) having two separate dimensions of trainable parameters: angle parameters affecting the output qubit states, and pole parameters associated with the output measurement basis. Exploiting this dyadic trainability as meta-learning capability, we propose quantum meta MARL (QM2ARL) that first applies angle training for meta-QNN learning, followed by pole training for few-shot or local-QNN training. To avoid overfitting, we develop an angle-to-pole regularization technique injecting noise into the pole domain during angle training. Furthermore, by exploiting the pole as the memory address of each trained QNN, we introduce the concept of pole memory allowing one to save and load trained QNNs using only two-parameter pole values. We theoretically prove the convergence of angle training under the angle-to-pole regularization, and by simulation corroborate the effectiveness of QM2ARL in achieving high reward and fast convergence, as well as of the pole memory in fast adaptation to a time-varying environment.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v37i9.26313"
    },
    {
        "id": 33695,
        "title": "MEWA: A Benchmark For Meta-Learning in Collaborative Working Agents",
        "authors": "Radu Stoican, Angelo Cangelosi, Thomas H. Weisswange",
        "published": "2023-12-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/ssci52147.2023.10371913"
    },
    {
        "id": 33696,
        "title": "Meta-Reinforcement Learning via Exploratory Task Clustering",
        "authors": "Zhendong Chu, Renqin Cai, Hongning Wang",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "Meta-reinforcement learning (meta-RL) aims to quickly solve new RL tasks by leveraging knowledge from prior tasks. Previous studies often assume a single-mode homogeneous task distribution, ignoring possible structured heterogeneity among tasks. Such an oversight can hamper effective exploration and adaptation, especially with limited samples. In this work, we harness the structured heterogeneity among tasks via clustering to improve meta-RL, which facilitates knowledge sharing at the cluster level. To facilitate exploration, we also develop a dedicated cluster-level exploratory policy to discover task clusters via divide-and-conquer. The knowledge from the discovered clusters helps to narrow the search space of task-specific policy learning, leading to more sample-efficient policy adaptation. We evaluate the proposed method on environments with parametric clusters (e.g., rewards and state dynamics in the MuJoCo suite) and non-parametric clusters (e.g., control skills in the Meta-World suite). The results demonstrate strong advantages of our solution against a set of representative meta-RL methods.",
        "keywords": "",
        "link": "http://dx.doi.org/10.1609/aaai.v38i10.29046"
    },
    {
        "id": 33697,
        "title": "Adaptive Parameter Sharing for Multi-Agent Reinforcement Learning",
        "authors": "Dapeng Li, Na Lou, Bin Zhang, Zhiwei Xu, Guoliang Fan",
        "published": "2024-4-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icassp48485.2024.10447262"
    },
    {
        "id": 33698,
        "title": "The learning adversary - An experimental investigation of adaptive pedagogical agents as opponents in educational videogames",
        "authors": "Steve Nebel, Maik Beege, Sascha Schneider, Günter Daniel Rey",
        "published": "2024-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1016/j.lindif.2024.102425"
    },
    {
        "id": 33699,
        "title": "Learning Multiagent Options for Tabular Reinforcement Learning using Factor Graphs",
        "authors": "Jiayu Chen, Jingdi Chen, Tian Lan, Vaneet Aggarwal",
        "published": "2023-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tai.2022.3195818"
    },
    {
        "id": 33700,
        "title": "A Meta-Analysis on the Effects of Using Artificial Intelligence&amp;amp;#8211;Powered Virtual Agents in Simulation-Based Learning",
        "authors": "Chih-Pu Dai",
        "published": "2023",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.3102/ip.23.2010380"
    },
    {
        "id": 33701,
        "title": "Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning",
        "authors": "Jing Tan, Ramin Khalili, Holger Karl",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/tits.2024.3378007"
    },
    {
        "id": 33702,
        "title": "Adaptive NGMA Scheme for IoT Networks: A Deep Reinforcement Learning Approach",
        "authors": "Yixuan Zou, Wenqiang Yi, Xiaodong Xu, Yue Liu, Kok Keong Chai, Yuanwei Liu",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/icc45041.2023.10278912"
    },
    {
        "id": 33703,
        "title": "Dynamic Resource Allocation for Satellite Edge Computing: An Adaptive Reinforcement Learning-based Approach",
        "authors": "Xiaoyu Tang, Zhaorong Tang, Shuyao Cui, Dantong Jin, Jibing Qiu",
        "published": "2023-11-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/satellite59115.2023.00018"
    },
    {
        "id": 33704,
        "title": "Research on Deep Reinforcement Learning of Unmanned Multi-sensor Fusion Based on Adaptive Reward Mechanism",
        "authors": "Guo Maotao, Liu Guichong",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "keywords": "",
        "link": "http://dx.doi.org/10.1109/auteee60196.2023.10407223"
    }
]