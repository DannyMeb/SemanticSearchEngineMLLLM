[
    {
        "id": 13671,
        "title": "VC-VQA: Visual Calibration Mechanism For Visual Question Answering",
        "authors": "Yanyuan Qiao, Zheng Yu, Jing Liu",
        "published": "2020-10",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip40778.2020.9190828"
    },
    {
        "id": 13672,
        "title": "CQ-VQA: Visual Question Answering on Categorized Questions",
        "authors": "Aakansha Mishra, Ashish Anand, Prithwijit Guha",
        "published": "2020-7",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn48605.2020.9206913"
    },
    {
        "id": 13673,
        "title": "VQA as a factoid question answering problem: A novel approach for knowledge-aware and explainable visual question answering",
        "authors": "Abhishek Narayanan, Abijna Rao, Abhishek Prasad, Natarajan S",
        "published": "2021-12",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.imavis.2021.104328"
    },
    {
        "id": 13674,
        "title": "VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis",
        "authors": "Argho Sarkar, Maryam Rahnemoonfar",
        "published": "2021-7-11",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss47720.2021.9553578"
    },
    {
        "id": 13675,
        "title": "Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering",
        "authors": "Nandita Naik, Christopher Potts, Elisa Kreiss",
        "published": "2023-10-2",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccvw60793.2023.00301"
    },
    {
        "id": 13676,
        "title": "RESCUENet-VQA: A Large-Scale Visual Question Answering Benchmark for Damage Assessment",
        "authors": "Argho Sarkar, Maryam Rahnemoonfar",
        "published": "2023-7-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/igarss52108.2023.10281747"
    },
    {
        "id": 13677,
        "title": "Visual Question Generation Answering (VQG-VQA) using Machine Learning Models",
        "authors": "Atul Kachare, Mukesh Kalla, Ashutosh Gupta",
        "published": "2023-6-28",
        "citations": 0,
        "abstract": "Presented automated visual question-answer system generates graphics-based question-answer pairs. The system consists of the Visual Query Generation (VQG) and Visual Question Answer (VQA) modules. VQG generates questions based on visual cues, and VQA provides matching answers to the VQG modules. VQG system generates questions using LSTM and VGG19 model, training parameters, and predicting words with the highest probability for output. VQA uses VGG-19 convolutional neural network for image encoding, embedding, and multilayer perceptron for high-quality responses. The proposed system reduces the need for human annotation and thus supports the traditional education sector by significantly reducing the human intervention required to generate text queries. The system can be used in interactive interfaces to help young children learn.",
        "link": "http://dx.doi.org/10.37394/23202.2023.22.67"
    },
    {
        "id": 13678,
        "title": "OCR-VQA: Visual Question Answering by Reading Text in Images",
        "authors": "Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, Anirban Chakraborty",
        "published": "2019-9",
        "citations": 61,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdar.2019.00156"
    },
    {
        "id": 13679,
        "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge",
        "authors": "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi",
        "published": "2019-6",
        "citations": 153,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2019.00331"
    },
    {
        "id": 13680,
        "title": "Post-Disaster Damage Detection using Aerial Footage: Visual Question Answering (VQA) Case Study",
        "authors": "Rafael De Sa Lowande, Arash Mahyari, Hakki Erhan Sevil",
        "published": "2022-10-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/aipr57179.2022.10092229"
    },
    {
        "id": 13681,
        "title": "A cascaded long short-term memory (LSTM) driven generic visual question answering (VQA)",
        "authors": "Iqbal Chowdhury, Kien Nguyen, Clinton Fookes, Sridha Sridharan",
        "published": "2017-9",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2017.8296600"
    },
    {
        "id": 13682,
        "title": "Using Markov Random Field (MRF) Hypergraph Transformer Method for Visual Question Answering (VQA) Application",
        "authors": "Jiawei Lin, Sei-Ichiro Kamata",
        "published": "2023-8-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/prai59366.2023.10332038"
    },
    {
        "id": 13683,
        "title": "Feasibility of Visual Question Answering (VQA) for Post-Disaster Damage Detection Using Aerial Footage",
        "authors": "Rafael De Sa Lowande, Hakki Erhan Sevil",
        "published": "2023-4-19",
        "citations": 1,
        "abstract": "Natural disasters are a major source of significant damage and costly repairs around the world. After a natural disaster occurs, there is usually a significant amount of damage, and with that, there are also a lot of costs involved with repairing and aiding all the people involved. In addition, the occurrence of natural phenomena has increased significantly in the past decade. With that in mind, post-disaster damage detection is usually performed manually by human operators. Taking into consideration all the areas one has to closely look into, as well as the difficult terrain and places with hard access, it becomes easy to understand how incredibly difficult it is for a surveyor to identify and annotate every single possible damage out there. Because of that, it has become essential to find new creative solutions for damage detection and classification in the case of natural disasters, especially hurricanes. This study focuses on the feasibility of using a Visual Question Answering (VQA) method for post-disaster damage detection, using aerial footage taken from an Unmanned Aerial Vehicle (UAV). Two other approaches are also utilized to provide comparison and to evaluate the performance of VQA. Our case study on our custom dataset collected after Hurricane Sally shows successful results using VQA for post-disaster damage detection applications.",
        "link": "http://dx.doi.org/10.3390/app13085079"
    },
    {
        "id": 13684,
        "title": "S-VQA: Sentence-Based Visual Question Answering",
        "authors": "Sanchit Pathak, Garima Singh, Ashish Anand, Prithwijit Guha",
        "published": "2023-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3627631.3627670"
    },
    {
        "id": 13685,
        "title": "CS-VQA: Visual Question Answering with Compressively Sensed Images",
        "authors": "Li-Chi Huang, Kuldeep Kulkarni, Anik Jha, Suhas Lohit, Suren Jayasuriya, Pavan Turaga",
        "published": "2018-10",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2018.8451445"
    },
    {
        "id": 13686,
        "title": "VQA-BC: Robust Visual Question Answering Via Bidirectional Chaining",
        "authors": "Mingrui Lao, Yanming Guo, Wei Chen, Nan Pu, Michael S. Lew",
        "published": "2022-5-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icassp43922.2022.9746493"
    },
    {
        "id": 13687,
        "title": "Embodied VQA",
        "authors": "Qi Wu, Peng Wang, Xin Wang, Xiaodong He, Wenwu Zhu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0964-1_10"
    },
    {
        "id": 13688,
        "title": "VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering",
        "authors": "Ekta Sood, Fabian Kögel, Florian Strohm, Prajit Dhar, Andreas Bulling",
        "published": "2021",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.conll-1.3"
    },
    {
        "id": 13689,
        "title": "Sim VQA: Exploring Simulated Environments for Visual Question Answering",
        "authors": "Paola Cascante-Bonilla, Hui Wu, Letao Wang, Rogerio Feris, Vicente Ordonez",
        "published": "2022-6",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr52688.2022.00500"
    },
    {
        "id": 13690,
        "title": "Towards Open Ended and Free Form Visual Question Answering: Modeling VQA as a Factoid Question Answering Problem",
        "authors": "Abhishek Narayanan, Abijna Rao, Abhishek Prasad, S. Natarajan",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-9774-9_69"
    },
    {
        "id": 13691,
        "title": "Medical VQA",
        "authors": "Qi Wu, Peng Wang, Xin Wang, Xiaodong He, Wenwu Zhu",
        "published": "2022",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0964-1_11"
    },
    {
        "id": 13692,
        "title": "Knowledge-Based VQA",
        "authors": "Qi Wu, Peng Wang, Xin Wang, Xiaodong He, Wenwu Zhu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0964-1_5"
    },
    {
        "id": 13693,
        "title": "From image to language: A critical analysis of Visual Question Answering (VQA) approaches, challenges, and opportunities",
        "authors": "Md. Farhan Ishmam, Md. Sakib Hossain Shovon, M.F. Mridha, Nilanjan Dey",
        "published": "2024-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.inffus.2024.102270"
    },
    {
        "id": 13694,
        "title": "Text-Based VQA",
        "authors": "Qi Wu, Peng Wang, Xin Wang, Xiaodong He, Wenwu Zhu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0964-1_12"
    },
    {
        "id": 13695,
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh",
        "published": "2017-7",
        "citations": 770,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2017.670"
    },
    {
        "id": 13696,
        "title": "ST-VQA: shrinkage transformer with accurate alignment for visual question answering",
        "authors": "Haiying Xia, Richeng Lan, Haisheng Li, Shuxiang Song",
        "published": "2023-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s10489-023-04564-x"
    },
    {
        "id": 13697,
        "title": "Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA",
        "authors": "Wentao Mo, Yang Liu",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combines 2D and 3D modalities and captures fine-grained correlations between modalities, allowing them mutually augmenting each other. Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh perspective on multi-modal transformer-based architectures for 3D-VQA. Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets and significantly outperforms existing solutions. Code is available at https://github.com/matthewdm0816/BridgeQA.",
        "link": "http://dx.doi.org/10.1609/aaai.v38i5.28222"
    },
    {
        "id": 13698,
        "title": "VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering",
        "authors": "Yanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, Jure Leskovec",
        "published": "2023-10-1",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv51070.2023.01973"
    },
    {
        "id": 13699,
        "title": "Visual Question Answering (VQA) on Images with Superimposed Text",
        "authors": "Venkat Kodali, Daniel Berleant",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-47457-6_18"
    },
    {
        "id": 13700,
        "title": "Vision-and-Language Pretraining for VQA",
        "authors": "Qi Wu, Peng Wang, Xin Wang, Xiaodong He, Wenwu Zhu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0964-1_6"
    },
    {
        "id": 13701,
        "title": "BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via Graph Representation Pretraining",
        "authors": "MinJun Kim, SeungWoo Song, YouHan Lee, Haneol Jang, KyungTae Lim",
        "published": "2024-3-24",
        "citations": 0,
        "abstract": "The current research direction in generative models, such as the recently developed GPT4, aims to find relevant knowledge information for multimodal and multilingual inputs to provide answers. Under these research circumstances, the demand for multilingual evaluation of visual question answering (VQA) tasks, a representative task of multimodal systems, has increased. Accordingly, we propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that can be extended to multilingualism. The proposed data include 17K images, 17K question-answer pairs for both Korean and English and 280K instances of knowledge information related to question-answer content. We also present a framework that can effectively inject knowledge information into a VQA system by pretraining the knowledge information of BOK-VQA data in the form of graph embeddings. Finally, through in-depth analysis, we demonstrated the actual effect of the knowledge information contained in the constructed training data on VQA.",
        "link": "http://dx.doi.org/10.1609/aaai.v38i16.29798"
    },
    {
        "id": 13702,
        "title": "Cross Modality Bias in Visual Question Answering: A Causal View with Possible Worlds VQA",
        "authors": "Ali Vosoughi, Shijian Deng, Songyang Zhang, Yapeng Tian, Chenliang Xu, Jiebo Luo",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tmm.2024.3380259"
    },
    {
        "id": 13703,
        "title": "A-VQA: Attention Based Visual Question Answering Technique for Handling Improper Count of Occluded Object",
        "authors": "Shivangi Modi, Dhatri Pandya",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-8297-4_27"
    },
    {
        "id": 13704,
        "title": "Outside Knowledge-based Visual Question Answering Empowered by Knowledge Graph Embedding",
        "authors": "Minjun Kim, Seungwoo Song, Dongjae Shin, Eunkyung Kim, Kyungtae Lim",
        "published": "2024-1-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5626/ktcp.2024.30.1.1"
    },
    {
        "id": 13705,
        "title": "Co-VQA : Answering by Interactive Sub Question Sequence",
        "authors": "Ruonan Wang, Yuxi Qian, Fangxiang Feng, Xiaojie Wang, Huixing Jiang",
        "published": "2022",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.findings-acl.188"
    },
    {
        "id": 13706,
        "title": "TRANS-VQA: Fully Transformer-Based Image Question-Answering Model Using Question-guided Vision Attention",
        "authors": "Dipali Koshti, Ashutosh Gupta, Mukesh Kalla, Arvind Sharma",
        "published": "2024-1-10",
        "citations": 0,
        "abstract": "Understanding multiple modalities and relating them is an easy task for humans. But for machines, this is a stimulating task. One such multi-modal reasoning task is Visual question answering which demands the machine to produce an answer for the natural language query asked based on the given image. Although plenty of work is done in this field, there is still a challenge of improving the answer prediction ability of the model and breaching human accuracy. A novel model for answering image-based questions based on a transformer has been proposed. The proposed model is a fully Transformer-based architecture that utilizes the power of a transformer for extracting language features as well as for performing joint understanding of question and image features. The proposed VQA model utilizes F-RCNN for image feature extraction. The retrieved language features and object-level image features are fed to a decoder inspired by the Bi-Directional Encoder Representation Transformer - BERT architecture that learns jointly the image characteristics directed by the question characteristics and rich representations of the image features are obtained. Extensive experimentation has been carried out to observe the effect of various hyperparameters on the performance of the model. The experimental results demonstrate that the model’s ability to predict the answer increases with the increase in the number of layers in the transformer’s encoder and decoder. The proposed model improves upon the previous models and is highly scalable due to the introduction of the BERT. Our best model reports 72.31% accuracy on the test-standard split of the VQAv2 dataset.",
        "link": "http://dx.doi.org/10.4114/intartif.vol27iss73pp111-128"
    },
    {
        "id": 13707,
        "title": "IQ-VQA: Intelligent Visual Question Answering",
        "authors": "Vatsal Goel, Mohit Chandak, Ashish Anand, Prithwijit Guha",
        "published": "2021",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-68790-8_28"
    },
    {
        "id": 13708,
        "title": "VQA-LOL: Visual Question Answering Under the Lens of Logic",
        "authors": "Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou Yang",
        "published": "2020",
        "citations": 21,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-58589-1_23"
    },
    {
        "id": 13709,
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "authors": "Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra, Devi Parikh",
        "published": "2019-4",
        "citations": 57,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s11263-018-1116-0"
    },
    {
        "id": 13710,
        "title": "PDID: Visual Discretization Intelligent Question Answering Model—VQA Model Based on Image Pixel Discretization and Image Semantic Discretization",
        "authors": "页名 陈",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.12677/csa.2023.1312243"
    },
    {
        "id": 13711,
        "title": "CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering",
        "authors": "Maitreya Patel, Tejas Gokhale, Chitta Baral, Yezhou Yang",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2022.emnlp-main.670"
    },
    {
        "id": 13712,
        "title": "Classical Visual Question Answering",
        "authors": "Qi Wu, Peng Wang, Xin Wang, Xiaodong He, Wenwu Zhu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0964-1_4"
    },
    {
        "id": 13713,
        "title": "Learning Neighbor-Enhanced Region Representations and Question-Guided Visual Representations for Visual Question Answering",
        "authors": "Ling Gao, Hongda Zhang, Nan Sheng, Lida Shi, Hao Xu",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4558736"
    },
    {
        "id": 13714,
        "title": "Visual Question Answering Analysis: Datasets, Methods, and Image Featurization Techniques",
        "authors": "Vijay Kumari, Abhimanyu Sethi, Yashvardhan Sharma, Lavika Goel",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011655900003411"
    },
    {
        "id": 13715,
        "title": "Text-Image Transformer with Cross-Attention for Visual Question Answering",
        "authors": "Mahdi Rezapour",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.169633350.04075219/v2"
    },
    {
        "id": 13716,
        "title": "Text-Image Transformer with Cross-Attention for Visual Question Answering",
        "authors": "Mahdi Rezapour",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.22541/au.169633350.04075219/v1"
    },
    {
        "id": 13717,
        "title": "TG-VQA: Ternary Game of Video Question Answering",
        "authors": "Hao Li, Peng Jin, Zesen Cheng, Songyang Zhang, Kai Chen, Zhennan Wang, Chang Liu, Jie Chen",
        "published": "2023-8",
        "citations": 4,
        "abstract": "Video question answering aims at answering a question about the video content by reasoning the alignment semantics within them. However, since relying heavily on human instructions, i.e., annotations or priors, current contrastive learning-based VideoQA methods remains challenging to perform fine-grained visual-linguistic alignments. In this work, we innovatively resort to game theory, which can simulate complicated relationships among multiple players with specific interaction strategies, e.g., video, question, and answer as ternary players, to achieve fine-grained alignment for VideoQA task. Specifically, we carefully design a VideoQA-specific interaction strategy to tailor the characteristics of VideoQA, which can mathematically generate the fine-grained visual-linguistic alignment label without label-intensive efforts. Our TG-VQA outperforms existing state-of-the-art by a large margin (more than 5%) on long-term and short-term VideoQA datasets, verifying its effectiveness and generalization ability. Thanks to the guidance of game-theoretic interaction, our model impressively convergences well on limited data (10^4 videos), surpassing most of those pre-trained on large-scale data (10^7 videos).",
        "link": "http://dx.doi.org/10.24963/ijcai.2023/116"
    },
    {
        "id": 13718,
        "title": "Surgical-VQA: Visual Question Answering in Surgical Scenes Using Transformer",
        "authors": "Lalithkumar Seenivasan, Mobarakol Islam, Adithya K Krishna, Hongliang Ren",
        "published": "2022",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-16449-1_4"
    },
    {
        "id": 13719,
        "title": "Multiple Context Learning Networks for Visual Question Answering",
        "authors": "Pufen Zhang, Hong Lan",
        "published": "No Date",
        "citations": 1,
        "abstract": "Abstract\nIn recently years, some visual question answering (VQA) methods that emphasize the simultaneous understanding of both the context of image and question have been proposed. Despite the effectiveness of these methods, they fail to explore a more comprehensive and generalized context learning tactics. To address this issue, we propose a novel Multiple Context Learning Networks (MCLN) to model the multiple contexts for VQA. Three kinds of contexts are investigated, namely visual context, textual context and a special visual-textual context that ignored by previous methods. Moreover, three corresponding context learning modules are proposed. These modules endow image and text representations with context-aware information based on a uniform context learning strategy. And they work together to form a multiple context learning layer (MCL). Such MCL can be stacked in depth and which describe high-level context information by associating intra-modal contexts with inter-modal context. On the VQA v2.0 datasets, the proposed model achieves 71.05% and 71.48% on test-dev set and test-std set respectively, and gains better performance than the previous state-of-the-art methods. In addition, extensive ablation studies have been carried out to examine the effectiveness of the proposed method.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-955099/v1"
    },
    {
        "id": 13720,
        "title": "SAM-VQA: Supervised Attention-Based Visual Question Answering Model for Post-Disaster Damage Assessment on Remote Sensing Imagery",
        "authors": "Argho Sarkar, Tashnim Chowdhury, Robin Roberson Murphy, Aryya Gangopadhyay, Maryam Rahnemoonfar",
        "published": "2023",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tgrs.2023.3276293"
    },
    {
        "id": 13721,
        "title": "Question-Agnostic Attention for Visual Question Answering",
        "authors": "Moshiur Farazi, Salman Khan, Nick Barnes",
        "published": "2021-1-10",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr48806.2021.9413330"
    },
    {
        "id": 13722,
        "title": "VQA-CLPR: Turning a Visual Question Answering Model into a Chinese License Plate Recognizer",
        "authors": "Gang Lv, Xuhao Jiang, Yining Sun, Weiya Ni, Fudong Nian",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-031-46308-2_29"
    },
    {
        "id": 13723,
        "title": "AI-VQA",
        "authors": "Rengang Li, Cong Xu, Zhenhua Guo, Baoyu Fan, Runze Zhang, Wei Liu, Yaqian Zhao, Weifeng Gong, Endong Wang",
        "published": "2022-10-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3503161.3548387"
    },
    {
        "id": 13724,
        "title": "R-VQA",
        "authors": "Pan Lu, Lei Ji, Wei Zhang, Nan Duan, Ming Zhou, Jianyong Wang",
        "published": "2018-7-19",
        "citations": 45,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3219819.3220036"
    },
    {
        "id": 13725,
        "title": "Video Question Answering",
        "authors": "Qi Wu, Peng Wang, Xin Wang, Xiaodong He, Wenwu Zhu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0964-1_8"
    },
    {
        "id": 13726,
        "title": "Visual Question Answering using Explicit Visual Attention",
        "authors": "Vasileios Lioutas, Nikolaos Passalis, Anastasios Tefas",
        "published": "2018",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iscas.2018.8351158"
    },
    {
        "id": 13727,
        "title": "Context-Aware Multi-Level Question Embedding Fusion for Visual Question Answering",
        "authors": "Shengdong Li, Chen Gong, Yuqing Zhu, Chuanwen Luo, Yi Hong, Xueqiang Lv",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4482214"
    },
    {
        "id": 13728,
        "title": "Question Answering (QA) Basics",
        "authors": "Qi Wu, Peng Wang, Xin Wang, Xiaodong He, Wenwu Zhu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0964-1_3"
    },
    {
        "id": 13729,
        "title": "Extracting Visual Semantic Information for Solving Visual Question Answering",
        "authors": "Yuxiang Huang",
        "published": "2023-10-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdsca59871.2023.10393030"
    },
    {
        "id": 13730,
        "title": "Inverse Visual Question Answering: A New Benchmark and VQA Diagnosis Tool",
        "authors": "Feng Liu, Tao Xiang, Timothy M. Hospedales, Wankou Yang, Changyin Sun",
        "published": "2020-2-1",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tpami.2018.2880185"
    },
    {
        "id": 13731,
        "title": "Pathological Visual Question Answering",
        "authors": "Xuehai He, Zhuo Cai, Wenlan Wei, Yichen Zhang, Luntian Mou, Eric Xing, Pengtao Xie",
        "published": "No Date",
        "citations": 9,
        "abstract": "We develop datasets and methods to perform visual question answering on pathology images.",
        "link": "http://dx.doi.org/10.36227/techrxiv.13127537"
    },
    {
        "id": 13732,
        "title": "Pathological Visual Question Answering",
        "authors": "Xuehai He, Zhuo Cai, Wenlan Wei, Yichen Zhang, Luntian Mou, Eric Xing, Pengtao Xie",
        "published": "No Date",
        "citations": 10,
        "abstract": "We develop datasets and methods to perform visual question answering on pathology images.",
        "link": "http://dx.doi.org/10.36227/techrxiv.13127537.v1"
    },
    {
        "id": 13733,
        "title": "Visual TTR - Modelling Visual Question Answering in Type Theory with Records",
        "authors": "Ronja Utescher",
        "published": "2019",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/w19-0602"
    },
    {
        "id": 13734,
        "title": "Advanced Models for Video Question Answering",
        "authors": "Qi Wu, Peng Wang, Xin Wang, Xiaodong He, Wenwu Zhu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0964-1_9"
    },
    {
        "id": 13735,
        "title": "Prior Visual Relationship Reasoning For Visual Question Answering",
        "authors": "Zhuoqian Yang, Zengchang Qin, Jing Yu, Tao Wan",
        "published": "2020-10",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip40778.2020.9190771"
    },
    {
        "id": 13736,
        "title": "A visual question answering method based on question intention",
        "authors": "Kai Wang, Yun Pan, Xiang Yao",
        "published": "2023-4-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icsp58490.2023.10248540"
    },
    {
        "id": 13737,
        "title": "Visual Question Generation as Dual Task of Visual Question Answering",
        "authors": "Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, Ming Zhou",
        "published": "2018-6",
        "citations": 80,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2018.00640"
    },
    {
        "id": 13738,
        "title": "Question-conditioned debiasing with focal visual context fusion for visual question answering",
        "authors": "Jin Liu, GuoXiang Wang, ChongFeng Fan, Fengyu Zhou, HuiJuan Xu",
        "published": "2023-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.knosys.2023.110879"
    },
    {
        "id": 13739,
        "title": "Multiple answers to a question: a new approach for visual question answering",
        "authors": "Sayedshayan Hashemi Hosseinabad, Mehran Safayani, Abdolreza Mirzaei",
        "published": "2021-1",
        "citations": 9,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s00371-019-01786-4"
    },
    {
        "id": 13740,
        "title": "An Analysis of Visual Question Answering Algorithms",
        "authors": "Kushal Kafle, Christopher Kanan",
        "published": "2017-10",
        "citations": 108,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv.2017.217"
    },
    {
        "id": 13741,
        "title": "Visual Question Answering Through Adversarial Learning of Multi-modal Representation",
        "authors": "Iqbal Chowdhury, Kien Nguyen Thanh, Clinton fookes, Sridha Sridharan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Solving the Visual Question Answering (VQA) task is a step towards achieving human-like reasoning capability of the machines. This paper proposes an approach to learn multimodal feature representation with adversarial training. The purpose of the adversarial training allows the model to learn from standard fusion methods in an unsupervised manner. The discriminator model is equipped with a siamese combinatin of two standard fusion method namely multimodal compact bilinear pooling and multimodal tucker fusion. Output multimodal feature representation from generator is a resultant of graph convolutional operation. The resultant multimodal representation of the adversarial training allows the proposed model to infer the correct answers from open-ended natural language questions from the VQA 2.0 dataset. An overall accuracy of 69.86\\% demonstrates the accuracy of the proposed model.",
        "link": "http://dx.doi.org/10.36227/techrxiv.12731948"
    },
    {
        "id": 13742,
        "title": "Towards Knowledge-Augmented Visual Question Answering",
        "authors": "Maryam Ziaeefard, Freddy Lecue",
        "published": "2020",
        "citations": 10,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2020.coling-main.169"
    },
    {
        "id": 13743,
        "title": "Simplifying Multimodal Composition: A Novel Zero-shot Framework to Visual Question Answering and Image Captioning",
        "authors": "Ethan Liu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nIn this work, we perform zero-shot image captioning and visual question answering on images using a simple model composition framework, composing the dense image captioning capabilities of a visual language model with the powerful reasoning abilities of a large language model, ChatGPT. The proposed method utilizes zero-shot learning to enable cross-modal integration of vision and language in order to create a comprehensive visual language model. We achieve zero-shot state-of-the-art performance on VQAv2, demonstrating its effectiveness and high accuracy. The method's simplicity makes it highly scalable and adaptable to a wide range of applications, including integration from OpenAI’s multimodal model, GPT-4, with audio language models in the future. The results demonstrate the vast potential of this simple zero-shot framework in improving the accuracy and relevance of vision and language applications, constituting an effective approach to image captioning and visual question answering, as well as future multimodal composition.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3027308/v1"
    },
    {
        "id": 13744,
        "title": "Addressing the Ambiguities of Interpretable Visual Question Answering",
        "authors": "Flewin DSouza, Hrugved Kolhe, Aditi Bodade, Paresh Chaudhari, Mangala Madankar",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4725930"
    },
    {
        "id": 13745,
        "title": "A Comparative Evaluation of Visual and Natural Language Question Answering over Linked Data",
        "authors": "Gerhard Wohlgenannt, Dmitry Mouromtsev, Dmitry Pavlov, Yury Emelyanov, Alexey Morozov",
        "published": "2019",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0008364704730478"
    },
    {
        "id": 13746,
        "title": "Language and Visual Relations Encoding for Visual Question Answering",
        "authors": "Fei Liu, Jing Liu, Zhiwei Fang, Hanqing Lu",
        "published": "2019-9",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip.2019.8803670"
    },
    {
        "id": 13747,
        "title": "Adversarial Learning to Improve Question Image Embedding in Medical Visual Question Answering",
        "authors": "Kaveesha Silva, Thanuja Maheepala, Kasun Tharaka, Thanuja D. Ambegoda",
        "published": "2022-7-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/mercon55799.2022.9906168"
    },
    {
        "id": 13748,
        "title": "Learning to Select Question-Relevant Relations for Visual Question Answering",
        "authors": "Jaewoong Lee, Heejoon Lee, Hwanhee Lee, Kyomin Jung",
        "published": "2021",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/2021.maiworkshop-1.13"
    },
    {
        "id": 13749,
        "title": "A Survey on Visual Question Answering",
        "authors": "Mrinal Banchhor, Pradeep Singh",
        "published": "2021-10-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcat52182.2021.9587797"
    },
    {
        "id": 13750,
        "title": "Visual Question Answering Through Adversarial Learning of Multi-modal Representation",
        "authors": "Iqbal Chowdhury, Kien Nguyen Thanh, Clinton fookes, Sridha Sridharan",
        "published": "No Date",
        "citations": 0,
        "abstract": "Solving the Visual Question Answering (VQA) task is a step towards achieving human-like reasoning capability of the machines. This paper proposes an approach to learn multimodal feature representation with adversarial training. The purpose of the adversarial training allows the model to learn from standard fusion methods in an unsupervised manner. The discriminator model is equipped with a siamese combinatin of two standard fusion method namely multimodal compact bilinear pooling and multimodal tucker fusion. Output multimodal feature representation from generator is a resultant of graph convolutional operation. The resultant multimodal representation of the adversarial training allows the proposed model to infer the correct answers from open-ended natural language questions from the VQA 2.0 dataset. An overall accuracy of 69.86\\% demonstrates the accuracy of the proposed model.",
        "link": "http://dx.doi.org/10.36227/techrxiv.12731948.v1"
    },
    {
        "id": 13751,
        "title": "Generating Question Relevant Captions to Aid Visual Question Answering",
        "authors": "Jialin Wu, Zeyuan Hu, Raymond Mooney",
        "published": "2019",
        "citations": 20,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.18653/v1/p19-1348"
    },
    {
        "id": 13752,
        "title": "On the role of question encoder sequence model in robust visual question answering",
        "authors": "Gouthaman KV, Anurag Mittal",
        "published": "2022-11",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.patcog.2022.108883"
    },
    {
        "id": 13753,
        "title": "Question-Guided Feature Pyramid Network for Medical Visual Question Answering",
        "authors": "Yonglin Yu, Haifeng Li, Hanrong Shi, Lin Li, Jun Xiao",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4087657"
    },
    {
        "id": 13754,
        "title": "QSAnglyzer: Visual Analytics for Prismatic Analysis of Question Answering System Evaluations",
        "authors": "Nan-Chen Chen, Been Kim",
        "published": "2017-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/vast.2017.8585733"
    },
    {
        "id": 13755,
        "title": "Overcoming Language Priors in Visual Question Answering with Cumulative Learning Strategy",
        "authors": "Aihua Mao, Feng Chen, Ziying Ma, Ken Lin",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4740502"
    },
    {
        "id": 13756,
        "title": "Visual Question Generation",
        "authors": "Qi Wu, Peng Wang, Xin Wang, Xiaodong He, Wenwu Zhu",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-0964-1_13"
    },
    {
        "id": 13757,
        "title": "RSVQA: Visual Question Answering for Remote Sensing Data",
        "authors": "Sylvain Lobry, Diego Marcos, Jesse Murray, Devis Tuia",
        "published": "2020-12",
        "citations": 84,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tgrs.2020.2988782"
    },
    {
        "id": 13758,
        "title": "Context Relation Fusion Model for Visual Question Answering",
        "authors": "Haotian Zhang, Wei Wu",
        "published": "2022-10-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icip46576.2022.9897563"
    },
    {
        "id": 13759,
        "title": "Indic Visual Question Answering",
        "authors": "Aditya Chandrasekar, Amey Shimpi, Dinesh Naik",
        "published": "2022-7-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/spcom55316.2022.9840835"
    },
    {
        "id": 13760,
        "title": "Question-oriented cross-modal co-attention networks for visual question answering",
        "authors": "Wei Guan, Zhenyu Wu, Wen Ping",
        "published": "2022-1-14",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccece54139.2022.9712726"
    },
    {
        "id": 13761,
        "title": "Visual-Semantic Dual Channel Network for Visual Question Answering",
        "authors": "Xin Wang, Qiaohong Chen, Ting Hu, Qi Sun, Yubo Jia",
        "published": "2021-7-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn52387.2021.9533855"
    },
    {
        "id": 13762,
        "title": "Question-Guided Graph Convolutional Network for Visual Question Answering Based on Object-Difference",
        "authors": "Minchang Huangfu, Yushui Geng",
        "published": "2023-8-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/swc57546.2023.10449195"
    },
    {
        "id": 13763,
        "title": "Sequential Visual Reasoning for Visual Question Answering",
        "authors": "Jinlai Liu, Chenfei Wu, Xiaojie Wang, Xuan Dong",
        "published": "2018-11",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccis.2018.8691361"
    },
    {
        "id": 13764,
        "title": "Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering",
        "authors": "Corentin Dancette, Remi Cadene, Damien Teney, Matthieu Cord",
        "published": "2021-10",
        "citations": 22,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccv48922.2021.00160"
    },
    {
        "id": 13765,
        "title": "A Novel Online Teaching Effect Evaluation Model Based on Visual Question Answering",
        "authors": "Yanqing Cui Yanqing Cui, Guangjie Han Yanqing Cui, Hongbo Zhu Guangjie Han",
        "published": "2022-1",
        "citations": 0,
        "abstract": "\n                        <p>The paper proposes a novel visual question answering (VQA)-based online teaching effect evaluation model. Based on the text interaction between teacher and students, we give a guide-attention (GA) model to discover the directive clues. Combining the self-attention (SA) models, we reweight the vital feature to locate the critical information on the whiteboard and students&rsquo; faces and further recognize their content and facial expressions. Three branches of information are encoded into the feature vectors to be fed into a bidirectional GRU network. With the real labels of the students’ answers annotated by two teachers and the predicted labels from the text and facial expression feedback, we train the chained network. Experiment reports a couple of competitive performance in the 2-class and 5-class tasks on the self-collected dataset, respectively.</p>\n<p>&nbsp;</p>\n                    ",
        "link": "http://dx.doi.org/10.53106/160792642022012301009"
    },
    {
        "id": 13766,
        "title": "Fusing attention with visual question answering",
        "authors": "Ryan Burt, Mihael Cudic, Jose C. Principe",
        "published": "2017-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2017.7965954"
    },
    {
        "id": 13767,
        "title": "Differential Attention for Visual Question Answering",
        "authors": "Badri Patro, Vinay P. Namboodiri",
        "published": "2018-6",
        "citations": 44,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cvpr.2018.00801"
    },
    {
        "id": 13768,
        "title": "Multi-stage Attention based Visual Question Answering",
        "authors": "Aakansha Mishra, Ashish Anand, Prithwijit Guha",
        "published": "2021-1-10",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr48806.2021.9413173"
    },
    {
        "id": 13769,
        "title": "Visual Question Answering with External Knowledge",
        "authors": "Santhosh Voruganti, Sairam U, Meghana S, Sravanthi M",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3853031"
    },
    {
        "id": 13770,
        "title": "Collaborative Modality Fusion for Mitigating Language Bias in Visual Question Answering",
        "authors": "Qiwen Lu, Shengbo Chen, Xiaoke Zhu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Language bias stands as a noteworthy concern in Visual Question Answering (VQA), wherein models tend to rely on spurious correlations between questions and answers for prediction. This prevents the models from effectively generalizing, leading to a decrease in performance. To address this bias, we propose a novel modality fusion collaborative de-biasing algorithm (CoD). In our approach, bias is considered as the model’s neglect of information from a particular modality during prediction. We employ a collaborative training approach to facilitate mutual modeling between different modalities, achieving efficient feature fusion and enabling the model to fully leverage multi-modal knowledge for prediction. Our experiments on various datasets, including VQA-CP v2, VQA v2, and VQA-VS, using different validation strategies, demonstrate the effectiveness of our approach. Notably, employing a basic baseline model resulted in an accuracy of 60.14% on VQA-CP v2.",
        "link": "http://dx.doi.org/10.20944/preprints202401.1211.v1"
    }
]