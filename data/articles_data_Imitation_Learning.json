[
    {
        "id": 18171,
        "title": "Learning by Imitation",
        "authors": "",
        "published": "2017",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-1-4899-7687-1_100248"
    },
    {
        "id": 18172,
        "title": "Contextual Online Imitation Learning (COIL): Using Guide Policies in Reinforcement Learning",
        "authors": "Alexander Hill, Marc Groefsema, Matthia Sabatelli, Raffaella Carloni, Marco Grzegorczyk",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012312700003636"
    },
    {
        "id": 18173,
        "title": "Imitation Learning",
        "authors": "",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-63416-2_300374"
    },
    {
        "id": 18174,
        "title": "Imitation Learning",
        "authors": "Zihan Ding",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-15-4095-0_8"
    },
    {
        "id": 18175,
        "title": "Imitation, Hayek, and the Significance of Cultural Learning",
        "authors": "H. Allen Orr",
        "published": "2019-11-5",
        "citations": 1,
        "abstract": "This chapter discusses biologist H. Allen Orr's two large and interesting questions about Robert Boyd's model of cultural learning. He wonders, first, whether Boyd exaggerates the contrast between the “Big Brain” model, which emphasizes cognitive explanations for human success, and the imitative model that Boyd prefers. Orr argues that successful imitation often requires considerable “neuronal firepower.” In addition, Orr usefully describes the partial convergence of Boyd's view with that advanced by the well-known free-market economist and social theorist Friedrich Hayek. Hayek also emphasized that social success and progress depend on the use of tacit and dispersed local knowledge, culturally transmitted social norms and ethical mores, and institutions that are the product of social evolution. Orr wonders whether scientists and social scientists pay less attention to Hayek than they should because of Hayek's politics.",
        "link": "http://dx.doi.org/10.23943/princeton/9780691195902.003.0004"
    },
    {
        "id": 18176,
        "title": "Imitation",
        "authors": "W. Sluckin",
        "published": "2018-2-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315151076-7"
    },
    {
        "id": 18177,
        "title": "Social learning: imitation",
        "authors": "Richard Hallam",
        "published": "2022-3-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003165507-55"
    },
    {
        "id": 18178,
        "title": "Mobile Robot Navigation Strategies Through Behavioral Cloning and Generative Adversarial Imitation Learning",
        "authors": "Kevin Silva, Rodrigo Calvo",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0011856700003467"
    },
    {
        "id": 18179,
        "title": "Learning by Observation",
        "authors": "Wanda Wyrwicka",
        "published": "2018-1-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781351292689-10"
    },
    {
        "id": 18180,
        "title": "Imitation or innovation: To what extent do exploitative learning and exploratory learning foster imitation strategy and innovation strategy for sustained competitive advantage?✰",
        "authors": "Murad Ali",
        "published": "2021-4",
        "citations": 55,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.techfore.2020.120527"
    },
    {
        "id": 18181,
        "title": "Mechanisms of copying, social learning, and imitation in animals",
        "authors": "Thomas R. Zentall",
        "published": "2022-11",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.lmot.2022.101844"
    },
    {
        "id": 18182,
        "title": "Learning for a Robot: Deep Reinforcement Learning, Imitation Learning, Transfer Learning",
        "authors": "Jiang Hua, Liangcai Zeng, Gongfa Li, Zhaojie Ju",
        "published": "2021-2-11",
        "citations": 98,
        "abstract": "Dexterous manipulation of the robot is an important part of realizing intelligence, but manipulators can only perform simple tasks such as sorting and packing in a structured environment. In view of the existing problem, this paper presents a state-of-the-art survey on an intelligent robot with the capability of autonomous deciding and learning. The paper first reviews the main achievements and research of the robot, which were mainly based on the breakthrough of automatic control and hardware in mechanics. With the evolution of artificial intelligence, many pieces of research have made further progresses in adaptive and robust control. The survey reveals that the latest research in deep learning and reinforcement learning has paved the way for highly complex tasks to be performed by robots. Furthermore, deep reinforcement learning, imitation learning, and transfer learning in robot control are discussed in detail. Finally, major achievements based on these methods are summarized and analyzed thoroughly, and future research challenges are proposed.",
        "link": "http://dx.doi.org/10.3390/s21041278"
    },
    {
        "id": 18183,
        "title": "Learning Through Imitation: an Experiment",
        "authors": "Marina Agranov, Gabriel Lopez-Moctezuma, Philipp Strack, Omer Tamuz",
        "published": "2022-4",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3386/w29962"
    },
    {
        "id": 18184,
        "title": "Efficient Robot Skill Learning: Grounded Simulation Learning and Imitation Learning from Observation",
        "authors": "Peter Stone",
        "published": "2021-4-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icarsc52212.2021.9429812"
    },
    {
        "id": 18185,
        "title": "Feedback-Driven Incremental Imitation Learning Using Sequential VAE",
        "authors": "Gabriela Sejnova, Karla Stepanova",
        "published": "2022-9-12",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icdl53763.2022.9962185"
    },
    {
        "id": 18186,
        "title": "Robot Manipulation Learning Using Generative Adversarial Imitation Learning",
        "authors": "Mohamed Khalil Jabri",
        "published": "2021-8",
        "citations": 4,
        "abstract": "Imitation learning allows learning complex behaviors given demonstrations. Early approaches belonging to either Behavior Cloning or Inverse Reinforcement Learning were however of limited scalability to complex environments. A more promising approach termed as Generative Adversarial Imitation Learning tackles the imitation learning problem by drawing a connection with Generative Adversarial Networks. In this work, we advocate the use of this class of methods and investigate possible extensions by endowing them with global temporal consistency, in particular through a contrastive learning based approach.",
        "link": "http://dx.doi.org/10.24963/ijcai.2021/678"
    },
    {
        "id": 18187,
        "title": "InfoSalGAIL: Visual Attention-empowered Imitation Learning of Pedestrian Behavior in Critical Traffic Scenarios",
        "authors": "Igor Vozniak, Matthias Klusch, André Antakli, Christian Müller",
        "published": "2020",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0010020003250337"
    },
    {
        "id": 18188,
        "title": "Learning Versus Imitation",
        "authors": "Mark Nielsen",
        "published": "2021",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-319-19650-3_2383"
    },
    {
        "id": 18189,
        "title": "Best-in-class imitation: Non-negative positive-unlabeled imitation learning from imperfect demonstrations",
        "authors": "Lin Zhang, Fei Zhu, Xinghong Ling, Quan Liu",
        "published": "2022-7",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ins.2022.04.015"
    },
    {
        "id": 18190,
        "title": "Inverse Reinforcement Learning and Imitation Learning",
        "authors": "Matthew F. Dixon, Igor Halperin, Paul Bilokon",
        "published": "2020",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-3-030-41068-1_11"
    },
    {
        "id": 18191,
        "title": "Imitation Learning: The Machine Learning Version of Discipleship",
        "authors": "Monica Vroman",
        "published": "2022-3-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.55409/math3ma2022-113"
    },
    {
        "id": 18192,
        "title": "A Hierarchical Autonomous Driving Framework Combining Reinforcement Learning and Imitation Learning",
        "authors": "Zeyu Li",
        "published": "2021-6",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccea53728.2021.00084"
    },
    {
        "id": 18193,
        "title": "Self-Imitation Guided High-Efficient Goal-Conditioned Reinforcement Learning",
        "authors": "Yao LI, YuHui Wang, XiaoYang Tan",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4419852"
    },
    {
        "id": 18194,
        "title": "Language Conditioned Imitation Learning Over Unstructured Data",
        "authors": "Corey Lynch*, Pierre Sermanet*",
        "published": "2021-7-12",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2021.xvii.047"
    },
    {
        "id": 18195,
        "title": "Relating movement primitive length to accuracy in imitation learning",
        "authors": "Sterling Holcomb, Rocio Alba-Flores",
        "published": "2017-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/secon.2017.7925335"
    },
    {
        "id": 18196,
        "title": "Learning to Drive Using Sparse Imitation Reinforcement Learning",
        "authors": "Yuci Han, Alper Yilmaz",
        "published": "2022-8-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icpr56361.2022.9956121"
    },
    {
        "id": 18197,
        "title": "Unsupervised Perceptual Rewards for Imitation Learning",
        "authors": "Pierre Sermanet, Kelvin Xu, Sergey Levine",
        "published": "2017-7-12",
        "citations": 33,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2017.xiii.050"
    },
    {
        "id": 18198,
        "title": "Limitations of self-imitation. Effects of self-imitation practice on L2 pronunciation with the use of a Golden Speaker Builder – an interactive tool for pronunciation training",
        "authors": "Ewa Kusz",
        "published": "2023-7-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.31274/psllt.15721"
    },
    {
        "id": 18199,
        "title": "Knowledge Management Strategies, Imitation, and Innovation",
        "authors": "V.T. Nguyen, T.A. Pham",
        "published": "2017",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/b978-0-08-100983-3.00004-6"
    },
    {
        "id": 18200,
        "title": "Learning the optimal state-feedback via supervised imitation learning",
        "authors": "Dharmesh Tailor, Dario Izzo",
        "published": "2019-12",
        "citations": 24,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/s42064-019-0054-0"
    },
    {
        "id": 18201,
        "title": "Adversarial Imitation Learning via Random Search",
        "authors": "MyungJae Shin, Joongheon Kim",
        "published": "2019-7",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn.2019.8852307"
    },
    {
        "id": 18202,
        "title": "An active inference model of hierarchical action understanding, learning and imitation",
        "authors": "Riccardo Proietti, Giovanni Pezzulo, Alessia Tessari",
        "published": "No Date",
        "citations": 3,
        "abstract": "We advance a novel computational model of the cognitive processing that underlies the acquisition of a hierarchical action repertoire and its use for observation, understanding and motor control. The model is grounded in a principled framework for understanding brain and cognition: active inference. We exemplify the functioning of the model by presenting four simulations of a tennis learner who observes a teacher performing tennis shots and forms hierarchical representations of the observed actions - including both actions that are already in her repertoire and novel actions - and finally imitates them. Our simulations show that the agent’s oculomotor activity implements an active information sampling strategy that permits inferring the kinematics aspects of the observed movement, which lie at the lowest level of the action hierarchy. In turn, this low-level kinematic inference supports higher-level inferences about deeper aspects of the observed actions, such as their proximal goals and intentions. Finally, the inferred action representations can steer imitative motor responses, but interfere with the execution of different actions. Taken together, our simulations show that the same hierarchical active inference model provides a unified account of action observation, understanding, learning and imitation. Finally, our model provides a computational rationale to explain the neurobiological underpinnings of visuomotor cognition, including the multiple routes for action understanding in the dorsal and ventral streams and mirror mechanisms.",
        "link": "http://dx.doi.org/10.31234/osf.io/ms95f"
    },
    {
        "id": 18203,
        "title": "Robot learning—Beyond imitation",
        "authors": "Guang-Zhong Yang",
        "published": "2019-1-30",
        "citations": 10,
        "abstract": "This special issue covers many aspects and applications of robot learning, including current progress, opportunities, and challenges.",
        "link": "http://dx.doi.org/10.1126/scirobotics.aaw3520"
    },
    {
        "id": 18204,
        "title": "Machine Learning",
        "authors": "",
        "published": "2021-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1017/9781009039604.022"
    },
    {
        "id": 18205,
        "title": "Synthesising Controllers for Quadrotors via Imitation Learning",
        "authors": "Yan Zong, Ningyun Lu, Bin Jiang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10451191"
    },
    {
        "id": 18206,
        "title": "Co-imitation: Learning Design and Behaviour by Imitation",
        "authors": "Chang Rajani, Karol Arndt, David Blanco-Mulero, Kevin Sebastian Luck, Ville Kyrki",
        "published": "2023-6-26",
        "citations": 1,
        "abstract": "The co-adaptation of robots has been a long-standing research endeavour with the goal of adapting both body and behaviour of a robot for a given task, inspired by the natural evolution of animals. Co-adaptation has the potential to eliminate costly manual hardware engineering as well as improve the performance of systems.\nThe standard approach to co-adaptation is to use a reward function for optimizing behaviour and morphology. However, defining and constructing such reward functions is notoriously difficult and often a significant engineering effort.\nThis paper introduces a new viewpoint on the co-adaptation problem, which we call co-imitation: finding a morphology and a policy that allow an imitator to closely match the behaviour of a demonstrator. To this end we propose a co-imitation methodology for adapting behaviour and morphology by matching state-distributions of the demonstrator. Specifically, we focus on the challenging scenario with mismatched state- and action-spaces between both agents. We find that co-imitation increases behaviour similarity across a variety of tasks and settings, and demonstrate co-imitation by transferring human walking, jogging and kicking skills onto a simulated humanoid.",
        "link": "http://dx.doi.org/10.1609/aaai.v37i5.25764"
    },
    {
        "id": 18207,
        "title": "Attention Guided Imitation Learning and Reinforcement Learning",
        "authors": "Ruohan Zhang",
        "published": "2019-7-17",
        "citations": 1,
        "abstract": "\r\n\r\n\r\nWe propose a framework that uses learned human visual attention model to guide the learning process of an imitation learning or reinforcement learning agent. We have collected high-quality human action and eye-tracking data while playing Atari games in a carefully controlled experimental setting. We have shown that incorporating a learned human gaze model into deep imitation learning yields promising results.\r\n\r\n\r\n",
        "link": "http://dx.doi.org/10.1609/aaai.v33i01.33019906"
    },
    {
        "id": 18208,
        "title": "IVDR: Imitation learning with Variational inference and Distributional Reinforcement learning to find Optimal Driving Strategy",
        "authors": "Kihyung Joo, Simon S. Woo",
        "published": "2021-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icmla52953.2021.00047"
    },
    {
        "id": 18209,
        "title": "Social Learning \"Imitation, Modeling\" During Early Childhood",
        "authors": "Tedjani Djeradi, Sirine Hadjer Zaabta",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.54246/1548-015-002-067"
    },
    {
        "id": 18210,
        "title": "Two Human-Like Imitation-Learning Bots with Probabilistic Behaviors",
        "authors": "Chris Pelling, Henry Gardner",
        "published": "2019-8",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cig.2019.8847995"
    },
    {
        "id": 18211,
        "title": "Off-Road Navigation With End-to-end Imitation Learning for Continuously Parameterized Control",
        "authors": "Crockett Hensley, Matthew Marshall",
        "published": "2022-3-26",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/southeastcon48659.2022.9763997"
    },
    {
        "id": 18212,
        "title": "Imitation as a model-free process in human reinforcement learning",
        "authors": "Anis Najar, Emmanuelle Bonnet, Bahador Bahrami, Stefano Palminteri",
        "published": "No Date",
        "citations": 1,
        "abstract": "While there is not doubt that social signals affect human reinforcement learning, there is still no consensus about their exact computational implementation. To address this issue, we compared three hypotheses about the algorithmic implementation of imitation in human reinforcement learning. A first hypothesis, decision biasing, postulates that imitation consists in transiently biasing the learner’s action selection without affecting her value function. According to the second hypothesis, model-based imitation, the learner infers the demonstrator’s value function through inverse reinforcement learning and uses it for action selection. Finally, according to the third hypothesis, value shaping, demonstrator’s actions directly affect the learner’s value function. We tested these three psychologically plausible hypotheses in two separate experiments (N = 24 and N = 44) featuring a new variant of a social reinforcement learning task, where we manipulated the quantity and the quality of the demonstrator’s choices. We show through model comparison that value shaping is favored, which provides a new perspective on how imitation is integrated into human reinforcement learning.",
        "link": "http://dx.doi.org/10.1101/797407"
    },
    {
        "id": 18213,
        "title": "Imitation Reinforcement Learning with Vision and Navigation for Autonomous Driving",
        "authors": "Lei He, Mingyue Ba, Yiren Wang, Ling Han",
        "published": "No Date",
        "citations": 0,
        "abstract": "\nAutonomous urban driving navigation remains an ongoing\nchallenge, with ample scope for improvement, particularly in navigating\nthrough unfamiliar and complex environments. The images captured by\ncameras provide a wealth of environmental information; however,\naccurately determining the positions of obstacles within these images\ncan be adversely affected by inclement weather conditions such as rain,\nsnow, or haze. In response to these challenges, this paper presents a\nhierarchical framework named CNS-DDPG. CNS, which stands for Conditional\nImitation Learning, involves the fusion of navigation state information\nwith global path and vehicle state data. DDPG, or Deterministic Policy\nGradient, is used for subsequent reinforcement learning. By carefully\nweighing the strengths and weaknesses of the image and perception\nmodule, our framework compensates for visual information captured by the\ncamera by incorporating navigation state data. This design allows our\nmodel to perform effectively even in adverse weather conditions.\nHowever, the limitations of imitation learning, particularly the\nscarcity of diverse training data, prompted us to employ the\nreinforcement learning method DDPG in the second stage of training. This\nstage benefits from the learned weights of the pre-trained and optimal\nCNS model. This approach reduces the reliance on imitation learning data\nand mitigates the challenge of low exploration efficiency associated\nwith randomly initialized weights in reinforcement learning.\nAdditionally, we implement image enhancement techniques to mitigate\noverfitting associated with simple image types. To evaluate the\neffectiveness of our approach, we conducted experiments using the CARLA\ndriving benchmark for urban driving. The car was controlled by a\nRaspberry Pi 4B, which was trained to navigate through an experimental\narea. Our experiments reveal that CNS-DDPG exhibits remarkable\ngeneralization capabilities, particularly in unfamiliar environments and\nchallenging navigation tasks.\n",
        "link": "http://dx.doi.org/10.22541/au.169957038.84697482/v1"
    },
    {
        "id": 18214,
        "title": "Investigation and Imitation of Human Captains’ Maneuver Using Inverse Reinforcement Learning",
        "authors": "Takefumi Higaki, Hirotada Hashimoto, Hitoshi Yoshioka",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nAutomatic collision avoidance is of significant importance to prevent maritime collisions. Although many studies have been conducted in recent years, autonomous system has not completely replaced human captains since it is still difficult to imitate their complicated decisions. Thus, the present paper tries to investigate and imitate experienced captains’ maneuver using maximum entropy inverse reinforcement learning (MaxEnt IRL). We firstly verify that MaxEnt IRL can reproduce appropriate reward function from demonstrative trajectories. Afterwards, we conduct an experiment on a simulator where well-experienced captains maneuver in congested sea and estimate reward from the trajectories. Searching the route which maximizes the obtained reward, finally, we demonstrate the optimized route can avoid collision against multiple ships in compliance with the International Regulations for Preventing Collisions at Sea (COLREGs).",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-1844861/v1"
    },
    {
        "id": 18215,
        "title": "Play it by Ear: Learning Skills amidst Occlusion through Audio-Visual Imitation Learning",
        "authors": "Maximilian Du, Olivia Y Lee, Suraj Nair, Chelsea Finn",
        "published": "2022-6-27",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2022.xviii.009"
    },
    {
        "id": 18216,
        "title": "Cooperative and Competitive Reinforcement and Imitation Learning for a Mixture of Heterogeneous Learning Modules",
        "authors": "Eiji Uchibe",
        "published": "2018-9-27",
        "citations": 5,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3389/fnbot.2018.00061"
    },
    {
        "id": 18217,
        "title": "An end-to-end learning of driving strategies based on DDPG and imitation learning",
        "authors": "Qijie Zou, Kang Xiong, Yingli Hou",
        "published": "2020-8",
        "citations": 8,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ccdc49329.2020.9164410"
    },
    {
        "id": 18218,
        "title": "Demand response strategy for HVAC based on reinforcement learning and imitation learning",
        "authors": "Y. He, H. Zhong, G. Zhang, G. Ruan",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1049/icp.2022.2096"
    },
    {
        "id": 18219,
        "title": "Robotic Manipulation with Reinforcement Learning, State Representation Learning, and Imitation Learning (Student Abstract)",
        "authors": "Hanxiao Chen",
        "published": "2021-5-18",
        "citations": 4,
        "abstract": "Humans possess the advanced ability to grab, hold, and manipulate objects with dexterous hands. What about robots? Can they interact with the surrounding world intelligently to achieve certain goals (e.g., grasping, object-relocation)? Actually, robotic manipulation is central to achieving the premise of robotics and represents immense potential to be widely applied in various scenarios like industries, hospitals, and homes. In this work, we aim to address multiple robotic manipulation tasks like grasping, button-pushing, and door-opening with reinforcement learning (RL), state representation learning (SRL), and imitation learning. For diverse missions, we self-built the PyBullet or MuJoCo simulated environments and independently explored three different learning-style methods to successfully solve such tasks: (1) Normal reinforcement learning methods; (2) Combined state representation learning (SRL) and RL approaches; (3) Imitation learning bootstrapped RL algorithms.",
        "link": "http://dx.doi.org/10.1609/aaai.v35i18.17881"
    },
    {
        "id": 18220,
        "title": "Sample Efficiency in Deep Reinforcement Learning based Recommender Systems with Imitation Learning",
        "authors": "Mohammad Mehdi Afsar, Trafford Crump, Behrouz Far",
        "published": "2022-5-27",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21428/594757db.6a36bb36"
    },
    {
        "id": 18221,
        "title": "Algorithmic Design of Autonomous Housekeeping Robots through Imitation Learning and Model Predictive Control",
        "authors": "Fangyu Zhu, Zhe Wu",
        "published": "2022-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cacml55074.2022.00024"
    },
    {
        "id": 18222,
        "title": "Improved Reinforcement Learning through Imitation Learning Pretraining Towards Image-based Autonomous Driving",
        "authors": "Tianqi Wang, Dong Eui Chang",
        "published": "2019-10",
        "citations": 13,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/iccas47443.2019.8971737"
    },
    {
        "id": 18223,
        "title": "Bilateral Control-Based Imitation Learning for Velocity-Controlled Robot",
        "authors": "Sho Sakaino",
        "published": "2021-6-20",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/isie45552.2021.9576326"
    },
    {
        "id": 18224,
        "title": "Self-Driving Vehicles Using End to End Deep Imitation Learning",
        "authors": "Ashraf Nabil, Ayman Kassem",
        "published": "2021-11-7",
        "citations": 0,
        "abstract": "Autonomous Driving is one of the difficult problems faced the automotive applications. Nowadays, it is restricted due to the presence of some laws that prevent cars from being fully autonomous for the fear of accidents occurrence. Researchers try to improve the accuracy and safety of their models with the aim of having a strong push against these restricted Laws.\r\nAutonomous driving is a sought-after solution which isn’t easily solved by classical approaches. Deep Learning is considered as a strong Artificial Intelligence paradigm which can teach machines how to behave in difficult situations. It proved its success in many differ domains, but it still has sometime in the automotive applications.\r\nThe presented work will use the end-to-end deep machine learning field in order to reach to our goal of having Full Autonomous Driving Vehicle that can behave correctly in different scenarios. CARLA simulator will be used to learn and test the deep neural networks. Results will show not only performance on CARLA’s simulator as an end-to-end solution for autonomous driving, but also how the same approach can be used on one of the most popular real datasets of automotive that includes camera images with the corresponding driver’s control action.",
        "link": "http://dx.doi.org/10.14738/tmlai.95.10795"
    },
    {
        "id": 18225,
        "title": "Self-Imitation Learning by Planning",
        "authors": "Sha Luo, Hamidreza Kasaei, Lambert Schomaker",
        "published": "2021-5-30",
        "citations": 6,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra48506.2021.9561411"
    },
    {
        "id": 18226,
        "title": "Improving Reward Estimation in Goal-Conditioned Imitation Learning with Counterfactual Data and Structural Causal Models",
        "authors": "Mohamed Jabri, Panagiotis Papadakis, Ehsan Abbasnejad, Gilles Coppin, Javen Shi",
        "published": "2023",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5220/0012268200003543"
    },
    {
        "id": 18227,
        "title": "Reinforcement learning building control approach harnessing imitation learning",
        "authors": "Sourav Dey, Thibault Marzullo, Xiangyu Zhang, Gregor Henze",
        "published": "2023-10",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.egyai.2023.100255"
    },
    {
        "id": 18228,
        "title": "Learning birdsong by imitation",
        "authors": "David F. Clayton",
        "published": "2019-10-4",
        "citations": 4,
        "abstract": "Transforming sensory information into vocal imitation allows young finches to sing",
        "link": "http://dx.doi.org/10.1126/science.aaz1552"
    },
    {
        "id": 18229,
        "title": "Efficient Imitation Learning for Game AI",
        "authors": "Chao Huang, Like Zhang, Yanqing Jing, Dajun Zhou",
        "published": "2020-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cog47356.2020.9231613"
    },
    {
        "id": 18230,
        "title": "Evolutionary Implications of Learning by Empathy, Imitation, and Identification",
        "authors": "Margaret Mead, Stephen Toulmin",
        "published": "2017-7-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781315080437-4"
    },
    {
        "id": 18231,
        "title": "Reinforcement Learning to Efficiently Recover Control Performance of Robots Using Imitation Learning After Failure",
        "authors": "Shoki Kobayashi, Takeshi Shibuya",
        "published": "2022-10-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/smc53654.2022.9945538"
    },
    {
        "id": 18232,
        "title": "Building Safe and Stable DNN Controllers using Deep Reinforcement Learning and Deep Imitation Learning",
        "authors": "Xudong He",
        "published": "2022-12",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/qrs57517.2022.00083"
    },
    {
        "id": 18233,
        "title": "Improving Learning by Imitation in Online Courses using Memorization, Learning by Doing and Lecture Architecture for Naive Programmers",
        "authors": "Siddharth Srivastava, Shalini Lamba, T.V. Prabhakar",
        "published": "2020-7",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icalt49669.2020.00064"
    },
    {
        "id": 18234,
        "title": "Self-Supervised Disentangled Representation Learning for Third-Person Imitation Learning",
        "authors": "Jinghuan Shang, Michael S. Ryoo",
        "published": "2021-9-27",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros51168.2021.9636363"
    },
    {
        "id": 18235,
        "title": "Robot learning by Single Shot Imitation for Manipulation Tasks",
        "authors": "Mohit Vohra, Laxmidhar Behera",
        "published": "2022-7-18",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ijcnn55064.2022.9892529"
    },
    {
        "id": 18236,
        "title": "Emergence of Chaotic Time Series by Adversarial Imitation Learning",
        "authors": "Seiya Yamazaki, Hiroyuki Iizuka, Masahito Yamamoto",
        "published": "2018",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1162/isal_a_00120"
    },
    {
        "id": 18237,
        "title": "Bridging the Gap Between Imitation Learning and Inverse Reinforcement Learning",
        "authors": "Bilal Piot, Matthieu Geist, Olivier Pietquin",
        "published": "2017-8",
        "citations": 45,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2016.2543000"
    },
    {
        "id": 18238,
        "title": "Social Learning between Groups: Imitation and the Role of Experience",
        "authors": "Karl H. Schlag",
        "published": "2022-9-6",
        "citations": 0,
        "abstract": "Social learning often occurs between groups with different levels of experience. Yet little is known about the ideal behavioral rules in such contexts. Existing insights only apply when individuals learn from each other in the same group. In this paper, we close this gap and consider two groups, novices and experienced. Experienced should not learn from novices. For novices learning from experienced, a particular form of probabilistic imitation is selected. Novices should imitate any experienced who is more successful, and sometimes but not always imitate an experienced who is less successful.",
        "link": "http://dx.doi.org/10.3390/g13050060"
    },
    {
        "id": 18239,
        "title": "Adversarial Imitation Learning from Incomplete Demonstrations",
        "authors": "Mingfei Sun, Xiaojuan Ma",
        "published": "2019-8",
        "citations": 8,
        "abstract": "Imitation learning targets deriving a mapping from states to actions, a.k.a. policy, from expert demonstrations. Existing methods for imitation learning typically require any actions in the demonstrations to be fully available, which is hard to ensure in real applications. Though algorithms for learning with unobservable actions have been proposed, they focus solely on state information and over- look the fact that the action sequence could still be partially available and provide useful information for policy deriving. In this paper, we propose a novel algorithm called Action-Guided Adversarial Imitation Learning (AGAIL) that learns a pol- icy from demonstrations with incomplete action sequences, i.e., incomplete demonstrations. The core idea of AGAIL is to separate demonstrations into state and action trajectories, and train a policy with state trajectories while using actions as auxiliary information to guide the training whenever applicable. Built upon the Generative Adversarial Imitation Learning, AGAIL has three components: a generator, a discriminator, and a guide. The generator learns a policy with rewards provided by the discriminator, which tries to distinguish state distributions between demonstrations and samples generated by the policy. The guide provides additional rewards to the generator when demonstrated actions for specific states are available. We com- pare AGAIL to other methods on benchmark tasks and show that AGAIL consistently delivers com- parable performance to the state-of-the-art methods even when the action sequence in demonstrations is only partially available.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/487"
    },
    {
        "id": 18240,
        "title": "Learning Through Imitation: an Experiment",
        "authors": "Marina Agranov, GABRIEL LOPEZ-MOCTEZUMA, Philipp Strack, Omer Tamuz",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4083318"
    },
    {
        "id": 18241,
        "title": "Imitation Learning for Financial Applications",
        "authors": "Sven Goluža, Tessa Bauman, Tomislav Kovačević, Zvonko Kostanjčar",
        "published": "2023-5-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/mipro57284.2023.10159778"
    },
    {
        "id": 18242,
        "title": "BAGAIL: Multi-modal imitation learning from imbalanced demonstrations",
        "authors": "Sijia Gu, Fei Zhu",
        "published": "2024-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.neunet.2024.106251"
    },
    {
        "id": 18243,
        "title": "Performance Metrics for State-Based Imitation Learning",
        "authors": "Mohamed Zalat, Babak Esfandiari",
        "published": "2021-4-18",
        "citations": 0,
        "abstract": "We propose five new domain-independent metrics for evaluating and comparing performance at imitating a state-based expert. We use two agents in the RoboCup environment to compare the performance metrics: an agent based on a Multi-Layer Perceptron (MLP) and an agent based on a Long Short-Term Memory (LSTM) neural network.",
        "link": "http://dx.doi.org/10.32473/flairs.v34i1.128479"
    },
    {
        "id": 18244,
        "title": "Imitation Learning",
        "authors": "Laxmidhar Behera, Swagat Kumar, Prem Kumar Patchaikani, Ranjith Ravindranathan Nair, Samrat Dutta",
        "published": "2020-4-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9780429486784-9"
    },
    {
        "id": 18245,
        "title": "Imitation Learning for Mean Field Games with Correlated Equilibria",
        "authors": "Zhiyu Zhao, Renyuan Xu, Haifeng Zhang, Jun Wang, Yaodong Yang",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nImitation learning (IL) is a powerful approach for acquiring optimal policies from demonstrated behaviors. However, applying IL to a large group of agents is arduous due to the exponential surge in interactions with an increase in population size. Mean Field Theory provides an efficient tool for analyzing multi-agent problems by gathering information at the population level. Although the approximation is tractable, restoring mean field Nash equilibria (MFNE) from demonstrations is challenging. Furthermore, many real-world problems, including traffic network equilibrium induced by public routing recommendations and pricing equilibrium of goods on E-commerce platforms, cannot be explained by the classic MFNE concept. In both cases, the intervention of the platform introduces correlation devices to the equilibrium. To address this issue, we propose a novel solution concept called Adaptive Mean Field Correlated Equilibrium (AMFCE) that generalizes MFNE. We establish a framework based on IL and AMFCE that recovers the AMFCE policy from real-world demonstrations. Our framework characterizes mean-field evolution using signatures from the rough path theory, and it has the significant benefit of recovering both the equilibrium policy and correlation device from data. We test our framework against state-of-the-art IL algorithms for mean field games (MFGs) on several tasks, including a real-world traffic flow prediction problem. Our results demonstrate the effectiveness of our proposed method and its potential for predicting and explaining large population behavior under correlated signals.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3108515/v1"
    },
    {
        "id": 18246,
        "title": "An Algorithmic Perspective on Imitation\n                  Learning",
        "authors": "Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J. Andrew Bagnell, Pieter Abbeel, Jan Peters",
        "published": "2018",
        "citations": 81,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1561/9781680834116"
    },
    {
        "id": 18247,
        "title": "Adaptive Information Gathering via Imitation Learning",
        "authors": "Sanjiban Choudhury, Ashish Kapoor, Gireeja Ranade, Sebastian Scherer, Debadeepta Dey",
        "published": "2017-7-12",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2017.xiii.041"
    },
    {
        "id": 18248,
        "title": "Learning Through Imitation: An Experiment",
        "authors": "Marina Agranov, GABRIEL LOPEZ-MOCTEZUMA, Philipp Strack, Omer Tamuz",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4092291"
    },
    {
        "id": 18249,
        "title": "Training Electric Vehicle Charging Controllers with Imitation Learning",
        "authors": "Martin Pilat",
        "published": "2021-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ictai52525.2021.00107"
    },
    {
        "id": 18250,
        "title": "Teleoperated Hexapod Robot for Imitation Learning Task Training",
        "authors": "Austin Gurley",
        "published": "2019-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iros40897.2019.8968540"
    },
    {
        "id": 18251,
        "title": "A Motion Capture and Imitation Learning-based Approach to Robot Control",
        "authors": "Peteris Racinskis, Janis Arents, Modris Greitans",
        "published": "No Date",
        "citations": 3,
        "abstract": "Imitation Learning is a discipline of Machine Learning primarily concerned with replicating observed behavior of agents known to perform well on a given task, collected in demonstration data sets. In this paper, we set out to introduce a pipeline for collecting demonstrations and training models that can produce motion plans for industrial robots. Object throwing is defined as the motivating use case. Multiple input data modalities are surveyed, and motion capture is selected as the most practicable. Two model architectures operating autoregressively are examined -- feedforward and recurrent neural networks. Trained models execute throws on a real robot successfully, and a battery of quantitative evaluation metrics is proposed, including extrapolated throw accuracy estimates. Recurrent neural networks outperform feedforward ones in most respects, with the best models having an assessed mean throw error on the order of 0.1...0.2 m at distances of 1.5...2.0 m. The data collection, pre-processing, and model training aspects of our proposed approach show promise, but further work is required in developing Cartesian motion planning tools before it is suitable for application in production.",
        "link": "http://dx.doi.org/10.20944/preprints202206.0427.v1"
    },
    {
        "id": 18252,
        "title": "CCIL: Context-conditioned imitation learning for urban driving",
        "authors": "Ke Guo, Wei Jing, Junbo Chen, Jia Pan",
        "published": "2023-7-10",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2023.xix.101"
    },
    {
        "id": 18253,
        "title": "Episodic Self-Imitation Learning with Hindsight",
        "authors": "Tianhong Dai, Hengyan Liu, Anil Anthony Bharath",
        "published": "2020-10-21",
        "citations": 9,
        "abstract": "Episodic self-imitation learning, a novel self-imitation algorithm with a trajectory selection module and an adaptive loss function, is proposed to speed up reinforcement learning. Compared to the original self-imitation learning algorithm, which samples good state–action pairs from the experience replay buffer, our agent leverages entire episodes with hindsight to aid self-imitation learning. A selection module is introduced to filter uninformative samples from each episode of the update. The proposed method overcomes the limitations of the standard self-imitation learning algorithm, a transitions-based method which performs poorly in handling continuous control environments with sparse rewards. From the experiments, episodic self-imitation learning is shown to perform better than baseline on-policy algorithms, achieving comparable performance to state-of-the-art off-policy algorithms in several simulated robot control tasks. The trajectory selection module is shown to prevent the agent learning undesirable hindsight experiences. With the capability of solving sparse reward problems in continuous control settings, episodic self-imitation learning has the potential to be applied to real-world problems that have continuous action spaces, such as robot guidance and manipulation.",
        "link": "http://dx.doi.org/10.3390/electronics9101742"
    },
    {
        "id": 18254,
        "title": "Online Imitation Learning for Self-Driving Simulation",
        "authors": "Zhe Zhang, Sanyuan Zhao",
        "published": "2021-8-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iccse51940.2021.9569543"
    },
    {
        "id": 18255,
        "title": "Behaviourism: Learning through Imitation and Reinforcement",
        "authors": "Susan Young",
        "published": "2023-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003331193-4"
    },
    {
        "id": 18256,
        "title": "Hierarchical Model-Based Imitation Learning for In-between Motion Synthesis",
        "authors": "Yuqi Yang, Yuehu Liu, Chi Zhang",
        "published": "2023-11-17",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cac59555.2023.10451613"
    },
    {
        "id": 18257,
        "title": "Deep reinforcement learning and imitation learning based on VizDoom",
        "authors": "Yingyu Xu",
        "published": "2022-10-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1145/3573428.3573729"
    },
    {
        "id": 18258,
        "title": "Learning Human Driving Behaviors with Sequential Causal Imitation Learning",
        "authors": "Kangrui Ruan, Xuan Di",
        "published": "2022-6-28",
        "citations": 3,
        "abstract": "Learning human driving behaviors is an efficient approach for self-driving vehicles. Traditional Imitation Learning (IL) methods assume that the expert demonstrations follow Markov Decision Processes (MDPs). However, in reality, this assumption does not always hold true. Spurious correlation may exist through the paths of historical variables because of the existence of unobserved confounders. Accounting for the latent causal relationships from unobserved variables to outcomes, this paper proposes Sequential Causal Imitation Learning (SeqCIL) for imitating driver behaviors. We develop a sequential causal template that generalizes the default MDP settings to one with Unobserved Confounders (MDPUC-HD). Then we develop a sufficient graphical criterion to determine when ignoring causality leads to poor performances in MDPUC-HD. Through the framework of Adversarial Imitation Learning, we develop a procedure to imitate the expert policy by blocking π-backdoor paths at each time step. Our methods are evaluated on a synthetic dataset and a real-world highway driving dataset, both demonstrating that the proposed procedure significantly outperforms non-causal imitation learning methods.",
        "link": "http://dx.doi.org/10.1609/aaai.v36i4.20382"
    },
    {
        "id": 18259,
        "title": "Motion Generation Using Bilateral Control-Based Imitation Learning With Autoregressive Learning",
        "authors": "Ayumu Sasagawa, Sho Sakaino, Toshiaki Tsuji",
        "published": "2021",
        "citations": 16,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/access.2021.3054960"
    },
    {
        "id": 18260,
        "title": "Learning Support by Visualizing Game Strategies From Imitation Learning Agents",
        "authors": "Ueno Masayuki, Takami Tomoyuki",
        "published": "2022-10-18",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/gcce56475.2022.10014419"
    },
    {
        "id": 18261,
        "title": "Learning by Watching via Keypoint Extraction and Imitation Learning",
        "authors": "Yin-Tung Albert Sun, Hsin-Chang Lin, Po-Yen Wu, Jung-Tang Huang",
        "published": "2022-11-9",
        "citations": 0,
        "abstract": "In recent years, the use of reinforcement learning and imitation learning to complete robot control tasks have become more popular. Demonstration and learning by experts have always been the goal of researchers. However, the lack of action data has been a significant limitation to learning by human demonstration. We propose an architecture based on a new 3D keypoint tracking model and generative adversarial imitation learning to learn from expert demonstrations. We used 3D keypoint tracking to make up for the lack of action data in simple images and then used image-to-image conversion to convert human hand demonstrations into robot images, which enabled subsequent generative adversarial imitation learning to learn smoothly. The estimation time of the 3D keypoint tracking model and the calculation time of the subsequent optimization algorithm was 30 ms. The coordinate errors of the model projected to the real 3D key point under correct detection were all within 1.8 cm. The tracking of key points did not require any sensors on the body; the operator did not need vision-related knowledge to correct the accuracy of the camera. By merely setting up a generic depth camera to track the mapping changes of key points after behavior clone training, the robot could learn human tasks by watching, including picking and placing an object and pouring water. We used pybullet to build an experimental environment to confirm our concept of the simplest behavioral cloning imitation to attest the success of the learning. The effectiveness of the proposed method was accomplished by a satisfactory performance requiring a sample efficiency of 20 sets for pick and place and 30 sets for pouring water.",
        "link": "http://dx.doi.org/10.3390/machines10111049"
    },
    {
        "id": 18262,
        "title": "Graph-based Subtask Representation Learning via Imitation Learning",
        "authors": "Se-Wook Yoo, Seung-Woo Seo",
        "published": "2022-2-6",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iceic54506.2022.9748273"
    },
    {
        "id": 18263,
        "title": "Online Baum-Welch algorithm for Hierarchical Imitation Learning",
        "authors": "Vittorio Giammarino, Ioannis Ch. Paschalidis",
        "published": "2021-12-14",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cdc45484.2021.9683044"
    },
    {
        "id": 18264,
        "title": "Creating Deep Learning-based Acrobatic Videos Using Imitation Videos",
        "authors": "",
        "published": "2021-2-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.3837/tiis.2021.02.018"
    },
    {
        "id": 18265,
        "title": "A Linearly Constrained Nonparametric Framework for Imitation Learning",
        "authors": "Yanlong Huang, Darwin G. Caldwell",
        "published": "2020-5",
        "citations": 4,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/icra40945.2020.9196821"
    },
    {
        "id": 18266,
        "title": "Neuro-computational account of arbitration between imitation and emulation during human observational learning",
        "authors": "Caroline C. Charpentier, Kiyohito Iigaya, John P. O’Doherty",
        "published": "No Date",
        "citations": 1,
        "abstract": "AbstractIn observational learning (OL), organisms learn from observing the behavior of others. There are at least two distinct strategies for OL. Imitation involves learning to repeat the previous actions of other agents, while in emulation, learning proceeds from inferring the goals and intentions of others. While putative neural correlates for these forms of learning have been identified, a fundamental question remains unaddressed: how does the brain decides which strategy to use in a given situation? Here we developed a novel computational model in which arbitration between the strategies is determined by the predictive reliability, such that control over behavior is adaptively weighted toward the strategy with the most reliable prediction. To test the theory, we designed a novel behavioral task in which our experimental manipulations produced dissociable effects on the reliability of the two strategies. Participants performed this task while undergoing fMRI in two independent studies (the second a pre-registered replication of the first). Behavior manifested patterns consistent with both emulation and imitation and flexibly changed between the two strategies as expected from the theory. Computational modelling revealed that behavior was best described by an arbitration model, in which the reliability of the emulation strategy determined the relative weights allocated to behavior for each strategy. Emulation reliability - the model’s arbitration signal - was encoded in the ventrolateral prefrontal cortex, temporoparietal junction and rostral cingulate cortex. Being replicated across two fMRI studies, these findings suggest a neuro-computational mechanism for allocating control between emulation and imitation during observational learning.",
        "link": "http://dx.doi.org/10.1101/828723"
    },
    {
        "id": 18267,
        "title": "Quantum Imitation Learning",
        "authors": "Zhihao Cheng, Kaining Zhang, Li Shen, Dacheng Tao",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/tnnls.2023.3275075"
    },
    {
        "id": 18268,
        "title": "Randomized Adversarial Imitation Learning for Autonomous Driving",
        "authors": "MyungJae Shin, Joongheon Kim",
        "published": "2019-8",
        "citations": 15,
        "abstract": "With the evolution of various advanced driver assistance system (ADAS) platforms, the design of autonomous driving system is becoming more complex and safety-critical. The autonomous driving system simultaneously activates multiple ADAS functions; and thus it is essential to coordinate various ADAS functions. This paper proposes a randomized adversarial imitation learning (RAIL) method that imitates the coordination of autonomous vehicle equipped with advanced sensors. The RAIL policies are trained through derivative-free optimization for the decision maker that coordinates the proper ADAS functions, e.g., smart cruise control and lane keeping system. Especially, the proposed method is also able to deal with the LIDAR data and makes decisions in complex multi-lane highways and multi-agent environments.",
        "link": "http://dx.doi.org/10.24963/ijcai.2019/638"
    },
    {
        "id": 18269,
        "title": "Robotic Object Manipulation with Full-Trajectory GAN-Based Imitation Learning",
        "authors": "Haoxu Wang, David Meger",
        "published": "2021-5",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/crv52889.2021.00016"
    },
    {
        "id": 18270,
        "title": "OIL: Observational Imitation Learning",
        "authors": "Guohao Li, Matthias Mueller, Vincent Michael Casser, Neil Smith, Dominik Michels, Bernard Ghanem",
        "published": "2019-6-22",
        "citations": 17,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.15607/rss.2019.xv.005"
    }
]