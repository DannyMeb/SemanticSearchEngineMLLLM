[
    {
        "id": 5201,
        "title": "Loss-functions matter, on optimizing score functions for the estimation of protein models accuracy",
        "authors": "Tomer Sidi, Chen Keasar",
        "published": "No Date",
        "citations": 3,
        "abstract": "AbstractMotivationMethods for protein structure prediction (PSP) generate multiple alternative structural models (aka decoys). Thus, supervised learning methods for the evaluation and ranking of these models are crucial elements of PSP. Supervised learning involves optimization of loss functions, but their influence on performance is typically overlooked. Here we put the loss functions in the spotlight, and study their effect on prediction performance.ResultsHere we report the performances of three variants of MESHI-score, a supervised learning method for the estimation of model accuracy (EMA). Each variant was trained with a different loss function and showed better performance in different aspects of the EMA problem. Most importantly, better discrimination between models of the same target, is gained by target centered loss functions.AvailabilityAll data is available at http://meshi1.cs.bgu.ac.il/SidiAndKeasar2018Data_download/. The MESHI-package (version 9.412) is available at https://github.com/meshiprot/meshi/releases).Contactchen.keasar@gmail.com",
        "link": "http://dx.doi.org/10.1101/651349"
    },
    {
        "id": 5202,
        "title": "Adjusted Distributionally Robust Bounds on Expected Loss Functions",
        "authors": "Yasemin Merzifonluoglu, Joseph Geunes",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4438326"
    },
    {
        "id": 5203,
        "title": "Matrices, Transfer Functions, and Insertion Loss",
        "authors": "",
        "published": "2017-12-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/b11543-16"
    },
    {
        "id": 5204,
        "title": "Interpreting the loss functions of Artificial neural networks in cancer research",
        "authors": "Karthik Rao",
        "published": "No Date",
        "citations": 0,
        "abstract": "Artificial Neural Networks (ANNs) have become a popular tool in cancer research for their ability to learn complex relationships between input variables and clinical outcomes. One of the crucial components of ANNs is the loss function, It measures the difference between the output that was anticipated and the output that was produced. In cancer research, different loss functions are used depending on the nature of the research question and the type of data being analyzed. The optimal loss function is critical to ensure optimal performance of the ANN model. The Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) are used in regression tasks, while Cross-Entropy (CE) is often used in classification tasks. The optimal selection of loss function depends on the specific research question and data being analyzed.\n",
        "link": "http://dx.doi.org/10.32388/0drdvr"
    },
    {
        "id": 5205,
        "title": "$L_p$ Loss Functions in Invariance Alignment and Haberman Linking",
        "authors": "Alexander Robitzsch",
        "published": "No Date",
        "citations": 2,
        "abstract": "The comparison of group means in latent variable models plays a vital role in empirical research in the social sciences. The present article discusses extensions of invariance alignment and Haberman linking concerning the choice of linking functions for comparisons of many groups. Robust linking functions are proposed for invariance alignment and robust Haberman linking that are particularly suited to item response data under partial invariance. In a simulation study, it is shown that both linking approaches have comparable performance, and in some conditions, the newly proposed robust Haberman linking outperforms invariance alignment.",
        "link": "http://dx.doi.org/10.20944/preprints202006.0034.v1"
    },
    {
        "id": 5206,
        "title": "The Loss of Municipal Functions",
        "authors": "W. A. Robson",
        "published": "2021-12-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4324/9781003273011-2"
    },
    {
        "id": 5207,
        "title": "Risk and Loss Functions",
        "authors": "",
        "published": "2018",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.7551/mitpress/4175.003.0006"
    },
    {
        "id": 5208,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Shahab Ansari",
        "published": "2023-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/j3eytu"
    },
    {
        "id": 5209,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Çağkan Yapar",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/t94hil"
    },
    {
        "id": 5210,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Jun Qi",
        "published": "2023-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/dgla25"
    },
    {
        "id": 5211,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Michele Piana",
        "published": "2023-5-31",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/yl0a3f"
    },
    {
        "id": 5212,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Hasan Tiryaki",
        "published": "2023-5-30",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/2jp3ft"
    },
    {
        "id": 5213,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Amirreza Fateh",
        "published": "2023-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/7lkf0s"
    },
    {
        "id": 5214,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Jyotir Moy",
        "published": "2023-6-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/bpmgcr"
    },
    {
        "id": 5215,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Jaume Vives",
        "published": "2023-6-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/wx6b19"
    },
    {
        "id": 5216,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Nathaniel Egwu",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/asp31y"
    },
    {
        "id": 5217,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Yang Jie",
        "published": "2023-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/9xzxst"
    },
    {
        "id": 5218,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Tang Mengyuan",
        "published": "2023-6-19",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/923n5d"
    },
    {
        "id": 5219,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Muhammad Azeem",
        "published": "2023-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/b8i675"
    },
    {
        "id": 5220,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Weishi Yin",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/mdxke6"
    },
    {
        "id": 5221,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Milad Farahani",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/txvfmb"
    },
    {
        "id": 5222,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Prasanalakshmi Balaji",
        "published": "2023-7-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/hfpws3"
    },
    {
        "id": 5223,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Eman Saber",
        "published": "2023-6-10",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/r1953u"
    },
    {
        "id": 5224,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Sreevalsan Menon",
        "published": "2023-6-1",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/qa8vmw"
    },
    {
        "id": 5225,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Sandhya Pundhir",
        "published": "2023-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/7nqzwn"
    },
    {
        "id": 5226,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Loris Nanni",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/6osunv"
    },
    {
        "id": 5227,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Thomas Stifter",
        "published": "2023-6-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/da3be4"
    },
    {
        "id": 5228,
        "title": "Loss Functions",
        "authors": "Sigrid Keydana",
        "published": "2023-2-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003275923-9"
    },
    {
        "id": 5229,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Meenachi L",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/pz3qy2"
    },
    {
        "id": 5230,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Hyunseok Seo",
        "published": "2023-6-8",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/y8407n"
    },
    {
        "id": 5231,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Vladimír Hlaváč",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/u8bc6x"
    },
    {
        "id": 5232,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Yadollah Waghei",
        "published": "2023-8-2",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/459ega"
    },
    {
        "id": 5233,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "OPhir Nave",
        "published": "2023-5-28",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/45545f"
    },
    {
        "id": 5234,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Majid Danesh",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/8aujae"
    },
    {
        "id": 5235,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Mohammad Edalatifar",
        "published": "2023-7-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/uj4eu0"
    },
    {
        "id": 5236,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Joseph Isabona",
        "published": "2023-6-23",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/efvmzj"
    },
    {
        "id": 5237,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Fahad Alturise",
        "published": "2023-5-25",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/i902d0"
    },
    {
        "id": 5238,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Anna Bosman",
        "published": "2023-6-20",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/byf4fb"
    },
    {
        "id": 5239,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Shengqiang Chi",
        "published": "2023-6-3",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/ejotan"
    },
    {
        "id": 5240,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Dr. AMANULLAH M",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/r8fo63"
    },
    {
        "id": 5241,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Arup Kumar Sahoo",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/kqx7eq"
    },
    {
        "id": 5242,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Rupesh Kumar Tipu",
        "published": "2023-5-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/kxlukp"
    },
    {
        "id": 5243,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Enriko Yudhistira Ramadhan",
        "published": "2023-6-21",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/2rlu3r"
    },
    {
        "id": 5244,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Tae Yoon Kim",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/ucn4q5"
    },
    {
        "id": 5245,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Ömer Ali Karaman",
        "published": "2023-6-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/1g8n11"
    },
    {
        "id": 5246,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Arup Kumar Sahoo",
        "published": "2023-6-5",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/jzfmkt"
    },
    {
        "id": 5247,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Yelda Karatepe Mumcu",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/xxrfsf"
    },
    {
        "id": 5248,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Zohreh Dehghani-Bidgoli",
        "published": "2023-5-27",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/fgkjs0"
    },
    {
        "id": 5249,
        "title": "Low Dose CT Denoising by ResNet With Fused Attention Modules and Integrated Loss Functions",
        "authors": "Javad Alirezaie",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>X-ray computed tomography (CT) is a non-invasive medical diagnostic tool that has raised public concerns due to the associated health risks of radiation dose to patients. Reducing the radiation dose leads to noise artifacts, making the low-dose CT images unreliable for</p>\n<p>diagnosis. Hence, low-dose CT (LDCT) image reconstruction techniques have offered a new research area. In this study, a deep neural network is proposed, specifically a residual network (ResNet) using dilated convolution, batch normalization, and rectified linear unit</p>\n<p>(ReLU) layers with fused spatial- and channel-attention modules to enhance the quality of LDCT images. The network is optimized using the integration of per-pixel loss, perceptual loss via VGG16-net, and dissimilarity index loss. Through an ablation experiment, these functions show that they could effectively prevent edge oversmoothing, improve image texture, and preserve the structural details. Finally, comparative experiments showed that the qualitative and quantitative results of the proposed network outperform state-of-the-art</p>\n<p>denoising models such as block-matching 3D filtering (BM3D), Markovian-based patch generative adversarial network (patch-GAN), and dilated residual network with edge detection (DRL-E-MP).</p>",
        "link": "http://dx.doi.org/10.32920/21263199.v1"
    },
    {
        "id": 5250,
        "title": "Economic Loss assessment of end-users based on an integrated method of quality loss functions",
        "authors": "Peng Zhang, Ling Pan, Xiang Shen",
        "published": "2018-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/ciced.2018.8592512"
    },
    {
        "id": 5251,
        "title": "Safety Management Functions",
        "authors": "Ron C. McKinnon",
        "published": "2023-3-7",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1201/9781003385943-19"
    },
    {
        "id": 5252,
        "title": "Comparative Analysis of 72 Flyback Transformers on 5τ Non-linear Battery with Loss Functions - Part II",
        "authors": "Abhishek Bansal",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This is the second part of the three part series paper where the novel idea of representing flyback transformer losses as loss functions using hyperbolic secant spline interpolation, and its use in determining flyback transformer core saturation and instability in the control feedback loop from the loss function using Lyapunov method has been presented with scientific proof and validity. In this paper, the comparative analysis of 36 different flyback transformers from 2W to 27W is being presented. These two-winding transformers are operated in continuous/discontinuous mode, switched from MOSFET controlled by microcontroller from 41 kHz to 97 kHz. Eighteen wattage’s which are used internationally at different voltage and current ratings have been analyzed on fifteen different cores - EPC13, EF12.6, EFD15, EE13, EE16, EEL16,EE19, EEL19,EFD20,EE25, E20/10/6, PR18x11,EEL22,RM6S/I, RM8/I.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19634352"
    },
    {
        "id": 5253,
        "title": "What Loss Functions Do Humans Optimize When They Perform Regression and Classification",
        "authors": "Hansol X. Ryu, Manoj Srinivasan",
        "published": "No Date",
        "citations": 0,
        "abstract": "AbstractStudying how humans perceive patterns in visually presented data is useful for understanding data-based decision-making and potentially understanding visually mediated sensorimotor control. We conducted experiments to examine how human subjects perform the simplest machine learning or statistical estimation tasks: linear regression and binary classification on 2D scatter plots. We used inverse optimization to infer the loss function humans optimize when they perform these tasks. Minimizing the sum of regression error raised to the power of 1.7 best-described human performing regression on sparse data. Loss functions with lower exponents, which are less sensitive to outliers, were better descriptors for regression tasks performed on less sparse data consisting of more data points. For the classification task, minimizing a logistic loss function was on average a better descriptor of human choices than an exponential loss function applied to only misclassified data. People changed their strategies as data density increased. These results represent overall trends across subjects and trials but there was large inter- and intra-subject variability in human choices. Future work may examine other loss function families and other tasks. Such understanding of human loss functions may inform the design of applications that interact with humans better and imitate humans more effectively.",
        "link": "http://dx.doi.org/10.1101/2023.09.19.558376"
    },
    {
        "id": 5254,
        "title": "Comparative Analysis of 72 Flyback Transformers on 5τ Non-linear Battery with Loss Functions - Part III",
        "authors": "Abhishek Bansal",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This is the final part of the three part series paper where the novel idea of representing flyback transformer losses as loss functions using hyperbolic secant spline interpolation,determination of the flyback transformer core saturation and instability in the control feedback loop from the loss function using Lyapunov method has been presented with scientific proof and validity. In this paper, the comparative analysis of 36 different flyback transformers from 30W to 85W is being presented.These two-winding transformers are operated in continuous/discontinuous mode,switched from MOSFET controlled by microcontroller from 60 kHz to 97 kHz. Eighteen wattage’s which are used internationally at different voltage and current ratings have been analyzed on fifteen different cores - EEL22, EE25, EEL25,EF25, EE28, EER28, EER28L, EE35, RM8/I, ETD29/16/10,ETD34/17/11, E25/13/7, PR30x19, ATQ27/18, RM12/I.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19634373.v1"
    },
    {
        "id": 5255,
        "title": "Robust Optimization of Discontinuous Loss Functions",
        "authors": "Daniel N. Wilke",
        "published": "2024",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1007/978-981-19-8851-6_1-1"
    },
    {
        "id": 5256,
        "title": "Categorical Forecasts and Non-Categorical Loss Functions",
        "authors": "Constantin Bürgi, Dorine Boumans",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3598751"
    },
    {
        "id": 5257,
        "title": "Comparative Analysis of 72 Flyback Transformers on 5τ Non-linear Battery with Loss Functions - Part I",
        "authors": "Abhishek Bansal",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This is a three part series paper, where the novel idea of representing flyback transformer losses as loss functions has been presented with scientific proof.Thirteen calculations for each transformer have been tabulated and used to derive mathematical loss function using hyperbolic secant spline interpolation which is subjected to Lyapunov method to predict core saturation and instability in the control loop. For scientific validity, 72 working transformers at thirty-six wattage’s which are being internationally used, have been examined to check, whether such proposition is valid or not.The definiteness of matrix,singular value decomposition,Lyapunov exponents are also tabulated. Then each of these transformers are simulated with non-linearies of Li-ion battery namely state of charge (SoC), self-discharge resistance, temperature-dependent leakage resistance,five time-constant dynamics, deterioration of battery performance over repeated charge and discharge cycles. The coefficients in the Jiles-Atherton model have been altered with fractional perturbations to obtain hysteresis curves.Various interesting comparisons in Bm,Bp,AC flux densities, effect on varying the coefficients in JA, residual errors, Lyapunov exponents, switching frequency vs MOSFET and transformer losses, reflected voltages, are tabulated and plotted.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19634340.v1"
    },
    {
        "id": 5258,
        "title": "Data informed hyperparmeter determinations and Inverse loss functions using AI for Thermomechanical Differential Equations",
        "authors": "Deepankar Pal, Grama Bhashyam",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.26226/morressier.612f6736bc9810372410086e"
    },
    {
        "id": 5259,
        "title": "Comparative Analysis of 72 Flyback Transformers on 5τ Non-linear Battery with Loss Functions - Part II",
        "authors": "Abhishek Bansal",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This is the second part of the three part series paper where the novel idea of representing flyback transformer losses as loss functions using hyperbolic secant spline interpolation, and its use in determining flyback transformer core saturation and instability in the control feedback loop from the loss function using Lyapunov method has been presented with scientific proof and validity. In this paper, the comparative analysis of 36 different flyback transformers from 2W to 27W is being presented. These two-winding transformers are operated in continuous/discontinuous mode, switched from MOSFET controlled by microcontroller from 41 kHz to 97 kHz. Eighteen wattage’s which are used internationally at different voltage and current ratings have been analyzed on fifteen different cores - EPC13, EF12.6, EFD15, EE13, EE16, EEL16,EE19, EEL19,EFD20,EE25, E20/10/6, PR18x11,EEL22,RM6S/I, RM8/I.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19634352.v1"
    },
    {
        "id": 5260,
        "title": "Comparative Analysis of 72 Flyback Transformers on 5τ Non-linear Battery with Loss Functions - Part I",
        "authors": "Abhishek Bansal",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This is a three part series paper, where the novel idea of representing flyback transformer losses as loss functions has been presented with scientific proof.Thirteen calculations for each transformer have been tabulated and used to derive mathematical loss function using hyperbolic secant spline interpolation which is subjected to Lyapunov method to predict core saturation and instability in the control loop. For scientific validity, 72 working transformers at thirty-six wattage’s which are being internationally used, have been examined to check, whether such proposition is valid or not.The definiteness of matrix,singular value decomposition,Lyapunov exponents are also tabulated. Then each of these transformers are simulated with non-linearies of Li-ion battery namely state of charge (SoC), self-discharge resistance, temperature-dependent leakage resistance,five time-constant dynamics, deterioration of battery performance over repeated charge and discharge cycles. The coefficients in the Jiles-Atherton model have been altered with fractional perturbations to obtain hysteresis curves.Various interesting comparisons in Bm,Bp,AC flux densities, effect on varying the coefficients in JA, residual errors, Lyapunov exponents, switching frequency vs MOSFET and transformer losses, reflected voltages, are tabulated and plotted.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19634340"
    },
    {
        "id": 5261,
        "title": "Measurement Error Sensitivity of Loss Functions for Distribution Forecasts",
        "authors": "Onno Kleen",
        "published": "No Date",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3476461"
    },
    {
        "id": 5262,
        "title": "Comparative Analysis of 72 Flyback Transformers on 5τ Non-linear Battery with Loss Functions - Part III",
        "authors": "Abhishek Bansal",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>This is the final part of the three part series paper where the novel idea of representing flyback transformer losses as loss functions using hyperbolic secant spline interpolation,determination of the flyback transformer core saturation and instability in the control feedback loop from the loss function using Lyapunov method has been presented with scientific proof and validity. In this paper, the comparative analysis of 36 different flyback transformers from 30W to 85W is being presented.These two-winding transformers are operated in continuous/discontinuous mode,switched from MOSFET controlled by microcontroller from 60 kHz to 97 kHz. Eighteen wattage’s which are used internationally at different voltage and current ratings have been analyzed on fifteen different cores - EEL22, EE25, EEL25,EF25, EE28, EER28, EER28L, EE35, RM8/I, ETD29/16/10,ETD34/17/11, E25/13/7, PR30x19, ATQ27/18, RM12/I.</p>",
        "link": "http://dx.doi.org/10.36227/techrxiv.19634373"
    },
    {
        "id": 5263,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Airton Monte Serrat Borin Junior",
        "published": "2023-5-29",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32388/26k99q"
    },
    {
        "id": 5264,
        "title": "Deriving Loss Functions for Regression and Classification from Humans",
        "authors": "Hansol Ryu, Manoj Srinivasan",
        "published": "2022",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.32470/ccn.2022.1237-0"
    },
    {
        "id": 5265,
        "title": "Fused Attention Modules Embedded in Artificial Neural Networks for Low Dose CT Denoising With Integrated Loss Functions",
        "authors": "Luella Marcos",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>X-ray Computed Tomography (CT) is a non-invasive medical diagnostic tool that has raised public concerns due to the associated health risks of radiation dose to patients. Reducing the radiation dose leads to noise artifacts, making the low-dose CT images unreliable for diagnosis. Hence, low-dose computed tomography (LDCT) image reconstruction techniques have offered a new challenge in the research area. This thesis focuses on reconstructing LDCT images using deep learning techniques to provide an efficient, effective, and accurate training regimes for LDCT image denoising. A fusion of spatial and channel attention modules integrated into a dilated residual network is proposed to improve the structural details of denoised LDCT images. Further, a combination of perceptual loss, per-pixel loss, and structural dissimilarity loss is used for the optimization of the overall network. These objective functions aim to preserve structural details, avoid edge over-smoothing and enhance the image texture, respectively. Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Metrics (SSIM) are used for measuring the quantitative results. A comparative experiment was done between the proposed model and the recent denoising models, such as Block Matching and 3D Filtering (BM3D), patch Markovnian Generative Adversarial Network (patch-GAN) and dilated residual learning with edge detection (DRL-E-MP). Not only with quantitative results, but these models were also compared visually. To further strengthen the validity of the outcomes, five different CT image datasets were used. The proposed model obtained the highest PSNR/SSIM value of 34.36/0.6971 while BM3D resulted in the lowest value with 30.24/0.4461 using the chest dataset from the Mayo Clinic. Overall, the proposed network demonstrated that it could outperform state-of-the-art models.</p>",
        "link": "http://dx.doi.org/10.32920/25413811"
    },
    {
        "id": 5266,
        "title": "Neighborhood-Based Loss Functions for Explainability of Autoencoders",
        "authors": "Denis Baskan, Patrick Erdelt",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4212995"
    },
    {
        "id": 5267,
        "title": "Review of: \"Interpreting the loss functions of Artificial neural networks in cancer research\"",
        "authors": "Pitoyo Hartono",
        "published": "2023-6-16",
        "citations": 0,
        "abstract": "",
        "link": "http://dx.doi.org/10.32388/9yym71"
    },
    {
        "id": 5268,
        "title": "Investigation of Loss Functions for Improving Deep Segmentation of Abdominal Organs from MRI",
        "authors": "Pedro Furtado",
        "published": "No Date",
        "citations": 0,
        "abstract": "Segmentation of Magnetic Resonance Images (MRI) of abdominal organs is useful for analysis prior to surgical procedures and for further processing. Deep Learning (DL) has become the standard, researchers have proposed improvements that include multiple views, ensembles and voting. Loss function alternatives, while being crucial to guide automated learning, have not been compared in detail. In this work we analyze limitations of popular metrics and their use as loss, study alternative loss variations based on those and other modifications and search for the best approach. An experimental setup was necessary to assess the alternatives. Results for the top scoring network and top scoring loss show improvements between 2 and 11 percentage points (pp) in Jaccard Index (JI), depending on organ and patient (sequence), for a total of 22 pp over 4 organs, all this being obtained just by choosing the best performing loss function instead of cross-entropy or dice. Our results apply directly to MRI of abdominal organs, with important practical implications for other architectures, as they can be applied easily to any of them. They also show the worth of variants of loss function and loss tuning, with future work needed to generalize and test in other contexts.",
        "link": "http://dx.doi.org/10.20944/preprints202011.0023.v1"
    },
    {
        "id": 5269,
        "title": "Bias, rationality and asymmetric loss functions",
        "authors": "Constantin Bürgi",
        "published": "2017-5",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.econlet.2017.03.002"
    },
    {
        "id": 5270,
        "title": "Robust boosting with truncated loss functions",
        "authors": "Zhu Wang",
        "published": "2018-1-1",
        "citations": 11,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1214/18-ejs1404"
    },
    {
        "id": 5271,
        "title": "Fused Attention Modules Embedded in Artificial Neural Networks for Low Dose CT Denoising With Integrated Loss Functions",
        "authors": "Luella Marcos",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>X-ray Computed Tomography (CT) is a non-invasive medical diagnostic tool that has raised public concerns due to the associated health risks of radiation dose to patients. Reducing the radiation dose leads to noise artifacts, making the low-dose CT images unreliable for diagnosis. Hence, low-dose computed tomography (LDCT) image reconstruction techniques have offered a new challenge in the research area. This thesis focuses on reconstructing LDCT images using deep learning techniques to provide an efficient, effective, and accurate training regimes for LDCT image denoising. A fusion of spatial and channel attention modules integrated into a dilated residual network is proposed to improve the structural details of denoised LDCT images. Further, a combination of perceptual loss, per-pixel loss, and structural dissimilarity loss is used for the optimization of the overall network. These objective functions aim to preserve structural details, avoid edge over-smoothing and enhance the image texture, respectively. Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Metrics (SSIM) are used for measuring the quantitative results. A comparative experiment was done between the proposed model and the recent denoising models, such as Block Matching and 3D Filtering (BM3D), patch Markovnian Generative Adversarial Network (patch-GAN) and dilated residual learning with edge detection (DRL-E-MP). Not only with quantitative results, but these models were also compared visually. To further strengthen the validity of the outcomes, five different CT image datasets were used. The proposed model obtained the highest PSNR/SSIM value of 34.36/0.6971 while BM3D resulted in the lowest value with 30.24/0.4461 using the chest dataset from the Mayo Clinic. Overall, the proposed network demonstrated that it could outperform state-of-the-art models.</p>",
        "link": "http://dx.doi.org/10.32920/25413811.v1"
    },
    {
        "id": 5272,
        "title": "Loss functions for Loss Given Default model comparison",
        "authors": "Christophe Hurlin, Jérémy Leymarie, Antoine Patin",
        "published": "2018-7",
        "citations": 25,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.ejor.2018.01.020"
    },
    {
        "id": 5273,
        "title": "AN EXTENDED STUDY TO DETERMINE THE BEST LOSS FUNCTIONS FOR ESTIMATING THE EXPONENTIAL DISTRIBUTION PARAMETER UNDERN EXTENDED STUDY TO DETERMINE THE BEST LOSS FUNCTIONS FOR ESTIMATING THE EXPONENTIAL DISTRIBUTION PARAMETER UNDER JEFFERY AND GAMMA PRIORS",
        "authors": "Zainab Falih Hamza",
        "published": "2023-3-30",
        "citations": 0,
        "abstract": "In this research, we compared the Bayesian estimators when estimating the scale parameter for the exponential distribution by using different loss functions under Jeffrey and Gamma priors, as most of the available symmetric and asymmetric loss functions were used, also the balanced and unbalanced loss functions. The simulation results proved the advantage of balanced loss functions with the Gamma prior, and the effectiveness of the balanced loss functions when using Jeffrey prior especially if the value of the weighted coefficient is equal to 0.5, so it is possible to use initial estimators as maximum likelihood estimator to compensate for the lack of prior information around the parameter to be estimated, also the advantage of the balanced general entropy loss function and the balanced weighted square error loss function under Jeffrey prior when the value of the scale parameter for the exponential distribution is less than 1, the preference of the balanced weighted square error loss function and the balanced K loss function if the value of the scale parameter for the exponential distribution is equal to 1, and the preference for the AL-Sayyad balanced loss function and the balanced AL-Bayyati loss function if the value of the scale parameter for the exponential distribution is greater or equal to 2.",
        "link": "http://dx.doi.org/10.26782/jmcms.2023.03.00001"
    },
    {
        "id": 5274,
        "title": "Non-Parametric Robust Model Risk Measurement with Path-Dependent Loss Functions",
        "authors": "Yu Feng",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.3659110"
    },
    {
        "id": 5275,
        "title": "A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation",
        "authors": "Danny Websdale, Ben Milner",
        "published": "2017-8-20",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2017-1504"
    },
    {
        "id": 5276,
        "title": "Investigations on Data Augmentation and Loss Functions for Deep Learning Based Speech-Background Separation",
        "authors": "Hakan Erdogan, Takuya Yoshioka",
        "published": "2018-9-2",
        "citations": 15,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.21437/interspeech.2018-2441"
    },
    {
        "id": 5277,
        "title": "Loss functions for Style Transfer with CycleGAN",
        "authors": "Xulu Wang",
        "published": "2022-1",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iwecai55315.2022.00020"
    },
    {
        "id": 5278,
        "title": "Loss Functions for Regression",
        "authors": "",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811254185_0010"
    },
    {
        "id": 5279,
        "title": "Robustness of Different Loss Functions and Their Impact on Network's Learning",
        "authors": "Vishal Rajput",
        "published": "No Date",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4065778"
    },
    {
        "id": 5280,
        "title": "Covariate balancing propensity score by tailored loss functions",
        "authors": "Qingyuan Zhao",
        "published": "2019-4-1",
        "citations": 49,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1214/18-aos1698"
    },
    {
        "id": 5281,
        "title": "Flexible Loss Functions for Binary Classification in Gradient-Boosted Decision Trees: An Application to Credit Scoring",
        "authors": "Jonah Mushava, Michael Murray",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4411806"
    },
    {
        "id": 5282,
        "title": "Review for \"Loss of the psychiatric risk factor SLC6A15 is associated with increased metabolic functions in primary hippocampal neurons\"",
        "authors": "Fiona Hollis",
        "published": "2020-9-6",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/ejn.14990/v3/review1"
    },
    {
        "id": 5283,
        "title": "Review for \"Loss of the psychiatric risk factor SLC6A15 is associated with increased metabolic functions in primary hippocampal neurons\"",
        "authors": "Fiona Hollis",
        "published": "2020-6-9",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/ejn.14990/v1/review1"
    },
    {
        "id": 5284,
        "title": "Review for \"Loss of the psychiatric risk factor SLC6A15 is associated with increased metabolic functions in primary hippocampal neurons\"",
        "authors": "Fiona Hollis",
        "published": "2020-7-14",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/ejn.14990/v2/review1"
    },
    {
        "id": 5285,
        "title": "On Loss Functions for Deep Neural Networks in Classification",
        "authors": "Katarzyna Janocha, Wojciech Marian Czarnecki",
        "published": "2017",
        "citations": 275,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.4467/20838476si.16.004.6185"
    },
    {
        "id": 5286,
        "title": "Remaining cycle time prediction: Temporal loss functions and prediction consistency",
        "authors": "Mike Riess",
        "published": "2023-8-6",
        "citations": 1,
        "abstract": "The usefulness of remaining cycle time models for predictive and prescriptive process monitoring depends not only on the overall accuracy of the predictions but also on their earliness: predictions should be as accurate as possible, as early as possible. To give this criterion more weight in model fitting, the paper evaluates three L1 loss functions with temporal decay. All have the property of increasing the weight of residuals from the early stages of a process relative to residuals from later stages but do so to different degrees. The loss functions are used in LSTM networks for training remaining throughout time models of four different business processes based on publicly available event log data sets. Compared to models trained with unweighted L1 loss, the suggested modifications yield small but significant improvements in earliness on out-of-sample data. Neither the unweighted L1 loss nor the modifications led to models with strictly monotonically decreasing predictions of the remaining time.",
        "link": "http://dx.doi.org/10.5617/nmi.10141"
    },
    {
        "id": 5287,
        "title": "Simulation-Based Comparison of Poisson Distribution Parameter Estimation Using Maximum Likelihood and Bayesian Method with Different Loss Functions",
        "authors": "Huda Alomari",
        "published": "No Date",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.2139/ssrn.4462323"
    },
    {
        "id": 5288,
        "title": "SemSegLoss: A python package of loss functions for semantic segmentation",
        "authors": "Shruti Jadon",
        "published": "2021-8",
        "citations": 3,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1016/j.simpa.2021.100078"
    },
    {
        "id": 5289,
        "title": "Low Dose CT Denoising by ResNet With Fused Attention Modules and Integrated Loss Functions",
        "authors": "Seyyedomid Badretale, Fariba Shaker, Paul Babyn, Javad Alirezaie",
        "published": "No Date",
        "citations": 0,
        "abstract": "<p>X-ray computed tomography (CT) is a non-invasive medical diagnostic tool that has raised public concerns due to the associated health risks of radiation dose to patients. Reducing the radiation dose leads to noise artifacts, making the low-dose CT images unreliable for</p>\n<p>diagnosis. Hence, low-dose CT (LDCT) image reconstruction techniques have offered a new research area. In this study, a deep neural network is proposed, specifically a residual network (ResNet) using dilated convolution, batch normalization, and rectified linear unit</p>\n<p>(ReLU) layers with fused spatial- and channel-attention modules to enhance the quality of LDCT images. The network is optimized using the integration of per-pixel loss, perceptual loss via VGG16-net, and dissimilarity index loss. Through an ablation experiment, these functions show that they could effectively prevent edge oversmoothing, improve image texture, and preserve the structural details. Finally, comparative experiments showed that the qualitative and quantitative results of the proposed network outperform state-of-the-art</p>\n<p>denoising models such as block-matching 3D filtering (BM3D), Markovian-based patch generative adversarial network (patch-GAN), and dilated residual network with edge detection (DRL-E-MP).</p>",
        "link": "http://dx.doi.org/10.32920/21263199.v2"
    },
    {
        "id": 5290,
        "title": "The Bayesian Posterior Estimators under Six Loss Functions for Unrestricted and Restricted Parameter Spaces",
        "authors": "Ying-Ying Zhang",
        "published": "2020-7-15",
        "citations": 1,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.5772/intechopen.88587"
    },
    {
        "id": 5291,
        "title": "A survey of loss functions for semantic segmentation",
        "authors": "Shruti Jadon",
        "published": "2020-10-27",
        "citations": 476,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/cibcb48159.2020.9277638"
    },
    {
        "id": 5292,
        "title": "Review for \"Loss of the psychiatric risk factor SLC6A15 is associated with increased metabolic functions in primary hippocampal neurons\"",
        "authors": " Mary Kay Lobo",
        "published": "2020-7-22",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/ejn.14990/v2/review2"
    },
    {
        "id": 5293,
        "title": "Loss Functions and Models for Classification",
        "authors": "",
        "published": "2022-12",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1142/9789811254185_0011"
    },
    {
        "id": 5294,
        "title": "New Loss Functions Using Control Function for Medical Image Registration",
        "authors": "Dong Xing, Shuge Lei, Guojun Liao, Zicong Zhou, Yongpei Zhu",
        "published": "No Date",
        "citations": 0,
        "abstract": "Abstract\nPurpose: Accurate registration of medical images is crucial for doctor’s diagnosis and quantitative analysis. Optimizing the loss function is an significant research direction in medical image registration. From variational method in differential geometry, control function is essential to generate better registration field ϕ. \nMethods: In this paper, we propose a novel registration loss function based on the VoxelMorph architecture, utilizing the control function and Lagrange multiplier. The proposed method consists of two steps. In the first step, we modify the gradient of the registration field ϕ in Lsmooth(ϕ) using the Laplacian operator. In the second step, we introduce the control function F into the Lsmooth(ϕ) from the first step, which is the main contribution of our method. And we discussed two extensions of our method based on Lagrange multiplier.In the first extension, we add a Lagrange multiplier to control the Lsmooth(ϕ) in the second step. The second extension is based on Lagrange multiplier and Jacobian determinant (JD). We change the Laplacian operator to JD, and control function F to monitor function f of JD. Our proposed method has been validated on two datasets, namely the ADNI and IBSR datasets. \nResults: The results of our method demonstrate a significant improvement in MR image registration compared to existing methods. The new loss function has better convergence than original loss and gets better average Dice (higher is better) and non-positive Jacobian locations (lower is better) compared with MIT’s original method. \nConclusion: The experimental findings indicate that our proposed technique outperforms other existing methods in the task of registering brain MRI images.",
        "link": "http://dx.doi.org/10.21203/rs.3.rs-3208997/v1"
    },
    {
        "id": 5295,
        "title": "Cross-Modal Common Representation Learning with Triplet Loss Functions",
        "authors": "Felix Ott, David Rugamer, Lucas Heublein, Bernd Bischl, Christopher Mutschler",
        "published": "No Date",
        "citations": 5,
        "abstract": "Common representation learning (CRL) learns a shared embedding between two or more modalities to improve in a given task over using only one of the modalities. CRL from different data types such as images and time-series data (e.g., audio or text data) requires a deep metric learning loss that minimizes the distance between the modality embeddings. In this paper, we propose to use the triplet loss, which uses positive and negative identities to create sample pairs with different labels, for CRL between image and time-series modalities. By adapting the triplet loss for CRL, higher accuracy in the main (time-series classification) task can be achieved by exploiting additional information of the auxiliary (image classification) task. Our experiments on synthetic data and handwriting recognition data from sensor-enhanced pens show an improved classification accuracy, faster convergence, and a better generalizability.",
        "link": "http://dx.doi.org/10.31219/osf.io/pbzd7"
    },
    {
        "id": 5296,
        "title": "Boost Loss Functions for Better Change Detection",
        "authors": "Ozan Peker, Fatih Uysal, Firat Hardalac",
        "published": "2022-12-15",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1109/iisec56263.2022.9998289"
    },
    {
        "id": 5297,
        "title": "An Analysis of Loss Functions for Heavily Imbalanced Lesion Segmentation",
        "authors": "Mariano Cabezas, Yago Diez",
        "published": "2024-3-20",
        "citations": 0,
        "abstract": "Heavily imbalanced datasets are common in lesion segmentation. Specifically, the lesions usually comprise less than 5% of the whole image volume when dealing with brain MRI. A common solution when training with a limited dataset is the use of specific loss functions that rebalance the effect of background and foreground voxels. These approaches are usually evaluated running a single cross-validation split without taking into account other possible random aspects that might affect the true improvement of the final metric (i.e., random weight initialisation or random shuffling). Furthermore, the evolution of the effect of the loss on the heavily imbalanced class is usually not analysed during the training phase. In this work, we present an analysis of different common loss metrics during training on public datasets dealing with brain lesion segmentation in heavy imbalanced datasets. In order to limit the effect of hyperparameter tuning and architecture, we chose a 3D Unet architecture due to its ability to provide good performance on different segmentation applications. We evaluated this framework on two public datasets and we observed that weighted losses have a similar performance on average, even though heavily weighting the gradient of the foreground class gives better performance in terms of true positive segmentation.",
        "link": "http://dx.doi.org/10.3390/s24061981"
    },
    {
        "id": 5298,
        "title": "Loss Functions for CNN-based Biometric Vein Recognition",
        "authors": "Ridvan Salih Kuzu, Emanuele Maiorana, Patrizio Campisi",
        "published": "2021-1-24",
        "citations": 7,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.23919/eusipco47968.2020.9287517"
    },
    {
        "id": 5299,
        "title": "Review for \"Loss of the psychiatric risk factor SLC6A15 is associated with increased metabolic functions in primary hippocampal neurons\"",
        "authors": " Mary Kay Lobo",
        "published": "2020-9-11",
        "citations": 0,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.1111/ejn.14990/v3/review2"
    },
    {
        "id": 5300,
        "title": "On the Impacts of Loss of Brain-Neural Functions on Creativity in Graphic Design",
        "authors": "Man Zhu",
        "published": "2018-5-27",
        "citations": 2,
        "abstract": "No Abstract or Keywords available",
        "link": "http://dx.doi.org/10.14704/nq.2018.16.5.1252"
    }
]